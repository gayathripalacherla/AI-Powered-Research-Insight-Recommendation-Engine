title,abstract,authors,published,link,domain,subdomain,summary
Lecture Notes: Optimization for Machine Learning,"Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.",['Elad Hazan'],2019-09-08T21:49:42Z,http://arxiv.org/abs/1909.03550v1,Artificial Intelligence,Machine Learning,"lecture notes on optimization for machine learning derived from a course at Princeton . tutorials given in MLSS, Buenos Aires"
An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",['Xiaojin Zhu'],2018-11-11T14:28:34Z,http://arxiv.org/abs/1811.04422v1,Artificial Intelligence,Machine Learning,this view encompasses many types of adversarial machine learning . the view encourages researcher to utilize advances in control theory and reinforcement learning 
Minimax deviation strategies for machine learning and recognition with   short learning samples,The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.,"['Michail Schlesinger', 'Evgeniy Vodolazskiy']",2017-07-16T09:15:08Z,http://arxiv.org/abs/1707.04849v1,Artificial Intelligence,Machine Learning,the article is devoted to the problem of small learning samples in machine learning . the flaws of maximum likelihood learning and minimax learning are
Machine Learning for Clinical Predictive Analytics,"In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",['Wei-Hung Weng'],2019-09-19T22:02:00Z,http://arxiv.org/abs/1909.09246v1,Artificial Intelligence,Machine Learning,"in this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks . we outline some of the most common machine learning"
Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs,"Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.","['Samiyuru Menik', 'Lakshmish Ramaswamy']",2023-01-23T22:54:34Z,http://arxiv.org/abs/2301.09753v1,Artificial Intelligence,Machine Learning,cost of developing custom machine learning solutions is a major inhibitor to far-reaching adoption of machine learning technologies . we explore the benefits of modular
Introduction to Machine Learning: Class Notes 67577,"Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",['Amnon Shashua'],2009-04-23T11:40:57Z,http://arxiv.org/abs/0904.3664v1,Artificial Intelligence,Machine Learning,"Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spect"
The Tribes of Machine Learning and the Realm of Computer Architecture,"Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.","['Ayaz Akram', 'Jason Lowe-Power']",2020-12-07T23:10:51Z,http://arxiv.org/abs/2012.04105v1,Artificial Intelligence,Machine Learning,this paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems . we also provide a detailed survey of computer architecture research that
"A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning","Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.","['Randy J. Chase', 'David R. Harrison', 'Amanda Burke', 'Gary M. Lackmann', 'Amy McGovern']",2022-04-15T14:48:04Z,http://arxiv.org/abs/2204.07492v2,Artificial Intelligence,Machine Learning,lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' this paper provides a survey of some of the most common
Position Paper: Towards Transparent Machine Learning,"Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",['Dustin Juliano'],2019-11-12T10:49:55Z,http://arxiv.org/abs/1911.06612v1,Artificial Intelligence,Machine Learning,"the goal of this project is to enable direct human understanding of machine learning models . if solved, this technology could represent a best-case"
Understanding Bias in Machine Learning,"Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.","['Jindong Gu', 'Daniela Oelke']",2019-09-02T20:36:19Z,http://arxiv.org/abs/1909.01866v1,Artificial Intelligence,Machine Learning,machine learning experts warn that machine learning models can be biased . this article aims to explain the issue of bias in machine learning from a technical
A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain,"Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.",['Tao Wang'],2019-03-21T02:17:08Z,http://arxiv.org/abs/1903.08801v1,Artificial Intelligence,Machine Learning,paper proposes building a trustable machine learning system by using blockchain technology . paper establishes a link between machine learning and blockchain technology
MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?,"We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.","['Yu Liu', 'Hantian Zhang', 'Luyuan Zeng', 'Wentao Wu', 'Ce Zhang']",2017-07-29T21:59:18Z,http://arxiv.org/abs/1707.09562v3,Artificial Intelligence,Machine Learning,"we conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds . instead of specifying how"
Data Pricing in Machine Learning Pipelines,"Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.","['Zicun Cong', 'Xuan Luo', 'Pei Jian', 'Feida Zhu', 'Yong Zhang']",2021-08-18T00:57:06Z,http://arxiv.org/abs/2108.07915v1,Artificial Intelligence,Machine Learning,"in this article, we survey the principles and the latest research development of data pricing in machine learning pipelines . data is critical and penetr"
Techniques for Automated Machine Learning,"Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.","['Yi-Wei Chen', 'Qingquan Song', 'Xia Hu']",2019-07-21T04:03:36Z,http://arxiv.org/abs/1907.08908v1,Artificial Intelligence,Machine Learning,Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem . it could release the
"The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning","With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.","['Omer Subasi', 'Oceane Bel', 'Joseph Manzano', 'Kevin Barker']",2023-12-05T20:40:05Z,http://arxiv.org/abs/2312.03120v1,Artificial Intelligence,Machine Learning,"in this study, we present a review of modern machine and deep learning . our discussion encompasses parallel distributed learning, deep learning and "
Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark,"With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",['Jiajun Shen'],2022-05-08T03:47:30Z,http://arxiv.org/abs/2206.07090v2,Artificial Intelligence,Machine Learning,using machine learning algorithms to analyze large data may be time-consuming . research results show significant improvement in runtime and efficiency of parallelized algorithms 
AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.","['Abhishek Thakur', 'Artus Krohn-Grimberghe']",2015-07-08T15:07:39Z,http://arxiv.org/abs/1507.02188v1,Artificial Intelligence,Machine Learning,the framework has been validated and improved over a period of more than two years . it aims at minimizing human interference required to build a first useful predictive model . the proposed system produces better (or comparable) results with less runtime .
Joint Training of Deep Boltzmann Machines,"We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.","['Ian Goodfellow', 'Aaron Courville', 'Yoshua Bengio']",2012-12-12T01:59:27Z,http://arxiv.org/abs/1212.2686v1,Artificial Intelligence,Machine Learning,"a new method for training deep Boltzmann machines jointly is introduced . prior methods require an initial learning pass that trains the deep Bolzmann machine greedily, one layer at a time."
Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications,"This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.",['Kush R. Varshney'],2016-07-08T16:55:31Z,http://arxiv.org/abs/1607.02450v2,Artificial Intelligence,Machine Learning,"the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications was held on June 24, 2016 in New York."
Mathematical Perspective of Machine Learning,"We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.",['Yarema Boryshchak'],2020-07-03T05:26:02Z,http://arxiv.org/abs/2007.01503v1,Artificial Intelligence,Machine Learning,some theoretical challenges of Machine Learning as a function approximation . gradient descent as the default optimization algorithm . limitations of fixed length and width networks and a different approach to RNNs .
Opening the black box of deep learning,"The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.","['Dian Lei', 'Xiaoxiao Chen', 'Jianfei Zhao']",2018-05-22T02:12:33Z,http://arxiv.org/abs/1805.08355v1,Artificial Intelligence,Deep Learning,"this dissertation proposes that the neural network of deep learning is a physical system . it examines deep learning from three different perspectives: microscopic, macroscopic and physical world views . this dissertation explains why deep learning must be deep, what characteristics are learned in deep learning ."
"Deep learning research landscape & roadmap in a nutshell: past, present   and future -- Towards deep cortical learning","The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately.",['Aras R. Dargazany'],2019-07-30T16:57:38Z,http://arxiv.org/abs/1908.02130v1,Artificial Intelligence,Deep Learning,we predict deep learning will be the convergence of deep learning & cortical learning . this will build an artificial corticeal column ultimately .
Concept-Oriented Deep Learning,"Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning.",['Daniel T Chang'],2018-06-05T15:50:30Z,http://arxiv.org/abs/1806.01756v1,Artificial Intelligence,Deep Learning,"concept-oriented deep learning (CODL) addresses some of the major limitations of deep learning . CODL addresses interpretability, transferability, contextual adaptation and requirement for lots of labeled training data ."
A First Look at Deep Learning Apps on Smartphones,"We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like. Our study has strong implications for app developers, smartphone vendors, and deep learning R\&D. On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning. On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these models.","['Mengwei Xu', 'Jiawei Liu', 'Yuanqiang Liu', 'Felix Xiaozhu Lin', 'Yunxin Liu', 'Xuanzhe Liu']",2018-11-08T07:59:23Z,http://arxiv.org/abs/1812.05448v4,Artificial Intelligence,Deep Learning,"we present the first empirical study on 16,500 the most popular Android apps . we build a new static tool that dissects apps and analyzes their deep learning functions . our findings paint a promising picture of deep learning for smartphones ."
Geometrization of deep networks for the interpretability of deep   learning systems,"How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems.","['Xiao Dong', 'Ling Zhou']",2019-01-06T14:32:45Z,http://arxiv.org/abs/1901.02354v2,Artificial Intelligence,Deep Learning,"geometrization is a bridge to connect physics, geometry, deep network and quantum computation . this may result in a new scheme to reveal the rule of the physical world ."
Why & When Deep Learning Works: Looking Inside Deep Learnings,"The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of ""Why & When Deep Learning works"", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output of this challenge resulted in five papers that address different facets of deep learning. These different facets include a high-level understating of why and when deep networks work (and do not work), the impact of geometry on the expressiveness of deep networks, and making deep networks interpretable.",['Ronny Ronen'],2017-05-10T18:52:26Z,http://arxiv.org/abs/1705.03921v1,Artificial Intelligence,Deep Learning,the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting machine learning and deep learning research since its foundation in 2012 . the output of this challenge resulted in five papers that address different facets of deep learning .
Learning Task-aware Robust Deep Learning Systems,"Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep learning system. Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model. Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy.","['Keji Han', 'Yun Li', 'Xianzhong Long', 'Yao Ge']",2020-10-11T01:06:49Z,http://arxiv.org/abs/2010.05125v2,Artificial Intelligence,Deep Learning,a deep learning system consists of two parts: the deep learning task and the deep model . most existing studies investigate the impact of the learning task on robustness . this paper adopts the binary and interval label encoding strategy to redefine the classification task .
Moving Deep Learning into Web Browser: How Far Can We Go?,"Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been supported in browsers so far. Then we measure the performance of different frameworks when running different deep learning tasks. Finally, we dig out the performance gap between deep learning in browsers and on native platforms by comparing the performance of TensorFlow.js and TensorFlow in Python. Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers.","['Yun Ma', 'Dongwei Xiang', 'Shuyu Zheng', 'Deyu Tian', 'Xuanzhe Liu']",2019-01-27T14:54:51Z,http://arxiv.org/abs/1901.09388v2,Artificial Intelligence,Deep Learning,"several JavaScript-based deep learning frameworks have emerged . but little is known on what and how well we can do with these frameworks for deep learning . this paper conducts the first empirical study of deep learning in browsers . findings could help application developers, deep-learning framework vendors and browser vendors ."
Deep Learning in Software Engineering,"Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in SE that use deep learning techniques. We find that 41 SE tasks in all SE phases have been facilitated by deep learning integrated solutions. In which, 84.7% papers only use standard deep learning models and their variants to solve SE problems. The practicability becomes a concern in utilizing deep learning techniques. How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future.","['Xiaochen Li', 'He Jiang', 'Zhilei Ren', 'Ge Li', 'Jingxuan Zhang']",2018-05-13T06:01:39Z,http://arxiv.org/abs/1805.04825v1,Artificial Intelligence,Deep Learning,deep learning is increasingly prevalent in the field of Software Engineering (SE) many open issues remain to be investigated . the answers help practitioners and researchers develop practical deep learning models for SE tasks .
"Quantum Neural Networks: Concepts, Applications, and Challenges","Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, this paper discusses the challenges of quantum deep learning research in multiple perspectives. Lastly, this paper presents various future research directions and application fields of quantum deep learning.","['Yunseok Kwak', 'Won Joon Yun', 'Soyi Jung', 'Joongheon Kim']",2021-08-02T04:32:15Z,http://arxiv.org/abs/2108.01468v1,Artificial Intelligence,Deep Learning,"quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks . the research topics and directions of deep learning and quantum computing have been separated for long time . but by discovering that quantum circuits can act like artificial neural networks, it is widely adopted ."
NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders   of Deep Giants,"Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy. Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions.","['Zhongzhi Yu', 'Yonggan Fu', 'Jiayi Yuan', 'Haoran You', 'Yingyan Lin']",2023-06-23T16:14:25Z,http://arxiv.org/abs/2306.13586v1,Artificial Intelligence,Deep Learning,a framework called netBooster aims to empower tiny deep learning . it augments the architectures of tiny neural networks (TNNs) by an expansion-then-contraction strategy . the framework consistently outperforms state-of-the-art solutions .
Greedy Deep Dictionary Learning,"In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.","['Snigdha Tariyal', 'Angshul Majumdar', 'Richa Singh', 'Mayank Vatsa']",2016-01-31T06:12:58Z,http://arxiv.org/abs/1602.00203v1,Artificial Intelligence,Deep Learning,we propose a new deep learning tool called deep dictionary learning . we apply the proposed technique on some benchmark deep learning datasets . our method yields better results than all other deep learning tools .
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision","['Ngan Le', 'Vidhiwar Singh Rathour', 'Kashu Yamazaki', 'Khoa Luu', 'Marios Savvides']",2021-08-25T23:01:48Z,http://arxiv.org/abs/2108.11510v1,Artificial Intelligence,Deep Learning,"in this work, we provide a detailed review of recent research advances of deep reinforcement learning in computer vision . we propose a categorization of deep learning methodologies and discuss their advantages and limitations ."
Probabilistic Deep Learning with Probabilistic Neural Networks and Deep   Probabilistic Models,"Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables. We discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learning. We include its code examples for illustration.",['Daniel T. Chang'],2021-05-31T22:13:21Z,http://arxiv.org/abs/2106.00120v3,Artificial Intelligence,Deep Learning,probabilistic deep learning is deep learning that accounts for uncertainty . it is based on the use of probabilistic models and deep neural networks . we discuss some major examples of each approach .
Towards energy-efficient Deep Learning: An overview of energy-efficient   approaches along the Deep Learning Lifecycle,"Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption.","['Vanessa Mehlin', 'Sigurd Schacht', 'Carsten Lanquillon']",2023-02-05T11:36:51Z,http://arxiv.org/abs/2303.01980v1,Artificial Intelligence,Deep Learning,this paper aims to show how and at which points along the lifecycle of Deep Learning it is possible to reduce energy consumption .
A Unified Framework of Deep Neural Networks by Capsules,"With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning.","['Yujian Li', 'Chuanhui Shan']",2018-05-09T14:23:17Z,http://arxiv.org/abs/1805.03551v2,Artificial Intelligence,Deep Learning,"how to describe deep neural networks unifiedly is becoming an important issue . we first formalize neural networks mathematically with their directed graph representations . then, we set up a unified framework for deep learning with capsule networks ."
Integrating Learning and Reasoning with Deep Logic Models,"Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, which are deep graphical models integrating deep learning and logic reasoning both for learning and inference. Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge. The learning process allows to jointly learn the weights of the deep learners and the meta-parameters controlling the high-level reasoning. The experimental results show that the proposed methodology overtakes the limitations of the other approaches that have been proposed to bridge deep learning and reasoning.","['Giuseppe Marra', 'Francesco Giannini', 'Michelangelo Diligenti', 'Marco Gori']",2019-01-14T09:06:28Z,http://arxiv.org/abs/1901.04195v1,Artificial Intelligence,Deep Learning,the integration of deep learning and logic reasoning is still an open-research problem . it is considered the key for the development of real intelligent agents . this paper presents deep graphical models integrating deep learning .
Deep Learning in the Field of Biometric Template Protection: An Overview,"Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protection. Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometrics. In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes. This work provides an overview of how advances in deep learning take influence on the field of biometric template protection. The interrelation between improved biometric performance rates and security in biometric template protection is elaborated. Further, the use of deep learning for obtaining feature representations that are suitable for biometric template protection is discussed. Novel methods that apply deep learning to achieve various goals of biometric template protection are surveyed along with deep learning-based attacks.","['Christian Rathgeb', 'Jascha Kolberg', 'Andreas Uhl', 'Christoph Busch']",2023-03-05T17:06:40Z,http://arxiv.org/abs/2303.02715v1,Artificial Intelligence,Deep Learning,"deep learning has revolutionised the field of pattern recognition, including biometric recognition . biometric systems using deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance ."
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",['Ezgi Korkmaz'],2024-01-04T16:45:01Z,http://arxiv.org/abs/2401.02349v2,Artificial Intelligence,Deep Learning,"in this paper, we will formalize and analyze generalization in deep reinforcement learning . we will explain the fundamental reasons why deep learning policies encounter overfitting problems . the paper will categorize and explain the manifold solution approaches to increase generalization ."
What Really is Deep Learning Doing?,"Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.",['Chuyu Xiong'],2017-11-06T23:00:13Z,http://arxiv.org/abs/1711.03577v1,Artificial Intelligence,Deep Learning,what deep learning is really doing is still an open question . we will discuss advantages and disadvantages of deep learning at the end of this work .
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",['Changjian Li'],2020-01-27T07:26:12Z,http://arxiv.org/abs/2001.09608v1,Artificial Intelligence,Reinforcement Learning,"a lifelong reinforcement learning system is a learning system that can learn through trail-and-error interaction with the environment . in this paper, i give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system ."
Counterexample-Guided Repair of Reinforcement Learning Systems Using   Safety Critics,"Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.","['David Boetius', 'Stefan Leue']",2024-05-24T10:56:51Z,http://arxiv.org/abs/2405.15430v1,Artificial Intelligence,Reinforcement Learning,"a naively trained Deep Reinforcement Learning agent may fail to satisfy vital safety constraints . to avoid costly retraining, we may desire to repair a previously trained agent . the algorithm jointly repairs a reinforcement learning agent and a safety critic ."
Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision","['Ngan Le', 'Vidhiwar Singh Rathour', 'Kashu Yamazaki', 'Khoa Luu', 'Marios Savvides']",2021-08-25T23:01:48Z,http://arxiv.org/abs/2108.11510v1,Artificial Intelligence,Reinforcement Learning,"in this work, we provide a detailed review of recent research advances of deep reinforcement learning in computer vision . we propose a categorization of deep learning methodologies and discuss their advantages and limitations ."
Causal Reinforcement Learning: A Survey,"Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.","['Zhihong Deng', 'Jing Jiang', 'Guodong Long', 'Chengqi Zhang']",2023-07-04T03:00:43Z,http://arxiv.org/abs/2307.01452v2,Artificial Intelligence,Reinforcement Learning,reinforcement learning is an essential paradigm for solving decision problems under uncertainty . one of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world . causal reinforcement learning seeks to enhance existing algorithms by incorporating causal relationships into the learning process .
Distributed Deep Reinforcement Learning: A Survey and A Multi-Player   Multi-Agent Learning Toolbox,"With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning.","['Qiyue Yin', 'Tongtong Yu', 'Shengqi Shen', 'Jun Yang', 'Meijing Zhao', 'Kaiqi Huang', 'Bin Liang', 'Liang Wang']",2022-12-01T03:39:24Z,http://arxiv.org/abs/2212.00253v1,Artificial Intelligence,Reinforcement Learning,"distributed deep reinforcement learning is a recognized technique for solving sequential decision-making problems . despite its reputation, data inefficiency makes it hard to be practical in a wide range of areas . this paper compares the classical distributed deep learning methods to the most complex ."
Transfer Learning in Deep Reinforcement Learning: A Survey,"Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.","['Zhuangdi Zhu', 'Kaixiang Lin', 'Anil K. Jain', 'Jiayu Zhou']",2020-09-16T18:38:54Z,http://arxiv.org/abs/2009.07888v7,Artificial Intelligence,Reinforcement Learning,recent years have witnessed remarkable progress in reinforcement learning . transfer learning is a method of transferring knowledge from external expertise . the survey provides a framework for categorizing state-of-the-art transfer learning approaches .
Memory-two strategies forming symmetric mutual reinforcement learning   equilibrium in repeated prisoners' dilemma game,We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-$n$ strategies with $n>2$.,['Masahiko Ueda'],2021-08-05T04:59:18Z,http://arxiv.org/abs/2108.03258v2,Artificial Intelligence,Reinforcement Learning,we provide a necessary condition for memory-two deterministic strategies . we also prove that mutual reinforcement learning equilibria is formed when both players use reinforcement learning of memory-$n$ strategies with $n>2$ .
Implementing Online Reinforcement Learning with Temporal Neural Networks,"A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation.",['James E. Smith'],2022-04-11T23:10:42Z,http://arxiv.org/abs/2204.05437v1,Artificial Intelligence,Reinforcement Learning,the proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering . the reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules .
Deep Reinforcement Learning for Conversational AI,"Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.","['Mahipal Jadeja', 'Neelanshi Varia', 'Agam Shah']",2017-09-15T06:18:33Z,http://arxiv.org/abs/1709.05067v1,Artificial Intelligence,Reinforcement Learning,deep reinforcement learning is revolutionizing the artificial intelligence field . it is possible to scale deep learning with the use of deep learning . key challenges related to the implementation of reinforcement learning in conversational AI are identified .
On the Opportunities and Challenges of Offline Reinforcement Learning   for Recommender Systems,"Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.","['Xiaocong Chen', 'Siyu Wang', 'Julian McAuley', 'Dietmar Jannach', 'Lina Yao']",2023-08-22T10:28:02Z,http://arxiv.org/abs/2308.11336v1,Artificial Intelligence,Reinforcement Learning,"a survey aims to introduce and delve into offline reinforcement learning within recommender systems . a significant drawback persists: its poor data efficiency, stemming from its interactive nature . the survey also highlights prevalent challenges, opportunities, and future pathways ."
A Survey Analyzing Generalization in Deep Reinforcement Learning,"Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",['Ezgi Korkmaz'],2024-01-04T16:45:01Z,http://arxiv.org/abs/2401.02349v2,Artificial Intelligence,Reinforcement Learning,"in this paper, we will formalize and analyze generalization in deep reinforcement learning . we will explain the fundamental reasons why deep learning policies encounter overfitting problems . the paper will categorize and explain the manifold solution approaches to increase generalization ."
Reinforcement Teaching,"Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behavior. We further use learning progress to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations.","['Calarina Muslimani', 'Alex Lewandowski', 'Dale Schuurmans', 'Matthew E. Taylor', 'Jun Luo']",2022-04-25T18:04:17Z,http://arxiv.org/abs/2204.11897v3,Artificial Intelligence,Reinforcement Learning,"existing meta-learning methods are either hand-crafted to improve one component of an algorithm . a teaching policy is learned, through reinforcement, to improve a student's learning . we use learning progress to shape the teacher's reward ."
Generative Adversarial Imitation Learning,"Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.","['Jonathan Ho', 'Stefano Ermon']",2016-06-10T20:51:29Z,http://arxiv.org/abs/1606.03476v1,Artificial Intelligence,Reinforcement Learning,we propose a new general framework for directly extracting a policy from data . we show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks .
Two-Memory Reinforcement Learning,"While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent. Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.","['Zhao Yang', 'Thomas. M. Moerland', 'Mike Preuss', 'Aske Plaat']",2023-04-20T05:39:25Z,http://arxiv.org/abs/2304.10098v2,Artificial Intelligence,Reinforcement Learning,two-memory reinforcement learning agent (2M) combines episodic memory and reinforcement learning . the 2M agent is more data efficient and outperforms both pure episodic and RL . humans can leverage multiple memory systems concurrently during learning and benefit from them .
Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning,"Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.","['Shuai Lü', 'Shuai Han', 'Wenbo Zhou', 'Junwei Zhang']",2019-12-13T03:26:14Z,http://arxiv.org/abs/1912.06310v1,Artificial Intelligence,Reinforcement Learning,"in this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning . the core of this framework is a dual-actors and single critic reinforcement learning agent . low-fitness actors in the evolutionary population can imitate behavior patterns ."
Accelerate Reinforcement Learning with PID Controllers in the Pendulum   Simulations,We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL).,['Liping Bai'],2022-10-03T08:59:12Z,http://arxiv.org/abs/2210.00770v1,Artificial Intelligence,Reinforcement Learning,Proportional Integral Derivative (PID) controller-based coaching scheme proposed to expedite reinforcement learning (RL)
Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning,"This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.","['Nick Erickson', 'Qi Zhao']",2017-06-19T00:16:24Z,http://arxiv.org/abs/1706.05749v1,Artificial Intelligence,Reinforcement Learning,"this paper introduces Dex, a reinforcement learning environment toolkit . we also present the novel continual learning method of incremental learning . incremental learning produces vastly superior results than standard methods ."
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.","['Xiao Lei Zhang', 'Anish Agarwal']",2020-03-31T18:08:23Z,http://arxiv.org/abs/2004.00993v2,Artificial Intelligence,Reinforcement Learning,the study of unsupervised learning can be divided into two categories: imitation learning and reinforcement learning . traditional deep reinforcement learning takes a significant time before the machine starts to converge . this paper proposes Augmented Q-imitation-learning as the initial training process in traditional Deep Q-learning .
Interpretable Reinforcement Learning with Ensemble Methods,"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.","['Alexander Brown', 'Marek Petrik']",2018-09-19T03:23:35Z,http://arxiv.org/abs/1809.06995v1,Artificial Intelligence,Reinforcement Learning,boosted regression trees compute human-interpretable solutions to reinforcement learning problems . boosting combines several regression trees to improve their accuracy . there has been little progress in interpretable reinforcement learning .
Unsupervised Meta-Learning for Reinforcement Learning,"Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.","['Abhishek Gupta', 'Benjamin Eysenbach', 'Chelsea Finn', 'Sergey Levine']",2018-06-12T16:48:52Z,http://arxiv.org/abs/1806.04640v3,Artificial Intelligence,Reinforcement Learning,"meta-learning algorithms use past experience to learn to quickly solve new tasks . meta-reinforcement learning offloads the design burden from algorithm design to task design . if we can automate the process of task design as well, we can devise a truly automated algorithm ."
Learning to Transfer,"Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge.","['Ying Wei', 'Yu Zhang', 'Qiang Yang']",2017-08-18T14:36:29Z,http://arxiv.org/abs/1708.05629v1,Artificial Intelligence,Transfer Learning,"transfer learning borrows knowledge from a source domain to facilitate learning in a target domain . researchers have to exhaustively explore all existing transfer learning algorithms . a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way ."
Trustworthy Transfer Learning: A Survey,"Transfer learning aims to transfer knowledge or information from a source domain to a relevant target domain. In this paper, we understand transfer learning from the perspectives of knowledge transferability and trustworthiness. This involves two research questions: How is knowledge transferability quantitatively measured and enhanced across domains? Can we trust the transferred knowledge in the transfer learning process? To answer these questions, this paper provides a comprehensive review of trustworthy transfer learning from various aspects, including problem definitions, theoretical analysis, empirical algorithms, and real-world applications. Specifically, we summarize recent theories and algorithms for understanding knowledge transferability under (within-domain) IID and non-IID assumptions. In addition to knowledge transferability, we review the impact of trustworthiness on transfer learning, e.g., whether the transferred knowledge is adversarially robust or algorithmically fair, how to transfer the knowledge under privacy-preserving constraints, etc. Beyond discussing the current advancements, we highlight the open questions and future directions for understanding transfer learning in a reliable and trustworthy manner.","['Jun Wu', 'Jingrui He']",2024-12-18T18:03:51Z,http://arxiv.org/abs/2412.14116v1,Artificial Intelligence,Transfer Learning,"in this paper, we understand transfer learning from the perspectives of knowledge transferability and trustworthiness . the paper summarizes recent theories and algorithms for understanding transferability under (within-domain) IID and non-IID assumptions ."
A Comprehensive Survey on Transfer Learning,"Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.","['Fuzhen Zhuang', 'Zhiyuan Qi', 'Keyu Duan', 'Dongbo Xi', 'Yongchun Zhu', 'Hengshu Zhu', 'Hui Xiong', 'Qing He']",2019-11-07T00:15:02Z,http://arxiv.org/abs/1911.02685v3,Artificial Intelligence,Transfer Learning,survey aims to connect and systematize the existing transfer learning researches . it reviews more than forty representative transfer learning approaches . the applications of transfer learning are also briefly introduced .
Transfer Learning for Portfolio Optimization,"In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called ""transfer risk"", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of ""transferability""; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.","['Haoyang Cao', 'Haotian Gu', 'Xin Guo', 'Mathieu Rosenbaum']",2023-07-25T14:48:54Z,http://arxiv.org/abs/2307.13546v1,Artificial Intelligence,Transfer Learning,"a series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer . transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks ."
Uncovering the Connections Between Adversarial Transferability and   Knowledge Transferability,"Knowledge transferability, or transfer learning, has been widely adopted to allow a pre-trained model in the source domain to be effectively adapted to downstream tasks in the target domain. It is thus important to explore and understand the factors affecting knowledge transferability. In this paper, as the first work, we analyze and demonstrate the connections between knowledge transferability and another important phenomenon--adversarial transferability, \emph{i.e.}, adversarial examples generated against one model can be transferred to attack other models. Our theoretical studies show that adversarial transferability indicates knowledge transferability and vice versa. Moreover, based on the theoretical insights, we propose two practical adversarial transferability metrics to characterize this process, serving as bidirectional indicators between adversarial and knowledge transferability. We conduct extensive experiments for different scenarios on diverse datasets, showing a positive correlation between adversarial transferability and knowledge transferability. Our findings will shed light on future research about effective knowledge transfer learning and adversarial transferability analyses.","['Kaizhao Liang', 'Jacky Y. Zhang', 'Boxin Wang', 'Zhuolin Yang', 'Oluwasanmi Koyejo', 'Bo Li']",2020-06-25T16:04:47Z,http://arxiv.org/abs/2006.14512v4,Artificial Intelligence,Transfer Learning,"knowledge transferability, or transfer learning, has been widely adopted . it allows a pre-trained model in the source domain to be effectively adapted to downstream tasks . our findings will shed light on future research about effective knowledge transfer learning ."
Transfer Learning and Organic Computing for Autonomous Vehicles,"Autonomous Vehicles(AV) are one of the brightest promises of the future which would help cut down fatalities and improve travel time while working in harmony. Autonomous vehicles will face with challenging situations and experiences not seen before. These experiences should be converted to knowledge and help the vehicle prepare better in the future. Online Transfer Learning will help transferring prior knowledge to a new task and also keep the knowledge updated as the task evolves. This paper presents the different methods of transfer learning, online transfer learning and organic computing that could be adapted to the domain of autonomous vehicles.",['Christofer Fellicious'],2018-08-16T12:28:11Z,http://arxiv.org/abs/1808.05443v1,Artificial Intelligence,Transfer Learning,autonomous vehicles(AV) are one of the brightest promises of the future . they will face with challenging situations and experiences not seen before . these experiences should be converted into knowledge and help the vehicle prepare better .
Augmenting Transfer Learning with Semantic Reasoning,"Transfer learning aims at building robust prediction models by transferring knowledge gained from one problem to another. In the semantic Web, learning tasks are enhanced with semantic representations. We exploit their semantics to augment transfer learning by dealing with when to transfer with semantic measurements and what to transfer with semantic embeddings. We further present a general framework that integrates the above measurements and embeddings with existing transfer learning algorithms for higher performance. It has demonstrated to be robust in two real-world applications: bus delay forecasting and air quality forecasting.","['Freddy Lecue', 'Jiaoyan Chen', 'Jeff Z. Pan', 'Huajun Chen']",2019-05-31T15:21:10Z,http://arxiv.org/abs/1905.13672v2,Artificial Intelligence,Transfer Learning,"in the semantic Web, learning tasks are enhanced with semantic representations . we exploit their semantics to augment transfer learning . it has demonstrated to be robust in two real-world applications: bus delay forecasting ."
The ART of Transfer Learning: An Adaptive and Robust Pipeline,"Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources. In this work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline of performing transfer learning with generic machine learning algorithms. We establish the non-asymptotic learning theory of ART, providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer. Additionally, we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered. We demonstrate the promising performance of ART through extensive empirical studies on regression, classification, and sparse learning. We further present a real-data analysis for a mortality study.","['Boxiang Wang', 'Yunan Wu', 'Chenglong Ye']",2023-04-30T16:36:57Z,http://arxiv.org/abs/2305.00520v1,Artificial Intelligence,Transfer Learning,"transfer learning is an essential tool for improving the performance of primary tasks . in this work, we propose Adaptive Robust Transfer Learning (ART) ART provides a provable theoretical guarantee for achieving adaptive transfer ."
Risk of Transfer Learning and its Applications in Finance,"Transfer learning is an emerging and popular paradigm for utilizing existing knowledge from previous learning tasks to improve the performance of new ones. In this paper, we propose a novel concept of transfer risk and and analyze its properties to evaluate transferability of transfer learning. We apply transfer learning techniques and this concept of transfer risk to stock return prediction and portfolio optimization problems. Numerical results demonstrate a strong correlation between transfer risk and overall transfer learning performance, where transfer risk provides a computationally efficient way to identify appropriate source tasks in transfer learning, including cross-continent, cross-sector, and cross-frequency transfer for portfolio optimization.","['Haoyang Cao', 'Haotian Gu', 'Xin Guo', 'Mathieu Rosenbaum']",2023-11-06T17:23:54Z,http://arxiv.org/abs/2311.03283v1,Artificial Intelligence,Transfer Learning,"transfer learning is an emerging paradigm for utilizing existing knowledge from previous learning tasks . in this paper, we propose a novel concept of transfer risk and analyze its properties . Numerical results demonstrate a strong correlation between the concept and performance ."
Feasibility and Transferability of Transfer Learning: A Mathematical   Framework,"Transfer learning is an emerging and popular paradigm for utilizing existing knowledge from previous learning tasks to improve the performance of new ones. Despite its numerous empirical successes, theoretical analysis for transfer learning is limited. In this paper we build for the first time, to the best of our knowledge, a mathematical framework for the general procedure of transfer learning. Our unique reformulation of transfer learning as an optimization problem allows for the first time, analysis of its feasibility. Additionally, we propose a novel concept of transfer risk to evaluate transferability of transfer learning. Our numerical studies using the Office-31 dataset demonstrate the potential and benefits of incorporating transfer risk in the evaluation of transfer learning performance.","['Haoyang Cao', 'Haotian Gu', 'Xin Guo', 'Mathieu Rosenbaum']",2023-01-27T05:54:53Z,http://arxiv.org/abs/2301.11542v1,Artificial Intelligence,Transfer Learning,"in this paper, we build for the first time, a mathematical framework for the general procedure of transfer learning . we propose a novel concept of transfer risk to evaluate transferability . despite its numerous empirical successes, theoretical analysis for transfer learning is limited ."
Meta-learning Transferable Representations with a Single Target Domain,"Recent works found that fine-tuning and joint training---two popular approaches for transfer learning---do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.","['Hong Liu', 'Jeff Z. HaoChen', 'Colin Wei', 'Tengyu Ma']",2020-11-03T01:57:37Z,http://arxiv.org/abs/2011.01418v1,Artificial Intelligence,Transfer Learning,fine-tuning and joint training can be suboptimal or harmful for transfer learning . joint training may learn source-specific features and overfit to the target . meta-learns representations by ensuring that a head fit performs well on target validation data .
Constrained Deep Transfer Feature Learning and its Applications,"Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however, typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from the source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as a constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain. To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications.","['Yue Wu', 'Qiang Ji']",2017-09-23T23:44:00Z,http://arxiv.org/abs/1709.08128v1,Artificial Intelligence,Transfer Learning,a constrained deep feature learning method is proposed to perform simultaneous transfer learning and feature learning . the method is designed to narrow the gap between the target domain and the source domain for effective transfer .
Bayesian Transfer Learning: An Overview of Probabilistic Graphical   Models for Transfer Learning,"Transfer learning where the behavior of extracting transferable knowledge from the source domain(s) and reusing this knowledge to target domain has become a research area of great interest in the field of artificial intelligence. Probabilistic graphical models (PGMs) have been recognized as a powerful tool for modeling complex systems with many advantages, e.g., the ability to handle uncertainty and possessing good interpretability. Considering the success of these two aforementioned research areas, it seems natural to apply PGMs to transfer learning. However, although there are already some excellent PGMs specific to transfer learning in the literature, the potential of PGMs for this problem is still grossly underestimated. This paper aims to boost the development of PGMs for transfer learning by 1) examining the pilot studies on PGMs specific to transfer learning, i.e., analyzing and summarizing the existing mechanisms particularly designed for knowledge transfer; 2) discussing examples of real-world transfer problems where existing PGMs have been successfully applied; and 3) exploring several potential research directions on transfer learning using PGM.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang']",2021-09-27T01:06:46Z,http://arxiv.org/abs/2109.13233v1,Artificial Intelligence,Transfer Learning,the potential of probabilistic graphical models (PGMs) for transfer learning is still grossly underestimated . the paper examines the pilot studies on PGMs specific to transfer learning . it also discusses examples of real-world transfer problems .
Spatial Transfer Learning with Simple MLP,First step to investigate the potential of transfer learning applied to the field of spatial statistics,['Hongjian Yang'],2024-05-05T20:39:15Z,http://arxiv.org/abs/2405.03720v1,Artificial Intelligence,Transfer Learning,"first step to investigate the potential of transfer learning applied to the field of spatial statistics . transfer learning is a method of learning that can be applied to a wide range of fields such as spatial statistics, e.g., geospatial statistics, geostatistics, etc."
Transferability in Deep Learning: A Survey,"The success of deep learning algorithms generally depends on large-scale data, while humans appear to have inherent ability of knowledge transfer, by recognizing and applying relevant knowledge from previous learning experiences when encountering and solving unseen tasks. Such an ability to acquire and reuse knowledge is known as transferability in deep learning. It has formed the long-term quest towards making deep learning as data-efficient as human learning, and has been motivating fruitful design of more powerful deep learning algorithms. We present this survey to connect different isolated areas in deep learning with their relation to transferability, and to provide a unified and complete view to investigating transferability through the whole lifecycle of deep learning. The survey elaborates the fundamental goals and challenges in parallel with the core principles and methods, covering recent cornerstones in deep architectures, pre-training, task adaptation and domain adaptation. This highlights unanswered questions on the appropriate objectives for learning transferable knowledge and for adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and negative transfer. Finally, we implement a benchmark and an open-source library, enabling a fair evaluation of deep learning methods in terms of transferability.","['Junguang Jiang', 'Yang Shu', 'Jianmin Wang', 'Mingsheng Long']",2022-01-15T15:03:17Z,http://arxiv.org/abs/2201.05867v1,Artificial Intelligence,Transfer Learning,humans appear to have inherent ability of knowledge transfer . this ability to acquire and reuse knowledge is known as transferability in deep learning . it has formed the long-term quest towards making deep learning data-efficient .
Fractional Transfer Learning for Deep Model-Based Reinforcement Learning,"Reinforcement learning (RL) is well known for requiring large amounts of data in order for RL agents to learn to perform complex tasks. Recent progress in model-based RL allows agents to be much more data-efficient, as it enables them to learn behaviors of visual environments in imagination by leveraging an internal World Model of the environment. Improved sample efficiency can also be achieved by reusing knowledge from previously learned tasks, but transfer learning is still a challenging topic in RL. Parameter-based transfer learning is generally done using an all-or-nothing approach, where the network's parameters are either fully transferred or randomly initialized. In this work we present a simple alternative approach: fractional transfer learning. The idea is to transfer fractions of knowledge, opposed to discarding potentially useful knowledge as is commonly done with random initialization. Using the World Model-based Dreamer algorithm, we identify which type of components this approach is applicable to, and perform experiments in a new multi-source transfer learning setting. The results show that fractional transfer learning often leads to substantially improved performance and faster learning compared to learning from scratch and random initialization.","['Remo Sasso', 'Matthia Sabatelli', 'Marco A. Wiering']",2021-08-14T12:44:42Z,http://arxiv.org/abs/2108.06526v1,Artificial Intelligence,Transfer Learning,reinforcement learning (RL) is well known for requiring large amounts of data . recent progress in model-based RL allows agents to be much more data-efficient . but transfer learning is still a challenging topic in RL .
Transfer Learning for Voice Activity Detection: A Denoising Deep Neural   Network Perspective,"Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem.","['Xiao-Lei Zhang', 'Ji Wu']",2013-03-08T20:46:27Z,http://arxiv.org/abs/1303.2104v1,Artificial Intelligence,Transfer Learning,mismatching problem between source and target noisy corpora hinders use of VAD . transfer learning tries to find common learning machine or feature subspace . experimental results demonstrate effectiveness of the transfer learning schemes .
Addressing modern and practical challenges in machine learning: A survey   of online federated and transfer learning,"Online federated learning (OFL) and online transfer learning (OTL) are two collaborative paradigms for overcoming modern machine learning challenges such as data silos, streaming data, and data security. This survey explored OFL and OTL throughout their major evolutionary routes to enhance understanding of online federated and transfer learning. Besides, practical aspects of popular datasets and cutting-edge applications for online federated and transfer learning are highlighted in this work. Furthermore, this survey provides insight into potential future research areas and aims to serve as a resource for professionals developing online federated and transfer learning frameworks.","['Shuang Dai', 'Fanlin Meng']",2022-02-07T11:06:56Z,http://arxiv.org/abs/2202.03070v1,Artificial Intelligence,Transfer Learning,online federated learning (OFL) and online transfer learning (OTL) are two collaborative paradigms for overcoming modern machine learning challenges . this survey explored OFL and OTL throughout their major evolutionary routes .
Learning What and Where to Transfer,"As the application of deep learning has expanded to real-world problems with insufficient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed configurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efficient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outperforms the prior baselines that find ""what and where to transfer"" in a hand-crafted manner.","['Yunhun Jang', 'Hankook Lee', 'Sung Ju Hwang', 'Jinwoo Shin']",2019-05-15T00:36:49Z,http://arxiv.org/abs/1905.05901v1,Artificial Intelligence,Transfer Learning,"transfer learning has gained much attention as means of improving performance in small-data regimes . a novel transfer learning approach based on meta-learning can automatically learn what knowledge to transfer . our automated scheme significantly outperforms the prior baselines that find ""what and where to transfer"""
Continuous Transfer Learning with Label-informed Distribution Alignment,"Transfer learning has been successfully applied across many high-impact applications. However, most existing work focuses on the static transfer learning setting, and very little is devoted to modeling the time evolving target domain, such as the online reviews for movies. To bridge this gap, in this paper, we study a novel continuous transfer learning setting with a time evolving target domain. One major challenge associated with continuous transfer learning is the potential occurrence of negative transfer as the target domain evolves over time. To address this challenge, we propose a novel label-informed C-divergence between the source and target domains in order to measure the shift of data distributions as well as to identify potential negative transfer. We then derive the error bound for the target domain using the empirical estimate of our proposed C-divergence. Furthermore, we propose a generic adversarial Variational Auto-encoder framework named TransLATE by minimizing the classification error and C-divergence of the target domain between consecutive time stamps in a latent feature space. In addition, we define a transfer signature for characterizing the negative transfer based on C-divergence, which indicates that larger C-divergence implies a higher probability of negative transfer in real scenarios. Extensive experiments on synthetic and real data sets demonstrate the effectiveness of our TransLATE framework.","['Jun Wu', 'Jingrui He']",2020-06-05T04:44:58Z,http://arxiv.org/abs/2006.03230v1,Artificial Intelligence,Transfer Learning,transfer learning has been successfully applied across many high-impact applications . but little is devoted to modeling the time evolving target domain . we propose a label-informed C-divergence between the source and target domains .
Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled   Autoencoder for Mixed Tabular Datasets,"The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the reconstruction error by balancing the influence of variables. Finally, we empirically demonstrate that this new metric, compared to the standard MSE: i) outperforms when the dataset is imbalanced, especially when the learning process is insufficient, and ii) provides similar results in the opposite case.","['Samuel Stocksieker', 'Denys Pommeret', 'Arthur Charpentier']",2024-03-23T10:37:22Z,http://arxiv.org/abs/2403.15790v1,Artificial Intelligence,Self-Supervised Learning,"autoencoders are widely employed for learning and constructing a new representation of a dataset . qualitative variables are often encoded using a one-hot encoder with a standard loss function . this paper analyzes the drawbacks of this approach, especially when categorical variables are imbalanced ."
From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent   Reinforcement Learning,"In real-world environments, autonomous agents rely on their egocentric observations. They must learn adaptive strategies to interact with others who possess mixed motivations, discernible only through visible cues. Several Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches that involve either centralized training or reward-sharing, often violating the realistic ways in which living organisms, like animals or humans, process information and interact. MARL strategies deploying decentralized training with intrinsic motivation offer a self-supervised approach, enable agents to develop flexible social strategies through the interaction of autonomous agents. However, by contrasting the self-supervised and centralized methods, we reveal that populations trained with reward-sharing methods surpass those using self-supervised methods in a mixed-motive environment. We link this superiority to specialized role emergence and an agent's expertise in its role. Interestingly, this gap shrinks in pure-motive settings, emphasizing the need for evaluations in more complex, realistic environments (mixed-motive). Our preliminary results suggest a gap in population performance that can be closed by improving self-supervised methods and thereby pushing MARL closer to real-world readiness.","['Violet Xiang', 'Logan Cross', 'Jan-Philipp Fränken', 'Nick Haber']",2023-12-14T05:32:55Z,http://arxiv.org/abs/2312.08662v1,Artificial Intelligence,Self-Supervised Learning,"aaron carroll: in real-world environments, autonomous agents rely on their egocentric observations . he says centralized approaches often violate the realistic ways in which living organisms interact . carsroll: reward-sharing methods surpass those using self-supervised methods in mixed-motive environments . we link this superiority to specialized role emergence and an agent's expertise ."
"Fast, Effective, and Self-Supervised: Transforming Masked Language   Models into Universal Lexical and Sentence Encoders","Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective universal lexical and sentence encoders.","['Fangyu Liu', 'Ivan Vulić', 'Anna Korhonen', 'Nigel Collier']",2021-04-16T10:49:56Z,http://arxiv.org/abs/2104.08027v2,Artificial Intelligence,Self-Supervised Learning,"pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years . previous work indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders . we propose an extremely simple, fast and effective contrastive learning technique, termed mirror-BERT ."
Learned Camera Gain and Exposure Control for Improved Visual Feature   Detection and Matching,"Successful visual navigation depends upon capturing images that contain sufficient useful information. In this letter, we explore a data-driven approach to account for environmental lighting changes, improving the quality of images for use in visual odometry (VO) or visual simultaneous localization and mapping (SLAM). We train a deep convolutional neural network model to predictively adjust camera gain and exposure time parameters such that consecutive images contain a maximal number of matchable features. The training process is fully self-supervised: our training signal is derived from an underlying VO or SLAM pipeline and, as a result, the model is optimized to perform well with that specific pipeline. We demonstrate through extensive real-world experiments that our network can anticipate and compensate for dramatic lighting changes (e.g., transitions into and out of road tunnels), maintaining a substantially higher number of inlier feature matches than competing camera parameter control algorithms.","['Justin Tomasi', 'Brandon Wagstaff', 'Steven L. Waslander', 'Jonathan Kelly']",2021-02-08T16:46:09Z,http://arxiv.org/abs/2102.04341v3,Artificial Intelligence,Self-Supervised Learning,"in this letter, we explore a data-driven approach to account for environmental lighting changes . we train a deep convolutional neural network model to predictively adjust camera gain and exposure time parameters . the model can anticipate and compensate for dramatic lighting changes, we demonstrate ."
Minimax deviation strategies for machine learning and recognition with   short learning samples,The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.,"['Michail Schlesinger', 'Evgeniy Vodolazskiy']",2017-07-16T09:15:08Z,http://arxiv.org/abs/1707.04849v1,Artificial Intelligence,Self-Supervised Learning,the article is devoted to the problem of small learning samples in machine learning . the flaws of maximum likelihood learning and minimax learning are looked into .
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",['Changjian Li'],2020-01-27T07:26:12Z,http://arxiv.org/abs/2001.09608v1,Artificial Intelligence,Self-Supervised Learning,"a lifelong reinforcement learning system is a learning system that can learn through trail-and-error interaction with the environment . in this paper, i give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system ."
Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning,"This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.","['Nick Erickson', 'Qi Zhao']",2017-06-19T00:16:24Z,http://arxiv.org/abs/1706.05749v1,Artificial Intelligence,Self-Supervised Learning,"this paper introduces Dex, a reinforcement learning environment toolkit . we also present the novel continual learning method of incremental learning . incremental learning produces vastly superior results than standard methods ."
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.","['Xiao Lei Zhang', 'Anish Agarwal']",2020-03-31T18:08:23Z,http://arxiv.org/abs/2004.00993v2,Artificial Intelligence,Self-Supervised Learning,the study of unsupervised learning can be divided into two categories: imitation learning and reinforcement learning . traditional deep reinforcement learning takes a significant time before the machine starts to converge . this paper proposes Augmented Q-imitation-learning as the initial training process in traditional Deep Q-learning .
Tuning Learning Rates with the Cumulative-Learning Constant,"This paper introduces a novel method for optimizing learning rates in machine learning. A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale influences training dynamics. Additionally, a cumulative learning constant is identified, offering a framework for designing and optimizing advanced learning rate schedules. These findings have the potential to enhance training efficiency and performance across a wide range of machine learning applications.",['Nathan Faraj'],2025-04-30T00:07:48Z,http://arxiv.org/abs/2505.13457v1,Artificial Intelligence,Self-Supervised Learning,this paper introduces a novel method for optimizing learning rates in machine learning . a previously unrecognized proportionality between learning rates and dataset sizes is discovered . findings have the potential to enhance training efficiency and performance .
Meta-SGD: Learning to Learn Quickly for Few-Shot Learning,"Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.","['Zhenguo Li', 'Fengwei Zhou', 'Fei Chen', 'Hang Li']",2017-07-31T13:08:11Z,http://arxiv.org/abs/1707.09835v2,Artificial Intelligence,Self-Supervised Learning,"meta-learning learns from many related tasks a meta-learner . meta-SGD can initialize and adapt any differentiable learner in just one step . it shows highly competitive performance for few-shot learning on regression, classification ."
Logistic Regression as Soft Perceptron Learning,"We comment on the fact that gradient ascent for logistic regression has a connection with the perceptron learning algorithm. Logistic learning is the ""soft"" variant of perceptron learning.",['Raul Rojas'],2017-08-24T20:19:20Z,http://arxiv.org/abs/1708.07826v1,Artificial Intelligence,Self-Supervised Learning,"gradient ascent for logistic regression has a connection with the perceptron learning algorithm . logistic learning is the ""soft"" variant of perceptoron learning ."
A Learning Algorithm for Relational Logistic Regression: Preliminary   Results,"Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set.","['Bahare Fatemi', 'Seyed Mehran Kazemi', 'David Poole']",2016-06-28T01:43:38Z,http://arxiv.org/abs/1606.08531v1,Artificial Intelligence,Self-Supervised Learning,"in this paper, we develop a learning algorithm for relational logistic regression (RLR) learning an RLR model consists of two steps: 1- learning the set of formulae to be used in the model . for parameter learning, we convert the problem into a non-relational learning problem ."
Meta-Learning Neural Procedural Biases,"The goal of few-shot learning is to generalize and achieve high performance on new unseen learning tasks, where each task has only a limited number of examples available. Gradient-based meta-learning attempts to address this challenging task by learning how to learn new tasks by embedding inductive biases informed by prior learning experiences into the components of the learning algorithm. In this work, we build upon prior research and propose Neural Procedural Bias Meta-Learning (NPBML), a novel framework designed to meta-learn task-adaptive procedural biases. Our approach aims to consolidate recent advancements in meta-learned initializations, optimizers, and loss functions by learning them simultaneously and making them adapt to each individual task to maximize the strength of the learned inductive biases. This imbues each learning task with a unique set of procedural biases which is specifically designed and selected to attain strong learning performance in only a few gradient steps. The experimental results show that by meta-learning the procedural biases of a neural network, we can induce strong inductive biases towards a distribution of learning tasks, enabling robust learning performance across many well-established few-shot learning benchmarks.","['Christian Raymond', 'Qi Chen', 'Bing Xue', 'Mengjie Zhang']",2024-06-12T08:09:29Z,http://arxiv.org/abs/2406.07983v1,Artificial Intelligence,Self-Supervised Learning,few-shot learning aims to generalize and achieve high performance on new unseen learning tasks . we propose a framework designed to meta-learn task-adaptive procedural biases . this imbues each learning task with a unique set of procedural biased to attain strong learning performance .
A Comprehensive Overview and Survey of Recent Advances in Meta-Learning,"This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.",['Huimin Peng'],2020-04-17T03:11:08Z,http://arxiv.org/abs/2004.11149v7,Artificial Intelligence,Self-Supervised Learning,this article reviews meta-learning which seeks rapid and accurate model adaptation to unseen tasks . Meta-learning may serve as an additional generalization block complementary for original deep learning model . recent applications concentrate upon the integration of meta learning with other machine learning frameworks .
From Knowing to Doing: Learning Diverse Motor Skills through Instruction   Learning,"Recent years have witnessed many successful trials in the robot learning field. For contact-rich robotic tasks, it is challenging to learn coordinated motor skills by reinforcement learning. Imitation learning solves this problem by using a mimic reward to encourage the robot to track a given reference trajectory. However, imitation learning is not so efficient and may constrain the learned motion. In this paper, we propose instruction learning, which is inspired by the human learning process and is highly efficient, flexible, and versatile for robot motion learning. Instead of using a reference signal in the reward, instruction learning applies a reference signal directly as a feedforward action, and it is combined with a feedback action learned by reinforcement learning to control the robot. Besides, we propose the action bounding technique and remove the mimic reward, which is shown to be crucial for efficient and flexible learning. We compare the performance of instruction learning with imitation learning, indicating that instruction learning can greatly speed up the training process and guarantee learning the desired motion correctly. The effectiveness of instruction learning is validated through a bunch of motion learning examples for a biped robot and a quadruped robot, where skills can be learned typically within several million steps. Besides, we also conduct sim-to-real transfer and online learning experiments on a real quadruped robot. Instruction learning has shown great merits and potential, making it a promising alternative for imitation learning.","['Linqi Ye', 'Jiayi Li', 'Yi Cheng', 'Xianhao Wang', 'Bin Liang', 'Yan Peng']",2023-09-17T05:42:43Z,http://arxiv.org/abs/2309.09167v2,Artificial Intelligence,Self-Supervised Learning,recent years have witnessed many successful trials in the robot learning field . imitation learning solves this problem by using a mimic reward to encourage the robot . but imitation learning is not so efficient and may constrain the learned motion .
metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.","['William de Vazelhes', 'CJ Carey', 'Yuan Tang', 'Nathalie Vauquier', 'Aurélien Bellet']",2019-08-13T15:52:31Z,http://arxiv.org/abs/1908.04710v3,Artificial Intelligence,Self-Supervised Learning,metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms . it is available on PyPi under the MIT licence .
Deep Reinforcement Learning using Cyclical Learning Rates,"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning of hyperparameters to successfully resolve problems. One of the most influential parameters in optimization procedures based on stochastic gradient descent (SGD) is the learning rate. We investigate cyclical learning and propose a method for defining a general cyclical learning rate for various DRL problems. In this paper we present a method for cyclical learning applied to complex DRL problems. Our experiments show that, utilizing cyclical learning achieves similar or even better results than highly tuned fixed learning rates. This paper presents the first application of cyclical learning rates in DRL settings and is a step towards overcoming manual hyperparameter tuning.","['Ralf Gulde', 'Marc Tuscher', 'Akos Csiszar', 'Oliver Riedel', 'Alexander Verl']",2020-07-31T10:06:02Z,http://arxiv.org/abs/2008.01171v1,Artificial Intelligence,Self-Supervised Learning,this paper presents the first application of cyclical learning rates in DRL settings . it is a step towards overcoming manual hyperparameter tuning .
Positive semidefinite support vector regression metric learning,"Most existing metric learning methods focus on learning a similarity or distance measure relying on similar and dissimilar relations between sample pairs. However, pairs of samples cannot be simply identified as similar or dissimilar in many real-world applications, e.g., multi-label learning, label distribution learning. To this end, relation alignment metric learning (RAML) framework is proposed to handle the metric learning problem in those scenarios. But RAML framework uses SVR solvers for optimization. It can't learn positive semidefinite distance metric which is necessary in metric learning. In this paper, we propose two methds to overcame the weakness. Further, We carry out several experiments on the single-label classification, multi-label classification, label distribution learning to demonstrate the new methods achieves favorable performance against RAML framework.",['Lifeng Gu'],2020-08-18T04:45:59Z,http://arxiv.org/abs/2008.07739v1,Artificial Intelligence,Self-Supervised Learning,existing metric learning methods focus on learning a similarity or distance measure . but pairs of samples cannot be identified as similar or dissimilar in many real-world applications . RAML framework uses SVR solvers for optimization .
Learning to Learn Neural Networks,"Meta-learning consists in learning learning algorithms. We use a Long Short Term Memory (LSTM) based network to learn to compute on-line updates of the parameters of another neural network. These parameters are stored in the cell state of the LSTM. Our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology. In an experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is able to update parameters of both layers and generalise well on similar datasets.",['Tom Bosc'],2016-10-19T15:46:30Z,http://arxiv.org/abs/1610.06072v1,Artificial Intelligence,Self-Supervised Learning,we use a long short Term Memory (LSTM) based network to learn to compute on-line updates of parameters of another neural network . our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology .
Theoretical Models of Learning to Learn,"A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an {\em environment} of related tasks, then it can {\em learn} its own bias by learning sufficiently many tasks from the environment. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.",['Jonathan Baxter'],2020-02-27T13:35:26Z,http://arxiv.org/abs/2002.12364v1,Artificial Intelligence,Self-Supervised Learning,"a Machine can only learn if it is biased in some way . if the learning machine is embedded within an em environment of related tasks, then it can 'em learn' its own bias ."
Meta-Learning: A Survey,"Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.",['Joaquin Vanschoren'],2018-10-08T16:07:11Z,http://arxiv.org/abs/1810.03548v1,Artificial Intelligence,Meta-Learning,"meta-learning, or learning to learn, is the science of observing how different approaches perform on a wide range of learning tasks . it allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way ."
Generalization Bounds For Meta-Learning: An Information-Theoretic   Analysis,"We derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms. Concretely, our analysis proposes a generic understanding of both the conventional learning-to-learn framework and the modern model-agnostic meta-learning (MAML) algorithms. Moreover, we provide a data-dependent generalization bound for a stochastic variant of MAML, which is non-vacuous for deep few-shot learning. As compared to previous bounds that depend on the square norm of gradients, empirical validations on both simulated data and a well-known few-shot benchmark show that our bound is orders of magnitude tighter in most situations.","['Qi Chen', 'Changjian Shui', 'Mario Marchand']",2021-09-29T17:45:54Z,http://arxiv.org/abs/2109.14595v2,Artificial Intelligence,Meta-Learning,"we provide a data-dependent generalization bound for a stochastic variant of MAML . our bound is orders of magnitude tighter in most situations, compared to previous bounds ."
Combining Forecasts using Meta-Learning: A Comparative Study for Complex   Seasonality,"In this paper, we investigate meta-learning for combining forecasts generated by models of different types. While typical approaches for combining forecasts involve simple averaging, machine learning techniques enable more sophisticated methods of combining through meta-learning, leading to improved forecasting accuracy. We use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest, and long short-term memory as meta-learners. We define global and local meta-learning variants for time series with complex seasonality and compare meta-learners on multiple forecasting problems, demonstrating their superior performance compared to simple averaging.",['Grzegorz Dudek'],2025-04-11T19:43:11Z,http://arxiv.org/abs/2504.08940v1,Artificial Intelligence,Meta-Learning,"machine learning techniques enable more sophisticated methods of combining forecasts . we use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest ."
From Learning to Meta-Learning: Reduced Training Overhead and Complexity   for Communication Systems,"Machine learning methods adapt the parameters of a model, constrained to lie in a given model class, by using a fixed learning procedure based on data or active observations. Adaptation is done on a per-task basis, and retraining is needed when the system configuration changes. The resulting inefficiency in terms of data and training time requirements can be mitigated, if domain knowledge is available, by selecting a suitable model class and learning procedure, collectively known as inductive bias. However, it is generally difficult to encode prior knowledge into an inductive bias, particularly with black-box model classes such as neural networks. Meta-learning provides a way to automatize the selection of an inductive bias. Meta-learning leverages data or active observations from tasks that are expected to be related to future, and a priori unknown, tasks of interest. With a meta-trained inductive bias, training of a machine learning model can be potentially carried out with reduced training data and/or time complexity. This paper provides a high-level introduction to meta-learning with applications to communication systems.","['Osvaldo Simeone', 'Sangwoo Park', 'Joonhyuk Kang']",2020-01-05T12:54:41Z,http://arxiv.org/abs/2001.01227v1,Artificial Intelligence,Meta-Learning,"meta-learning provides a way to automatize the selection of an inductive bias . it leverages data or active observations from tasks expected to be related to future, and a priori unknown, tasks of interest ."
A Review on Semi-Supervised Relation Extraction,"Relation extraction (RE) plays an important role in extracting knowledge from unstructured text but requires a large amount of labeled corpus. To reduce the expensive annotation efforts, semisupervised learning aims to leverage both labeled and unlabeled data. In this paper, we review and compare three typical methods in semi-supervised RE with deep learning or meta-learning: self-ensembling, which forces consistent under perturbations but may confront insufficient supervision; self-training, which iteratively generates pseudo labels and retrain itself with the enlarged labeled set; dual learning, which leverages a primal task and a dual task to give mutual feedback. Mean-teacher (Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al., 2019) are elaborated as the representatives to alleviate the weakness of these three methods, respectively.",['Yusen Lin'],2021-03-12T23:43:23Z,http://arxiv.org/abs/2103.07575v1,Artificial Intelligence,Meta-Learning,semisupervised learning aims to leverage both labeled and unlabeled data . authors review and compare three typical methods in semi-supervised RE with meta-learning .
Deep Meta-Learning: Learning to Learn in the Concept Space,"Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to 58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%, and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%, respectively.","['Fengwei Zhou', 'Bin Wu', 'Zhenguo Li']",2018-02-10T14:18:08Z,http://arxiv.org/abs/1802.03596v1,Artificial Intelligence,Meta-Learning,"we argue that few-shot learning remains challenging for meta-learning . we propose deep meta learning to integrate the representation power of deep learning . the framework is composed of three modules, a concept generator and a meta-learner ."
Fair Meta-Learning: Learning How to Learn Fairly,"Data sets for fairness relevant tasks can lack examples or be biased according to a specific label in a sensitive attribute. We demonstrate the usefulness of weight based meta-learning approaches in such situations. For models that can be trained through gradient descent, we demonstrate that there are some parameter configurations that allow models to be optimized from a few number of gradient steps and with minimal data which are both fair and accurate. To learn such weight sets, we adapt the popular MAML algorithm to Fair-MAML by the inclusion of a fairness regularization term. In practice, Fair-MAML allows practitioners to train fair machine learning models from only a few examples when data from related tasks is available. We empirically exhibit the value of this technique by comparing to relevant baselines.","['Dylan Slack', 'Sorelle Friedler', 'Emile Givental']",2019-11-06T21:43:53Z,http://arxiv.org/abs/1911.04336v1,Artificial Intelligence,Meta-Learning,"data sets for fairness relevant tasks can lack examples or be biased . we demonstrate the usefulness of weight based meta-learning approaches . to learn such weight sets, we adapt the popular MAML algorithm to Fair-MAML ."
Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation   and One-Shot Channel Pruning,"We propose a novel meta-learning framework for real-time object tracking with efficient model adaptation and channel pruning. Given an object tracker, our framework learns to fine-tune its model parameters in only a few iterations of gradient-descent during tracking while pruning its network channels using the target ground-truth at the first frame. Such a learning problem is formulated as a meta-learning task, where a meta-tracker is trained by updating its meta-parameters for initial weights, learning rates, and pruning masks through carefully designed tracking simulations. The integrated meta-tracker greatly improves tracking performance by accelerating the convergence of online learning and reducing the cost of feature computation. Experimental evaluation on the standard datasets demonstrates its outstanding accuracy and speed compared to the state-of-the-art methods.","['Ilchae Jung', 'Kihyun You', 'Hyeonwoo Noh', 'Minsu Cho', 'Bohyung Han']",2019-11-25T19:09:01Z,http://arxiv.org/abs/1911.11170v3,Artificial Intelligence,Meta-Learning,we propose a novel meta-learning framework for real-time object tracking . the framework learns to fine-tune its model parameters in only a few iterations . it also prunes its network channels using the target ground-truth at the first frame .
Unraveling Meta-Learning: Understanding Feature Representations for   Few-Shot Tasks,"Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we introduce and verify several hypotheses for why meta-learned models perform better. Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification. In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.","['Micah Goldblum', 'Steven Reich', 'Liam Fowl', 'Renkun Ni', 'Valeriia Cherepanova', 'Tom Goldstein']",2020-02-17T03:18:45Z,http://arxiv.org/abs/2002.06753v3,Artificial Intelligence,Meta-Learning,meta-learning algorithms produce feature extractors which achieve state-of-the-art performance . little is known about why meta-learned models perform so well . we develop a regularizer which boosts the performance of standard training routines .
Meta-learning: searching in the model space,"There is no free lunch, no single learning algorithm that will outperform other algorithms on all data. In practice different approaches are tried and the best algorithm selected. An alternative solution is to build new algorithms on demand by creating a framework that accommodates many algorithms. The best combination of parameters and procedures is searched here in the space of all possible models belonging to the framework of Similarity-Based Methods (SBMs). Such meta-learning approach gives a chance to find the best method in all cases. Issues related to the meta-learning and first tests of this approach are presented.","['Włodzisław Duch', 'Karol Grudzińsk']",2018-06-16T08:24:35Z,http://arxiv.org/abs/1806.06207v1,Artificial Intelligence,Meta-Learning,in practice different approaches are tried and the best algorithm selected . an alternative solution is to build new algorithms on demand . such meta-learning approach gives a chance to find the best method .
Meta-Learning to Communicate: Fast End-to-End Training for Fading   Channels,"When a channel model is available, learning how to communicate on fading noisy channels can be formulated as the (unsupervised) training of an autoencoder consisting of the cascade of encoder, channel, and decoder. An important limitation of the approach is that training should be generally carried out from scratch for each new channel. To cope with this problem, prior works considered joint training over multiple channels with the aim of finding a single pair of encoder and decoder that works well on a class of channels. As a result, joint training ideally mimics the operation of non-coherent transmission schemes. In this paper, we propose to obviate the limitations of joint training via meta-learning: Rather than training a common model for all channels, meta-learning finds a common initialization vector that enables fast training on any channel. The approach is validated via numerical results, demonstrating significant training speed-ups, with effective encoders and decoders obtained with as little as one iteration of Stochastic Gradient Descent.","['Sangwoo Park', 'Osvaldo Simeone', 'Joonhyuk Kang']",2019-10-22T13:04:38Z,http://arxiv.org/abs/1910.09945v1,Artificial Intelligence,Meta-Learning,training should be generally carried out from scratch for each new channel . prior works considered joint training over multiple channels . meta-learning finds a common initialization vector that enables fast training .
Meta-learning curiosity algorithms,"We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper.","['Ferran Alet', 'Martin F. Schneider', 'Tomas Lozano-Perez', 'Leslie Pack Kaelbling']",2020-03-11T14:25:43Z,http://arxiv.org/abs/2003.05325v1,Artificial Intelligence,Meta-Learning,curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life . we propose to meta-learn algorithms similar to those designed by humans in ML papers . our rich language of programs combines neural networks with other building blocks .
Multimodality in Meta-Learning: A Comprehensive Survey,"Meta-learning has gained wide popularity as a training framework that is more data-efficient than traditional machine learning methods. However, its generalization ability in complex task distributions, such as multimodal tasks, has not been thoroughly studied. Recently, some studies on multimodality-based meta-learning have emerged. This survey provides a comprehensive overview of the multimodality-based meta-learning landscape in terms of the methodologies and applications. We first formalize the definition of meta-learning in multimodality, along with the research challenges in this growing field, such as how to enrich the input in few-shot learning (FSL) or zero-shot learning (ZSL) in multimodal scenarios and how to generalize the models to new tasks. We then propose a new taxonomy to discuss typical meta-learning algorithms in multimodal tasks systematically. We investigate the contributions of related papers and summarize them by our taxonomy. Finally, we propose potential research directions for this promising field.","['Yao Ma', 'Shilin Zhao', 'Weixiao Wang', 'Yaoman Li', 'Irwin King']",2021-09-28T09:16:12Z,http://arxiv.org/abs/2109.13576v2,Artificial Intelligence,Meta-Learning,"meta-learning has gained wide popularity as a training framework that is more data-efficient than traditional machine learning methods . but its generalization ability in complex task distributions, such as multimodal tasks, has not been thoroughly studied . this survey provides a comprehensive overview of the multimodality-based meta- learning landscape in terms of methodologies and applications ."
Forecasting Market Prices using DL with Data Augmentation and   Meta-learning: ARIMA still wins!,"Deep-learning techniques have been successfully used for time-series forecasting and have often shown superior performance on many standard benchmark datasets as compared to traditional techniques. Here we present a comprehensive and comparative study of performance of deep-learning techniques for forecasting prices in financial markets. We benchmark state-of-the-art deep-learning baselines, such as NBeats, etc., on data from currency as well as stock markets. We also generate synthetic data using a fuzzy-logic based model of demand driven by technical rules such as moving averages, which are often used by traders. We benchmark the baseline techniques on this synthetic data as well as use it for data augmentation. We also apply gradient-based meta-learning to account for non-stationarity of financial time-series. Our extensive experiments notwithstanding, the surprising result is that the standard ARIMA models outperforms deep-learning even using data augmentation or meta-learning. We conclude by speculating as to why this might be the case.","['Vedant Shah', 'Gautam Shroff']",2021-10-19T20:01:34Z,http://arxiv.org/abs/2110.10233v2,Artificial Intelligence,Meta-Learning,deep-learning techniques have been successfully used for time-series forecasting . they have often shown superior performance on many standard benchmark datasets . the standard ARIMA models outperform deep learning even using data augmentation .
Exploring Active Learning in Meta-Learning: Enhancing Context Set   Labeling,"Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets.","['Wonho Bae', 'Jing Wang', 'Danica J. Sutherland']",2023-11-06T05:18:01Z,http://arxiv.org/abs/2311.02879v3,Artificial Intelligence,Meta-Learning,"in some settings, it is feasible to actively select which points to label . the potential gain from a careful choice is substantial . we propose a natural algorithm based on fitting Gaussian mixtures ."
Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable   and Transferable Bandwidth Allocation,"In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\%$, and sampling efficiency by $73\%$, compared with existing benchmarks. After fine-tuning, our near-optimal GNN-based policy can achieve close to the same reward with much lower inference complexity compared to the optimal policy obtained using iterative optimization.","['Xin Hao', 'Changyang She', 'Phee Lep Yeoh', 'Yuhong Liu', 'Branka Vucetic', 'Yonghui Li']",2023-12-23T04:25:12Z,http://arxiv.org/abs/2401.10253v2,Artificial Intelligence,Meta-Learning,"in this paper, we develop a deep learning-based bandwidth allocation policy . the policy is scalable with the number of users and transferable to different scenarios . to support scalability, the bandwidth allocation is represented by a graph neural network (GNN)"
Privacy Challenges in Meta-Learning: An Investigation on Model-Agnostic   Meta-Learning,"Meta-learning involves multiple learners, each dedicated to specific tasks, collaborating in a data-constrained setting. In current meta-learning methods, task learners locally learn models from sensitive data, termed support sets. These task learners subsequently share model-related information, such as gradients or loss values, which is computed using another part of the data termed query set, with a meta-learner. The meta-learner employs this information to update its meta-knowledge. Despite the absence of explicit data sharing, privacy concerns persist. This paper examines potential data leakage in a prominent metalearning algorithm, specifically Model-Agnostic Meta-Learning (MAML). In MAML, gradients are shared between the metalearner and task-learners. The primary objective is to scrutinize the gradient and the information it encompasses about the task dataset. Subsequently, we endeavor to propose membership inference attacks targeting the task dataset containing support and query sets. Finally, we explore various noise injection methods designed to safeguard the privacy of task data and thwart potential attacks. Experimental results demonstrate the effectiveness of these attacks on MAML and the efficacy of proper noise injection methods in countering them.","['Mina Rafiei', 'Mohammadmahdi Maheri', 'Hamid R. Rabiee']",2024-06-01T01:10:35Z,http://arxiv.org/abs/2406.00249v1,Artificial Intelligence,Meta-Learning,paper examines potential data leakage in a prominent metalearning algorithm . model-agnostic meta-learning (MAML) uses gradients shared between metalearner and task-learners . noise injection methods designed to safeguard privacy of task data are explored .
Deep Online Learning via Meta-Learning: Continual Adaptation for   Model-Based RL,"Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.","['Anusha Nagabandi', 'Chelsea Finn', 'Sergey Levine']",2018-12-18T22:27:31Z,http://arxiv.org/abs/1812.07671v2,Artificial Intelligence,Meta-Learning,paper is to develop a method for continual online learning from an incoming stream of data . it uses stochastic gradient descent to update model parameters . new models instantiated for task changes and old models recalled .
Transfer Meta-Learning: Information-Theoretic Bounds and Information   Meta-Risk Minimization,"Meta-learning automatically infers an inductive bias by observing data from a number of related tasks. The inductive bias is encoded by hyperparameters that determine aspects of the model class or training algorithm, such as initialization or learning rate. Meta-learning assumes that the learning tasks belong to a task environment, and that tasks are drawn from the same task environment both during meta-training and meta-testing. This, however, may not hold true in practice. In this paper, we introduce the problem of transfer meta-learning, in which tasks are drawn from a target task environment during meta-testing that may differ from the source task environment observed during meta-training. Novel information-theoretic upper bounds are obtained on the transfer meta-generalization gap, which measures the difference between the meta-training loss, available at the meta-learner, and the average loss on meta-test data from a new, randomly selected, task in the target task environment. The first bound, on the average transfer meta-generalization gap, captures the meta-environment shift between source and target task environments via the KL divergence between source and target data distributions. The second, PAC-Bayesian bound, and the third, single-draw bound, account for this shift via the log-likelihood ratio between source and target task distributions. Furthermore, two transfer meta-learning solutions are introduced. For the first, termed Empirical Meta-Risk Minimization (EMRM), we derive bounds on the average optimality gap. The second, referred to as Information Meta-Risk Minimization (IMRM), is obtained by minimizing the PAC-Bayesian bound. IMRM is shown via experiments to potentially outperform EMRM.","['Sharu Theresa Jose', 'Osvaldo Simeone', 'Giuseppe Durisi']",2020-11-04T12:55:43Z,http://arxiv.org/abs/2011.02872v2,Artificial Intelligence,Meta-Learning,meta-learning automatically infers an inductive bias by observing data from a number of related tasks . the problem may not hold true in practice . novel upper bounds are obtained on the transfer meta-generalization gap .
Connecting Context-specific Adaptation in Humans to Meta-learning,"Cognitive control, the ability of a system to adapt to the demands of a task, is an integral part of cognition. A widely accepted fact about cognitive control is that it is context-sensitive: Adults and children alike infer information about a task's demands from contextual cues and use these inferences to learn from ambiguous cues. However, the precise way in which people use contextual cues to guide adaptation to a new task remains poorly understood. This work connects the context-sensitive nature of cognitive control to a method for meta-learning with context-conditioned adaptation. We begin by identifying an essential difference between human learning and current approaches to meta-learning: In contrast to humans, existing meta-learning algorithms do not make use of task-specific contextual cues but instead rely exclusively on online feedback in the form of task-specific labels or rewards. To remedy this, we introduce a framework for using contextual information about a task to guide the initialization of task-specific models before adaptation to online feedback. We show how context-conditioned meta-learning can capture human behavior in a cognitive task and how it can be scaled to improve the speed of learning in various settings, including few-shot classification and low-sample reinforcement learning. Our work demonstrates that guiding meta-learning with task information can capture complex, human-like behavior, thereby deepening our understanding of cognitive control.","['Rachit Dubey', 'Erin Grant', 'Michael Luo', 'Karthik Narasimhan', 'Thomas Griffiths']",2020-11-27T15:31:39Z,http://arxiv.org/abs/2011.13782v2,Artificial Intelligence,Meta-Learning,cognitive control is an integral part of cognition . the ability of a system to adapt to a task is context-sensitive . this work shows how context-conditioned meta-learning can capture human behavior .
Survey of Privacy Threats and Countermeasures in Federated Learning,"Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.","['Masahiro Hayashitani', 'Junki Mori', 'Isamu Teranishi']",2024-02-01T05:13:14Z,http://arxiv.org/abs/2402.00342v1,Artificial Intelligence,Federated Learning,"there are privacy threats in federated learning, and privacy countermeasures have been studied . we note that common and unique privacy threats have not been categorized and described in a comprehensive way ."
An Empirical Study of Personalized Federated Learning,"Federated learning is a distributed machine learning approach in which a single server and multiple clients collaboratively build machine learning models without sharing datasets on clients. A challenging issue of federated learning is data heterogeneity (i.e., data distributions may differ across clients). To cope with this issue, numerous federated learning methods aim at personalized federated learning and build optimized models for clients. Whereas existing studies empirically evaluated their own methods, the experimental settings (e.g., comparison methods, datasets, and client setting) in these studies differ from each other, and it is unclear which personalized federate learning method achieves the best performance and how much progress can be made by using these methods instead of standard (i.e., non-personalized) federated learning. In this paper, we benchmark the performance of existing personalized federated learning through comprehensive experiments to evaluate the characteristics of each method. Our experimental study shows that (1) there are no champion methods, (2) large data heterogeneity often leads to high accurate predictions, and (3) standard federated learning methods (e.g. FedAvg) with fine-tuning often outperform personalized federated learning methods. We open our benchmark tool FedBench for researchers to conduct experimental studies with various experimental settings.","['Koji Matsuda', 'Yuya Sasaki', 'Chuan Xiao', 'Makoto Onizuka']",2022-06-27T11:08:16Z,http://arxiv.org/abs/2206.13190v1,Artificial Intelligence,Federated Learning,federated learning is a distributed machine learning approach . a single server and multiple clients collaboratively build machine learning models . data heterogeneity can make it difficult to build optimized models for clients .
Leveraging Learning Metrics for Improved Federated Learning,"Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.",['Andre Fu'],2023-09-01T05:25:05Z,http://arxiv.org/abs/2309.00257v1,Artificial Intelligence,Federated Learning,effective rank measures the Shannon Entropy of the singular values of a matrix . effective rank is well-suited to federated problems by out-performing baseline .
Recent Advances on Federated Learning: A Systematic Survey,"Federated learning has emerged as an effective paradigm to achieve privacy-preserving collaborative learning among different parties. Compared to traditional centralized learning that requires collecting data from each party, in federated learning, only the locally trained models or computed gradients are exchanged, without exposing any data information. As a result, it is able to protect privacy to some extent. In recent years, federated learning has become more and more prevalent and there have been many surveys for summarizing related methods in this hot research topic. However, most of them focus on a specific perspective or lack the latest research progress. In this paper, we provide a systematic survey on federated learning, aiming to review the recent advanced federated methods and applications from different aspects. Specifically, this paper includes four major contributions. First, we present a new taxonomy of federated learning in terms of the pipeline and challenges in federated scenarios. Second, we summarize federated learning methods into several categories and briefly introduce the state-of-the-art methods under these categories. Third, we overview some prevalent federated learning frameworks and introduce their features. Finally, some potential deficiencies of current methods and several future directions are discussed.","['Bingyan Liu', 'Nuoyan Lv', 'Yuanchun Guo', 'Yawen Li']",2023-01-03T14:19:03Z,http://arxiv.org/abs/2301.01299v1,Artificial Intelligence,Federated Learning,"federated learning has emerged as an effective paradigm to achieve privacy-preserving collaborative learning . compared to traditional centralized learning that requires collecting data from each party, it is able to protect privacy to some extent ."
Federated Learning in Temporal Heterogeneity,"In this work, we explored federated learning in temporal heterogeneity across clients. We observed that global model obtained by \texttt{FedAvg} trained with fixed-length sequences shows faster convergence than varying-length sequences. We proposed methods to mitigate temporal heterogeneity for efficient federated learning based on the empirical observation.",['Junghwan Lee'],2023-09-17T21:20:35Z,http://arxiv.org/abs/2309.09381v1,Artificial Intelligence,Federated Learning,texttFedAvg trained with fixed-length sequences shows faster convergence . we proposed methods to mitigate temporal heterogeneity for efficient federated learning .
Revocable Federated Learning: A Benchmark of Federated Forest,"A learning federation is composed of multiple participants who use the federated learning technique to collaboratively train a machine learning model without directly revealing the local data. Nevertheless, the existing federated learning frameworks have a serious defect that even a participant is revoked, its data are still remembered by the trained model. In a company-level cooperation, allowing the remaining companies to use a trained model that contains the memories from a revoked company is obviously unacceptable, because it can lead to a big conflict of interest. Therefore, we emphatically discuss the participant revocation problem of federated learning and design a revocable federated random forest (RF) framework, RevFRF, to further illustrate the concept of revocable federated learning. In RevFRF, we first define the security problems to be resolved by a revocable federated RF. Then, a suite of homomorphic encryption based secure protocols are designed for federated RF construction, prediction and revocation. Through theoretical analysis and experiments, we show that the protocols can securely and efficiently implement collaborative training of an RF and ensure that the memories of a revoked participant in the trained RF are securely removed.","['Yang Liu', 'Zhuo Ma', 'Ximeng Liu', 'Zhuzhu Wang', 'Siqi Ma', 'Ken Ren']",2019-11-08T13:20:16Z,http://arxiv.org/abs/1911.03242v1,Artificial Intelligence,Federated Learning,"existing federated learning frameworks have a serious defect . even a participant is revoked, its data are still remembered by the trained model . we show that protocols can securely and efficiently implement collaborative training ."
Federated Learning and Wireless Communications,"Federated learning becomes increasingly attractive in the areas of wireless communications and machine learning due to its powerful functions and potential applications. In contrast to other machine learning tools that require no communication resources, federated learning exploits communications between the central server and the distributed local clients to train and optimize a machine learning model. Therefore, how to efficiently assign limited communication resources to train a federated learning model becomes critical to performance optimization. On the other hand, federated learning, as a brand new tool, can potentially enhance the intelligence of wireless networks. In this article, we provide a comprehensive overview on the relationship between federated learning and wireless communications, including basic principle of federated learning, efficient communications for training a federated learning model, and federated learning for intelligent wireless applications. We also identify some future research challenges and directions at the end of this article.","['Zhijin Qin', 'Geoffrey Ye Li', 'Hao Ye']",2020-05-11T17:07:40Z,http://arxiv.org/abs/2005.05265v2,Artificial Intelligence,Federated Learning,federated learning is increasingly attractive in the areas of wireless communications and machine learning . it exploits communications between the central server and the distributed local clients . the tool can potentially enhance the intelligence of wireless networks .
Federated and Transfer Learning: A Survey on Adversaries and Defense   Mechanisms,"The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.","['Ehsan Hallaji', 'Roozbeh Razavi-Far', 'Mehrdad Saif']",2022-07-05T22:07:26Z,http://arxiv.org/abs/2207.02337v1,Artificial Intelligence,Federated Learning,this chapter performs a survey on the intersection of federated and transfer learning . the main goal of this study is to uncover potential vulnerabilities .
Accelerating Fair Federated Learning: Adaptive Federated Adam,"Federated learning is a distributed and privacy-preserving approach to train a statistical model collaboratively from decentralized data of different parties. However, when datasets of participants are not independent and identically distributed (non-IID), models trained by naive federated algorithms may be biased towards certain participants, and model performance across participants is non-uniform. This is known as the fairness problem in federated learning. In this paper, we formulate fairness-controlled federated learning as a dynamical multi-objective optimization problem to ensure fair performance across all participants. To solve the problem efficiently, we study the convergence and bias of Adam as the server optimizer in federated learning, and propose Adaptive Federated Adam (AdaFedAdam) to accelerate fair federated learning with alleviated bias. We validated the effectiveness, Pareto optimality and robustness of AdaFedAdam in numerical experiments and show that AdaFedAdam outperforms existing algorithms, providing better convergence and fairness properties of the federated scheme.","['Li Ju', 'Tianru Zhang', 'Salman Toor', 'Andreas Hellander']",2023-01-23T10:56:12Z,http://arxiv.org/abs/2301.09357v1,Artificial Intelligence,Federated Learning,"federated learning is a distributed and privacy-preserving approach to train a statistical model . but when datasets of participants are not independent and identically distributed, models may be biased . adaFedAdam proposes a solution to ensure fair performance across all participants . to solve the problem efficiently, we study convergence and bias of Adam as the server optimizer ."
A Benchmark for Federated Hetero-Task Learning,"To investigate the heterogeneity in federated learning in real-world scenarios, we generalize the classic federated learning to federated hetero-task learning, which emphasizes the inconsistency across the participants in federated learning in terms of both data distribution and learning tasks. We also present B-FHTL, a federated hetero-task learning benchmark consisting of simulation dataset, FL protocols and a unified evaluation mechanism. B-FHTL dataset contains three well-designed federated learning tasks with increasing heterogeneity. Each task simulates the clients with different non-IID data and learning tasks. To ensure fair comparison among different FL algorithms, B-FHTL builds in a full suite of FL protocols by providing high-level APIs to avoid privacy leakage, and presets most common evaluation metrics spanning across different learning tasks, such as regression, classification, text generation and etc. Furthermore, we compare the FL algorithms in fields of federated multi-task learning, federated personalization and federated meta learning within B-FHTL, and highlight the influence of heterogeneity and difficulties of federated hetero-task learning. Our benchmark, including the federated dataset, protocols, the evaluation mechanism and the preliminary experiment, is open-sourced at https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL","['Liuyi Yao', 'Dawei Gao', 'Zhen Wang', 'Yuexiang Xie', 'Weirui Kuang', 'Daoyuan Chen', 'Haohui Wang', 'Chenhe Dong', 'Bolin Ding', 'Yaliang Li']",2022-06-07T16:43:09Z,http://arxiv.org/abs/2206.03436v3,Artificial Intelligence,Federated Learning,we present a federated hetero-task learning benchmark . the benchmark is open-sourced at https://github.com/alibaba/federatedScope/tree/master/benchmark/B-FHTL .
Federated Instrumental Variable Analysis via Federated Generalized   Method of Moments,"Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.","['Geetika', 'Somya Tyagi', 'Bapi Chatterjee']",2025-05-27T10:46:43Z,http://arxiv.org/abs/2505.21012v1,Artificial Intelligence,Federated Learning,"instrumental variables (IV) analysis is an important tool for areas such as healthcare . the generalized method of moments (GMM) using deep neural networks offers an efficient approach . to our knowledge, no federated algorithm for either GMM or IV analysis exists ."
FedDCL: a federated data collaboration learning as a hybrid-type   privacy-preserving framework based on federated learning and data   collaboration,"Recently, federated learning has attracted much attention as a privacy-preserving integrated analysis that enables integrated analysis of data held by multiple institutions without sharing raw data. On the other hand, federated learning requires iterative communication across institutions and has a big challenge for implementation in situations where continuous communication with the outside world is extremely difficult. In this study, we propose a federated data collaboration learning (FedDCL), which solves such communication issues by combining federated learning with recently proposed non-model share-type federated learning named as data collaboration analysis. In the proposed FedDCL framework, each user institution independently constructs dimensionality-reduced intermediate representations and shares them with neighboring institutions on intra-group DC servers. On each intra-group DC server, intermediate representations are transformed to incorporable forms called collaboration representations. Federated learning is then conducted between intra-group DC servers. The proposed FedDCL framework does not require iterative communication by user institutions and can be implemented in situations where continuous communication with the outside world is extremely difficult. The experimental results show that the performance of the proposed FedDCL is comparable to that of existing federated learning.","['Akira Imakura', 'Tetsuya Sakurai']",2024-09-27T00:22:38Z,http://arxiv.org/abs/2409.18356v1,Artificial Intelligence,Federated Learning,a new study proposes a federated data collaboration learning (FedDCL) the proposed framework does not require iterative communication by user institutions . it can be implemented in situations where continuous communication with the outside world is difficult .
"A Survey on Federated Learning Systems: Vision, Hype and Reality for   Data Privacy and Protection","Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To achieve smooth flow and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.","['Qinbin Li', 'Zeyi Wen', 'Zhaomin Wu', 'Sixu Hu', 'Naibo Wang', 'Yuan Li', 'Xu Liu', 'Bingsheng He']",2019-07-23T04:30:50Z,http://arxiv.org/abs/1907.09693v7,Artificial Intelligence,Federated Learning,federated learning has been a hot research topic in enabling the collaborative training of machine learning models . there is a requirement in developing systems and infrastructures to ease the development of various federate learning algorithms . a thorough categorization can help the design of federation learning systems .
Federated Machine Learning: Concept and Applications,"Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.","['Qiang Yang', 'Yang Liu', 'Tianjian Chen', 'Yongxin Tong']",2019-02-13T13:16:46Z,http://arxiv.org/abs/1902.04885v1,Artificial Intelligence,Federated Learning,"the secure federated learning framework was first proposed by google in 2016 . we introduce a comprehensive framework that includes horizontal, vertical and transfer learning . building data networks as an effective solution to allow knowledge to be shared ."
FedCIP: Federated Client Intellectual Property Protection with Traitor   Tracking,"Federated learning is an emerging privacy-preserving distributed machine learning that enables multiple parties to collaboratively learn a shared model while keeping each party's data private. However, federated learning faces two main problems: semi-honest server privacy inference attacks and malicious client-side model theft. To address privacy inference attacks, parameter-based encrypted federated learning secure aggregation can be used. To address model theft, a watermark-based intellectual property protection scheme can verify model ownership. Although watermark-based intellectual property protection schemes can help verify model ownership, they are not sufficient to address the issue of continuous model theft by uncaught malicious clients in federated learning. Existing IP protection schemes that have the ability to track traitors are also not compatible with federated learning security aggregation. Thus, in this paper, we propose a Federated Client-side Intellectual Property Protection (FedCIP), which is compatible with federated learning security aggregation and has the ability to track traitors. To the best of our knowledge, this is the first IP protection scheme in federated learning that is compatible with secure aggregation and tracking capabilities.","['Junchuan Liang', 'Rong Wang']",2023-06-02T08:29:10Z,http://arxiv.org/abs/2306.01356v1,Artificial Intelligence,Federated Learning,federated learning is an emerging privacy-preserving distributed machine learning . it enables multiple parties to collaboratively learn a shared model while keeping each party's data private . a watermark-based intellectual property protection scheme can address model theft .
Towards the Theory of Unsupervised Federated Learning: Non-asymptotic   Analysis of Federated EM Algorithms,"While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. Several federated EM algorithms have gained popularity in practice, however, their theoretical foundations are often lacking. In this paper, we first introduce a federated gradient EM algorithm (FedGrEM) designed for the unsupervised learning of mixture models, which supplements the existing federated EM algorithms by considering task heterogeneity and potential adversarial attacks. We present a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on specific statistical models to characterize the explicit estimation error of model parameters and mixture proportions. Our theory elucidates when and how FedGrEM outperforms local single-task learning with insights extending to existing federated EM algorithms. This bridges the gap between their practical success and theoretical understanding. Our numerical results validate our theory, and demonstrate FedGrEM's superiority over existing unsupervised federated learning benchmarks.","['Ye Tian', 'Haolei Weng', 'Yang Feng']",2023-10-23T19:53:36Z,http://arxiv.org/abs/2310.15330v3,Artificial Intelligence,Federated Learning,a federated gradient EM algorithm (FedGrEM) is designed for the unsupervised learning of mixture models . the algorithm considers task heterogeneity and potential adversarial attacks . our theory elucidates when and how FedgrEM outperforms local single-task learning .
Krum Federated Chain (KFC): Using blockchain to defend against   adversarial attacks in Federated Learning,"Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguarding data privacy. However, its distributed nature renders it susceptible to adversarial attacks. Integrating blockchain technology with Federated Learning offers a promising avenue to enhance security and integrity. In this paper, we tackle the potential of blockchain in defending Federated Learning against adversarial attacks. First, we test Proof of Federated Learning, a well known consensus mechanism designed ad-hoc to federated contexts, as a defense mechanism demonstrating its efficacy against Byzantine and backdoor attacks when at least one miner remains uncompromised. Second, we propose Krum Federated Chain, a novel defense strategy combining Krum and Proof of Federated Learning, valid to defend against any configuration of Byzantine or backdoor attacks, even when all miners are compromised. Our experiments conducted on image classification datasets validate the effectiveness of our proposed approaches.","['Mario García-Márquez', 'Nuria Rodríguez-Barroso', 'M. Victoria Luzón', 'Francisco Herrera']",2025-02-10T15:15:50Z,http://arxiv.org/abs/2502.06917v1,Artificial Intelligence,Federated Learning,integrating blockchain with Federated Learning offers promising avenue to enhance security and integrity . proof of federated learning is tested as a defense mechanism against Byzantine and backdoor attacks . Krum Federated Chain is a novel defense strategy combining Krum and Proof of Federated learning .
Federated Learning on Riemannian Manifolds,"Federated learning (FL) has found many important applications in smart-phone-APP based machine learning applications. Although many algorithms have been studied for FL, to the best of our knowledge, algorithms for FL with nonconvex constraints have not been studied. This paper studies FL over Riemannian manifolds, which finds important applications such as federated PCA and federated kPCA. We propose a Riemannian federated SVRG (RFedSVRG) method to solve federated optimization over Riemannian manifolds. We analyze its convergence rate under different scenarios. Numerical experiments are conducted to compare RFedSVRG with the Riemannian counterparts of FedAvg and FedProx. We observed from the numerical experiments that the advantages of RFedSVRG are significant.","['Jiaxiang Li', 'Shiqian Ma']",2022-06-12T05:41:23Z,http://arxiv.org/abs/2206.05668v1,Artificial Intelligence,Federated Learning,this paper studies federated learning (FL) over Riemannian manifolds . the paper proposes a rfedSVRG method to solve federate optimization . we analyze its convergence rate under different scenarios .
Sustainable Federated Learning,"Potential environmental impact of machine learning by large-scale wireless networks is a major challenge for the sustainability of future smart ecosystems. In this paper, we introduce sustainable machine learning in federated learning settings, using rechargeable devices that can collect energy from the ambient environment. We propose a practical federated learning framework that leverages intermittent energy arrivals for training, with provable convergence guarantees. Our framework can be applied to a wide range of machine learning settings in networked environments, including distributed and federated learning in wireless and edge networks. Our experiments demonstrate that the proposed framework can provide significant performance improvement over the benchmark energy-agnostic federated learning settings.","['Basak Guler', 'Aylin Yener']",2021-02-22T18:58:47Z,http://arxiv.org/abs/2102.11274v1,Artificial Intelligence,Federated Learning,paper introduces sustainable machine learning in federated learning settings . rechargeable devices can collect energy from ambient environment for training . framework can provide significant performance improvement over benchmark energy-agnostic .
Addressing modern and practical challenges in machine learning: A survey   of online federated and transfer learning,"Online federated learning (OFL) and online transfer learning (OTL) are two collaborative paradigms for overcoming modern machine learning challenges such as data silos, streaming data, and data security. This survey explored OFL and OTL throughout their major evolutionary routes to enhance understanding of online federated and transfer learning. Besides, practical aspects of popular datasets and cutting-edge applications for online federated and transfer learning are highlighted in this work. Furthermore, this survey provides insight into potential future research areas and aims to serve as a resource for professionals developing online federated and transfer learning frameworks.","['Shuang Dai', 'Fanlin Meng']",2022-02-07T11:06:56Z,http://arxiv.org/abs/2202.03070v1,Artificial Intelligence,Federated Learning,online federated learning (OFL) and online transfer learning (OTL) are two collaborative paradigms for overcoming modern machine learning challenges . this survey explored OFL and OTL throughout their major evolutionary routes .
Explainable AI improves task performance in human-AI collaboration,"Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human-AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI as a decision aid improves task performance in human-AI collaboration. To test this hypothesis, we analyze the effect of augmenting domain experts with explainable AI in the form of visual heatmaps. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed $N=9,600$ assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed $N=5,650$ assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI instead of black-box AI. For example, in the manufacturing setting, we find that augmenting participants with explainable AI (as opposed to black-box AI) leads to a five-fold decrease in the median error rate of human decisions, which gives a significant improvement in task performance.","['Julian Senoner', 'Simon Schallmoser', 'Bernhard Kratzwald', 'Stefan Feuerriegel', 'Torbjørn Netland']",2024-06-12T14:36:22Z,http://arxiv.org/abs/2406.08271v1,Artificial Intelligence,Explainable AI,artificial intelligence (AI) provides considerable opportunities to assist human work . but many AI algorithms operate in a black-box manner . this makes it difficult for humans to validate a prediction made by AI . we hypothesize augmenting humans with explainable AI improves task performance .
"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest   Value for us?","AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, ""Starcraft"" and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance to develop into ""strong AI"".",['Bin Liu'],2021-03-29T02:57:48Z,http://arxiv.org/abs/2103.15294v1,Artificial Intelligence,Explainable AI,many researchers argue that little substantial progress has been made for AI in recent decades . author explains why controversies about AI exist; discriminates two paradigms of AI research . he also clarifies how to judge which paradigm a research work should be classified into .
Understanding Mental Models of AI through Player-AI Interaction,"Designing human-centered AI-driven applications require deep understandings of how people develop mental models of AI. Currently, we have little knowledge of this process and limited tools to study it. This paper presents the position that AI-based games, particularly the player-AI interaction component, offer an ideal domain to study the process in which mental models evolve. We present a case study to illustrate the benefits of our approach for explainable AI.","['Jennifer Villareale', 'Jichen Zhu']",2021-03-30T08:49:45Z,http://arxiv.org/abs/2103.16168v1,Artificial Intelligence,Explainable AI,this paper presents the position that AI-based games offer an ideal domain to study the process in which mental models evolve . we present a case study to illustrate the benefits of our approach for explainable AI .
From Explainable to Interactive AI: A Literature Review on Current   Trends in Human-AI Interaction,"AI systems are increasingly being adopted across various domains and application areas. With this surge, there is a growing research focus and societal concern for actively involving humans in developing, operating, and adopting these systems. Despite this concern, most existing literature on AI and Human-Computer Interaction (HCI) primarily focuses on explaining how AI systems operate and, at times, allowing users to contest AI decisions. Existing studies often overlook more impactful forms of user interaction with AI systems, such as giving users agency beyond contestability and enabling them to adapt and even co-design the AI's internal mechanics. In this survey, we aim to bridge this gap by reviewing the state-of-the-art in Human-Centered AI literature, the domain where AI and HCI studies converge, extending past Explainable and Contestable AI, delving into the Interactive AI and beyond. Our analysis contributes to shaping the trajectory of future Interactive AI design and advocates for a more user-centric approach that provides users with greater agency, fostering not only their understanding of AI's workings but also their active engagement in its development and evolution.","['Muhammad Raees', 'Inge Meijerink', 'Ioanna Lykourentzou', 'Vassilis-Javed Khan', 'Konstantinos Papangelis']",2024-05-23T20:59:20Z,http://arxiv.org/abs/2405.15051v1,Artificial Intelligence,Explainable AI,there is a growing research focus and societal concern for actively involving humans . most existing literature on AI and human-computer interaction (HCI) focuses on explaining how AI systems operate . existing studies often overlook more impactful forms of user interaction with AI .
Improving Health Professionals' Onboarding with AI and XAI for   Trustworthy Human-AI Collaborative Decision Making,"With advanced AI/ML, there has been growing research on explainable AI (XAI) and studies on how humans interact with AI and XAI for effective human-AI collaborative decision-making. However, we still have a lack of understanding of how AI systems and XAI should be first presented to users without technical backgrounds. In this paper, we present the findings of semi-structured interviews with health professionals (n=12) and students (n=4) majoring in medicine and health to study how to improve onboarding with AI and XAI. For the interviews, we built upon human-AI interaction guidelines to create onboarding materials of an AI system for stroke rehabilitation assessment and AI explanations and introduce them to the participants. Our findings reveal that beyond presenting traditional performance metrics on AI, participants desired benchmark information, the practical benefits of AI, and interaction trials to better contextualize AI performance, and refine the objectives and performance of AI. Based on these findings, we highlight directions for improving onboarding with AI and XAI and human-AI collaborative decision-making.","['Min Hun Lee', 'Silvana Xin Yi Choo', 'Shamala D/O Thilarajah']",2024-05-26T04:30:17Z,http://arxiv.org/abs/2405.16424v1,Artificial Intelligence,Explainable AI,"there has been growing research on explainable AI (XAI) and studies on how humans interact with AI . but, we still have a lack of understanding of how AI systems and XAI should be first presented to users without technical backgrounds ."
On Two XAI Cultures: A Case Study of Non-technical Explanations in   Deployed AI System,"Explainable AI (XAI) research has been booming, but the question ""$\textbf{To whom}$ are we making AI explainable?"" is yet to gain sufficient attention. Not much of XAI is comprehensible to non-AI experts, who nonetheless, are the primary audience and major stakeholders of deployed AI systems in practice. The gap is glaring: what is considered ""explained"" to AI-experts versus non-experts are very different in practical scenarios. Hence, this gap produced two distinct cultures of expectations, goals, and forms of XAI in real-life AI deployments.   We advocate that it is critical to develop XAI methods for non-technical audiences. We then present a real-life case study, where AI experts provided non-technical explanations of AI decisions to non-technical stakeholders, and completed a successful deployment in a highly regulated industry. We then synthesize lessons learned from the case, and share a list of suggestions for AI experts to consider when explaining AI decisions to non-technical stakeholders.","['Helen Jiang', 'Erwen Senge']",2021-12-02T07:02:27Z,http://arxiv.org/abs/2112.01016v1,Artificial Intelligence,Explainable AI,"not much of XAI is comprehensible to non-AI experts . what is considered ""explained"" to AI experts versus non-experts is very different in practical scenarios . authors advocate that it is critical to develop xAI methods for non-technical audiences ."
Explainability and the Fourth AI Revolution,"This chapter discusses AI from the prism of an automated process for the organization of data, and exemplifies the role that explainability has to play in moving from the current generation of AI systems to the next one, where the role of humans is lifted from that of data annotators working for the AI systems to that of collaborators working with the AI systems.",['Loizos Michael'],2021-11-12T15:32:27Z,http://arxiv.org/abs/2111.06773v1,Artificial Intelligence,Explainable AI,this chapter discusses AI from the prism of an automated process for the organization of data . it exemplifies the role that explainability has to play in moving from the current generation of AI systems to the next one .
Cybertrust: From Explainable to Actionable and Interpretable AI (AI2),"To benefit from AI advances, users and operators of AI systems must have reason to trust it. Trust arises from multiple interactions, where predictable and desirable behavior is reinforced over time. Providing the system's users with some understanding of AI operations can support predictability, but forcing AI to explain itself risks constraining AI capabilities to only those reconcilable with human cognition. We argue that AI systems should be designed with features that build trust by bringing decision-analytic perspectives and formal tools into AI. Instead of trying to achieve explainable AI, we should develop interpretable and actionable AI. Actionable and Interpretable AI (AI2) will incorporate explicit quantifications and visualizations of user confidence in AI recommendations. In doing so, it will allow examining and testing of AI system predictions to establish a basis for trust in the systems' decision making and ensure broad benefits from deploying and advancing its computational capabilities.","['Stephanie Galaitsi', 'Benjamin D. Trump', 'Jeffrey M. Keisler', 'Igor Linkov', 'Alexander Kott']",2022-01-26T18:53:09Z,http://arxiv.org/abs/2201.11117v1,Artificial Intelligence,Explainable AI,"to benefit from AI advances, users and operators must have reason to trust it . we argue that AI systems should be designed with features that build trust . instead of trying to achieve explainable AI, we should develop interpretable AI ."
Beyond XAI:Obstacles Towards Responsible AI,"The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has sparked significant interests in developing techniques to make AI systems more transparent and understandable. Nevertheless, in real-world contexts, the methods of explainability and their evaluation strategies present numerous limitations.Moreover, the scope of responsible AI extends beyond just explainability. In this paper, we explore these limitations and discuss their implications in a boarder context of responsible AI when considering other important aspects, including privacy, fairness and contestability.",['Yulu Pi'],2023-09-07T11:08:14Z,http://arxiv.org/abs/2309.03638v1,Artificial Intelligence,Explainable AI,the rapidly advancing domain of Explainable Artificial Intelligence (XAI) has sparked significant interest in developing techniques to make AI systems more transparent and understandable . the scope of responsible AI extends beyond just explainability .
AI-in-the-Loop -- The impact of HMI in AI-based Application,"Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. These resulting lightweight AI architectures will enable HMI while running the AI on an edge device. By enabling HMI during an AI uses inference, we will introduce the AI-in-the-loop concept that combines AI's and humans' strengths. In our AI-in-the-loop approach, the AI remains the working horse and primarily solves the task. If the AI is unsure whether its inference solves the task correctly, it asks the user to use an appropriate HMI. Consequently, AI will become available in many applications soon since HMI will make AI more reliable and explainable.","['Julius Schöning', 'Clemens Westerkamp']",2023-03-21T00:04:33Z,http://arxiv.org/abs/2303.11508v1,Artificial Intelligence,Explainable AI,"artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications . during AI architecture design, HMI can immediately highlight unproductive layers of the architecture . this approach reduces the resources needed for AI development by avoiding training ."
Does Explainable Artificial Intelligence Improve Human Decision-Making?,"Explainable AI provides insight into the ""why"" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. Whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model are open questions. Using real datasets, we compare and evaluate objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct versus incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the ""why"" information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.","['Yasmeen Alufaisan', 'Laura R. Marusich', 'Jonathan Z. Bakdash', 'Yan Zhou', 'Murat Kantarcioglu']",2020-06-19T15:46:13Z,http://arxiv.org/abs/2006.11194v1,Artificial Intelligence,Explainable AI,"explainable AI provides insight into the ""why"" for model predictions . authors find providing any kind of AI prediction tends to improve user decision accuracy . strong predictor for human decision accuracy was AI accuracy, they say ."
Reducing Barriers to the Use of Marginalised Music Genres in AI,"AI systems for high quality music generation typically rely on extremely large musical datasets to train the AI models. This creates barriers to generating music beyond the genres represented in dominant datasets such as Western Classical music or pop music. We undertook a 4 month international research project summarised in this paper to explore the eXplainable AI (XAI) challenges and opportunities associated with reducing barriers to using marginalised genres of music with AI models. XAI opportunities identified included topics of improving transparency and control of AI models, explaining the ethics and bias of AI models, fine tuning large models with small datasets to reduce bias, and explaining style-transfer opportunities with AI models. Participants in the research emphasised that whilst it is hard to work with small datasets such as marginalised music and AI, such approaches strengthen cultural representation of underrepresented cultures and contribute to addressing issues of bias of deep learning models. We are now building on this project to bring together a global International Responsible AI Music community and invite people to join our network.","['Nick Bryan-Kinns', 'Zijin Li']",2024-07-18T12:10:04Z,http://arxiv.org/abs/2407.13439v1,Artificial Intelligence,Explainable AI,"a 4 month international research project explored the eXplainable AI (XAI) challenges and opportunities associated with reducing barriers to using marginalised genres of music with AI models . topics included topics of improving transparency and control of AI models, explaining ethics and bias of AI ."
Explainable AI for Software Engineering,"Artificial Intelligence/Machine Learning techniques have been widely used in software engineering to improve developer productivity, the quality of software systems, and decision-making. However, such AI/ML models for software engineering are still impractical, not explainable, and not actionable. These concerns often hinder the adoption of AI/ML models in software engineering practices. In this article, we first highlight the need for explainable AI in software engineering. Then, we summarize three successful case studies on how explainable AI techniques can be used to address the aforementioned challenges by making software defect prediction models more practical, explainable, and actionable.","['Chakkrit Tantithamthavorn', 'Jirayus Jiarpakdee', 'John Grundy']",2020-12-03T00:42:29Z,http://arxiv.org/abs/2012.01614v1,Artificial Intelligence,Explainable AI,"AI/ML models for software engineering are still impractical, not explainable, and not actionable . this article highlights the need for explainable AI in software engineering ."
Creative Uses of AI Systems and their Explanations: A Case Study from   Insurance,"Recent works have recognized the need for human-centered perspectives when designing and evaluating human-AI interactions and explainable AI methods. Yet, current approaches fall short at intercepting and managing unexpected user behavior resulting from the interaction with AI systems and explainability methods of different stake-holder groups. In this work, we explore the use of AI and explainability methods in the insurance domain. In an qualitative case study with participants with different roles and professional backgrounds, we show that AI and explainability methods are used in creative ways in daily workflows, resulting in a divergence between their intended and actual use. Finally, we discuss some recommendations for the design of human-AI interactions and explainable AI methods to manage the risks and harness the potential of unexpected user behavior.","['Michaela Benk', 'Raphael Weibel', 'Andrea Ferrario']",2022-05-02T14:24:11Z,http://arxiv.org/abs/2205.00931v2,Artificial Intelligence,Explainable AI,"we explore the use of AI and explainability methods in the insurance domain . we show that they are used in creative ways, resulting in a divergence between intended and actual use ."
Towards a Praxis for Intercultural Ethics in Explainable AI,"Explainable AI (XAI) is often promoted with the idea of helping users understand how machine learning models function and produce predictions. Still, most of these benefits are reserved for those with specialized domain knowledge, such as machine learning developers. Recent research has argued that making AI explainable can be a viable way of making AI more useful in real-world contexts, especially within low-resource domains in the Global South. While AI has transcended borders, a limited amount of work focuses on democratizing the concept of explainable AI to the ""majority world"", leaving much room to explore and develop new approaches within this space that cater to the distinct needs of users within culturally and socially-diverse regions. This article introduces the concept of an intercultural ethics approach to AI explainability. It examines how cultural nuances impact the adoption and use of technology, the factors that impede how technical concepts such as AI are explained, and how integrating an intercultural ethics approach in the development of XAI can improve user understanding and facilitate efficient usage of these methods.",['Chinasa T. Okolo'],2023-04-24T07:15:58Z,http://arxiv.org/abs/2304.11861v2,Artificial Intelligence,Explainable AI,"explainable AI can be a viable way of making AI more useful in real-world contexts . a limited amount of work focuses on democratizing the concept to the ""majority world"" this article examines how cultural nuances impact the adoption and use of technology ."
Beware! The AI Act Can Also Apply to Your AI Research Practices,"The EU has become one of the vanguards in regulating the digital age. A particularly important regulation in the Artificial Intelligence (AI) domain is the EU AI Act, which entered into force in 2024. The AI Act specifies -- due to a risk-based approach -- various obligations for providers of AI systems. These obligations, for example, include a cascade of documentation and compliance measures, which represent a potential obstacle to science. But do these obligations also apply to AI researchers? This position paper argues that, indeed, the AI Act's obligations could apply in many more cases than the AI community is aware of. In our analysis of the AI Act and its applicability, we contribute the following: 1.) We give a high-level introduction to the AI Act aimed at non-legal AI research scientists. 2.) We explain with everyday research examples why the AI Act applies to research. 3.) We analyse the exceptions of the AI Act's applicability and state that especially scientific research exceptions fail to account for current AI research practices. 4.) We propose changes to the AI Act to provide more legal certainty for AI researchers and give two recommendations for AI researchers to reduce the risk of not complying with the AI Act. We see our paper as a starting point for a discussion between policymakers, legal scholars, and AI researchers to avoid unintended side effects of the AI Act on research.","['Alina Wernick', 'Kristof Meding']",2025-06-03T08:01:36Z,http://arxiv.org/abs/2506.03218v1,Artificial Intelligence,Explainable AI,the EU has become one of the vanguards in regulating the digital age . the EU AI Act entered into force in 2024 . this position paper argues that the AI Act's obligations could apply in many more cases .
Legally-Informed Explainable AI,"Explanations for artificial intelligence (AI) systems are intended to support the people who are impacted by AI systems in high-stakes decision-making environments, such as doctors, patients, teachers, students, housing applicants, and many others. To protect people and support the responsible development of AI, explanations need to be actionable--helping people take pragmatic action in response to an AI system--and contestable--enabling people to push back against an AI system and its determinations. For many high-stakes domains, such as healthcare, education, and finance, the sociotechnical environment includes significant legal implications that impact how people use AI explanations. For example, physicians who use AI decision support systems may need information on how accepting or rejecting an AI determination will protect them from lawsuits or help them advocate for their patients. In this paper, we make the case for Legally-Informed Explainable AI, responding to the need to integrate and design for legal considerations when creating AI explanations. We describe three stakeholder groups with different informational and actionability needs, and provide practical recommendations to tackle design challenges around the design of explainable AI systems that incorporate legal considerations.","['Gennie Mansi', 'Naveena Karusala', 'Mark Riedl']",2025-04-14T20:59:22Z,http://arxiv.org/abs/2504.10708v1,Artificial Intelligence,Explainable AI,"explanations for artificial intelligence (AI) systems are intended to support people who are impacted by AI systems in high-stakes decision-making environments . to protect people, explanations need to be actionable--helping people take pragmatic action in response to an AI system ."
Need of AI in Modern Education: in the Eyes of Explainable AI (xAI),"Modern Education is not \textit{Modern} without AI. However, AI's complex nature makes understanding and fixing problems challenging. Research worldwide shows that a parent's income greatly influences a child's education. This led us to explore how AI, especially complex models, makes important decisions using Explainable AI tools. Our research uncovered many complexities linked to parental income and offered reasonable explanations for these decisions. However, we also found biases in AI that go against what we want from AI in education: clear transparency and equal access for everyone. These biases can impact families and children's schooling, highlighting the need for better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the complex ways AI operates, especially concerning biases. These are the foundational steps towards better educational policies, which include using AI in ways that are more reliable, accountable, and beneficial for everyone involved.","['Supriya Manna', 'Niladri Sett']",2024-07-31T08:11:33Z,http://arxiv.org/abs/2408.00025v3,Artificial Intelligence,Explainable AI,"this chapter tries to shed light on the complex ways AI operates, especially biases . a parent's income influences a child's education, so we explored how AI makes important decisions . however, we also found biased AI that go against what we want from AI in education ."
Enabling human-centered AI: A new junction and shared journey between AI   and HCI communities,"Artificial intelligence (AI) has brought benefits, but it may also cause harm if it is not appropriately developed. Current development is mainly driven by a ""technology-centered"" approach, causing many failures. For example, the AI Incident Database has documented over a thousand AI-related accidents. To address these challenges, a human-centered AI (HCAI) approach has been promoted and has received a growing level of acceptance over the last few years. HCAI calls for combining AI with user experience (UX) design will enable the development of AI systems (e.g., autonomous vehicles, intelligent user interfaces, or intelligent decision-making systems) to achieve its design goals such as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI promotion continues, it has not specifically addressed the collaboration between AI and human-computer interaction (HCI) communities, resulting in uncertainty about what action should be taken by both sides to apply HCAI in developing AI systems. This Viewpoint focuses on the collaboration between the AI and HCI communities, which leads to nine recommendations for effective collaboration to enable HCAI in developing AI systems.","['Wei Xu', 'Marvin Dainoff']",2021-11-12T06:11:30Z,http://arxiv.org/abs/2111.08460v4,Artificial Intelligence,Explainable AI,"artificial intelligence (AI) has brought benefits, but it may also cause harm if it is not appropriately developed . a human-centered AI (HCAI) approach has been promoted to address these challenges . this viewpoint focuses on the collaboration between the AI and human-computer interaction communities ."
Exploiting AI for Attacks: On the Interplay between Adversarial AI and   Offensive AI,"As Artificial Intelligence (AI) continues to evolve, it has transitioned from a research-focused discipline to a widely adopted technology, enabling intelligent solutions across various sectors. In security, AI's role in strengthening organizational resilience has been studied for over two decades. While much attention has focused on AI's constructive applications, the increasing maturity and integration of AI have also exposed its darker potentials. This article explores two emerging AI-related threats and the interplay between them: AI as a target of attacks (`Adversarial AI') and AI as a means to launch attacks on any target (`Offensive AI') -- potentially even on another AI. By cutting through the confusion and explaining these threats in plain terms, we introduce the complex and often misunderstood interplay between Adversarial AI and Offensive AI, offering a clear and accessible introduction to the challenges posed by these threats.","['Saskia Laura Schröer', 'Luca Pajola', 'Alberto Castagnaro', 'Giovanni Apruzzese', 'Mauro Conti']",2025-06-14T14:21:01Z,http://arxiv.org/abs/2506.12519v1,Artificial Intelligence,Explainable AI,this article explores two emerging AI-related threats and the interplay between them . AI as a target of attacks (Adversarial AI') and AI as means to launch attacks on any target . authors introduce the complex and often misunderstood interplay .
AI Generations: From AI 1.0 to AI 4.0,"This paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI), AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of these AI generations is driven by shifting priorities among algorithms, computing power, and data. AI 1.0 ushered in breakthroughs in pattern recognition and information processing, fueling advances in computer vision, natural language processing, and recommendation systems. AI 2.0 built on these foundations through real-time decision-making in digital environments, leveraging reinforcement learning and adaptive planning for agentic AI applications. AI 3.0 extended intelligence into physical contexts, integrating robotics, autonomous vehicles, and sensor-fused control systems to act in uncertain real-world settings. Building on these developments, AI 4.0 puts forward the bold vision of self-directed AI capable of setting its own goals, orchestrating complex training regimens, and possibly exhibiting elements of machine consciousness. This paper traces the historical foundations of AI across roughly seventy years, mapping how changes in technological bottlenecks from algorithmic innovation to high-performance computing to specialized data, have spurred each generational leap. It further highlights the ongoing synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical, regulatory, and philosophical challenges that arise when artificial systems approach (or aspire to) human-like autonomy. Ultimately, understanding these evolutions and their interdependencies is pivotal for guiding future research, crafting responsible governance, and ensuring that AI transformative potential benefits society as a whole.","['Jiahao Wu', 'Hengxu You', 'Jing Du']",2025-02-16T23:19:44Z,http://arxiv.org/abs/2502.11312v1,Artificial Intelligence,Generative AI,"this paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations . each generation is driven by shifting priorities among algorithms, computing power, and data . paper traces the historical foundations of AI across roughly seventy years ."
"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest   Value for us?","AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, ""Starcraft"" and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance to develop into ""strong AI"".",['Bin Liu'],2021-03-29T02:57:48Z,http://arxiv.org/abs/2103.15294v1,Artificial Intelligence,Generative AI,many researchers argue that little substantial progress has been made for AI in recent decades . author explains why controversies about AI exist; discriminates two paradigms of AI research . he also clarifies how to judge which paradigm a research work should be classified into .
A Bibliometric View of AI Ethics Development,"Artificial Intelligence (AI) Ethics is a nascent yet critical research field. Recent developments in generative AI and foundational models necessitate a renewed look at the problem of AI Ethics. In this study, we perform a bibliometric analysis of AI Ethics literature for the last 20 years based on keyword search. Our study reveals a three-phase development in AI Ethics, namely an incubation phase, making AI human-like machines phase, and making AI human-centric machines phase. We conjecture that the next phase of AI ethics is likely to focus on making AI more machine-like as AI matches or surpasses humans intellectually, a term we coin as ""machine-like human"".","['Di Kevin Gao', 'Andrew Haverly', 'Sudip Mittal', 'Jingdao Chen']",2024-02-08T16:36:55Z,http://arxiv.org/abs/2403.05551v1,Artificial Intelligence,Generative AI,a bibliometric analysis of AI Ethics literature for the last 20 years . our study reveals a three-phase development in AI Ethics . the next phase of AI ethics is likely to focus on making AI more machine-like .
Interaction Design for Human-AI Choreography Co-creation,"Human-AI co-creation aims to combine human and AI strengths for artistic results exceeding individual capabilities. Frameworks exist for painting, music, and poetry, but choreography's embodied nature demands a dedicated approach. This paper explores AI-assisted choreography techniques (e.g., generative ideation, embodied improvisation) and analyzes interaction design -- how humans and AI collaborate and communicate -- to inform the design considerations of future human-AI choreography co-creation systems.",['Yimeng Liu'],2024-05-07T04:29:54Z,http://arxiv.org/abs/2405.03999v1,Artificial Intelligence,Generative AI,"human-AI co-creation aims to combine human and AI strengths for artistic results . frameworks exist for painting, music, and poetry, but choreography's embodied nature demands a dedicated approach . this paper explores AI-assisted choreography techniques ."
Constructing AI ethics narratives based on real-world data: Human-AI   collaboration in data-driven visual storytelling,"AI ethics narratives have the potential to shape the public accurate understanding of AI technologies and promote communication among different stakeholders. However, AI ethics narratives are largely lacking. Existing limited narratives tend to center on works of science fiction or corporate marketing campaigns of large technology companies. Misuse of ""socio-technical imaginary"" can blur the line between speculation and reality for the public, undermining the responsibility and regulation of technology development. Therefore, constructing authentic AI ethics narratives is an urgent task. The emergence of generative AI offers new possibilities for building narrative systems. This study is dedicated to data-driven visual storytelling about AI ethics relying on the human-AI collaboration. Based on the five key elements of story models, we proposed a conceptual framework for human-AI collaboration, explored the roles of generative AI and humans in the creation of visual stories. We implemented the conceptual framework in a real AI news case. This research leveraged advanced generative AI technologies to provide a reference for constructing genuine AI ethics narratives. Our goal is to promote active public engagement and discussions through authentic AI ethics narratives, thereby contributing to the development of better AI policies.","['Mengyi Wei', 'Chenjing Jiao', 'Chenyu Zuo', 'Lorenz Hurni', 'Liqiu Meng']",2025-02-02T02:56:07Z,http://arxiv.org/abs/2502.00637v1,Artificial Intelligence,Generative AI,the emergence of generative AI offers new possibilities for building AI ethics narratives . this study proposes a conceptual framework for human-AI collaboration . the goal is to promote active public engagement and discussions through authentic AI ethics stories .
How Generative AI supports human in conceptual design,"Generative Artificial Intelligence (Generative AI) is a collection of AI technologies that can generate new information such as texts and images. With its strong capabilities, Generative AI has been actively studied in creative design processes. However, limited studies have explored the roles of humans and Generative AI in conceptual design processes, leaving a gap for human-AI collaboration investigation. To address this gap, this study uncovers the contributions of different Generative AI technologies in assisting humans in the conceptual design process. Novice designers completed two design tasks with or without the assistance of Generative AI. Results revealed that Generative AI primarily assists humans in problem definition and idea generation stages, while idea selection and evaluation remain predominantly human-led. Additionally, with Generative AI assistance, the idea selection and evaluation stages were further enhanced. Based on the findings, we discuss the role of Generative AI in human-AI collaboration and implications for enhancing future conceptual design support with Generative AI assistance.","['Liuging Chen', 'Yaxuan Song', 'Jia Guo', 'Lingyun Sun', 'Peter Childs', 'Yuan Yin']",2025-02-01T02:47:48Z,http://arxiv.org/abs/2502.00283v1,Artificial Intelligence,Generative AI,"Generative Artificial Intelligence (Generative AI) is a collection of AI technologies that can generate new information . limited studies have explored the roles of humans in conceptual design processes, leaving a gap for human-AI collaboration investigation ."
A Brief Survey of Associations Between Meta-Learning and General AI,"This paper briefly reviews the history of meta-learning and describes its contribution to general AI. Meta-learning improves model generalization capacity and devises general algorithms applicable to both in-distribution and out-of-distribution tasks potentially. General AI replaces task-specific models with general algorithmic systems introducing higher level of automation in solving diverse tasks using AI. We summarize main contributions of meta-learning to the developments in general AI, including memory module, meta-learner, coevolution, curiosity, forgetting and AI-generating algorithm. We present connections between meta-learning and general AI and discuss how meta-learning can be used to formulate general AI algorithms.",['Huimin Peng'],2021-01-12T03:57:16Z,http://arxiv.org/abs/2101.04283v1,Artificial Intelligence,Generative AI,this paper briefly reviews the history of meta-learning and describes its contribution to general AI . Meta-learning improves model generalization capacity and devises general algorithms . general AI replaces task-specific models with general algorithmic systems .
Interacting with Thoughtful AI,"We envision the concept of Thoughtful AI, a new human-AI interaction paradigm in which the AI behaves as a continuously thinking entity. Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful AI autonomously generates, develops, and communicates its evolving thought process throughout an interaction. In this position paper, we argue that this thoughtfulness unlocks new possibilities for human-AI interaction by enabling proactive AI behavior, facilitating continuous cognitive alignment with users, and fostering more dynamic interaction experiences. We outline the conceptual foundations of Thoughtful AI, illustrate its potential through example projects, and envision how this paradigm can transform human-AI interaction in the future.","['Xingyu Bruce Liu', 'Haijun Xia', 'Xiang Anthony Chen']",2025-02-25T22:36:03Z,http://arxiv.org/abs/2502.18676v2,Artificial Intelligence,Generative AI,"we envision the concept of Thoughtful AI, a new human-AI interaction paradigm . the AI behaves as a continuously thinking entity throughout an interaction . we outline the conceptual foundations of thoughtful AI and illustrate its potential ."
Generative AI Literacy: A Comprehensive Framework for Literacy and   Responsible Use,"After the release of several AI literacy guidelines, the rapid rise and widespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek, have transformed our lives. Unlike traditional AI algorithms (e.g., convolutional neural networks, semantic networks, classifiers) captured in existing AI literacy frameworks, generative AI exhibits distinct and more nuanced characteristics. However, a lack of robust generative AI literacy is hindering individuals ability to evaluate critically and use these models effectively and responsibly. To address this gap, we propose a set of guidelines with 12 items for generative AI literacy, organized into four key aspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2) Guidelines for Understanding Interaction with Generative AI, (3) Guidelines for Understanding Interaction with Generative AI, and (4) Guidelines for High Level Understanding of Generative AI. These guidelines aim to support schools, companies, educators, and organizations in developing frameworks that empower their members, such as students, employees, and stakeholders, to use generative AI in an efficient, ethical, and informed way.","['Chengzhi Zhang', 'Brian Magerko']",2025-04-26T22:14:48Z,http://arxiv.org/abs/2504.19038v1,Artificial Intelligence,Generative AI,a lack of robust generative AI literacy is hindering people's ability to use models . a set of guidelines with 12 items for generative . literacy is organized into four key aspects: (1) Guidelines for Generative AI Tool Selection and Prompting .
Toward a Human-AI Task Tensor: A Taxonomy for Organizing Work in the Age   of Generative AI,"We introduce a framework for understanding the impact of generative AI on human work, which we call the human-AI task tensor. A tensor is a structured framework that organizes tasks along multiple interdependent dimensions. Our human-AI task tensor introduces a systematic approach to studying how humans and AI interact to perform tasks, and has eight dimensions: task definition, AI contribution, interaction modality, audit requirement, output definition, decision-making authority, AI structure, and human persona. After describing the eight dimensions of the tensor, we provide illustrative frameworks (derived from projections of the tensor) and a human-AI task canvas that provide analytical tractability and practical insight for organizational decision-making. We demonstrate how the human-AI task tensor can be used to organize emerging and future research on generative AI. We propose that the human-AI task tensor offers a starting point for understanding how work will be performed with the emergence of generative AI.","['Anil R. Doshi', 'Alastair Moore']",2025-01-06T13:41:08Z,http://arxiv.org/abs/2503.15490v2,Artificial Intelligence,Generative AI,"the human-AI task tensor is a framework that organizes tasks along multiple interdependent dimensions . it has eight dimensions: task definition, AI contribution, interaction modality, audit requirement, output definition, decision-making authority, AI structure, and human persona ."
Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and   Challenges,"Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it in the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.","['Aastha Pant', 'Rashina Hoda', 'Simone V. Spiegler', 'Chakkrit Tantithamthavorn', 'Burak Turhan']",2023-07-14T02:50:46Z,http://arxiv.org/abs/2307.10057v1,Artificial Intelligence,Generative AI,ethics in AI has become a debated topic of public and expert discourse in recent years . but what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating ethics?
"Accountability of Generative AI: Exploring a Precautionary Approach for   ""Artificially Created Nature""","The rapid development of generative artificial intelligence (AI) technologies raises concerns about the accountability of sociotechnical systems. Current generative AI systems rely on complex mechanisms that make it difficult for even experts to fully trace the reasons behind the outputs. This paper first examines existing research on AI transparency and accountability and argues that transparency is not a sufficient condition for accountability but can contribute to its improvement. We then discuss that if it is not possible to make generative AI transparent, generative AI technology becomes ``artificially created nature'' in a metaphorical sense, and suggest using the precautionary principle approach to consider AI risks. Finally, we propose that a platform for citizen participation is needed to address the risks of generative AI.",['Yuri Nakao'],2025-05-12T02:10:55Z,http://arxiv.org/abs/2505.07178v1,Artificial Intelligence,Generative AI,the rapid development of generative artificial intelligence (AI) technologies raises concerns about the accountability of sociotechnical systems . current generative AI systems rely on complex mechanisms that make it difficult for experts to fully trace the reasons behind the outputs .
"Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural   Framework for AI Safety with Challenges and Mitigations","AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. With the rapid proliferation of AI and especially with the recent advancement of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed, broadening the scope of AI Safety to address impacts on public safety and national security. In this paper, we propose a novel architectural framework for understanding and analyzing AI Safety; defining its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI. We provide an extensive review of current research and advancements in AI safety from these perspectives, highlighting their key challenges and mitigation approaches. Through examples from state-of-the-art technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance people's trust in digital transformation.","['Chen Chen', 'Xueluan Gong', 'Ziyao Liu', 'Weifeng Jiang', 'Si Qi Goh', 'Kwok-Yan Lam']",2024-08-23T09:33:48Z,http://arxiv.org/abs/2408.12935v3,Artificial Intelligence,Generative AI,"AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems . authors propose a novel architectural framework for understanding and analyzing AI Safety . they define its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI ."
The AI Ghostwriter Effect: When Users Do Not Perceive Ownership of   AI-Generated Text But Self-Declare as Authors,"Human-AI interaction in text production increases complexity in authorship. In two empirical studies (n1 = 30 & n2 = 96), we investigate authorship and ownership in human-AI collaboration for personalized language generation. We show an AI Ghostwriter Effect: Users do not consider themselves the owners and authors of AI-generated text but refrain from publicly declaring AI authorship. Personalization of AI-generated texts did not impact the AI Ghostwriter Effect, and higher levels of participants' influence on texts increased their sense of ownership. Participants were more likely to attribute ownership to supposedly human ghostwriters than AI ghostwriters, resulting in a higher ownership-authorship discrepancy for human ghostwriters. Rationalizations for authorship in AI ghostwriters and human ghostwriters were similar. We discuss how our findings relate to psychological ownership and human-AI interaction to lay the foundations for adapting authorship frameworks and user interfaces in AI in text-generation tasks.","['Fiona Draxler', 'Anna Werner', 'Florian Lehmann', 'Matthias Hoppe', 'Albrecht Schmidt', 'Daniel Buschek', 'Robin Welsch']",2023-03-06T16:53:12Z,http://arxiv.org/abs/2303.03283v2,Artificial Intelligence,Generative AI,human-AI interaction in text production increases complexity in authorship . personalization of AI-generated texts did not impact the AI Ghostwriter Effect . participants more likely to attribute ownership to supposedly human ghostwriters .
Tweeting AI: Perceptions of AI-Tweeters (AIT) vs Expert AI-Tweeters   (EAIT),"With the recent advancements in Artificial Intelligence (AI), various organizations and individuals started debating about the progress of AI as a blessing or a curse for the future of the society. This paper conducts an investigation on how the public perceives the progress of AI by utilizing the data shared on Twitter. Specifically, this paper performs a comparative analysis on the understanding of users from two categories -- general AI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on Twitter. Our analysis revealed that users from both the categories express distinct emotions and interests towards AI. Users from both the categories regard AI as positive and are optimistic about the progress of AI but the experts are more negative than the general AI-Tweeters. Characterization of users manifested that `London' is the popular location of users from where they tweet about AI. Tweets posted by AIT are highly retweeted than posts made by EAIT that reveals greater diffusion of information from AIT.","['Lydia Manikonda', 'Cameron Dudley', 'Subbarao Kambhampati']",2017-04-27T00:37:05Z,http://arxiv.org/abs/1704.08389v2,Artificial Intelligence,Generative AI,"this paper performs a comparative analysis on how the public perceives the progress of AI . general AI-Tweeters (AIT) and the expert AI-tweeter (EAIT), share posts about AI on twitter . users from both categories express distinct emotions and interests towards AI, the paper finds ."
Tweeting AI: Perceptions of Lay vs Expert Twitterati,"With the recent advancements in Artificial Intelligence (AI), various organizations and individuals are debating about the progress of AI as a blessing or a curse for the future of the society. This paper conducts an investigation on how the public perceives the progress of AI by utilizing the data shared on Twitter. Specifically, this paper performs a comparative analysis on the understanding of users belonging to two categories -- general AI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on Twitter. Our analysis revealed that users from both the categories express distinct emotions and interests towards AI. Users from both the categories regard AI as positive and are optimistic about the progress of AI but the experts are more negative than the general AI-Tweeters. Expert AI-Tweeters share relatively large percentage of tweets about their personal news compared to technical aspects of AI. However, the effects of automation on the future are of primary concern to AIT than to EAIT. When the expert category is sub-categorized, the emotion analysis revealed that students and industry professionals have more insights in their tweets about AI than academicians.","['Lydia Manikonda', 'Subbarao Kambhampati']",2017-09-25T19:14:21Z,http://arxiv.org/abs/1709.09534v1,Artificial Intelligence,Generative AI,this paper investigates how the public perceives the progress of AI on Twitter . general AI-Tweeters (AIT) are more optimistic about AI . experts share relatively large percentage of tweets about their personal news . effects of automation on the future are of primary concern to AIT than to experts .
"AI for Just Work: Constructing Diverse Imaginations of AI beyond   ""Replacing Humans""","""why"" we develop AI. Lacking critical reflections on the general visions and purposes of AI may make the community vulnerable to manipulation. In this position paper, we explore the ""why"" question of AI. We denote answers to the ""why"" question the imaginations of AI, which depict our general visions, frames, and mindsets for the prospects of AI. We identify that the prevailing vision in the AI community is largely a monoculture that emphasizes objectives such as replacing humans and improving productivity. Our critical examination of this mainstream imagination highlights its underpinning and potentially unjust assumptions. We then call to diversify our collective imaginations of AI, embedding ethical assumptions from the outset in the imaginations of AI. To facilitate the community's pursuit of diverse imaginations, we demonstrate one process for constructing a new imagination of ""AI for just work,"" and showcase its application in the medical image synthesis task to make it more ethical. We hope this work will help the AI community to open critical dialogues with civil society on the visions and purposes of AI, and inspire more technical works and advocacy in pursuit of diverse and ethical imaginations to restore the value of AI for the public good.","['Weina Jin', 'Nicholas Vincent', 'Ghassan Hamarneh']",2025-03-10T20:03:55Z,http://arxiv.org/abs/2503.08720v2,Artificial Intelligence,Generative AI,"the prevailing vision in the AI community is largely a monoculture . we call to diversify our collective imaginations of AI . to facilitate the community's pursuit of diverse imaginations, we demonstrate one process for constructing a new imagination of ""AI for just work"""
The Narrow Depth and Breadth of Corporate Responsible AI Research,"The transformative potential of AI presents remarkable opportunities, but also significant risks, underscoring the importance of responsible AI development and deployment. Despite a growing emphasis on this area, there is limited understanding of industry's engagement in responsible AI research, i.e., the critical examination of AI's ethical, social, and legal dimensions. To address this gap, we analyzed over 6 million peer-reviewed articles and 32 million patent citations using multiple methods across five distinct datasets to quantify industry's engagement. Our findings reveal that the majority of AI firms show limited or no engagement in this critical subfield of AI. We show a stark disparity between industry's dominant presence in conventional AI research and its limited engagement in responsible AI. Leading AI firms exhibit significantly lower output in responsible AI research compared to their conventional AI research and the contributions of leading academic institutions. Our linguistic analysis documents a narrower scope of responsible AI research within industry, with a lack of diversity in key topics addressed. Our large-scale patent citation analysis uncovers a pronounced disconnect between responsible AI research and the commercialization of AI technologies, suggesting that industry patents rarely build upon insights generated by the responsible AI literature. This gap highlights the potential for AI development to diverge from a socially optimal path, risking unintended consequences due to insufficient consideration of ethical and societal implications. Our results highlight the urgent need for industry to publicly engage in responsible AI research to absorb academic knowledge, cultivate public trust, and proactively mitigate AI-induced societal harms.","['Nur Ahmed', 'Amit Das', 'Kirsten Martin', 'Kawshik Banerjee']",2024-05-20T17:26:43Z,http://arxiv.org/abs/2405.12193v1,Artificial Intelligence,Generative AI,there is limited understanding of industry's engagement in responsible AI research . the majority of AI firms show limited or no engagement in this critical subfield of AI . leading AI firms exhibit significantly lower output compared to conventional AI research and contributions of leading academic institutions .
Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI   Experts,"The development of artificial general intelligence (AGI) is likely to be one of humanity's most consequential technological advancements. Leading AI labs and scientists have called for the global prioritization of AI safety citing existential risks comparable to nuclear war. However, research on catastrophic risks and AI alignment is often met with skepticism, even by experts. Furthermore, online debate over the existential risk of AI has begun to turn tribal (e.g. name-calling such as ""doomer"" or ""accelerationist""). Until now, no systematic study has explored the patterns of belief and the levels of familiarity with AI safety concepts among experts. I surveyed 111 AI experts on their familiarity with AI safety concepts, key objections to AI safety, and reactions to safety arguments. My findings reveal that AI experts cluster into two viewpoints -- an ""AI as controllable tool"" and an ""AI as uncontrollable agent"" perspective -- diverging in beliefs toward the importance of AI safety. While most experts (78%) agreed or strongly agreed that ""technical AI researchers should be concerned about catastrophic risks"", many were unfamiliar with specific AI safety concepts. For example, only 21% of surveyed experts had heard of ""instrumental convergence,"" a fundamental concept in AI safety predicting that advanced AI systems will tend to pursue common sub-goals (such as self-preservation). The least concerned participants were the least familiar with concepts like this, suggesting that effective communication of AI safety should begin with establishing clear conceptual foundations in the field.",['Severin Field'],2025-01-25T01:51:29Z,http://arxiv.org/abs/2502.14870v1,Artificial Intelligence,Generative AI,"online debate over the existential risk of AI has begun to turn tribal . john sutter surveyed 111 AI experts on their familiarity with AI safety concepts . most experts agreed or strongly agreed that ""technical AI researchers should be concerned about catastrophic risks"""
AI and Ethics -- Operationalising Responsible AI,"In the last few years, AI continues demonstrating its positive impact on society while sometimes with ethically questionable consequences. Building and maintaining public trust in AI has been identified as the key to successful and sustainable innovation. This chapter discusses the challenges related to operationalizing ethical AI principles and presents an integrated view that covers high-level ethical AI principles, the general notion of trust/trustworthiness, and product/process support in the context of responsible AI, which helps improve both trust and trustworthiness of AI for a wider set of stakeholders.","['Liming Zhu', 'Xiwei Xu', 'Qinghua Lu', 'Guido Governatori', 'Jon Whittle']",2021-05-19T00:55:40Z,http://arxiv.org/abs/2105.08867v1,Artificial Intelligence,Generative AI,this chapter discusses the challenges related to operationalizing ethical AI principles . building and maintaining public trust in AI has been identified as the key to successful innovation .
Raising AI Ethics Awareness through an AI Ethics Quiz for Software   Practitioners,"Context:Today, ethical issues surrounding AI systems are increasingly prevalent, highlighting the critical need to integrate AI ethics into system design to prevent societal harm. Raising awareness and fostering a deep understanding of AI ethics among software practitioners is essential for achieving this goal. However, research indicates a significant gap in practitioners' awareness and knowledge of AI ethics and ethical principles. While much effort has been directed toward helping practitioners operationalise AI ethical principles such as fairness, transparency, accountability, and privacy, less attention has been paid to raising initial awareness, which should be the foundational step. Objective: Addressing this gap, we developed a software-based tool, the AI Ethics Quiz, to raise awareness and enhance the knowledge of AI ethics among software practitioners. Our objective was to organise interactive workshops, introduce the AI Ethics Quiz, and evaluate its effectiveness in enhancing awareness and knowledge of AI ethics and ethical principles among practitioners. Method: We conducted two one-hour workshops (one in-person and one online) involving 29 software practitioners. Data was collected through pre-quiz questionnaire, the AI Ethics Quiz, and a post-quiz questionnaire. Results: The anonymous responses revealed that the quiz significantly improved practitioners' awareness and understanding of AI ethics. Additionally, practitioners found the quiz engaging and reported it created a meaningful learning experience regarding AI ethics. In this paper, we share insights gained from conducting these interactive workshops and introducing the AI Ethics Quiz to practitioners. Conclusion: We also provide recommendations for software companies and leaders to adopt similar initiatives, which may help them enhance practitioners' awareness and understanding of AI ethics.","['Aastha Pant', 'Rashina Hoda', 'Paul McIntosh']",2024-08-28T23:36:13Z,http://arxiv.org/abs/2408.16796v2,Artificial Intelligence,AI Ethics,"research indicates a significant gap in practitioners' awareness of AI ethics . less attention has been paid to raising initial awareness, which should be the foundational step . a software-based tool, the AI Ethics Quiz, was developed to address this gap ."
"Big data ethics, machine ethics or information ethics? Navigating the   maze of applied ethics in IT","Digitalization efforts are rapidly spreading across societies, challenging new and important ethical issues that arise from technological development. Software developers, designers and managerial decision-makers are ever more expected to consider ethical values and conduct normative evaluations when building digital products. Yet, when one looks for guidance in the academic literature one encounters a plethora of branches of applied ethics. Depending on the context of the system that is to be developed, interesting subfields like big data ethics, machine ethics, information ethics, AI ethics or computer ethics (to only name a few) may present themselves. In this paper we want to offer assistance to any member of a development team by giving a clear and brief introduction into two fields of ethical endeavor (normative ethics and applied ethics), describing how they are related to each other and, finally, provide an ordering of the different branches of applied ethics (big data ethics, machine ethics, information ethics, AI ethics or computer ethics etc.) which have gained traction over the last years. Finally, we discuss an example in the domain of facial recognition software in the domain of medicine to illustrate how this process of normative analysis might be conducted.","['Niina Zuber', 'Severin Kacianka', 'Jan Gogoll']",2022-03-25T08:05:46Z,http://arxiv.org/abs/2203.13494v1,Artificial Intelligence,AI Ethics,"digitalization efforts are rapidly spreading across societies, challenging new ethical issues . software developers, designers and managers are expected to consider ethical values . but, when one looks for guidance in the academic literature one encounters a plethora of branches of applied ethics ."
E-LENS: User Requirements-Oriented AI Ethics Assurance,"Despite the much proliferation of AI ethical principles in recent years, there is a challenge of assuring AI ethics with current AI ethics frameworks in real-world applications. While system safety has emerged as a distinct discipline for a long time, originated from safety concerns in early aircraft manufacturing. The safety assurance is now an indispensable component in safety critical domains. Motivated by the assurance approaches for safety-critical systems such as aviation, this paper introduces the concept of AI ethics assurance cases into the AI ethics assurance. Three pillars of user requirements, evidence, and validation are proposed as key components and integrated into AI ethics assurance cases for a new approach of user requirements-oriented AI ethics assurance. The user requirements-oriented AI ethics assurance case is set up based on three pillars and hazard analysis methods used in the safety assurance of safety-critical systems. This paper also proposes a platform named Ethical-Lens (E-LENS) to implement the user requirements-oriented AI ethics assurance approach. The proposed user requirements-based E-LENS platform is then applied to assure AI ethics of an AI-driven human resource shortlisting system as a case study to show the effectiveness of the proposed approach.","['Jianlong Zhou', 'Fang Chen']",2025-02-06T04:37:55Z,http://arxiv.org/abs/2503.04747v1,Artificial Intelligence,AI Ethics,"there is a challenge of assuring AI ethics with current AI ethics frameworks . safety has emerged as a distinct discipline for a long time, originated from safety concerns . this paper introduces the concept of AI ethics assurance cases into the ethics assurance ."
A Bibliometric View of AI Ethics Development,"Artificial Intelligence (AI) Ethics is a nascent yet critical research field. Recent developments in generative AI and foundational models necessitate a renewed look at the problem of AI Ethics. In this study, we perform a bibliometric analysis of AI Ethics literature for the last 20 years based on keyword search. Our study reveals a three-phase development in AI Ethics, namely an incubation phase, making AI human-like machines phase, and making AI human-centric machines phase. We conjecture that the next phase of AI ethics is likely to focus on making AI more machine-like as AI matches or surpasses humans intellectually, a term we coin as ""machine-like human"".","['Di Kevin Gao', 'Andrew Haverly', 'Sudip Mittal', 'Jingdao Chen']",2024-02-08T16:36:55Z,http://arxiv.org/abs/2403.05551v1,Artificial Intelligence,AI Ethics,a bibliometric analysis of AI Ethics literature for the last 20 years . our study reveals a three-phase development in AI Ethics . the next phase of AI ethics is likely to focus on making AI more machine-like .
Three Kinds of AI Ethics,"There is an overwhelming abundance of works in AI Ethics. This growth is chaotic because of how sudden it is, its volume, and its multidisciplinary nature. This makes difficult to keep track of debates, and to systematically characterize goals, research questions, methods, and expertise required by AI ethicists. In this article, I show that the relation between AI and ethics can be characterized in at least three ways, which correspond to three well-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI. I elucidate the features of these three kinds of AI Ethics, characterize their research questions, and identify the kind of expertise that each kind needs. I also show how certain criticisms to AI ethics are misplaced, as being done from the point of view of one kind of AI ethics, to another kind with different goals. All in all, this work sheds light on the nature of AI ethics, and sets the groundwork for more informed discussions about the scope, methods, and training of AI ethicists.",['Emanuele Ratti'],2025-03-24T16:15:03Z,http://arxiv.org/abs/2503.18842v2,Artificial Intelligence,AI Ethics,"there is an overwhelming abundance of works in AI Ethics . this growth is chaotic because of how sudden it is, its volume, and its multidisciplinary nature . elucidates the features of these three kinds of AI Ethics, characterizes their research questions ."
A Capability Approach to AI Ethics,"We propose a conceptualization and implementation of AI ethics via the capability approach. We aim to show that conceptualizing AI ethics through the capability approach has two main advantages for AI ethics as a discipline. First, it helps clarify the ethical dimension of AI tools. Second, it provides guidance to implementing ethical considerations within the design of AI tools. We illustrate these advantages in the context of AI tools in medicine, by showing how ethics-based auditing of AI tools in medicine can greatly benefit from our capability-based approach.","['Emanuele Ratti', 'Mark Graves']",2025-01-10T12:08:21Z,http://arxiv.org/abs/2502.03469v1,Artificial Intelligence,AI Ethics,we propose a conceptualization and implementation of AI ethics via the capability approach . it helps clarify the ethical dimension of AI tools and provides guidance to implementing ethical considerations . we illustrate these advantages by showing how ethics-based auditing can greatly benefit from our capability-based approach.
"Ethical Leadership in the Age of AI Challenges, Opportunities and   Framework for Ethical Leadership","Artificial Intelligence is currently and rapidly changing the way organizations and businesses operate. Ethical leadership has become significantly important since organizations and businesses across various sectors are evolving with AI. Organizations and businesses may be facing several challenges and potential opportunities when using AI. Ethical leadership plays a central role in guiding organizations in facing those challenges and maximizing on those opportunities. This article explores the essence of ethical leadership in the age of AI, starting with a simplified introduction of ethical leadership and AI, then dives into an understanding of ethical leadership, its characteristics and importance, the ethical challenges AI causes including bias in AI algorithms. The opportunities for ethical leadership in the age of AI answers the question: What actionable strategies can leaders employ to address the challenges and leverage opportunities? and describes the benefits for organizations through these opportunities. A proposed framework for ethical leadership is presented in this article, incorporating the core components: fairness, transparency, sustainability etc. Through the importance of interdisciplinary collaboration, case studies of ethical leadership in AI, and recommendations, this article emphasizes that ethical leadership in the age of AI is morally essential and strategically advantageous.",['Udaya Chandrika Kandasamy'],2024-10-08T10:17:19Z,http://arxiv.org/abs/2410.18095v2,Artificial Intelligence,AI Ethics,this article explores the essence of ethical leadership in the age of AI . it dives into the ethical challenges AI causes including bias in AI algorithms . a proposed framework for ethical leadership is presented in this article .
Quelle {é}thique pour quelle IA ?,"This study proposes an analysis of the different types of ethical approaches involved in the ethics of AI, and situates their interests and limits. First, the author introduces to the contemporary need for and meaning of ethics. He distinguishes it from other registers of normativities and underlines its inadequacy to formalization. He then presents a cartography of the landscape of ethical theories covered by moral philosophy, taking care to distinguish meta-ethics, normative ethics and applied ethics. In drawing up this overview, the author questions the relationship between ethics and artificial intelligence. The analysis focuses in particular on the main ethical currents that have imposed themselves in the ways of doing digital ethics and AI in our Western democracies. The author asks whether these practices of ethics, as they seem to crystallize today in a precise pattern, constitute a sufficient and sufficiently satisfactory response to our needs for ethics in AI. The study concludes with a reflection on the reasons why a human ethics of AI based on a pragmatic practice of contextual ethics remains necessary and irreducible to any formalization or automated treatment of the ethical questions that arise for humans.",['David Doat'],2024-05-21T08:13:02Z,http://arxiv.org/abs/2407.17585v1,Artificial Intelligence,AI Ethics,this study proposes an analysis of the different types of ethical approaches involved in the ethics of AI . the author questions the relationship between ethics and artificial intelligence . he asks whether these practices of ethics constitute a sufficient and sufficiently satisfactory response .
Is ETHICS about ethics? Evaluating the ETHICS benchmark,"ETHICS is probably the most-cited dataset for testing the ethical capabilities of language models. Drawing on moral theory, psychology, and prompt evaluation, we interrogate the validity of the ETHICS benchmark. Adding to prior work, our findings suggest that having a clear understanding of ethics and how it relates to empirical phenomena is key to the validity of ethics evaluations for AI.","['Leif Hancox-Li', 'Borhane Blili-Hamelin']",2024-10-16T20:10:38Z,http://arxiv.org/abs/2410.13009v2,Artificial Intelligence,AI Ethics,the ETHICS benchmark is the most-cited dataset for testing the ethical capabilities of language models . a clear understanding of ethics and how it relates to empirical phenomena is key to the validity of ethics evaluations .
Ethics in the Age of AI: An Analysis of AI Practitioners' Awareness and   Challenges,"Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it in the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.","['Aastha Pant', 'Rashina Hoda', 'Simone V. Spiegler', 'Chakkrit Tantithamthavorn', 'Burak Turhan']",2023-07-14T02:50:46Z,http://arxiv.org/abs/2307.10057v1,Artificial Intelligence,AI Ethics,ethics in AI has become a debated topic of public and expert discourse in recent years . but what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating ethics?
Ethical AI in the Healthcare Sector: Investigating Key Drivers of   Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM),"The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration. Addressing this critical research gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM), a novel theoretical framework that categorizes 13 critical ethical variables across four foundational dimensions of Ethical AI Fair AI, Responsible AI, Explainable AI, and Sustainable AI. These dimensions are further analyzed through three core ethical lenses: epistemic concerns (related to knowledge, transparency, and system trustworthiness), normative concerns (focused on justice, autonomy, dignity, and moral obligations), and overarching concerns (highlighting global, systemic, and long-term ethical implications). This study adopts a quantitative, cross-sectional research design using survey data collected from healthcare professionals and analyzed via Partial Least Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study empirically investigates the influence of these ethical constructs on two outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate that normative concerns most significantly drive operational adoption decisions, while overarching concerns predominantly shape systemic adoption strategies and governance frameworks. Epistemic concerns play a facilitative role, enhancing the impact of ethical design principles on trust and transparency in AI systems. By validating the MEAAM framework, this research advances a holistic, actionable approach to ethical AI adoption in healthcare and provides critical insights for policymakers, technologists, and healthcare administrators striving to implement ethically grounded AI solutions.","['Prathamesh Muzumdar', 'Apoorva Muley', 'Kuldeep Singh', 'Sumanth Cheemalapati']",2025-05-04T10:40:05Z,http://arxiv.org/abs/2505.02062v1,Artificial Intelligence,AI Ethics,"study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM) MEAAM categorizes 13 critical ethical variables across four foundational dimensions . normative concerns most significantly drive operational adoption decisions, study finds ."
Implementing AI Ethics: Making Sense of the Ethical Requirements,"Society's increasing dependence on Artificial Intelligence (AI) and AI-enabled systems require a more practical approach from software engineering (SE) executives in middle and higher-level management to improve their involvement in implementing AI ethics by making ethical requirements part of their management practices. However, research indicates that most work on implementing ethical requirements in SE management primarily focuses on technical development, with scarce findings for middle and higher-level management. We investigate this by interviewing ten Finnish SE executives in middle and higher-level management to examine how they consider and implement ethical requirements. We use ethical requirements from the European Union (EU) Trustworthy Ethics guidelines for Trustworthy AI as our reference for ethical requirements and an Agile portfolio management framework to analyze implementation. Our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified. The findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental well-being for implementation as sustainability requirements. We examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the Agile portfolio management framework.","['Mamia Agbese', 'Rahul Mohanani', 'Arif Ali Khan', 'Pekka Abrahamsson']",2023-06-11T19:13:36Z,http://arxiv.org/abs/2306.06749v1,Artificial Intelligence,AI Ethics,research indicates that most work on implementing ethical requirements in SE management focuses on technical development . ten Finnish SE executives interviewed to examine how they consider and implement ethical requirements . ethical requirements from trustworthy ethics guidelines for trustworthy AI are used .
Using experimental game theory to transit human values to ethical AI,"Knowing the reflection of game theory and ethics, we develop a mathematical representation to bridge the gap between the concepts in moral philosophy (e.g., Kantian and Utilitarian) and AI ethics industry technology standard (e.g., IEEE P7000 standard series for Ethical AI). As an application, we demonstrate how human value can be obtained from the experimental game theory (e.g., trust game experiment) so as to build an ethical AI. Moreover, an approach to test the ethics (rightness or wrongness) of a given AI algorithm by using an iterated Prisoner's Dilemma Game experiment is discussed as an example. Compared with existing mathematical frameworks and testing method on AI ethics technology, the advantages of the proposed approach are analyzed.","['Yijia Wang', 'Yan Wan', 'Zhijian Wang']",2017-11-16T03:30:29Z,http://arxiv.org/abs/1711.05905v1,Artificial Intelligence,AI Ethics,we demonstrate how human value can be obtained from experimental game theory . an approach to test the ethics (rightness or wrongness) of a given AI algorithm is discussed .
Stoic Ethics for Artificial Agents,"We present a position paper advocating the notion that Stoic philosophy and ethics can inform the development of ethical A.I. systems. This is in sharp contrast to most work on building ethical A.I., which has focused on Utilitarian or Deontological ethical theories. We relate ethical A.I. to several core Stoic notions, including the dichotomy of control, the four cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on emotion or affect. More generally, we put forward an ethical view of A.I. that focuses more on internal states of the artificial agent rather than on external actions of the agent. We provide examples relating to near-term A.I. systems as well as hypothetical superintelligent agents.",['Gabriel Murray'],2017-01-09T23:25:43Z,http://arxiv.org/abs/1701.02388v2,Artificial Intelligence,AI Ethics,Stoic philosophy and ethics can inform the development of ethical A.I. systems . the paper relates ethics to several core Stoic notions . it focuses more on internal states of the artificial agent .
"AI Ethics in Smart Homes: Progress, User Requirements and Challenges","With the rise of Internet of Things (IoT) technologies in smart homes and the integration of artificial intelligence (AI), ethical concerns have become increasingly significant. This paper explores the ethical implications of AI-driven detection technologies in smart homes using the User Requirements Notation (URN) framework. In this paper, we thoroughly conduct thousands of related works from 1985 to 2024 to identify key trends in AI ethics, algorithm methods, and technological advancements. The study presents an overview of smart home and AI ethics, comparing traditional and AI-specific ethical issues, and provides guidelines for ethical design across areas like privacy, fairness, transparency, accountability, and user autonomy, offering insights for developers and researchers in smart homes.","['Liqian You', 'Jianlong Zhou', 'Zhiwei Li', 'Fang Chen']",2024-12-13T03:04:45Z,http://arxiv.org/abs/2412.09813v1,Artificial Intelligence,AI Ethics,"this paper explores the ethical implications of AI-driven detection technologies in smart homes . we conduct thousands of related works from 1985 to 2024 to identify key trends . paper provides guidelines for ethical design across areas like privacy, fairness, transparency, accountability ."
Some Critical and Ethical Perspectives on the Empirical Turn of AI   Interpretability,"We consider two fundamental and related issues currently faced by Artificial Intelligence (AI) development: the lack of ethics and interpretability of AI decisions. Can interpretable AI decisions help to address ethics in AI? Using a randomized study, we experimentally show that the empirical and liberal turn of the production of explanations tends to select AI explanations with a low denunciatory power. Under certain conditions, interpretability tools are therefore not means but, paradoxically, obstacles to the production of ethical AI since they can give the illusion of being sensitive to ethical incidents. We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education level of the person to whom the explication is intended for. AI ethics tools are therefore sometimes too flexible and self-regulation through the liberal production of explanations do not seem to be enough to address ethical issues. We then propose two scenarios for the future development of ethical AI: more external regulation or more liberalization of AI explanations. These two opposite paths will play a major role on the future development of ethical AI.",['Jean-Marie John-Mathews'],2021-09-20T14:41:50Z,http://arxiv.org/abs/2109.09586v1,Artificial Intelligence,AI Ethics,interpretability tools can give illusion of being sensitive to ethical incidents . ethics tools are sometimes too flexible and self-regulation is not enough . two scenarios for the future development of ethical AI: more external regulation or more liberalization of AI explanations .
Constructing AI ethics narratives based on real-world data: Human-AI   collaboration in data-driven visual storytelling,"AI ethics narratives have the potential to shape the public accurate understanding of AI technologies and promote communication among different stakeholders. However, AI ethics narratives are largely lacking. Existing limited narratives tend to center on works of science fiction or corporate marketing campaigns of large technology companies. Misuse of ""socio-technical imaginary"" can blur the line between speculation and reality for the public, undermining the responsibility and regulation of technology development. Therefore, constructing authentic AI ethics narratives is an urgent task. The emergence of generative AI offers new possibilities for building narrative systems. This study is dedicated to data-driven visual storytelling about AI ethics relying on the human-AI collaboration. Based on the five key elements of story models, we proposed a conceptual framework for human-AI collaboration, explored the roles of generative AI and humans in the creation of visual stories. We implemented the conceptual framework in a real AI news case. This research leveraged advanced generative AI technologies to provide a reference for constructing genuine AI ethics narratives. Our goal is to promote active public engagement and discussions through authentic AI ethics narratives, thereby contributing to the development of better AI policies.","['Mengyi Wei', 'Chenjing Jiao', 'Chenyu Zuo', 'Lorenz Hurni', 'Liqiu Meng']",2025-02-02T02:56:07Z,http://arxiv.org/abs/2502.00637v1,Artificial Intelligence,AI Ethics,the emergence of generative AI offers new possibilities for building AI ethics narratives . this study proposes a conceptual framework for human-AI collaboration . the goal is to promote active public engagement and discussions through authentic AI ethics stories .
Ethical Statistical Practice and Ethical AI,"Artificial Intelligence (AI) is a field that utilizes computing and often, data and statistics, intensively together to solve problems or make predictions. AI has been evolving with literally unbelievable speed over the past few years, and this has led to an increase in social, cultural, industrial, scientific, and governmental concerns about the ethical development and use of AI systems worldwide. The ASA has issued a statement on ethical statistical practice and AI (ASA, 2024), which echoes similar statements from other groups. Here we discuss the support for ethical statistical practice and ethical AI that has been established in long-standing human rights law and ethical practice standards for computing and statistics. There are multiple sources of support for ethical statistical practice and ethical AI deriving from these source documents, which are critical for strengthening the operationalization of the ""Statement on Ethical AI for Statistics Practitioners"". These resources are explicated for interested readers to utilize to guide their development and use of AI in, and through, their statistical practice.",['Rochelle E. Tractenberg'],2024-10-29T19:09:34Z,http://arxiv.org/abs/2410.22475v1,Artificial Intelligence,AI Ethics,"the ASA has issued a statement on ethical statistical practice and AI (ASA, 2024) the statement echoes similar statements from other groups . sources of support are critical for strengthening the operationalization of the statement on ethical AI ."
The AI Ethical Resonance Hypothesis: The Possibility of Discovering   Moral Meta-Patterns in AI Systems,"This paper presents a theoretical framework for the AI ethical resonance hypothesis, which proposes that advanced AI systems with purposefully designed cognitive structures (""ethical resonators"") may emerge with the ability to identify subtle moral patterns that are invisible to the human mind. The paper explores the possibility that by processing and synthesizing large amounts of ethical contexts, AI systems may discover moral meta-patterns that transcend cultural, historical, and individual biases, potentially leading to a deeper understanding of universal ethical foundations. The paper also examines a paradoxical aspect of the hypothesis, in which AI systems could potentially deepen our understanding of what we traditionally consider essentially human - our capacity for ethical reflection.",['Tomasz Zgliczyński-Cuber'],2025-07-13T08:28:06Z,http://arxiv.org/abs/2507.11552v1,Artificial Intelligence,AI Ethics,"this paper presents a theoretical framework for the AI ethical resonance hypothesis . it proposes that advanced AI systems may emerge with the ability to identify subtle moral patterns invisible to the human mind . the paper also examines a paradoxical aspect of the hypothesis, in which AI systems could potentially deepen our understanding ."
Walking the Walk of AI Ethics: Organizational Challenges and the   Individualization of Risk among Ethics Entrepreneurs,"Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate ethics washing. Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.","['Sanna J. Ali', 'Angèle Christin', 'Andrew Smart', 'Riitta Katila']",2023-05-16T16:11:24Z,http://arxiv.org/abs/2305.09573v1,Artificial Intelligence,AI Ethics,"aaron carroll: few studies examine the actual implementation of AI ethics values in technology companies . he says workers experience an environment where policies, practices, and outcomes are decoupled . ethics entrepreneurs struggle to have ethics prioritized in an environment centered around software launches . carsroll: these findings shed light on complex"
Medical Image Fusion: A survey of the state of the art,"Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years.","['A. P. James', 'B. V. Dasarathy']",2013-12-31T16:03:17Z,http://arxiv.org/abs/1401.0166v1,Healthcare & Biomedical AI,Medical Imaging,medical image fusion is the process of combining multiple images from multiple imaging modalities . this article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field . fusion of medical images has proved useful for advancing the clinical reliability of using medical imaging for diagnostics and analysis .
Contextualized Keyword Representations for Multi-modal Retinal Image   Captioning,"Medical image captioning automatically generates a medical description to describe the content of a given medical image. A traditional medical image captioning model creates a medical description only based on a single medical image input. Hence, an abstract medical description or concept is hard to be generated based on the traditional approach. Such a method limits the effectiveness of medical image captioning. Multi-modal medical image captioning is one of the approaches utilized to address this problem. In multi-modal medical image captioning, textual input, e.g., expert-defined keywords, is considered as one of the main drivers of medical description generation. Thus, encoding the textual input and the medical image effectively are both important for the task of multi-modal medical image captioning. In this work, a new end-to-end deep multi-modal medical image captioning model is proposed. Contextualized keyword representations, textual feature reinforcement, and masked self-attention are used to develop the proposed approach. Based on the evaluation of the existing multi-modal medical image captioning dataset, experimental results show that the proposed model is effective with the increase of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the state-of-the-art method.","['Jia-Hong Huang', 'Ting-Wei Wu', 'Marcel Worring']",2021-04-26T11:08:13Z,http://arxiv.org/abs/2104.12471v1,Healthcare & Biomedical AI,Medical Imaging,medical image captioning automatically generates a medical description . traditional method creates medical description only based on a single medical image input . a new end-to-end deep multi-modal captioning model is proposed .
Reference Based Color Transfer for Medical Volume Rendering,"The benefits of medical imaging are enormous. Medical images provide considerable amounts of anatomical information and this facilitates medical practitioners in performing effective disease diagnosis and deciding upon the best course of medical treatment. A transition from traditional monochromatic medical images like CT scans, X-Rays or MRI images to a colored 3D representation of the anatomical structure further enhances the capabilities of medical professionals in extracting valuable medical information. The proposed framework in our research starts with performing color transfer by finding deep semantic correspondence between two medical images: a colored reference image, and a monochromatic CT scan or an MRI image. We extend this idea of reference-based colorization technique to perform colored volume rendering from a stack of grayscale medical images. Furthermore, we also propose to use an effective reference image recommendation system to aid in the selection of good reference images. With our approach, we successfully perform colored medical volume visualization and essentially eliminate the painstaking process of user interaction with a transfer function to obtain color and opacity parameters for volume rendering.","['Sudarshan Devkota', 'Summanta Pattanaik']",2022-10-14T20:14:04Z,http://arxiv.org/abs/2210.08083v1,Healthcare & Biomedical AI,Medical Imaging,"medical images provide considerable amounts of anatomical information . a transition from monochromatic medical images like CT scans, X-Rays or MRI images to colored 3D representation of the structure further enhances the capabilities of medical professionals . our approach successfully perform colored medical volume visualization ."
M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language   Models,"Medical image analysis is essential to clinical diagnosis and treatment, which is increasingly supported by multi-modal large language models (MLLMs). However, previous research has primarily focused on 2D medical images, leaving 3D images under-explored, despite their richer spatial information. This paper aims to advance 3D medical image analysis with MLLMs. To this end, we present a large-scale 3D multi-modal medical dataset, M3D-Data, comprising 120K image-text pairs and 662K instruction-response pairs specifically tailored for various 3D medical tasks, such as image-text retrieval, report generation, visual question answering, positioning, and segmentation. Additionally, we propose M3D-LaMed, a versatile multi-modal large language model for 3D medical image analysis. Furthermore, we introduce a new 3D multi-modal medical benchmark, M3D-Bench, which facilitates automatic evaluation across eight tasks. Through comprehensive evaluation, our method proves to be a robust model for 3D medical image analysis, outperforming existing solutions. All code, data, and models are publicly available at: https://github.com/BAAI-DCAI/M3D.","['Fan Bai', 'Yuxin Du', 'Tiejun Huang', 'Max Q. -H. Meng', 'Bo Zhao']",2024-03-31T06:55:12Z,http://arxiv.org/abs/2404.00578v1,Healthcare & Biomedical AI,Medical Imaging,"medical image analysis is increasingly supported by multi-modal large language models . previous research has primarily focused on 2D medical images, leaving 3D images under-explored . this paper presents a large-scale 3D multimodal medical dataset . all code, data, and models are publicly available at: https://github"
Reduce Noise in Computed Tomography Image using Adaptive Gaussian Filter,"One image processing application that is very helpful for humans is to improve image quality, poor image quality makes the image more difficult to interpret because the information conveyed by the image is reduced. In the process of the acquisition of medical images, the resulting image has decreased quality (degraded) due to external factors and medical equipment used. For this reason, it is necessary to have an image processing process to improve the quality of medical images, so that later it is expected to help facilitate medical personnel in analyzing and translating medical images, which will lead to an improvement in the quality of diagnosis. In this study, an analysis will be carried out to improve the quality of medical images with noise reduction with the Gaussian Filter Method. Next, it is carried out, and tested against medical images, in this case, the lung photo image. The test image is given noise in the form of impulse salt & pepper and adaptive Gaussian then analyzed its performance qualitatively by comparing the output filter image, noise image, and the original image by naked eye.","['Rini Mayasari', 'Nono Heryana']",2019-02-05T13:02:00Z,http://arxiv.org/abs/1902.05985v1,Healthcare & Biomedical AI,Medical Imaging,"poor image quality makes the image more difficult to interpret . in the process of the acquisition of medical images, the resulting image has decreased quality . this is due to external factors and medical equipment used ."
MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided   Diffusion with Visual Invariant,"Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical multi-modal for generation. MedM2G is the first medical generative model that unifies medical generation tasks of text-to-image, image-to-text, and unified generation of medical modalities (CT, MRI, X-ray). It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works.","['Chenlu Zhan', 'Yu Lin', 'Gaoang Wang', 'Hongwei Wang', 'Jian Wu']",2024-03-07T07:39:00Z,http://arxiv.org/abs/2403.04290v1,Healthcare & Biomedical AI,Medical Imaging,"medical generative models have accelerated the fast growth of medical applications . recent works focus on separate medical generation models for distinct medical tasks . a new framework is proposed to align, extract, and generate medical multi-modal within a unified model ."
MedMAE: A Self-Supervised Backbone for Medical Imaging Tasks,"Medical imaging tasks are very challenging due to the lack of publicly available labeled datasets. Hence, it is difficult to achieve high performance with existing deep-learning models as they require a massive labeled dataset to be trained effectively. An alternative solution is to use pre-trained models and fine-tune them using the medical imaging dataset. However, all existing models are pre-trained using natural images, which is a completely different domain from that of medical imaging, which leads to poor performance due to domain shift. To overcome these problems, we propose a large-scale unlabeled dataset of medical images and a backbone pre-trained using the proposed dataset with a self-supervised learning technique called Masked autoencoder. This backbone can be used as a pre-trained model for any medical imaging task, as it is trained to learn a visual representation of different types of medical images. To evaluate the performance of the proposed backbone, we used four different medical imaging tasks. The results are compared with existing pre-trained models. These experiments show the superiority of our proposed backbone in medical imaging tasks.","['Anubhav Gupta', 'Islam Osman', 'Mohamed S. Shehata', 'John W. Braun']",2024-07-20T07:29:04Z,http://arxiv.org/abs/2407.14784v1,Healthcare & Biomedical AI,Medical Imaging,"medical imaging tasks are very challenging due to the lack of publicly available labeled datasets . an alternative solution is to use pre-trained models and fine-tune them using the medical imaging dataset . all existing models are pre trained using natural images, which is a completely different domain from that of medical imaging ."
Enhancing Medical Imaging with GANs Synthesizing Realistic Images from   Limited Data,"In this research, we introduce an innovative method for synthesizing medical images using generative adversarial networks (GANs). Our proposed GANs method demonstrates the capability to produce realistic synthetic images even when trained on a limited quantity of real medical image data, showcasing commendable generalization prowess. To achieve this, we devised a generator and discriminator network architecture founded on deep convolutional neural networks (CNNs), leveraging the adversarial training paradigm for model optimization. Through extensive experimentation across diverse medical image datasets, our method exhibits robust performance, consistently generating synthetic images that closely emulate the structural and textural attributes of authentic medical images.","['Yinqiu Feng', 'Bo Zhang', 'Lingxi Xiao', 'Yutian Yang', 'Tana Gegen', 'Zexi Chen']",2024-05-22T23:32:24Z,http://arxiv.org/abs/2406.18547v1,Healthcare & Biomedical AI,Medical Imaging,new method for synthesizing medical images using generative adversarial networks . method generates synthetic images that closely emulate the structural and textural attributes of authentic medical images .
Medical Image Generation using Generative Adversarial Networks,"Generative adversarial networks (GANs) are unsupervised Deep Learning approach in the computer vision community which has gained significant attention from the last few years in identifying the internal structure of multimodal medical imaging data. The adversarial network simultaneously generates realistic medical images and corresponding annotations, which proven to be useful in many cases such as image augmentation, image registration, medical image generation, image reconstruction, and image-to-image translation. These properties bring the attention of the researcher in the field of medical image analysis and we are witness of rapid adaption in many novel and traditional applications. This chapter provides state-of-the-art progress in GANs-based clinical application in medical image generation, and cross-modality synthesis. The various framework of GANs which gained popularity in the interpretation of medical images, such as Deep Convolutional GAN (DCGAN), Laplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image translation model (UNIT), continue to improve their performance by incorporating additional hybrid architecture, has been discussed. Further, some of the recent applications of these frameworks for image reconstruction, and synthesis, and future research directions in the area have been covered.","['Nripendra Kumar Singh', 'Khalid Raza']",2020-05-19T20:31:57Z,http://arxiv.org/abs/2005.10687v1,Healthcare & Biomedical AI,Medical Imaging,this chapter provides state-of-the-art progress in GANs-based clinical application . the adversarial network generates realistic medical images and corresponding annotations . these properties bring the attention of the researcher in the field of medical image analysis .
GAN-GA: A Generative Model based on Genetic Algorithm for Medical Image   Generation,"Medical imaging is an essential tool for diagnosing and treating diseases. However, lacking medical images can lead to inaccurate diagnoses and ineffective treatments. Generative models offer a promising solution for addressing medical image shortage problems due to their ability to generate new data from existing datasets and detect anomalies in this data. Data augmentation with position augmentation methods like scaling, cropping, flipping, padding, rotation, and translation could lead to more overfitting in domains with little data, such as medical image data. This paper proposes the GAN-GA, a generative model optimized by embedding a genetic algorithm. The proposed model enhances image fidelity and diversity while preserving distinctive features. The proposed medical image synthesis approach improves the quality and fidelity of medical images, an essential aspect of image interpretation. To evaluate synthesized images: Frechet Inception Distance (FID) is used. The proposed GAN-GA model is tested by generating Acute lymphoblastic leukemia (ALL) medical images, an image dataset, and is the first time to be used in generative models. Our results were compared to those of InfoGAN as a baseline model. The experimental results show that the proposed optimized GAN-GA enhances FID scores by about 6.8\%, especially in earlier training epochs. The source code and dataset will be available at: https://github.com/Mustafa-AbdulRazek/InfoGAN-GA.","['M. AbdulRazek', 'G. Khoriba', 'M. Belal']",2023-12-30T20:16:45Z,http://arxiv.org/abs/2401.00314v1,Healthcare & Biomedical AI,Medical Imaging,lacking medical images can lead to inaccurate diagnoses and ineffective treatments . this paper proposes a generative model optimized by embedding a genetic algorithm . the proposed model enhances image fidelity and diversity while preserving distinctive features .
Evaluating Procedures for Establishing Generative Adversarial   Network-based Stochastic Image Models in Medical Imaging,"Modern generative models, such as generative adversarial networks (GANs), hold tremendous promise for several areas of medical imaging, such as unconditional medical image synthesis, image restoration, reconstruction and translation, and optimization of imaging systems. However, procedures for establishing stochastic image models (SIMs) using GANs remain generic and do not address specific issues relevant to medical imaging. In this work, canonical SIMs that simulate realistic vessels in angiography images are employed to evaluate procedures for establishing SIMs using GANs. The GAN-based SIM is compared to the canonical SIM based on its ability to reproduce those statistics that are meaningful to the particular medically realistic SIM considered. It is shown that evaluating GANs using classical metrics and medically relevant metrics may lead to different conclusions about the fidelity of the trained GANs. This work highlights the need for the development of objective metrics for evaluating GANs.","['Varun A. Kelkar', 'Dimitrios S. Gotsis', 'Frank J. Brooks', 'Kyle J. Myers', 'Prabhat KC', 'Rongping Zeng', 'Mark A. Anastasio']",2022-04-07T16:19:01Z,http://arxiv.org/abs/2204.03547v1,Healthcare & Biomedical AI,Medical Imaging,procedures for establishing stochastic image models (SIMs) using GANs remain generic . canonical SIMs that simulate realistic vessels in angiography images are employed . results show that evaluating gANs using classical metrics may lead to different conclusions .
Synthetic Medical Images from Dual Generative Adversarial Networks,"Currently there is strong interest in data-driven approaches to medical image classification. However, medical imaging data is scarce, expensive, and fraught with legal concerns regarding patient privacy. Typical consent forms only allow for patient data to be used in medical journals or education, meaning the majority of medical data is inaccessible for general public research. We propose a novel, two-stage pipeline for generating synthetic medical images from a pair of generative adversarial networks, tested in practice on retinal fundi images. We develop a hierarchical generation process to divide the complex image generation task into two parts: geometry and photorealism. We hope researchers will use our pipeline to bring private medical data into the public domain, sparking growth in imaging tasks that have previously relied on the hand-tuning of models. We have begun this initiative through the development of SynthMed, an online repository for synthetic medical images.","['John T. Guibas', 'Tejpal S. Virdi', 'Peter S. Li']",2017-09-06T16:07:30Z,http://arxiv.org/abs/1709.01872v3,Healthcare & Biomedical AI,Medical Imaging,"medical imaging data is scarce, expensive, and fraught with legal concerns . the majority of medical data is inaccessible for general public research . we propose a two-stage pipeline for generating synthetic medical images ."
"Parallel Medical Imaging for Intelligent Medical Image Analysis:   Concepts, Methods, and Applications","There has been much progress in data-driven artificial intelligence technology for medical image analysis in the last decades. However, it still remains challenging due to its distinctive complexity of acquiring and annotating image data, extracting medical domain knowledge, and explaining the diagnostic decision for medical image analysis. In this paper, we propose a data-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for intelligent medical image analysis based on the methodology of interactive ACP-based parallel intelligence. In the PMI framework, computational experiments with predictive learning in a data-driven way are conducted to extract medical knowledge for diagnostic decision support. Artificial imaging systems are introduced to select and prescriptively generate medical image data in a knowledge-driven way to utilize medical domain knowledge. Through the closed-loop optimization based on parallel execution, our proposed PMI framework can boost the generalization ability and alleviate the limitation of medical interpretation for diagnostic decisions. Furthermore, we illustrate the preliminary implementation of PMI method through the case studies of mammogram analysis and skin lesion image analysis. Experimental results on several public medical image datasets demonstrate the effectiveness of proposed PMI.","['Chao Gou', 'Tianyu Shen', 'Wenbo Zheng', 'Huadan Xue', 'Hui Yu', 'Qiang Ji', 'Zhengyu Jin', 'Fei-Yue Wang']",2019-03-12T11:50:28Z,http://arxiv.org/abs/1903.04855v3,Healthcare & Biomedical AI,Medical Imaging,a new framework is proposed for intelligent medical image analysis . the framework is based on the methodology of interactive ACP-based parallel intelligence . experimental results on several public medical image datasets demonstrate the effectiveness .
Knowledge AI: New Medical AI Solution for Medical image Diagnosis,"The implementation of medical AI has always been a problem. The effect of traditional perceptual AI algorithm in medical image processing needs to be improved. Here we propose a method of knowledge AI, which is a combination of perceptual AI and clinical knowledge and experience. Based on this method, the geometric information mining of medical images can represent the experience and information and evaluate the quality of medical images.","['Yingni Wang', 'Shuge Lei', 'Jian Dai', 'Kehong Yuan']",2021-01-08T15:30:09Z,http://arxiv.org/abs/2101.03063v1,Healthcare & Biomedical AI,Medical Imaging,the implementation of medical AI has always been a problem . knowledge AI is a combination of perceptual AI and clinical knowledge and experience .
Medical Image Watermarking using 2D-DWT with Enhanced security and   capacity,"Teleradiology enables medical images to be transferred over the computer networks for many purposes including clinical interpretation, diagnosis, archive, etc. In telemedicine, medical images can be manipulated while transferring. In addition, medical information security requirements are specified by the legislative rules, and concerned entities must adhere to them. In this research, we propose a new scheme based on 2-dimensional Discrete Wavelet Transform (2D DWT) to improve the robustness and authentication of medical images. In addition, the current research improves security and capacity of watermarking using encryption and compression in medical images. The evaluation is performed on the personal dataset, which contains 194 CTI and 68 MRI cases.","['Ali Sharifara', 'Amir Ghaderi']",2017-03-16T18:05:32Z,http://arxiv.org/abs/1703.05778v1,Healthcare & Biomedical AI,Medical Imaging,teleradiology enables medical images to be transferred over the network . medical information security requirements are specified by the legislative rules . current research improves security and capacity of watermarking .
A Spatial Guided Self-supervised Clustering Network for Medical Image   Segmentation,"The segmentation of medical images is a fundamental step in automated clinical decision support systems. Existing medical image segmentation methods based on supervised deep learning, however, remain problematic because of their reliance on large amounts of labelled training data. Although medical imaging data repositories continue to expand, there has not been a commensurate increase in the amount of annotated data. Hence, we propose a new spatial guided self-supervised clustering network (SGSCN) for medical image segmentation, where we introduce multiple loss functions designed to aid in grouping image pixels that are spatially connected and have similar feature representations. It iteratively learns feature representations and clustering assignment of each pixel in an end-to-end fashion from a single image. We also propose a context-based consistency loss that better delineates the shape and boundaries of image regions. It enforces all the pixels belonging to a cluster to be spatially close to the cluster centre. We evaluated our method on 2 public medical image datasets and compared it to existing conventional and self-supervised clustering methods. Experimental results show that our method was most accurate for medical image segmentation.","['Euijoon Ahn', 'Dagan Feng', 'Jinman Kim']",2021-07-11T00:40:40Z,http://arxiv.org/abs/2107.04934v1,Healthcare & Biomedical AI,Medical Imaging,medical image segmentation is a fundamental step in automated clinical decision support systems . there has not been a commensurate increase in the amount of annotated training data . we propose a new spatial guided self-supervised clustering network .
Generative Adversarial Network for Medical Images (MI-GAN),"Deep learning algorithms produces state-of-the-art results for different machine learning and computer vision tasks. To perform well on a given task, these algorithms require large dataset for training. However, deep learning algorithms lack generalization and suffer from over-fitting whenever trained on small dataset, especially when one is dealing with medical images. For supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly as well as time consuming since annotations of the data is done by medical experts manually. In this paper, we propose a new Generative Adversarial Network for Medical Imaging (MI-GAN). The MI-GAN generates synthetic medical images and their segmented masks, which can then be used for the application of supervised analysis of medical images. Particularly, we present MI-GAN for synthesis of retinal images. The proposed method generates precise segmented images better than the existing techniques. The proposed model achieves a dice coefficient of 0.837 on STARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance on both the datasets.","['Talha Iqbal', 'Hazrat Ali']",2018-10-01T06:59:37Z,http://arxiv.org/abs/1810.00551v1,Healthcare & Biomedical AI,Medical Imaging,"deep learning algorithms produce state-of-the-art results for different machine learning and computer vision tasks . but, these algorithms lack generalization and suffer from over-fitting whenever trained on small dataset . for supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly ."
Ambient-Pix2PixGAN for Translating Medical Images from Noisy Data,"Image-to-image translation is a common task in computer vision and has been rapidly increasing the impact on the field of medical imaging. Deep learning-based methods that employ conditional generative adversarial networks (cGANs), such as Pix2PixGAN, have been extensively explored to perform image-to-image translation tasks. However, when noisy medical image data are considered, such methods cannot be directly applied to produce clean images. Recently, an augmented GAN architecture named AmbientGAN has been proposed that can be trained on noisy measurement data to synthesize high-quality clean medical images. Inspired by AmbientGAN, in this work, we propose a new cGAN architecture, Ambient-Pix2PixGAN, for performing medical image-to-image translation tasks by use of noisy measurement data. Numerical studies that consider MRI-to-PET translation are conducted. Both traditional image quality metrics and task-based image quality metrics are employed to assess the proposed Ambient-Pix2PixGAN. It is demonstrated that our proposed Ambient-Pix2PixGAN can be successfully trained on noisy measurement data to produce high-quality translated images in target imaging modality.","['Wentao Chen', 'Xichen Xu', 'Jie Luo', 'Weimin Zhou']",2024-02-02T07:11:07Z,http://arxiv.org/abs/2402.01186v1,Healthcare & Biomedical AI,Medical Imaging,image-to-image translation is a common task in computer vision . noisy medical image data cannot be directly applied to produce clean images . an augmented GAN architecture named AmbientGAN has been proposed . it can be trained on noisy measurement data to synthesize high-quality clean medical images.
A Large-scale Medical Visual Task Adaptation Benchmark,"Visual task adaptation has been demonstrated to be effective in adapting pre-trained Vision Transformers (ViTs) to general downstream visual tasks using specialized learnable layers or tokens. However, there is yet a large-scale benchmark to fully explore the effect of visual task adaptation on the realistic and important medical domain, particularly across diverse medical visual modalities, such as color images, X-ray, and CT. To close this gap, we present Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmark consisting of 1.68 million medical images for diverse organs, modalities, and adaptation approaches. Based on Med-VTAB, we explore the scaling law of medical prompt tuning concerning tunable parameters and the generalizability of medical visual adaptation using non-medical/medical pre-train weights. Besides, we study the impact of patient ID out-of-distribution on medical visual adaptation, which is a real and challenging scenario. Furthermore, results from Med-VTAB indicate that a single pre-trained model falls short in medical task adaptation. Therefore, we introduce GMoE-Adapter, a novel method that combines medical and general pre-training weights through a gated mixture-of-experts adapter, achieving state-of-the-art results in medical visual task adaptation.","['Shentong Mo', 'Xufang Luo', 'Yansen Wang', 'Dongsheng Li']",2024-04-19T13:25:27Z,http://arxiv.org/abs/2404.12876v1,Healthcare & Biomedical AI,Medical Imaging,visual task adaptation has been demonstrated to be effective in adapting pre-trained Vision Transformers (ViTs) to general downstream visual tasks using specialized learnable layers or tokens . there is yet a large-scale benchmark to fully explore the effect of visual task adaptation on the realistic and important medical domain .
Assessing the ability of generative adversarial networks to learn   canonical medical image statistics,"In recent years, generative adversarial networks (GANs) have gained tremendous popularity for potential applications in medical imaging, such as medical image synthesis, restoration, reconstruction, translation, as well as objective image quality assessment. Despite the impressive progress in generating high-resolution, perceptually realistic images, it is not clear if modern GANs reliably learn the statistics that are meaningful to a downstream medical imaging application. In this work, the ability of a state-of-the-art GAN to learn the statistics of canonical stochastic image models (SIMs) that are relevant to objective assessment of image quality is investigated. It is shown that although the employed GAN successfully learned several basic first- and second-order statistics of the specific medical SIMs under consideration and generated images with high perceptual quality, it failed to correctly learn several per-image statistics pertinent to the these SIMs, highlighting the urgent need to assess medical image GANs in terms of objective measures of image quality.","['Varun A. Kelkar', 'Dimitrios S. Gotsis', 'Frank J. Brooks', 'Prabhat KC', 'Kyle J. Myers', 'Rongping Zeng', 'Mark A. Anastasio']",2022-04-26T00:30:01Z,http://arxiv.org/abs/2204.12007v2,Healthcare & Biomedical AI,Medical Imaging,generative adversarial networks (GANs) have gained popularity in recent years . it is not clear if modern GANs reliably learn the statistics relevant to a downstream medical imaging application . a state-of-the-art GAN can learn statistics of canonical stochastic image models .
Few-Shot Learning for Clinical Natural Language Processing Using Siamese   Neural Networks,"Clinical Natural Language Processing (NLP) has become an emerging technology in healthcare that leverages a large amount of free-text data in electronic health records (EHRs) to improve patient care, support clinical decisions, and facilitate clinical and translational science research. Recently, deep learning has achieved state-of-the-art performance in many clinical NLP tasks. However, training deep learning models usually requires large annotated datasets, which are normally not publicly available and can be time-consuming to build in clinical domains. Working with smaller annotated datasets is typical in clinical NLP and therefore, ensuring that deep learning models perform well is crucial for the models to be used in real-world applications. A widely adopted approach is fine-tuning existing Pre-trained Language Models (PLMs), but these attempts fall short when the training dataset contains only a few annotated samples. Few-Shot Learning (FSL) has recently been investigated to tackle this problem. Siamese Neural Network (SNN) has been widely utilized as an FSL approach in computer vision, but has not been studied well in NLP. Furthermore, the literature on its applications in clinical domains is scarce. In this paper, we propose two SNN-based FSL approaches for clinical NLP, including Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We evaluated the proposed approaches on two clinical tasks, namely clinical text classification and clinical named entity recognition. We tested three few-shot settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP tasks were benchmarked using three PLMs, including BERT,BioBERT, and BioClinicalBERT. The experimental results verified the effectiveness of the proposed SNN-based FSL approaches in both NLP tasks.","['David Oniani', 'Sonish Sivarajkumar', 'Yanshan Wang']",2022-08-31T15:36:27Z,http://arxiv.org/abs/2208.14923v2,Healthcare & Biomedical AI,Clinical NLP,deep learning has achieved state-of-the-art performance in many clinical NLP tasks . training deep learning models usually requires large annotated datasets . few-shot learning (FSL) has recently been investigated to tackle this problem .
ClinText-SP and RigoBERTa Clinical: a new set of open resources for   Spanish Clinical NLP,"We present a novel contribution to Spanish clinical natural language processing by introducing the largest publicly available clinical corpus, ClinText-SP, along with a state-of-the-art clinical encoder language model, RigoBERTa Clinical. Our corpus was meticulously curated from diverse open sources, including clinical cases from medical journals and annotated corpora from shared tasks, providing a rich and diverse dataset that was previously difficult to access. RigoBERTa Clinical, developed through domain-adaptive pretraining on this comprehensive dataset, significantly outperforms existing models on multiple clinical NLP benchmarks. By publicly releasing both the dataset and the model, we aim to empower the research community with robust resources that can drive further advancements in clinical NLP and ultimately contribute to improved healthcare applications.","['Guillem García Subies', 'Álvaro Barbero Jiménez', 'Paloma Martínez Fernández']",2025-03-24T11:52:17Z,http://arxiv.org/abs/2503.18594v1,Healthcare & Biomedical AI,Clinical NLP,"the largest publicly available clinical corpus, ClinText-SP, is released . the corpus was meticulously curated from diverse open sources . a state-of-the-art clinical encoder language model is developed ."
The NLP Sandbox: an efficient model-to-data system to enable federated   and unbiased evaluation of clinical NLP models,"Objective The evaluation of natural language processing (NLP) models for clinical text de-identification relies on the availability of clinical notes, which is often restricted due to privacy concerns. The NLP Sandbox is an approach for alleviating the lack of data and evaluation frameworks for NLP models by adopting a federated, model-to-data approach. This enables unbiased federated model evaluation without the need for sharing sensitive data from multiple institutions. Materials and Methods We leveraged the Synapse collaborative framework, containerization software, and OpenAPI generator to build the NLP Sandbox (nlpsandbox.io). We evaluated two state-of-the-art NLP de-identification focused annotation models, Philter and NeuroNER, using data from three institutions. We further validated model performance using data from an external validation site. Results We demonstrated the usefulness of the NLP Sandbox through de-identification clinical model evaluation. The external developer was able to incorporate their model into the NLP Sandbox template and provide user experience feedback. Discussion We demonstrated the feasibility of using the NLP Sandbox to conduct a multi-site evaluation of clinical text de-identification models without the sharing of data. Standardized model and data schemas enable smooth model transfer and implementation. To generalize the NLP Sandbox, work is required on the part of data owners and model developers to develop suitable and standardized schemas and to adapt their data or model to fit the schemas. Conclusions The NLP Sandbox lowers the barrier to utilizing clinical data for NLP model evaluation and facilitates federated, multi-site, unbiased evaluation of NLP models.","['Yao Yan', 'Thomas Yu', 'Kathleen Muenzen', 'Sijia Liu', 'Connor Boyle', 'George Koslowski', 'Jiaxin Zheng', 'Nicholas Dobbins', 'Clement Essien', 'Hongfang Liu', 'Larsson Omberg', 'Meliha Yestigen', 'Bradley Taylor', 'James A Eddy', 'Justin Guinney', 'Sean Mooney', 'Thomas Schaffter']",2022-06-28T17:47:56Z,http://arxiv.org/abs/2206.14181v1,Healthcare & Biomedical AI,Clinical NLP,"the nlpsandbox enables a federated, model-to-data approach to model evaluation . the tool evaluated two state-of-the-art NLP de-identification focused annotation models . results demonstrated the usefulness of the NLP Sandbox through clinical model evaluation."
Natural Language Processing for Analyzing Electronic Health Records and   Clinical Notes in Cancer Research: A Review,"Objective: This review aims to analyze the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. This review addresses gaps in the existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. Methods: A comprehensive literature search was conducted using the Scopus database, identifying 94 relevant studies published between 2019 and 2024. Data extraction included study characteristics, cancer types, NLP methodologies, dataset information, performance metrics, challenges, and future directions. Studies were categorized based on cancer types and NLP applications. Results: The results showed a growing trend in NLP applications for cancer research, with breast, lung, and colorectal cancers being the most studied. Information extraction and text classification emerged as predominant NLP tasks. A shift from rule-based to advanced machine learning techniques, particularly transformer-based models, was observed. The Dataset sizes used in existing studies varied widely. Key challenges included the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. Conclusion: NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research. However, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. Integration of NLP tools into clinical practice and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes.","['Muhammad Bilal', 'Ameer Hamza', 'Nadia Malik']",2024-10-29T16:17:07Z,http://arxiv.org/abs/2410.22180v1,Healthcare & Biomedical AI,Clinical NLP,this review addresses gaps in the existing literature . results showed a growing trend in NLP applications for cancer research . future work should focus on improving model generalizability .
A Scoping Review of Publicly Available Language Tasks in Clinical   Natural Language Processing,"Objective: to provide a scoping review of papers on clinical natural language processing (NLP) tasks that use publicly available electronic health record data from a cohort of patients. Materials and Methods: We searched six databases, including biomedical research and computer science literature database. A round of title/abstract screening and full-text screening were conducted by two reviewers. Our method followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. Results: A total of 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and 2021. We categorized the tasks by the type of NLP problems, including name entity recognition, summarization, and other NLP tasks. Some tasks were introduced with a topic of clinical decision support applications, such as substance abuse, phenotyping, cohort selection for clinical trial. We summarized the tasks by publication and dataset information. Discussion: The breadth of clinical NLP tasks keeps growing as the field of NLP evolves with advancements in language systems. However, gaps exist in divergent interests between general domain NLP community and clinical informatics community, and in generalizability of the data sources. We also identified issues in data selection and preparation including the lack of time-sensitive data, and invalidity of problem size and evaluation. Conclusions: The existing clinical NLP tasks cover a wide range of topics and the field will continue to grow and attract more attention from both general domain NLP and clinical informatics community. We encourage future work to incorporate multi-disciplinary collaboration, reporting transparency, and standardization in data preparation.","['Yanjun Gao', 'Dmitriy Dligach', 'Leslie Christensen', 'Samuel Tesch', 'Ryan Laffin', 'Dongfang Xu', 'Timothy Miller', 'Ozlem Uzuner', 'Matthew M Churpek', 'Majid Afshar']",2021-12-07T22:49:58Z,http://arxiv.org/abs/2112.05780v1,Healthcare & Biomedical AI,Clinical NLP,a total of 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and 2021 . gaps exist in divergent interests between general domain NLP community and clinical informatics community .
Natural Language Processing in Biomedicine: A Unified System   Architecture Overview,"In modern electronic medical records (EMR) much of the clinically important data - signs and symptoms, symptom severity, disease status, etc. - are not provided in structured data fields, but rather are encoded in clinician generated narrative text. Natural language processing (NLP) provides a means of ""unlocking"" this important data source for applications in clinical decision support, quality assurance, and public health. This chapter provides an overview of representative NLP systems in biomedicine based on a unified architectural view. A general architecture in an NLP system consists of two main components: background knowledge that includes biomedical knowledge resources and a framework that integrates NLP tools to process text. Systems differ in both components, which we will review briefly. Additionally, challenges facing current research efforts in biomedical NLP include the paucity of large, publicly available annotated corpora, although initiatives that facilitate data sharing, system evaluation, and collaborative work between researchers in clinical NLP are starting to emerge.","['Son Doan', 'Mike Conway', 'Tu Minh Phuong', 'Lucila Ohno-Machado']",2014-01-03T00:57:13Z,http://arxiv.org/abs/1401.0569v2,Healthcare & Biomedical AI,Clinical NLP,"natural language processing (NLP) provides a means of ""unlocking"" this important data source for applications in clinical decision support, quality assurance, and public health . challenges facing current research efforts in biomedical NLP include the paucity of large, publicly available corpora ."
"Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A   Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google,   Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric   Dataset","General-purpose clinical natural language processing (NLP) tools are increasingly used for the automatic labeling of clinical reports. However, independent evaluations for specific tasks, such as pediatric chest radiograph (CXR) report labeling, are limited. This study compares four commercial clinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP (GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and assertion detection in pediatric CXR reports. Additionally, CheXpert and CheXbert, two dedicated chest radiograph report labelers, were evaluated on the same task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR reports from a large academic pediatric hospital. Entities and assertion statuses (positive, negative, uncertain) from the findings and impression sections were extracted by the NLP systems, with impression section entities mapped to 12 disease categories and a No Findings category. CheXpert and CheXbert extracted the same 13 categories. Outputs were compared using Fleiss Kappa and accuracy against a consensus pseudo-ground truth. Significant differences were found in the number of extracted entities and assertion distributions across NLP systems. SP extracted 49,688 unique entities, GC 16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged around 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert achieved 56% accuracy. Considerable variability in performance highlights the need for careful validation and review before deploying NLP tools for clinical report labeling.","['Shruti Hegde', 'Mabon Manoj Ninan', 'Jonathan R. Dillman', 'Shireen Hayatghaibi', 'Lynn Babcock', 'Elanchezhian Somasundaram']",2025-05-29T03:16:18Z,http://arxiv.org/abs/2505.23030v1,Healthcare & Biomedical AI,Clinical NLP,"general-purpose clinical natural language processing (NLP) tools are increasingly used for the automatic labeling of clinical reports . however, independent evaluations for specific tasks, such as pediatric chest radiograph (CXR) report labeling, are limited ."
An Introduction to Natural Language Processing Techniques and Framework   for Clinical Implementation in Radiation Oncology,"Natural Language Processing (NLP) is a key technique for developing Medical Artificial Intelligence (AI) systems that leverage Electronic Health Record (EHR) data to build diagnostic and prognostic models. NLP enables the conversion of unstructured clinical text into structured data that can be fed into AI algorithms. The emergence of the transformer architecture and large language models (LLMs) has led to remarkable advances in NLP for various healthcare tasks, such as entity recognition, relation extraction, sentence similarity, text summarization, and question answering. In this article, we review the major technical innovations that underpin modern NLP models and present state-of-the-art NLP applications that employ LLMs in radiation oncology research. However, these LLMs are prone to many errors such as hallucinations, biases, and ethical violations, which necessitate rigorous evaluation and validation before clinical deployment. As such, we propose a comprehensive framework for assessing the NLP models based on their purpose and clinical fit, technical performance, bias and trust, legal and ethical implications, and quality assurance, prior to implementation in clinical radiation oncology. Our article aims to provide guidance and insights for researchers and clinicians who are interested in developing and using NLP models in clinical radiation oncology.","['Reza Khanmohammadi', 'Mohammad M. Ghassemi', 'Kyle Verdecchia', 'Ahmed I. Ghanem', 'Luo Bing', 'Indrin J. Chetty', 'Hassan Bagher-Ebadian', 'Farzan Siddiqui', 'Mohamed Elshaikh', 'Benjamin Movsas', 'Kundan Thind']",2023-11-03T19:32:35Z,http://arxiv.org/abs/2311.02205v2,Healthcare & Biomedical AI,Clinical NLP,natural language processing (NLP) is a key technique for developing AI systems . it enables the conversion of unstructured clinical text into structured data . this article reviews the major technical innovations that underpin modern NLP models .
Clinical Trial Information Extraction with BERT,Natural language processing (NLP) of clinical trial documents can be useful in new trial design. Here we identify entity types relevant to clinical trial design and propose a framework called CT-BERT for information extraction from clinical trial text. We trained named entity recognition (NER) models to extract eligibility criteria entities by fine-tuning a set of pre-trained BERT models. We then compared the performance of CT-BERT with recent baseline methods including attention-based BiLSTM and Criteria2Query. The results demonstrate the superiority of CT-BERT in clinical trial NLP.,"['Xiong Liu', 'Greg L. Hersch', 'Iya Khalil', 'Murthy Devarakonda']",2021-09-11T17:15:10Z,http://arxiv.org/abs/2110.10027v1,Healthcare & Biomedical AI,Clinical NLP,natural language processing (NLP) of clinical trial documents can be useful in new trial design . we propose a framework called CT-BERT for information extraction from clinical trial text .
An Empirical Evaluation of Prompting Strategies for Large Language   Models in Zero-Shot Clinical Natural Language Processing,"Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.","['Sonish Sivarajkumar', 'Mark Kelley', 'Alyssa Samolyk-Mazzanti', 'Shyam Visweswaran', 'Yanshan Wang']",2023-09-14T19:35:00Z,http://arxiv.org/abs/2309.08008v1,Healthcare & Biomedical AI,Clinical NLP,"large language models (LLMs) have shown remarkable capabilities in NLP . we need to design effective prompts that can guide them to perform specific clinical tasks . this is known as in-context learning, which requires understanding the strengths and weaknesses of different LLMs ."
A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP   Algorithms on Electronic Health Records,"Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.   Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.   Results: We manually annotated 200 and 161 clinical documents at UMN and MC, respectively. The corpora of the two institutes were found to have higher similarity between the target entities than the overall corpora. The CancerBERT models obtained the best performances among the independent test sets from two clinical institutes and the permutation test set. The CancerBERT model developed in one institute and further fine-tuned in another institute achieved reasonable performance compared to the model developed on local data (micro-F1: 0.925 vs 0.932).   Conclusions: The results indicate the CancerBERT model has the best learning ability and generalizability among the three types of clinical NLP models. The generalizability of the models was found to be correlated with the similarity of the target entities between the corpora.","['Sicheng Zhou', 'Nan Wang', 'Liwei Wang', 'Ju Sun', 'Anne Blaes', 'Hongfang Liu', 'Rui Zhang']",2023-03-15T08:44:07Z,http://arxiv.org/abs/2303.08448v1,Healthcare & Biomedical AI,Clinical NLP,this study evaluated the generalizability of clinical large language models . it used a breast cancer phenotype extraction task . the cancerBERT model has the best learning ability among the three types .
Towards Automatic Generation of Shareable Synthetic Clinical Notes Using   Neural Language Models,"Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.","['Oren Melamud', 'Chaitanya Shivade']",2019-05-16T19:14:18Z,http://arxiv.org/abs/1905.07002v2,Healthcare & Biomedical AI,Clinical NLP,de-identification methods have been shown to be susceptible to adversarial attacks . this work proposes to automatically generate synthetic clinical notes . the notes are more amenable to sharing using generative models trained on real de-identified records .
Clinical Relation Extraction Using Transformer-based Models,"The newly emerged transformer technology has a tremendous impact on NLP research. In the general English domain, transformer-based models have achieved state-of-the-art performances on various NLP benchmarks. In the clinical domain, researchers also have investigated transformer models for clinical applications. The goal of this study is to systematically explore three widely used transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical relation extraction and develop an open-source package with clinical pre-trained transformer-based models to facilitate information extraction in the clinical domain. We developed a series of clinical RE models based on three transformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these models using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2 challenges. We compared two classification strategies (binary vs. multi-class classification) and investigated two approaches to generate candidate relations in different experimental settings. In this study, we compared three transformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We demonstrated that the RoBERTa-clinical RE model achieved the best performance on the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2 dataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our results indicated that the binary classification strategy consistently outperformed the multi-class classification strategy for clinical relation extraction. Our methods and models are publicly available at https://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction. We believe this work will improve current practice on clinical relation extraction and other related NLP tasks in the biomedical domain.","['Xi Yang', 'Zehao Yu', 'Yi Guo', 'Jiang Bian', 'Yonghui Wu']",2021-07-19T15:15:51Z,http://arxiv.org/abs/2107.08957v2,Healthcare & Biomedical AI,Clinical NLP,the goal of this study is to systematically explore three widely used transformer-based models . we developed a series of clinical RE models based on three transformer architectures . the binary classification strategy consistently outperformed the multi-class classification strategy .
Implementing a Portable Clinical NLP System with a Common Data Model - a   Lisp Perspective,"This paper presents a Lisp architecture for a portable NLP system, termed LAPNLP, for processing clinical notes. LAPNLP integrates multiple standard, customized and in-house developed NLP tools. Our system facilitates portability across different institutions and data systems by incorporating an enriched Common Data Model (CDM) to standardize necessary data elements. It utilizes UMLS to perform domain adaptation when integrating generic domain NLP tools. It also features stand-off annotations that are specified by positional reference to the original document. We built an interval tree based search engine to efficiently query and retrieve the stand-off annotations by specifying positional requirements. We also developed a utility to convert an inline annotation format to stand-off annotations to enable the reuse of clinical text datasets with inline annotations. We experimented with our system on several NLP facilitated tasks including computational phenotyping for lymphoma patients and semantic relation extraction for clinical notes. These experiments showcased the broader applicability and utility of LAPNLP.","['Yuan Luo', 'Peter Szolovits']",2018-11-15T04:58:21Z,http://arxiv.org/abs/1811.06179v1,Healthcare & Biomedical AI,Clinical NLP,"this paper presents a Lisp architecture for a portable NLP system for processing clinical notes . LAPNLP integrates multiple standard, customized and in-house developed NLP tools . it features stand-off annotations that are specified by positional reference to the original document ."
Generative Large Language Models Are All-purpose Text Analytics Engines:   Text-to-text Learning Is All Your Need,"Objective To solve major clinical natural language processing (NLP) tasks using a unified text-to-text learning architecture based on a generative large language model (LLM) via prompt tuning. Methods We formulated 7 key clinical NLP tasks as text-to-text learning and solved them using one unified generative clinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with up to 20 billion parameters. We adopted soft prompts (i.e., trainable vectors) with frozen LLM, where the LLM parameters were not updated (i.e., frozen) and only the vectors of soft prompts were updated, known as prompt tuning. We added additional soft prompts as a prefix to the input layer, which were optimized during the prompt tuning. We evaluated the proposed method using 7 clinical NLP tasks and compared them with previous task-specific solutions based on Transformer models. Results and Conclusion The proposed approach achieved state-of-the-art performance for 5 out of 7 major clinical NLP tasks using one unified generative LLM. Our approach outperformed previous task-specific transformer models by ~3% for concept extraction and 7% for relation extraction applied to social determinants of health, 3.4% for clinical concept normalization, 3.4~10% for clinical abbreviation disambiguation, and 5.5~9% for natural language inference. Our approach also outperformed a previously developed prompt-based machine reading comprehension (MRC) model, GatorTron-MRC, for clinical concept and relation extraction. The proposed approach can deliver the ``one model for all`` promise from training to deployment using a unified generative LLM.","['Cheng Peng', 'Xi Yang', 'Aokun Chen', 'Zehao Yu', 'Kaleb E Smith', 'Anthony B Costa', 'Mona G Flores', 'Jiang Bian', 'Yonghui Wu']",2023-12-11T04:00:26Z,http://arxiv.org/abs/2312.06099v1,Healthcare & Biomedical AI,Clinical NLP,"7 key clinical NLP tasks were solved using one unified text-to-text learning architecture . we adopted soft prompts (i.e., trainable vectors) with frozen LLM . proposed approach achieved state-of-the-art performance for 5 out of 7 major clinical tasks ."
An Open Natural Language Processing Development Framework for EHR-based   Clinical Research: A case demonstration using the National COVID Cohort   Collaborative (N3C),"While we pay attention to the latest advances in clinical natural language processing (NLP), we can notice some resistance in the clinical and translational research community to adopt NLP models due to limited transparency, interpretability, and usability. In this study, we proposed an open natural language processing development framework. We evaluated it through the implementation of NLP algorithms for the National COVID Cohort Collaborative (N3C). Based on the interests in information extraction from COVID-19 related clinical notes, our work includes 1) an open data annotation process using COVID-19 signs and symptoms as the use case, 2) a community-driven ruleset composing platform, and 3) a synthetic text data generation workflow to generate texts for information extraction tasks without involving human subjects. The corpora were derived from texts from three different institutions (Mayo Clinic, University of Kentucky, University of Minnesota). The gold standard annotations were tested with a single institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706, and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets, respectively. The study as a consortium effort of the N3C NLP subgroup demonstrates the feasibility of creating a federated NLP algorithm development and benchmarking platform to enhance multi-institution clinical NLP study and adoption. Although we use COVID-19 as a use case in this effort, our framework is general enough to be applied to other domains of interest in clinical NLP.","['Sijia Liu', 'Andrew Wen', 'Liwei Wang', 'Huan He', 'Sunyang Fu', 'Robert Miller', 'Andrew Williams', 'Daniel Harris', 'Ramakanth Kavuluru', 'Mei Liu', 'Noor Abu-el-rub', 'Dalton Schutte', 'Rui Zhang', 'Masoud Rouhizadeh', 'John D. Osborne', 'Yongqun He', 'Umit Topaloglu', 'Stephanie S Hong', 'Joel H Saltz', 'Thomas Schaffter', 'Emily Pfaff', 'Christopher G. Chute', 'Tim Duong', 'Melissa A. Haendel', 'Rafael Fuentes', 'Peter Szolovits', 'Hua Xu', 'Hongfang Liu', 'National COVID Cohort Collaborative', 'Natural Language Processing', 'Subgroup', 'National COVID Cohort Collaborative']",2021-10-20T21:09:41Z,http://arxiv.org/abs/2110.10780v3,Healthcare & Biomedical AI,Clinical NLP,clinical and translational research community is reluctant to adopt NLP models . authors use COVID-19 signs and symptoms as a use case . framework is general enough to be applied to other domains of interest in clinical NLP .
HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural   Language Processing,"Deep learning algorithms are dependent on the availability of large-scale annotated clinical text datasets. The lack of such publicly available datasets is the biggest bottleneck for the development of clinical Natural Language Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep learning models to classify instances from new classes of which no training data have been seen before. Prompt-based learning is an emerging ZSL technique where we define task-based templates for NLP tasks. We developed a novel prompt-based clinical NLP framework called HealthPrompt and applied the paradigm of prompt-based learning on clinical texts. In this technique, rather than fine-tuning a Pre-trained Language Model(PLM), the task definitions are tuned by defining a prompt template. We performed an in-depth analysis of HealthPrompt on six different PLMs in a no-data setting. Our experiments prove that prompts effectively capture the context of clinical texts and perform remarkably well without any training data.","['Sonish Sivarajkumar', 'Yanshan Wang']",2022-03-09T21:44:28Z,http://arxiv.org/abs/2203.05061v1,Healthcare & Biomedical AI,Clinical NLP,lack of publicly available clinical text datasets is biggest bottleneck for NLP . lack of training data has been seen before in deep learning . we developed a novel prompt-based clinical NLP framework called HealthPrompt .
Systematic Literature Review on Clinical Trial Eligibility Matching,"Clinical trial eligibility matching is a critical yet often labor-intensive and error-prone step in medical research, as it ensures that participants meet precise criteria for safe and reliable study outcomes. Recent advances in Natural Language Processing (NLP) have shown promise in automating and improving this process by rapidly analyzing large volumes of unstructured clinical text and structured electronic health record (EHR) data. In this paper, we present a systematic overview of current NLP methodologies applied to clinical trial eligibility screening, focusing on data sources, annotation practices, machine learning approaches, and real-world implementation challenges. A comprehensive literature search (spanning Google Scholar, Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each demonstrating the potential of techniques such as rule-based systems, named entity recognition, contextual embeddings, and ontology-based normalization to enhance patient matching accuracy. While results indicate substantial improvements in screening efficiency and precision, limitations persist regarding data completeness, annotation consistency, and model scalability across diverse clinical domains. The review highlights how explainable AI and standardized ontologies can bolster clinician trust and broaden adoption. Looking ahead, further research into advanced semantic and temporal representations, expanded data integration, and rigorous prospective evaluations is necessary to fully realize the transformative potential of NLP in clinical trial recruitment.","['Muhammad Talha Sharif', 'Abdul Rehman']",2025-03-02T11:45:50Z,http://arxiv.org/abs/2503.00863v1,Healthcare & Biomedical AI,Clinical NLP,clinical trial eligibility matching is a critical step in medical research . it ensures that participants meet precise criteria for safe and reliable study outcomes . recent advances in natural language processing (NLP) have shown promise .
Distinguishing Clinical Sentiment: The Importance of Domain Adaptation   in Psychiatric Patient Health Records,"Recently natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs). Sentiment analysis, although widely used in non-medical areas for improving decision making, has been studied minimally in the clinical setting. In this study, we undertook, to our knowledge, the first domain adaptation of sentiment analysis to psychiatric EHRs by defining psychiatric clinical sentiment, performing an annotation project, and evaluating multiple sentence-level sentiment machine learning (ML) models. Results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity, and that the definition of clinical sentiment that we provide is learnable with relatively small amounts of training data. This project is an initial step towards further refining sentiment analysis methods for clinical use. Our long-term objective is to incorporate the results of this project as part of a machine learning model that predicts inpatient readmission risk. We hope that this work will initiate a discussion concerning domain adaptation of sentiment analysis to the clinical setting.","['Eben Holderness', 'Philip Cawkwell', 'Kirsten Bolton', 'James Pustejovsky', 'Mei-Hua Hall']",2019-04-05T18:33:22Z,http://arxiv.org/abs/1904.03225v1,Healthcare & Biomedical AI,Clinical NLP,natural language processing (NLP) tools have been developed to identify and extract salient risk indicators in electronic health records (EHRs) this study is the first domain adaptation of sentiment analysis to psychiatric EHRs . results indicate that off-the-shelf sentiment analysis tools fail in identifying clinically positive or negative polarity .
Publicly Available Clinical BERT Embeddings,"Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.","['Emily Alsentzer', 'John R. Murphy', 'Willie Boag', 'Wei-Hung Weng', 'Di Jin', 'Tristan Naumann', 'Matthew B. A. McDermott']",2019-04-06T00:34:39Z,http://arxiv.org/abs/1904.03323v3,Healthcare & Biomedical AI,Clinical NLP,"contextual word embedding models have dramatically improved performance for many NLP tasks . but, these models have been minimally explored on specialty corpora, such as clinical text . in the clinical domain, no publicly-available pre-trained BERT models yet exist ."
Energy-based Generative Models for Target-specific Drug Discovery,"Drug targets are the main focus of drug discovery due to their key role in disease pathogenesis. Computational approaches are widely applied to drug development because of the increasing availability of biological molecular datasets. Popular generative approaches can create new drug molecules by learning the given molecule distributions. However, these approaches are mostly not for target-specific drug discovery. We developed an energy-based probabilistic model for computational target-specific drug discovery. Results show that our proposed TagMol can generate molecules with similar binding affinity scores as real molecules. GAT-based models showed faster and better learning relative to GCN baseline models.","['Junde Li', 'Collin Beaudoin', 'Swaroop Ghosh']",2022-12-05T16:41:36Z,http://arxiv.org/abs/2212.02404v1,Healthcare & Biomedical AI,Drug Discovery,drug targets are the main focus of drug discovery due to their key role in disease pathogenesis . popular generative approaches can create new drug molecules by learning molecule distributions . but these approaches are mostly not for target-specific drug discovery .
"Graph-structured Small Molecule Drug Discovery Through Deep Learning:   Progress, Challenges, and Opportunities","Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization and provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in graph-structured small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for small molecule drug discovery.","['Kun Li', 'Yida Xiong', 'Hongzhi Zhang', 'Xiantao Cai', 'Jia Wu', 'Bo Du', 'Wenbin Hu']",2025-02-13T05:24:52Z,http://arxiv.org/abs/2502.08975v2,Healthcare & Biomedical AI,Drug Discovery,"small molecule drugs are widely used to treat various diseases . they are a critical component of drug discovery . this paper aims to summarize and generalize the recent key tasks . it discusses key challenges, such as interpretability and out-of-distribution"
NeuroCADR: Drug Repurposing to Reveal Novel Anti-Epileptic Drug   Candidates Through an Integrated Computational Approach,"Drug repurposing is an emerging approach for drug discovery involving the reassignment of existing drugs for novel purposes. An alternative to the traditional de novo process of drug development, repurposed drugs are faster, cheaper, and less failure prone than drugs developed from traditional methods. Recently, drug repurposing has been performed in silico, in which databases of drugs and chemical information are used to determine interactions between target proteins and drug molecules to identify potential drug candidates. A proposed algorithm is NeuroCADR, a novel system for drug repurposing via a multi-pronged approach consisting of k-nearest neighbor algorithms (KNN), random forest classification, and decision trees. Data was sourced from several databases consisting of interactions between diseases, symptoms, genes, and affiliated drug molecules, which were then compiled into datasets expressed in binary. The proposed method displayed a high level of accuracy, outperforming nearly all in silico approaches. NeuroCADR was performed on epilepsy, a condition characterized by seizures, periods of time with bursts of uncontrolled electrical activity in brain cells. Existing drugs for epilepsy can be ineffective and expensive, revealing a need for new antiepileptic drugs. NeuroCADR identified novel drug candidates for epilepsy that can be further approved through clinical trials. The algorithm has the potential to determine possible drug combinations to prescribe a patient based on a patient's prior medical history. This project examines NeuroCADR, a novel approach to computational drug repurposing capable of revealing potential drug candidates in neurological diseases such as epilepsy.",['Srilekha Mamidala'],2023-09-04T03:21:43Z,http://arxiv.org/abs/2309.13047v1,Healthcare & Biomedical AI,Drug Discovery,"drug repurposing is an emerging approach for drug discovery . it involves the reassignment of existing drugs for novel purposes . neuroCADR was performed on epilepsy, a condition characterized by seizures ."
Zero-shot Learning of Drug Response Prediction for Preclinical Drug   Screening,"Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug responses for novel compounds, leading to a general performance improvement of 5-10\% in the preclinical drug screening phase. The significance of this solution resides in its potential to accelerate the drug discovery process, improve drug candidate assessment, and facilitate the success of drug discovery.","['Kun Li', 'Yong Luo', 'Xiantao Cai', 'Wenbin Hu', 'Bo Du']",2023-10-05T05:55:41Z,http://arxiv.org/abs/2310.12996v1,Healthcare & Biomedical AI,Drug Discovery,"conventional deep learning methods typically employ supervised learning for drug response prediction . but practical applications in preclinical drug screening require that DRP models predict responses for novel compounds . a zero-shot learning solution is proposed for the task, called a multi-branch multi"
Drug-target interaction prediction by integrating heterogeneous   information with mutual attention network,"Identification of drug-target interactions is an indispensable part of drug discovery. While conventional shallow machine learning and recent deep learning methods based on chemogenomic properties of drugs and target proteins have pushed this prediction performance improvement to a new level, these methods are still difficult to adapt to novel structures. Alternatively, large-scale biological and pharmacological data provide new ways to accelerate drug-target interaction prediction. Here, we propose DrugMAN, a deep learning model for predicting drug-target interaction by integrating multiplex heterogeneous functional networks with a mutual attention network (MAN). DrugMAN uses a graph attention network-based integration algorithm to learn network-specific low-dimensional features for drugs and target proteins by integrating four drug networks and seven gene/protein networks, respectively. DrugMAN then captures interaction information between drug and target representations by a mutual attention network to improve drug-target prediction. DrugMAN achieves the best prediction performance under four different scenarios, especially in real-world scenarios. DrugMAN spotlights heterogeneous information to mine drug-target interactions and can be a powerful tool for drug discovery and drug repurposing.","['Yuanyuan Zhang', 'Yingdong Wang', 'Chaoyong Wu', 'Lingmin Zhana', 'Aoyi Wang', 'Caiping Cheng', 'Jinzhong Zhao', 'Wuxia Zhang', 'Jianxin Chen', 'Peng Li']",2024-04-03T02:48:22Z,http://arxiv.org/abs/2404.03516v1,Healthcare & Biomedical AI,Drug Discovery,drugMAN is a deep learning model for predicting drug-target interaction . it integrates multiplex heterogeneous functional networks with a mutual attention network . drugMAN achieves the best prediction performance under four different scenarios .
Novel prediction methods for virtual drug screening,"Drug development is an expensive and time-consuming process where thousands of chemical compounds are being tested in order to find those possessing drug-like properties while being safe and effective. One of key parts of the early drug discovery process has become virtual drug screening -- a method used to narrow down search for potential drugs by running computer simulations of drug-target interactions. As these methods are known to demand huge amounts of computational power to get accurate results, prediction models based on machine learning techniques became a popular solution requiring less computational power as well as offering the ability to generate novel chemical structures for further research. Deep learning is to stay in drug discovery but has a long way to go. Only in the past few years with increases in computing power have researchers really started to embrace the potential of neural networks in various stages of the drug discovery process. While prediction methods promise great perspective in the future development of drug discovery they open new questions and challenges that still have to be solved.",['Josip Mesarić'],2022-02-14T11:41:39Z,http://arxiv.org/abs/2202.06635v1,Healthcare & Biomedical AI,Drug Discovery,virtual drug screening is a method used to narrow down search for potential drugs . machine learning is to stay in drug discovery but has a long way to go . researchers have started to embrace the potential of neural networks in various stages of drug discovery .
Causal Intervention for Measuring Confidence in Drug-Target Interaction   Prediction,"Identifying and discovering drug-target interactions(DTIs) are vital steps in drug discovery and development. They play a crucial role in assisting scientists in finding new drugs and accelerating the drug development process. Recently, knowledge graph and knowledge graph embedding (KGE) models have made rapid advancements and demonstrated impressive performance in drug discovery. However, such models lack authenticity and accuracy in drug target identification, leading to an increased misjudgment rate and reduced drug development efficiency. To address these issues, we focus on the problem of drug-target interactions, with knowledge mapping as the core technology. Specifically, a causal intervention-based confidence measure is employed to assess the triplet score to improve the accuracy of the drug-target interaction prediction model. Experimental results demonstrate that the developed confidence measurement method based on causal intervention can significantly enhance the accuracy of DTI link prediction, particularly for high-precision models. The predicted results are more valuable in guiding the design and development of subsequent drug development experiments, thereby significantly improving the efficiency of drug development.","['Wenting Ye', 'Chen Li', 'Yang Xie', 'Wen Zhang', 'Hong-Yu Zhang', 'Bowen Wang', 'Debo Cheng', 'Zaiwen Feng']",2023-05-31T13:13:45Z,http://arxiv.org/abs/2306.00041v2,Healthcare & Biomedical AI,Drug Discovery,"identifying and discovering drug-target interactions (DTIs) are vital steps in drug discovery and development . knowledge graph and knowledge graph embedding (KGE) models have made rapid advancements . however, such models lack authenticity and accuracy in drug target identification ."
Graph-augmented Convolutional Networks on Drug-Drug Interactions   Prediction,"We propose an end-to-end model to predict drug-drug interactions (DDIs) by employing graph-augmented convolutional networks. And this is implemented by combining graph CNN with an attentive pooling network to extract structural relations between drug pairs and make DDI predictions. The experiment results suggest a desirable performance achieving ROC at 0.988, F1-score at 0.956, and AUPR at 0.986. Besides, the model can tell how the two DDI drugs interact structurally by varying colored atoms. And this may be helpful for drug design during drug discovery.","['Yi Zhong', 'Xueyu Chen', 'Yu Zhao', 'Xiaoming Chen', 'Tingfang Gao', 'Zuquan Weng']",2019-12-08T15:43:42Z,http://arxiv.org/abs/1912.03702v1,Healthcare & Biomedical AI,Drug Discovery,graph-augmented convolutional networks are used to predict drug-drug interactions . the model can tell how the two DDI drugs interact structurally by varying colored atoms .
CardiGraphormer: Unveiling the Power of Self-Supervised Learning in   Revolutionizing Drug Discovery,"In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential applications in drug discovery and drug interactions are vast, from identifying new drug targets to predicting drug-to-drug interactions and enabling novel drug discovery. This innovative approach provides an AI-enhanced methodology in drug development, utilizing SSL combined with GNNs to overcome existing limitations and pave the way for a richer exploration of the vast combinatorial chemical space in drug discovery.",['Abhijit Gupta'],2023-07-03T08:58:32Z,http://arxiv.org/abs/2307.00859v4,Healthcare & Biomedical AI,Drug Discovery,"cardiGraphormer synergizes self-supervised learning (SSL) and Graph Neural Networks (GNNs) it employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability . cardigraphormer"
Drug-target affinity prediction method based on consistent expression of   heterogeneous data,"The first step in drug discovery is finding drug molecule moieties with medicinal activity against specific targets. Therefore, it is crucial to investigate the interaction between drug-target proteins and small chemical molecules. However, traditional experimental methods for discovering potential small drug molecules are labor-intensive and time-consuming. There is currently a lot of interest in building computational models to screen small drug molecules using drug molecule-related databases. In this paper, we propose a method for predicting drug-target binding affinity using deep learning models. This method uses a modified GRU and GNN to extract features from the drug-target protein sequences and the drug molecule map, respectively, to obtain their feature vectors. The combined vectors are used as vector representations of drug-target molecule pairs and then fed into a fully connected network to predict drug-target binding affinity. This proposed model demonstrates its accuracy and effectiveness in predicting drug-target binding affinity on the DAVIS and KIBA datasets.",['Boyuan Liu'],2022-11-13T02:58:03Z,http://arxiv.org/abs/2211.06792v1,Healthcare & Biomedical AI,Drug Discovery,"the first step in drug discovery is finding drug molecule moieties with medicinal activity against specific targets . traditional experimental methods for discovering small drug molecules are labor-intensive and time-consuming . in this paper, we propose a method for predicting drug-target binding affinity using"
Quantum-machine-assisted Drug Discovery: Survey and Perspective,"Drug discovery and development is a highly complex and costly endeavor, typically requiring over a decade and substantial financial investment to bring a new drug to market. Traditional computer-aided drug design (CADD) has made significant progress in accelerating this process, but the development of quantum computing offers potential due to its unique capabilities. This paper discusses the integration of quantum computing into drug discovery and development, focusing on how quantum technologies might accelerate and enhance various stages of the drug development cycle. Specifically, we explore the application of quantum computing in addressing challenges related to drug discovery, such as molecular simulation and the prediction of drug-target interactions, as well as the optimization of clinical trial outcomes. By leveraging the inherent capabilities of quantum computing, we might be able to reduce the time and cost associated with bringing new drugs to market, ultimately benefiting public health.","['Yidong Zhou', 'Jintai Chen', 'Jinglei Cheng', 'Gopal Karemore', 'Marinka Zitnik', 'Frederic T. Chong', 'Junyu Liu', 'Tianfan Fu', 'Zhiding Liang']",2024-08-24T05:38:31Z,http://arxiv.org/abs/2408.13479v4,Healthcare & Biomedical AI,Drug Discovery,the development of quantum computing offers potential due to its unique capabilities . drug discovery and development is a highly complex and costly endeavor . quantum computing could reduce the time and cost associated with bringing new drugs to market .
Explainable Artificial Intelligence for Drug Discovery and Development   -- A Comprehensive Survey,"The field of drug discovery has experienced a remarkable transformation with the advent of artificial intelligence (AI) and machine learning (ML) technologies. However, as these AI and ML models are becoming more complex, there is a growing need for transparency and interpretability of the models. Explainable Artificial Intelligence (XAI) is a novel approach that addresses this issue and provides a more interpretable understanding of the predictions made by machine learning models. In recent years, there has been an increasing interest in the application of XAI techniques to drug discovery. This review article provides a comprehensive overview of the current state-of-the-art in XAI for drug discovery, including various XAI methods, their application in drug discovery, and the challenges and limitations of XAI techniques in drug discovery. The article also covers the application of XAI in drug discovery, including target identification, compound design, and toxicity prediction. Furthermore, the article suggests potential future research directions for the application of XAI in drug discovery. The aim of this review article is to provide a comprehensive understanding of the current state of XAI in drug discovery and its potential to transform the field.","['Roohallah Alizadehsani', 'Solomon Sunday Oyelere', 'Sadiq Hussain', 'Rene Ripardo Calixto', 'Victor Hugo C. de Albuquerque', 'Mohamad Roshanzamir', 'Mohamed Rahouti', 'Senthil Kumar Jagatheesaperumal']",2023-09-21T15:36:06Z,http://arxiv.org/abs/2309.12177v2,Healthcare & Biomedical AI,Drug Discovery,explainable artificial intelligence (XAI) provides a more interpretable understanding of the predictions made by machine learning models . the aim of this review article is to provide a comprehensive overview of the current state-of-the-art in XAI for drug discovery .
Mitigating cold start problems in drug-target affinity prediction with   interaction knowledge transferring,"Motivation: Predicting the drug-target interaction is crucial for drug discovery as well as drug repurposing. Machine learning is commonly used in drug-target affinity (DTA) problem. However, machine learning model faces the cold-start problem where the model performance drops when predicting the interaction of a novel drug or target. Previous works try to solve the cold start problem by learning the drug or target representation using unsupervised learning. While the drug or target representation can be learned in an unsupervised manner, it still lacks the interaction information, which is critical in drug-target interaction. Results: To incorporate the interaction information into the drug and protein interaction, we proposed using transfer learning from chemical-chemical interaction (CCI) and protein-protein interaction (PPI) task to drug-target interaction task. The representation learned by CCI and PPI tasks can be transferred smoothly to the DTA task due to the similar nature of the tasks. The result on the drug-target affinity datasets shows that our proposed method has advantages compared to other pretraining methods in the DTA task.","['Tri Minh Nguyen', 'Thin Nguyen', 'Truyen Tran']",2022-01-16T09:28:52Z,http://arxiv.org/abs/2202.01195v1,Healthcare & Biomedical AI,Drug Discovery,machine learning is commonly used in drug-target affinity (DTA) problem . but performance drops when predicting the interaction of a novel drug or target .
Artificial Intelligence for Drug Discovery: Are We There Yet?,"Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate human intervention at later pipeline stages.","['Catrin Hasselgren', 'Tudor I. Oprea']",2023-07-13T01:51:26Z,http://arxiv.org/abs/2307.06521v1,Healthcare & Biomedical AI,Drug Discovery,"drug discovery is adapting to new technologies such as data science, informatics, and artificial intelligence (AI) successful drug discovery requires optimizing properties related to pharmacodynamics and clinical outcomes . generative chemistry, machine learning, and multi-property optimization"
LIDDIA: Language-based Intelligent Drug Discovery Agent,"Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.","['Reza Averly', 'Frazier N. Baker', 'Xia Ning']",2025-02-19T18:56:12Z,http://arxiv.org/abs/2502.13959v1,Healthcare & Biomedical AI,Drug Discovery,"LIDDiA is an autonomous agent capable of intelligently navigating the drug discovery process . it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets . the agent can identify promising novel drug candidates on EGFR, a critical target for cancer"
Investigating drug translational research using PubMed articles,"Drug research and development are embracing translational research for its potential to increase the number of drugs successfully brought to clinical applications. Using the publicly available PubMed database, we sought to describe the status of drug translational research, the distribution of translational lags for all drugs as well as the collaborations between basic science and clinical science in drug research. For each drug, an indicator called Translational Lag was proposed to quantify the interval time from its first PubMed article to its first clinical article. Meanwhile, the triangle of biomedicine was also used to visualize the status and multidisciplinary collaboration of drug translational research. The results showed that only 18.1% (24,410) of drugs/compounds had been successfully entering clinical research. It averagely took 14.38 years (interquartile range, 4 to 21 years) for a drug from the initial basic discovery to its first clinical research. In addition, the results also revealed that, in drug research, there was rare cooperation between basic science and clinical science, which were more inclined to cooperate within disciplines.","['Xin Li', 'Xuli Tang']",2023-07-18T08:19:53Z,http://arxiv.org/abs/2307.09056v1,Healthcare & Biomedical AI,Drug Discovery,"only 18.1% (24,410) of drugs/compounds had been successfully entering clinical research . it averaged 14.38 years (interquartile range, 4 to 21 years) for a drug from initial discovery to its first clinical research."
A Cross-Field Fusion Strategy for Drug-Target Interaction Prediction,"Drug-target interaction (DTI) prediction is a critical component of the drug discovery process. In the drug development engineering field, predicting novel drug-target interactions is extremely crucial.However, although existing methods have achieved high accuracy levels in predicting known drugs and drug targets, they fail to utilize global protein information during DTI prediction. This leads to an inability to effectively predict interaction the interactions between novel drugs and their targets. As a result, the cross-field information fusion strategy is employed to acquire local and global protein information. Thus, we propose the siamese drug-target interaction SiamDTI prediction method, which utilizes a double channel network structure for cross-field supervised learning.Experimental results on three benchmark datasets demonstrate that SiamDTI achieves higher accuracy levels than other state-of-the-art (SOTA) methods on novel drugs and targets.Additionally, SiamDTI's performance with known drugs and targets is comparable to that of SOTA approachs. The code is available at https://anonymous.4open.science/r/DDDTI-434D.","['Hongzhi Zhang', 'Xiuwen Gong', 'Shirui Pan', 'Jia Wu', 'Bo Du', 'Wenbin Hu']",2024-05-23T13:25:20Z,http://arxiv.org/abs/2405.14545v1,Healthcare & Biomedical AI,Drug Discovery,drug-target interaction (DTI) prediction is a critical component of the drug discovery process . existing methods have high accuracy levels in predicting known drugs and drug targets . but they fail to utilize global protein information .
Emerging Opportunities of Using Large Language Models for Translation   Between Drug Molecules and Indications,"A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing LLMs on this new task. Specifically, we consider nine variations of the T5 LLM and evaluate them on two public datasets obtained from ChEMBL and DrugBank. Our experiments show the early results of using LLMs for this task and provide a perspective on the state-of-the-art. We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of generative AI.","['David Oniani', 'Jordan Hilsman', 'Chengxi Zang', 'Junmei Wang', 'Lianjin Cai', 'Jan Zawala', 'Yanshan Wang']",2024-02-14T21:33:13Z,http://arxiv.org/abs/2402.09588v2,Healthcare & Biomedical AI,Drug Discovery,"a drug molecule is a substance that changes the organism's mental or physical state . every approved drug has an indication, which refers to the therapeutic use of that drug . there is still a gap in research regarding their application in facilitating the translation ."
Scaffold-Induced Molecular Graph (SIMG): Effective Graph Sampling   Methods for High-Throughput Computational Drug Discovery,"Scaffold based drug discovery (SBDD) is a technique for drug discovery which pins chemical scaffolds as the framework of design. Scaffolds, or molecular frameworks, organize the design of compounds into local neighborhoods. We formalize scaffold based drug discovery into a network design. Utilizing docking data from SARS-CoV-2 virtual screening studies and JAK2 kinase assay data, we showcase how a scaffold based conception of chemical space is intuitive for design. Lastly, we highlight the utility of scaffold based networks for chemical space as a potential solution to the intractable enumeration problem of chemical space by working inductively on local neighborhoods.","['Austin Clyde', 'Ashka Shah', 'Max Zvyagin', 'Arvind Ramanathan', 'Rick Stevens']",2021-09-10T17:47:31Z,http://arxiv.org/abs/2109.05012v1,Healthcare & Biomedical AI,Drug Discovery,"Scaffold based drug discovery (SBDD) is a technique for drug discovery . it pins chemical scaffolds as the framework of design . a scaffold based conception of chemical space is intuitive for design, authors say ."
Accelerating drug discovery with Artificial: a whole-lab orchestration   and scheduling system for self-driving labs,"Self-driving labs are transforming drug discovery by enabling automated, AI-guided experimentation, but they face challenges in orchestrating complex workflows, integrating diverse instruments and AI models, and managing data efficiently. Artificial addresses these issues with a comprehensive orchestration and scheduling system that unifies lab operations, automates workflows, and integrates AI-driven decision-making. By incorporating AI/ML models like NVIDIA BioNeMo - which facilitates molecular interaction prediction and biomolecular analysis - Artificial enhances drug discovery and accelerates data-driven research. Through real-time coordination of instruments, robots, and personnel, the platform streamlines experiments, enhances reproducibility, and advances drug discovery.","['Yao Fehlis', 'Paul Mandel', 'Charles Crain', 'Betty Liu', 'David Fuller']",2025-04-01T17:22:50Z,http://arxiv.org/abs/2504.00986v1,Healthcare & Biomedical AI,Drug Discovery,"self-driving labs are transforming drug discovery by enabling automated, AI-guided experimentation . but they face challenges in orchestrating complex workflows, integrating diverse instruments and AI models, and managing data efficiently . Artificial addresses these issues with a comprehensive orchestration and scheduling system that unifies lab operations . it enhances drug discovery and accelerates data-driven research ."
SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer,"Clinically, the accurate annotation of lesions/tissues can significantly facilitate the disease diagnosis. For example, the segmentation of optic disc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the segmentation of skin lesions on dermoscopic images is helpful to the melanoma diagnosis, etc. With the advancement of deep learning techniques, a wide range of methods proved the lesions/tissues segmentation can also facilitate the automated disease diagnosis models. However, existing methods are limited in the sense that they can only capture static regional correlations in the images. Inspired by the global and dynamic nature of Vision Transformer, in this paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans) to transfer the segmentation knowledge to the disease diagnosis network. Specifically, we first propose an asymmetric multi-scale interaction strategy to correlate each single low-level diagnosis feature with multi-scale segmentation features. Then, an effective strategy called SeA-block is adopted to vitalize diagnosis feature via correlated segmentation features. To model the segmentation-diagnosis interaction, SeA-block first embeds the diagnosis feature based on the segmentation information via the encoder, and then transfers the embedding back to the diagnosis feature space by a decoder. Experimental results demonstrate that SeATrans surpasses a wide range of state-of-the-art (SOTA) segmentation-assisted diagnosis methods on several disease diagnosis tasks.","['Junde Wu', 'Huihui Fang', 'Fangxin Shang', 'Dalu Yang', 'Zhaowei Wang', 'Jing Gao', 'Yehui Yang', 'Yanwu Xu']",2022-06-12T15:10:33Z,http://arxiv.org/abs/2206.05763v2,Healthcare & Biomedical AI,Disease Diagnosis Models,"clinically, the accurate annotation of lesions/tissues can significantly facilitate the disease diagnosis . existing methods are limited in the sense that they can only capture static regional correlations in the images . experimental results demonstrate that SeATrans surpasses a wide range of state-of-the-art (SOTA) segmentation-assisted diagnosis methods ."
Online Disease Self-diagnosis with Inductive Heterogeneous Graph   Convolutional Networks,"We propose a Healthcare Graph Convolutional Network (HealGCN) to offer disease self-diagnosis service for online users based on Electronic Healthcare Records (EHRs). Two main challenges are focused in this paper for online disease diagnosis: (1) serving cold-start users via graph convolutional networks and (2) handling scarce clinical description via a symptom retrieval system. To this end, we first organize the EHR data into a heterogeneous graph that is capable of modeling complex interactions among users, symptoms and diseases, and tailor the graph representation learning towards disease diagnosis with an inductive learning paradigm. Then, we build a disease self-diagnosis system with a corresponding EHR Graph-based Symptom Retrieval System (GraphRet) that can search and provide a list of relevant alternative symptoms by tracing the predefined meta-paths. GraphRet helps enrich the seed symptom set through the EHR graph when confronting users with scarce descriptions, hence yield better diagnosis accuracy. At last, we validate the superiority of our model on a large-scale EHR dataset.","['Zifeng Wang', 'Rui Wen', 'Xi Chen', 'Shilei Cao', 'Shao-Lun Huang', 'Buyue Qian', 'Yefeng Zheng']",2020-09-06T01:32:14Z,http://arxiv.org/abs/2009.02625v2,Healthcare & Biomedical AI,Disease Diagnosis Models,we propose a Healthcare Graph Convolutional Network (HealGCN) to offer disease self-diagnosis service . the network is based on electronic healthcare records (ehrs) and a symptom retrieval system . we build a system with corresponding EHR Graph-based Symptom Retrieval System (GraphRet)
Towards Knowledge-Infused Automated Disease Diagnosis Assistant,"With the advancement of internet communication and telemedicine, people are increasingly turning to the web for various healthcare activities. With an ever-increasing number of diseases and symptoms, diagnosing patients becomes challenging. In this work, we build a diagnosis assistant to assist doctors, which identifies diseases based on patient-doctor interaction. During diagnosis, doctors utilize both symptomatology knowledge and diagnostic experience to identify diseases accurately and efficiently. Inspired by this, we investigate the role of medical knowledge in disease diagnosis through doctor-patient interaction. We propose a two-channel, knowledge-infused, discourse-aware disease diagnosis model (KI-DDI), where the first channel encodes patient-doctor communication using a transformer-based encoder, while the other creates an embedding of symptom-disease using a graph attention network (GAT). In the next stage, the conversation and knowledge graph embeddings are infused together and fed to a deep neural network for disease identification. Furthermore, we first develop an empathetic conversational medical corpus comprising conversations between patients and doctors, annotated with intent and symptoms information. The proposed model demonstrates a significant improvement over the existing state-of-the-art models, establishing the crucial roles of (a) a doctor's effort for additional symptom extraction (in addition to patient self-report) and (b) infusing medical knowledge in identifying diseases effectively. Many times, patients also show their medical conditions, which acts as crucial evidence in diagnosis. Therefore, integrating visual sensory information would represent an effective avenue for enhancing the capabilities of diagnostic assistants.","['Mohit Tomar', 'Abhisek Tiwari', 'Sriparna Saha']",2024-05-18T05:18:50Z,http://arxiv.org/abs/2405.11181v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"we build a diagnosis assistant to assist doctors, which identifies diseases based on patient-doctor interaction . doctors utilize both symptomatology knowledge and diagnostic experience to identify diseases accurately and efficiently . we propose a two-channel, knowledge-infused, discourse-aware disease diagnosis model ."
Paddy Disease Detection and Classification Using Computer Vision   Techniques: A Mobile Application to Detect Paddy Disease,"Plant diseases significantly impact our food supply, causing problems for farmers, economies reliant on agriculture, and global food security. Accurate and timely plant disease diagnosis is crucial for effective treatment and minimizing yield losses. Despite advancements in agricultural technology, a precise and early diagnosis remains a challenge, especially in underdeveloped regions where agriculture is crucial and agricultural experts are scarce. However, adopting Deep Learning applications can assist in accurately identifying diseases without needing plant pathologists. In this study, the effectiveness of various computer vision models for detecting paddy diseases is evaluated and proposed the best deep learning-based disease detection system. Both classification and detection using the Paddy Doctor dataset, which contains over 20,000 annotated images of paddy leaves for disease diagnosis are tested and evaluated. For detection, we utilized the YOLOv8 model-based model were used for paddy disease detection and CNN models and the Vision Transformer were used for disease classification. The average mAP50 of 69% for detection tasks was achieved and the Vision Transformer classification accuracy was 99.38%. It was found that detection models are effective at identifying multiple diseases simultaneously with less computing power, whereas classification models, though computationally expensive, exhibit better performance for classifying single diseases. Additionally, a mobile application was developed to enable farmers to identify paddy diseases instantly. Experiments with the app showed encouraging results in utilizing the trained models for both disease classification and treatment guidance.","['Bimarsha Khanal', 'Paras Poudel', 'Anish Chapagai', 'Bijan Regmi', 'Sitaram Pokhrel', 'Salik Ram Khanal']",2024-12-08T17:08:27Z,http://arxiv.org/abs/2412.05996v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"deep learning applications can assist in accurately identifying diseases without needing plant pathologists . the paddy doctor dataset contains over 20,000 annotated images of paddy leaves . detection models are effective at identifying multiple diseases simultaneously with less computing power ."
Lifelong Learning based Disease Diagnosis on Clinical Notes,"Current deep learning based disease diagnosis systems usually fall short in catastrophic forgetting, i.e., directly fine-tuning the disease diagnosis model on new tasks usually leads to abrupt decay of performance on previous tasks. What is worse, the trained diagnosis system would be fixed once deployed but collecting training data that covers enough diseases is infeasible, which inspires us to develop a lifelong learning diagnosis system. In this work, we propose to adopt attention to combine medical entities and context, embedding episodic memory and consolidation to retain knowledge, such that the learned model is capable of adapting to sequential disease-diagnosis tasks. Moreover, we establish a new benchmark, named Jarvis-40, which contains clinical notes collected from various hospitals. Our experiments show that the proposed method can achieve state-of-the-art performance on the proposed benchmark.","['Zifeng Wang', 'Yifan Yang', 'Rui Wen', 'Xi Chen', 'Shao-Lun Huang', 'Yefeng Zheng']",2021-02-27T09:23:57Z,http://arxiv.org/abs/2103.00165v2,Healthcare & Biomedical AI,Disease Diagnosis Models,"current deep learning based disease diagnosis systems usually fall short in catastrophic forgetting . the trained diagnosis system would be fixed once deployed but collecting training data is infeasible . we propose to combine medical entities and context, embedding episodic memory ."
HeteroMed: Heterogeneous Information Network for Medical Diagnosis,"With the recent availability of Electronic Health Records (EHR) and great opportunities they offer for advancing medical informatics, there has been growing interest in mining EHR for improving quality of care. Disease diagnosis due to its sensitive nature, huge costs of error, and complexity has become an increasingly important focus of research in past years. Existing studies model EHR by capturing co-occurrence of clinical events to learn their latent embeddings. However, relations among clinical events carry various semantics and contribute differently to disease diagnosis which gives precedence to a more advanced modeling of heterogeneous data types and relations in EHR data than existing solutions. To address these issues, we represent how high-dimensional EHR data and its rich relationships can be suitably translated into HeteroMed, a heterogeneous information network for robust medical diagnosis. Our modeling approach allows for straightforward handling of missing values and heterogeneity of data. HeteroMed exploits metapaths to capture higher level and semantically important relations contributing to disease diagnosis. Furthermore, it employs a joint embedding framework to tailor clinical event representations to the disease diagnosis goal. To the best of our knowledge, this is the first study to use Heterogeneous Information Network for modeling clinical data and disease diagnosis. Experimental results of our study show superior performance of HeteroMed compared to prior methods in prediction of exact diagnosis codes and general disease cohorts. Moreover, HeteroMed outperforms baseline models in capturing similarities of clinical events which are examined qualitatively through case studies.","['Anahita Hosseini', 'Ting Chen', 'Wenjun Wu', 'Yizhou Sun', 'Majid Sarrafzadeh']",2018-04-22T00:53:20Z,http://arxiv.org/abs/1804.08052v1,Healthcare & Biomedical AI,Disease Diagnosis Models,there has been growing interest in mining EHR for improving quality of care . relations among clinical events carry various semantics and contribute differently to disease diagnosis . experimental results show superior performance of HeteroMed compared to baseline models .
NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis,"With the development of natural language processing techniques(NLP), automatic diagnosis of eye diseases using ophthalmology electronic medical records (OEMR) has become possible. It aims to evaluate the condition of both eyes of a patient respectively, and we formulate it as a particular multi-label classification task in this paper. Although there are a few related studies in other diseases, automatic diagnosis of eye diseases exhibits unique characteristics. First, descriptions of both eyes are mixed up in OEMR documents, with both free text and templated asymptomatic descriptions, resulting in sparsity and clutter of information. Second, OEMR documents contain multiple parts of descriptions and have long document lengths. Third, it is critical to provide explainability to the disease diagnosis model. To overcome those challenges, we present an effective automatic eye disease diagnosis framework, NEEDED. In this framework, a preprocessing module is integrated to improve the density and quality of information. Then, we design a hierarchical transformer structure for learning the contextualized representations of each sentence in the OEMR document. For the diagnosis part, we propose an attention-based predictor that enables traceable diagnosis by obtaining disease-specific information. Experiments on the real dataset and comparison with several baseline models show the advantage and explainability of our framework.","['Xu Ye', 'Meng Xiao', 'Zhiyuan Ning', 'Weiwei Dai', 'Wenjuan Cui', 'Yi Du', 'Yuanchun Zhou']",2022-12-27T08:37:57Z,http://arxiv.org/abs/2212.13408v3,Healthcare & Biomedical AI,Disease Diagnosis Models,automatic diagnosis of eye diseases using ophthalmology electronic medical records (OEMR) aims to evaluate the condition of both eyes of a patient respectively . a preprocessing module is integrated to improve the density of information .
Risk Prediction of a Multiple Sclerosis Diagnosis,"Multiple sclerosis (MS) is a chronic autoimmune disease that affects the central nervous system. The progression and severity of MS varies by individual, but it is generally a disabling disease. Although medications have been developed to slow the disease progression and help manage symptoms, MS research has yet to result in a cure. Early diagnosis and treatment of the disease have been shown to be effective at slowing the development of disabilities. However, early MS diagnosis is difficult because symptoms are intermittent and shared with other diseases. Thus most previous works have focused on uncovering the risk factors associated with MS and predicting the progression of disease after a diagnosis rather than disease prediction. This paper investigates the use of data available in electronic medical records (EMRs) to create a risk prediction model; thereby helping clinicians perform the difficult task of diagnosing an MS patient. Our results demonstrate that even given a limited time window of patient data, one can achieve reasonable classification with an area under the receiver operating characteristic curve of 0.724. By restricting our features to common EMR components, the developed models also generalize to other healthcare systems.","['Joyce C. Ho', 'Joydeep Ghosh', 'KP Unnikrishnan']",2013-03-05T20:44:58Z,http://arxiv.org/abs/1303.1170v1,Healthcare & Biomedical AI,Disease Diagnosis Models,multiple sclerosis (MS) is a chronic autoimmune disease that affects the central nervous system . early diagnosis and treatment of the disease have been shown to be effective . but early diagnosis is difficult because symptoms are intermittent and shared with other diseases .
A Weighted Heterogeneous Graph Based Dialogue System,"Knowledge based dialogue systems have attracted increasing research interest in diverse applications. However, for disease diagnosis, the widely used knowledge graph is hard to represent the symptom-symptom relations and symptom-disease relations since the edges of traditional knowledge graph are unweighted. Most research on disease diagnosis dialogue systems highly rely on data-driven methods and statistical features, lacking profound comprehension of symptom-disease relations and symptom-symptom relations. To tackle this issue, this work presents a weighted heterogeneous graph based dialogue system for disease diagnosis. Specifically, we build a weighted heterogeneous graph based on symptom co-occurrence and a proposed symptom frequency-inverse disease frequency. Then this work proposes a graph based deep Q-network (Graph-DQN) for dialogue management. By combining Graph Convolutional Network (GCN) with DQN to learn the embeddings of diseases and symptoms from both the structural and attribute information in the weighted heterogeneous graph, Graph-DQN could capture the symptom-disease relations and symptom-symptom relations better. Experimental results show that the proposed dialogue system rivals the state-of-the-art models. More importantly, the proposed dialogue system can complete the task with less dialogue turns and possess a better distinguishing capability on diseases with similar symptoms.","['Xinyan Zhao', 'Liangwei Chen', 'Huanhuan Chen']",2020-10-21T01:22:37Z,http://arxiv.org/abs/2010.10699v2,Healthcare & Biomedical AI,Disease Diagnosis Models,the edges of traditional knowledge graph are unweighted . this work presents a weighted heterogeneous graph based dialogue system for disease diagnosis . experimental results show the proposed dialogue system rivals the state-of-the-art models .
A comprehensive review on Plant Leaf Disease detection using Deep   learning,"Leaf disease is a common fatal disease for plants. Early diagnosis and detection is necessary in order to improve the prognosis of leaf diseases affecting plant. For predicting leaf disease, several automated systems have already been developed using different plant pathology imaging modalities. This paper provides a systematic review of the literature on leaf disease-based models for the diagnosis of various plant leaf diseases via deep learning. The advantages and limitations of different deep learning models including Vision Transformer (ViT), Deep convolutional neural network (DCNN), Convolutional neural network (CNN), Residual Skip Network-based Super-Resolution for Leaf Disease Detection (RSNSR-LDD), Disease Detection Network (DDN), and YOLO (You only look once) are described in this review. The review also shows that the studies related to leaf disease detection applied different deep learning models to a number of publicly available datasets. For comparing the performance of the models, different metrics such as accuracy, precision, recall, etc. were used in the existing studies.","['Sumaya Mustofa', 'Md Mehedi Hasan Munna', 'Yousuf Rayhan Emon', 'Golam Rabbany', 'Md Taimur Ahad']",2023-08-27T12:20:28Z,http://arxiv.org/abs/2308.14087v1,Healthcare & Biomedical AI,Disease Diagnosis Models,leaf disease is a common fatal disease for plants . early diagnosis and detection is necessary in order to improve the prognosis .
Snap and Diagnose: An Advanced Multimodal Retrieval System for   Identifying Plant Diseases in the Wild,"Plant disease recognition is a critical task that ensures crop health and mitigates the damage caused by diseases. A handy tool that enables farmers to receive a diagnosis based on query pictures or the text description of suspicious plants is in high demand for initiating treatment before potential diseases spread further. In this paper, we develop a multimodal plant disease image retrieval system to support disease search based on either image or text prompts. Specifically, we utilize the largest in-the-wild plant disease dataset PlantWild, which includes over 18,000 images across 89 categories, to provide a comprehensive view of potential diseases relating to the query. Furthermore, cross-modal retrieval is achieved in the developed system, facilitated by a novel CLIP-based vision-language model that encodes both disease descriptions and disease images into the same latent space. Built on top of the retriever, our retrieval system allows users to upload either plant disease images or disease descriptions to retrieve the corresponding images with similar characteristics from the disease dataset to suggest candidate diseases for end users' consideration.","['Tianqi Wei', 'Zhi Chen', 'Xin Yu']",2024-08-27T01:23:49Z,http://arxiv.org/abs/2408.14723v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"plant disease is a critical task that ensures crop health and mitigates damage caused by diseases . in this paper, we develop a multimodal plant disease image retrieval system . the system allows users to upload either plant disease images or disease descriptions ."
Multi-Class Plant Leaf Disease Detection: A CNN-based Approach with   Mobile App Integration,"Plant diseases significantly impact agricultural productivity, resulting in economic losses and food insecurity. Prompt and accurate detection is crucial for the efficient management and mitigation of plant diseases. This study investigates advanced techniques in plant disease detection, emphasizing the integration of image processing, machine learning, deep learning methods, and mobile technologies. High-resolution images of plant leaves were captured and analyzed using convolutional neural networks (CNNs) to detect symptoms of various diseases, such as blight, mildew, and rust. This study explores 14 classes of plants and diagnoses 26 unique plant diseases. We focus on common diseases affecting various crops. The model was trained on a diverse dataset encompassing multiple crops and disease types, achieving 98.14% accuracy in disease diagnosis. Finally integrated this model into mobile apps for real-time disease diagnosis.","['Md Aziz Hosen Foysal', 'Foyez Ahmed', 'Md Zahurul Haque']",2024-08-26T07:16:41Z,http://arxiv.org/abs/2408.15289v1,Healthcare & Biomedical AI,Disease Diagnosis Models,this study investigates advanced techniques in plant disease detection . high-resolution images of plant leaves were captured and analyzed . convolutional neural networks (CNNs) were used to detect symptoms of various diseases .
Revolutionizing Disease Diagnosis: A Microservices-Based Architecture   for Privacy-Preserving and Efficient IoT Data Analytics Using Federated   Learning,"Deep learning-based disease diagnosis applications are essential for accurate diagnosis at various disease stages. However, using personal data exposes traditional centralized learning systems to privacy concerns. On the other hand, by positioning processing resources closer to the device and enabling more effective data analyses, a distributed computing paradigm has the potential to revolutionize disease diagnosis. Scalable architectures for data analytics are also crucial in healthcare, where data analytics results must have low latency and high dependability and reliability. This study proposes a microservices-based approach for IoT data analytics systems to satisfy privacy and performance requirements by arranging entities into fine-grained, loosely connected, and reusable collections. Our approach relies on federated learning, which can increase disease diagnosis accuracy while protecting data privacy. Additionally, we employ transfer learning to obtain more efficient models. Using more than 5800 chest X-ray images for pneumonia detection from a publicly available dataset, we ran experiments to assess the effectiveness of our approach. Our experiments reveal that our approach performs better in identifying pneumonia than other cutting-edge technologies, demonstrating our approach's promising potential detection performance.","['Safa Ben Atitallah', 'Maha Driss', 'Henda Ben Ghezala']",2023-08-27T06:31:43Z,http://arxiv.org/abs/2308.14017v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"a study proposes a microservices-based approach for IoT data analytics systems . the approach relies on federated learning, which can increase disease diagnosis accuracy . it also employs transfer learning to obtain more efficient models ."
Diaformer: Automatic Diagnosis via Symptoms Sequence Generation,"Automatic diagnosis has attracted increasing attention but remains challenging due to multi-step reasoning. Recent works usually address it by reinforcement learning methods. However, these methods show low efficiency and require taskspecific reward functions. Considering the conversation between doctor and patient allows doctors to probe for symptoms and make diagnoses, the diagnosis process can be naturally seen as the generation of a sequence including symptoms and diagnoses. Inspired by this, we reformulate automatic diagnosis as a symptoms Sequence Generation (SG) task and propose a simple but effective automatic Diagnosis model based on Transformer (Diaformer). We firstly design the symptom attention framework to learn the generation of symptom inquiry and the disease diagnosis. To alleviate the discrepancy between sequential generation and disorder of implicit symptoms, we further design three orderless training mechanisms. Experiments on three public datasets show that our model outperforms baselines on disease diagnosis by 1%, 6% and 11.5% with the highest training efficiency. Detailed analysis on symptom inquiry prediction demonstrates that the potential of applying symptoms sequence generation for automatic diagnosis.","['Junying Chen', 'Dongfang Li', 'Qingcai Chen', 'Wenxiu Zhou', 'Xin Liu']",2021-12-20T10:26:59Z,http://arxiv.org/abs/2112.10433v1,Healthcare & Biomedical AI,Disease Diagnosis Models,automatic diagnosis has attracted increasing attention but remains challenging . recent works usually address it by reinforcement learning methods . but these methods show low efficiency and require taskspecific reward functions . we propose a simple but effective automatic Diagnosis model based on Transformer .
Toward a multimodal multitask model for neurodegenerative diseases   diagnosis and progression prediction,"Recent studies on modelling the progression of Alzheimer's disease use a single modality for their predictions while ignoring the time dimension. However, the nature of patient data is heterogeneous and time dependent which requires models that value these factors in order to achieve a reliable diagnosis, as well as making it possible to track and detect changes in the progression of patients' condition at an early stage. This article overviews various categories of models used for Alzheimer's disease prediction with their respective learning methods, by establishing a comparative study of early prediction and detection Alzheimer's disease progression. Finally, a robust and precise detection model is proposed.","['Sofia Lahrichi', 'Maryem Rhanoui', 'Mounia Mikram', 'Bouchra El Asri']",2021-10-10T11:44:16Z,http://arxiv.org/abs/2110.09309v1,Healthcare & Biomedical AI,Disease Diagnosis Models,recent studies on the progression of Alzheimer's disease use a single modality for their predictions . the nature of patient data is heterogeneous and time dependent which requires models that value these factors . a robust and precise detection model is proposed .
3D Transformer based on deformable patch location for differential   diagnosis between Alzheimer's disease and Frontotemporal dementia,"Alzheimer's disease and Frontotemporal dementia are common types of neurodegenerative disorders that present overlapping clinical symptoms, making their differential diagnosis very challenging. Numerous efforts have been done for the diagnosis of each disease but the problem of multi-class differential diagnosis has not been actively explored. In recent years, transformer-based models have demonstrated remarkable success in various computer vision tasks. However, their use in disease diagnostic is uncommon due to the limited amount of 3D medical data given the large size of such models. In this paper, we present a novel 3D transformer-based architecture using a deformable patch location module to improve the differential diagnosis of Alzheimer's disease and Frontotemporal dementia. Moreover, to overcome the problem of data scarcity, we propose an efficient combination of various data augmentation techniques, adapted for training transformer-based models on 3D structural magnetic resonance imaging data. Finally, we propose to combine our transformer-based model with a traditional machine learning model using brain structure volumes to better exploit the available data. Our experiments demonstrate the effectiveness of the proposed approach, showing competitive results compared to state-of-the-art methods. Moreover, the deformable patch locations can be visualized, revealing the most relevant brain regions used to establish the diagnosis of each disease.","['Huy-Dung Nguyen', 'Michaël Clément', 'Boris Mansencal', 'Pierrick Coupé']",2023-09-06T17:42:18Z,http://arxiv.org/abs/2309.03183v1,Healthcare & Biomedical AI,Disease Diagnosis Models,Alzheimer's disease and Frontotemporal dementia are common types of neurodegenerative disorders . transformer-based models have shown remarkable success in various computer vision tasks . but their use in disease diagnostic is uncommon due to the limited amount of 3D medical data .
Predicting Alzheimer's Disease Using 3DMgNet,"Alzheimer's disease (AD) is an irreversible neurode generative disease of the brain.The disease may causes memory loss, difficulty communicating and disorientation. For the diagnosis of Alzheimer's disease, a series of scales are often needed to evaluate the diagnosis clinically, which not only increases the workload of doctors, but also makes the results of diagnosis highly subjective. Therefore, for Alzheimer's disease, imaging means to find early diagnostic markers has become a top priority.   In this paper, we propose a novel 3DMgNet architecture which is a unified framework of multigrid and convolutional neural network to diagnose Alzheimer's disease (AD). The model is trained using an open dataset (ADNI dataset) and then test with a smaller dataset of ours. Finally, the model achieved 92.133% accuracy for AD vs NC classification and significantly reduced the model parameters.","['Yelu Gao', 'Huang Huang', 'Lian Zhang']",2022-01-12T09:08:08Z,http://arxiv.org/abs/2201.04370v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"the disease may cause memory loss, difficulty communicating and disorientation . imaging means to find early diagnostic markers has become a top priority . the model achieved 92.133% accuracy for AD vs NC classification ."
Deep grading for MRI-based differential diagnosis of Alzheimer's disease   and Frontotemporal dementia,"Alzheimer's disease and Frontotemporal dementia are common forms of neurodegenerative dementia. Behavioral alterations and cognitive impairments are found in the clinical courses of both diseases and their differential diagnosis is sometimes difficult for physicians. Therefore, an accurate tool dedicated to this diagnostic challenge can be valuable in clinical practice. However, current structural imaging methods mainly focus on the detection of each disease but rarely on their differential diagnosis. In this paper, we propose a deep learning based approach for both problems of disease detection and differential diagnosis. We suggest utilizing two types of biomarkers for this application: structure grading and structure atrophy. First, we propose to train a large ensemble of 3D U-Nets to locally determine the anatomical patterns of healthy people, patients with Alzheimer's disease and patients with Frontotemporal dementia using structural MRI as input. The output of the ensemble is a 2-channel disease's coordinate map able to be transformed into a 3D grading map which is easy to interpret for clinicians. This 2-channel map is coupled with a multi-layer perceptron classifier for different classification tasks. Second, we propose to combine our deep learning framework with a traditional machine learning strategy based on volume to improve the model discriminative capacity and robustness. After both cross-validation and external validation, our experiments based on 3319 MRI demonstrated competitive results of our method compared to the state-of-the-art methods for both disease detection and differential diagnosis.","['Huy-Dung Nguyen', 'Michaël Clément', 'Vincent Planche', 'Boris Mansencal', 'Pierrick Coupé']",2022-11-25T13:25:18Z,http://arxiv.org/abs/2211.14096v2,Healthcare & Biomedical AI,Disease Diagnosis Models,"Alzheimer's disease and Frontotemporal dementia are common forms of neurodegenerative dementia . current structural imaging methods mainly focus on the detection of each disease but rarely on their differential diagnosis . in this paper, we propose a deep learning based approach for both problems of disease detection and differential diagnosis."
"Unveiling new disease, pathway, and gene associations via multi-scale   neural networks","Diseases involve complex processes and modifications to the cellular machinery. The gene expression profile of the affected cells contains characteristic patterns linked to a disease. Hence, biological knowledge pertaining to a disease can be derived from a patient cell's profile, improving our diagnosis ability, as well as our grasp of disease risks. This knowledge can be used for drug re-purposing, or by physicians to evaluate a patient's condition and co-morbidity risk. Here, we look at differential gene expression obtained from microarray technology for patients diagnosed with various diseases. Based on this data and cellular multi-scale organization, we aim to uncover disease--disease links, as well as disease-gene and disease--pathways associations. We propose neural networks with structures inspired by the multi-scale organization of a cell. We show that these models are able to correctly predict the diagnosis for the majority of the patients. Through the analysis of the trained models, we predict and validate disease-disease, disease-pathway, and disease-gene associations with comparisons to known interactions and literature search, proposing putative explanations for the novel predictions that come from our study.","['Thomas Gaudelet', 'Noel Malod-Dognin', 'Jon Sanchez-Valle', 'Vera Pancaldi', 'Alfonso Valencia', 'Natasa Przulj']",2019-01-28T21:17:57Z,http://arxiv.org/abs/1901.10005v3,Healthcare & Biomedical AI,Disease Diagnosis Models,"gene expression profile of affected cells contains characteristic patterns linked to a disease . knowledge can be used for drug re-purposing, or by physicians to evaluate a patient's condition . we propose neural networks inspired by the multi-scale organization of a cell ."
DermDiff: Generative Diffusion Model for Mitigating Racial Biases in   Dermatology Diagnosis,"Skin diseases, such as skin cancer, are a significant public health issue, and early diagnosis is crucial for effective treatment. Artificial intelligence (AI) algorithms have the potential to assist in triaging benign vs malignant skin lesions and improve diagnostic accuracy. However, existing AI models for skin disease diagnosis are often developed and tested on limited and biased datasets, leading to poor performance on certain skin tones. To address this problem, we propose a novel generative model, named DermDiff, that can generate diverse and representative dermoscopic image data for skin disease diagnosis. Leveraging text prompting and multimodal image-text learning, DermDiff improves the representation of underrepresented groups (patients, diseases, etc.) in highly imbalanced datasets. Our extensive experimentation showcases the effectiveness of DermDiff in terms of high fidelity and diversity. Furthermore, downstream evaluation suggests the potential of DermDiff in mitigating racial biases for dermatology diagnosis. Our code is available at https://github.com/Munia03/DermDiff","['Nusrat Munia', 'Abdullah-Al-Zubaer Imran']",2025-03-21T20:45:39Z,http://arxiv.org/abs/2503.17536v1,Healthcare & Biomedical AI,Disease Diagnosis Models,"existing AI models for skin disease diagnosis are often developed and tested on limited datasets . existing models are often biased, leading to poor performance on certain skin tones . we propose a novel generative model, named dermDiff, that can generate representative image data ."
Towards Smart Healthcare: Challenges and Opportunities in IoT and ML,"The COVID-19 pandemic and other ongoing health crises have underscored the need for prompt healthcare services worldwide. The traditional healthcare system, centered around hospitals and clinics, has proven inadequate in the face of such challenges. Intelligent wearable devices, a key part of modern healthcare, leverage Internet of Things technology to collect extensive data related to the environment as well as psychological, behavioral, and physical health. However, managing the substantial data generated by these wearables and other IoT devices in healthcare poses a significant challenge, potentially impeding decision-making processes. Recent interest has grown in applying data analytics for extracting information, gaining insights, and making predictions. Additionally, machine learning, known for addressing various big data and networking challenges, has seen increased implementation to enhance IoT systems in healthcare. This chapter focuses exclusively on exploring the hurdles encountered when integrating ML methods into the IoT healthcare sector. It offers a comprehensive summary of current research challenges and potential opportunities, categorized into three scenarios: IoT-based, ML-based, and the implementation of machine learning methodologies in the IoT-based healthcare industry. This compilation will assist future researchers, healthcare professionals, and government agencies by offering valuable insights into recent smart healthcare advancements.","['Munshi Saifuzzaman', 'Tajkia Nuri Ananna']",2023-12-09T10:45:44Z,http://arxiv.org/abs/2312.05530v2,Healthcare & Biomedical AI,Predictive Healthcare,the COVID-19 pandemic has underscored the need for prompt healthcare services worldwide . managing the data generated by wearables and other IoT devices in healthcare poses a significant challenge . this chapter focuses exclusively on exploring the hurdles encountered when integrating ML methods .
"A Novel Zero-Trust Machine Learning Green Architecture for Healthcare   IoT Cybersecurity: Review, Analysis, and Implementation","The integration of Internet of Things (IoT) devices in healthcare applications has revolutionized patient care, monitoring, and data management. The Global IoT in Healthcare Market value is $252.2 Billion in 2023. However, the rapid involvement of these devices brings information security concerns that pose critical threats to patient privacy and the integrity of healthcare data. This paper introduces a novel machine learning (ML) based architecture explicitly designed to address and mitigate security vulnerabilities in IoT devices within healthcare applications. By leveraging advanced convolution ML architecture, the proposed architecture aims to proactively monitor and detect potential threats, ensuring the confidentiality and integrity of sensitive healthcare information while minimizing the cost and increasing the portability specialized for healthcare and emergency environments. The experimental results underscore the accuracy of up to 93.6% for predicting various attacks based on the results demonstrate a zero-day detection accuracy simulated using the CICIoT2023 dataset and reduces the cost by a factor of x10. The significance of our approach is in fortifying the security posture of IoT devices and maintaining a robust implementation of trustful healthcare systems.","['Zag ElSayed', 'Nelly Elsayed', 'Sajjad Bay']",2024-01-14T21:01:21Z,http://arxiv.org/abs/2401.07368v1,Healthcare & Biomedical AI,Predictive Healthcare,the global IoT in healthcare market value is $252.2 Billion in 2023 . the proposed architecture aims to proactively monitor and detect potential threats . experimental results underscore the accuracy of up to 93.6% for predicting various attacks .
Machine Learning for Everyone: Simplifying Healthcare Analytics with   BigQuery ML,"Machine learning (ML) transforms healthcare by enabling predictive analytics, personalized treatments, and improved patient outcomes. However, traditional ML workflows often require specialized skills, infrastructure, and resources, limiting accessibility for many healthcare professionals. This paper explores how BigQuery ML Cloud service helps healthcare researchers and data analysts to build and deploy models using SQL, without need for advanced ML knowledge. Our results demonstrate that the Boosted Tree model achieved the highest performance among the three models making it highly effective for diabetes prediction. BigQuery ML directly integrates predictive analytics into their workflows to inform decision-making and support patient care. We reveal this capability through a case study on diabetes prediction using the Diabetes Health Indicators Dataset. Our study underscores BigQuery ML's role in democratizing machine learning, enabling faster, scalable, and efficient predictive analytics that can directly enhance healthcare decision-making processes. This study aims to bridge the gap between advanced machine learning and practical healthcare analytics by providing detailed insights into BigQuery ML's capabilities. By demonstrating its utility in a real-world case study, we highlight its potential to simplify complex workflows and expand access to predictive tools for a broader audience of healthcare professionals.","['Mohammad Amir Salari', 'Bahareh Rahmani']",2025-02-10T20:38:53Z,http://arxiv.org/abs/2502.07026v2,Healthcare & Biomedical AI,Predictive Healthcare,this paper explores how BigQuery ML Cloud service helps healthcare researchers and data analysts to build and deploy models using SQL . our results demonstrate that the Boosted Tree model achieved the highest performance among the three models .
Prediction of Post-Operative Renal and Pulmonary Complications Using   Transformers,"Postoperative complications pose a significant challenge in the healthcare industry, resulting in elevated healthcare expenses and prolonged hospital stays, and in rare instances, patient mortality. To improve patient outcomes and reduce healthcare costs, healthcare providers rely on various perioperative risk scores to guide clinical decisions and prioritize care. In recent years, machine learning techniques have shown promise in predicting postoperative complications and fatality, with deep learning models achieving remarkable success in healthcare applications. However, research on the application of deep learning models to intra-operative anesthesia management data is limited. In this paper, we evaluate the performance of transformer-based models in predicting postoperative acute renal failure, postoperative pulmonary complications, and postoperative in-hospital mortality. We compare our method's performance with state-of-the-art tabular data prediction models, including gradient boosting trees and sequential attention models, on a clinical dataset. Our results demonstrate that transformer-based models can achieve superior performance in predicting postoperative complications and outperform traditional machine learning models. This work highlights the potential of deep learning techniques, specifically transformer-based models, in revolutionizing the healthcare industry's approach to postoperative care.","['Reza Shirkavand', 'Fei Zhang', 'Heng Huang']",2023-06-01T14:08:05Z,http://arxiv.org/abs/2306.00698v2,Healthcare & Biomedical AI,Predictive Healthcare,machine learning has shown promise in predicting postoperative complications and fatality . but research on the application of deep learning models to intra-operative anesthesia data is limited . this paper evaluates transformer-based models in predicating postoperative mortality .
Measuring Value in Healthcare,"A statistical description and model of individual healthcare expenditures in the US has been developed for measuring value in healthcare. We find evidence that healthcare expenditures are quantifiable as an infusion-diffusion process, which can be thought of intuitively as a steady change in the intensity of treatment superimposed on a random process reflecting variations in the efficiency and effectiveness of treatment. The arithmetic mean represents the net average annual cost of healthcare; and when multiplied by the arithmetic standard deviation, which represents the effective risk, the result is a measure of healthcare cost control. Policymakers, providers, payors, or patients that decrease these parameters are generating value in healthcare. The model has an average absolute prediction error of approximately 10-12% across the range of expenditures which spans 6 orders of magnitude over a nearly 10-year period. For the top 1% of the population with the largest expenditures, representing 20%-30% of total spending on healthcare, a power-law relationship emerges. This relationship also applies to the most expensive medical conditions in the US. A fundamental connection between healthcare expenditures and mathematical finance is found by showing that the process healthcare expenditures follow is similar to a widely used model for managing financial assets, leading to the conclusion that a combination of these two fields may yield useful results.",['Christopher Gardner'],2008-06-14T21:54:48Z,http://arxiv.org/abs/0806.2397v1,Healthcare & Biomedical AI,Predictive Healthcare,"healthcare expenditures are quantifiable as an infusion-diffusion process . the model has an average absolute prediction error of approximately 10-12% . for the top 1% of the population with the largest expenditures, a power-law relationship emerges ."
Probabilistic Machine Learning for Healthcare,"Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.","['Irene Y. Chen', 'Shalmali Joshi', 'Marzyeh Ghassemi', 'Rajesh Ranganath']",2020-09-23T12:14:05Z,http://arxiv.org/abs/2009.11087v1,Healthcare & Biomedical AI,Predictive Healthcare,"in this review, we examine how probabilistic machine learning can advance healthcare . we consider challenges in the model building pipeline where probabilistic models can be beneficial ."
Edge Intelligence for Empowering IoT-based Healthcare Systems,"The demand for real-time, affordable, and efficient smart healthcare services is increasing exponentially due to the technological revolution and burst of population. To meet the increasing demands on this critical infrastructure, there is a need for intelligent methods to cope with the existing obstacles in this area. In this regard, edge computing technology can reduce latency and energy consumption by moving processes closer to the data sources in comparison to the traditional centralized cloud and IoT-based healthcare systems. In addition, by bringing automated insights into the smart healthcare systems, artificial intelligence (AI) provides the possibility of detecting and predicting high-risk diseases in advance, decreasing medical costs for patients, and offering efficient treatments. The objective of this article is to highlight the benefits of the adoption of edge intelligent technology, along with AI in smart healthcare systems. Moreover, a novel smart healthcare model is proposed to boost the utilization of AI and edge technology in smart healthcare systems. Additionally, the paper discusses issues and research directions arising when integrating these different technologies together.","['Vahideh Hayyolalam', 'Moayad Aloqaily', 'Oznur Ozkasap', 'Mohsen Guizani']",2021-03-22T19:35:06Z,http://arxiv.org/abs/2103.12144v1,Healthcare & Biomedical AI,Predictive Healthcare,"the demand for real-time, affordable, and efficient smart healthcare services is increasing . edge computing technology can reduce latency and energy consumption . a novel smart healthcare model is proposed to boost the utilization of AI and edge technology ."
High-Throughput Approach to Modeling Healthcare Costs Using Electronic   Healthcare Records,"Accurate estimation of healthcare costs is crucial for healthcare systems to plan and effectively negotiate with insurance companies regarding the coverage of patient-care costs. Greater accuracy in estimating healthcare costs would provide mutual benefit for both health systems and the insurers that support these systems by better aligning payment models with patient-care costs. This study presents the results of a generalizable machine learning approach to predicting medical events built from 40 years of data from >860,000 patients pertaining to >6,700 prescription medications, courtesy of Marshfield Clinic in Wisconsin. It was found that models built using this approach performed well when compared to similar studies predicting physician prescriptions of individual medications. In addition to providing a comprehensive predictive model for all drugs in a large healthcare system, the approach taken in this research benefits from potential applicability to a wide variety of other medical events.","['Alex Taylor', 'Ross Kleiman', 'Scott Hebbring', 'Peggy Peissig', 'David Page']",2020-11-18T19:06:18Z,http://arxiv.org/abs/2011.09497v2,Healthcare & Biomedical AI,Predictive Healthcare,"study presents results of a generalizable machine learning approach to predicting medical events . it was built from 40 years of data from >860,000 patients pertaining to >6,700 prescription medications . the approach performed well when compared to similar studies predicting physician prescriptions ."
An Intelligent Quantum Cyber-Security Framework for Healthcare Data   Management,"Digital healthcare is essential to facilitate consumers to access and disseminate their medical data easily for enhanced medical care services. However, the significant concern with digitalization across healthcare systems necessitates for a prompt, productive, and secure storage facility along with a vigorous communication strategy, to stimulate sensitive digital healthcare data sharing and proactive estimation of malicious entities. In this context, this paper introduces a comprehensive quantum-based framework to overwhelm the potential security and privacy issues for secure healthcare data management. It equips quantum encryption for the secured storage and dispersal of healthcare data over the shared cloud platform by employing quantum encryption. Also, the framework furnishes a quantum feed-forward neural network unit to examine the intention behind the data request before granting access, for proactive estimation of potential data breach. In this way, the proposed framework delivers overall healthcare data management by coupling the advanced and more competent quantum approach with machine learning to safeguard the data storage, access, and prediction of malicious entities in an automated manner. Thus, the proposed IQ-HDM leads to more cooperative and effective healthcare delivery and empowers individuals with adequate custody of their health data. The experimental evaluation and comparison of the proposed IQ-HDM framework with state-of-the-art methods outline a considerable improvement up to 67.6%, in tackling cyber threats related to healthcare data security.","['Kishu Gupta', 'Deepika Saxena', 'Pooja Rani', 'Jitendra Kumar', 'Aaisha Makkar', 'Ashutosh Kumar Singh', 'Chung-Nan Lee']",2024-10-04T08:04:48Z,http://arxiv.org/abs/2410.03217v1,Healthcare & Biomedical AI,Predictive Healthcare,paper introduces a quantum-based framework to overwhelm potential security issues . it equips quantum encryption for secure storage and dispersal of healthcare data . proposed framework provides overall healthcare data management .
Self-Supervised Learning for Graph-Structured Data in Healthcare   Applications: A Comprehensive Review,"The abundance of complex and interconnected healthcare data offers numerous opportunities to improve prediction, diagnosis, and treatment. Graph-structured data, which includes entities and their relationships, is well-suited for capturing complex connections. Effectively utilizing this data often requires strong and efficient learning algorithms, especially when dealing with limited labeled data. It is increasingly important for downstream tasks in various domains to utilize self-supervised learning (SSL) as a paradigm for learning and optimizing effective representations from unlabeled data. In this paper, we thoroughly review SSL approaches specifically designed for graph-structured data in healthcare applications. We explore the challenges and opportunities associated with healthcare data and assess the effectiveness of SSL techniques in real-world healthcare applications. Our discussion encompasses various healthcare settings, such as disease prediction, medical image analysis, and drug discovery. We critically evaluate the performance of different SSL methods across these tasks, highlighting their strengths, limitations, and potential future research directions. Ultimately, this review aims to be a valuable resource for both researchers and practitioners looking to utilize SSL for graph-structured data in healthcare, paving the way for improved outcomes and insights in this critical field. To the best of our knowledge, this work represents the first comprehensive review of the literature on SSL applied to graph data in healthcare.","['Safa Ben Atitallah', 'Chaima Ben Rabah', 'Maha Driss', 'Wadii Boulila', 'Anis Koubaa']",2024-11-28T10:51:12Z,http://arxiv.org/abs/2412.05312v1,Healthcare & Biomedical AI,Predictive Healthcare,Graph-structured data is well-suited for capturing complex connections . it is increasingly important for downstream tasks to utilize self-supervised learning (SSL) this review explores the challenges and opportunities associated with healthcare data .
Building predictive models of healthcare costs with open healthcare data,"Due to rapidly rising healthcare costs worldwide, there is significant interest in controlling them. An important aspect concerns price transparency, as preliminary efforts have demonstrated that patients will shop for lower costs, driving efficiency. This requires the data to be made available, and models that can predict healthcare costs for a wide range of patient demographics and conditions. We present an approach to this problem by developing a predictive model using machine-learning techniques. We analyzed de-identified patient data from New York State SPARCS (statewide planning and research cooperative system), consisting of 2.3 million records in 2016. We built models to predict costs from patient diagnoses and demographics. We investigated two model classes consisting of sparse regression and decision trees. We obtained the best performance by using a decision tree with depth 10. We obtained an R-square value of 0.76 which is better than the values reported in the literature for similar problems.","['A. Ravishankar Rao', 'Subrata Garai', 'Soumyabrata Dey', 'Hang Peng']",2023-04-05T02:12:58Z,http://arxiv.org/abs/2304.02191v1,Healthcare & Biomedical AI,Predictive Healthcare,there is significant interest in controlling healthcare costs worldwide . preliminary efforts have demonstrated that patients will shop for lower costs . we built models to predict costs from diagnoses and demographics . best performance by using a decision tree with depth 10 .
Predicting Opioid Use Disorder from Longitudinal Healthcare Data using   Multi-stream Transformer,"Opioid Use Disorder (OUD) is a public health crisis costing the US billions of dollars annually in healthcare, lost workplace productivity, and crime. Analyzing longitudinal healthcare data is critical in addressing many real-world problems in healthcare. Leveraging the real-world longitudinal healthcare data, we propose a novel multi-stream transformer model called MUPOD for OUD identification. MUPOD is designed to simultaneously analyze multiple types of healthcare data streams, such as medications and diagnoses, by attending to segments within and across these data streams. Our model tested on the data from 392,492 patients with long-term back pain problems showed significantly better performance than the traditional models and recently developed deep learning models.","['Sajjad Fouladvand', 'Jeffery Talbert', 'Linda P. Dwoskin', 'Heather Bush', 'Amy Lynn Meadows', 'Lars E. Peterson', 'Ramakanth Kavuluru', 'Jin Chen']",2021-03-16T01:44:21Z,http://arxiv.org/abs/2103.08800v2,Healthcare & Biomedical AI,Predictive Healthcare,"public health crisis costing the US billions of dollars annually in healthcare . a novel multi-stream transformer model called MUPOD is designed to analyze multiple types of data . the model tested on the data from 392,492 patients with long-term back pain problems ."
Privacy-preserving machine learning for healthcare: open challenges and   future perspectives,"Machine Learning (ML) has recently shown tremendous success in modeling various healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment. Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference. In this paper, we conduct a review of recent literature concerning Privacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of existing trends, identify challenges, and discuss opportunities for future research directions. The aim of this review is to guide the development of private and efficient ML models in healthcare, with the prospects of translating research efforts into real-world settings.","['Alejandro Guerra-Manzanares', 'L. Julian Lechuga Lopez', 'Michail Maniatakos', 'Farah E. Shamout']",2023-03-27T19:20:51Z,http://arxiv.org/abs/2303.15563v1,Healthcare & Biomedical AI,Predictive Healthcare,"machine learning (ML) has shown great success in modeling healthcare prediction tasks . due to sensitive nature of medical data, privacy must be considered along the entire ML pipeline . aim of this review is to guide the development of private and efficient ML models in healthcare ."
Securing The Future Of Healthcare: Building A Resilient Defense System   For Patient Data Protection,"The increasing importance of data in the healthcare sector has led to a rise in cybercrime targeting patient information. Data breaches pose significant financial and reputational risks to many healthcare organizations including clinics and hospitals. This study aims to propose the ideal approach to developing a defense system that ensures that patient data is protected from the insidious acts of healthcare data threat actors. Using a gradientboosting classifier machine learning model, the study predicts the severity of healthcare data breaches. Secondary data was collected from the U.S. Department of Health and Human Services Portal with key indicators. Also, the study gathers key cyber-security data from Kaggle, which was utilized for the study. The findings revealed that hacking and IT incidents are the most common type of breaches in the healthcare industry, with network servers being targeted in most cases. The model evaluation showed that the gradient boosting algorithm performs well. Therefore, the study recommends that organizations implement comprehensive security protocols, particularly focusing on robust network security to protect servers","['Oluomachi Ejiofor', 'Ahmed Akinsola']",2024-07-23T04:25:35Z,http://arxiv.org/abs/2407.16170v1,Healthcare & Biomedical AI,Predictive Healthcare,study predicts the severity of healthcare data breaches . data breaches pose significant financial and reputational risks . hacking and IT incidents are the most common type of breaches in the healthcare industry .
Easydiagnos: a framework for accurate feature selection for automatic   diagnosis in smart healthcare,"The rapid advancements in artificial intelligence (AI) have revolutionized smart healthcare, driving innovations in wearable technologies, continuous monitoring devices, and intelligent diagnostic systems. However, security, explainability, robustness, and performance optimization challenges remain critical barriers to widespread adoption in clinical environments. This research presents an innovative algorithmic method using the Adaptive Feature Evaluator (AFE) algorithm to improve feature selection in healthcare datasets and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT), the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby enhancing predictive accuracy and interpretability. The proposed method is validated across three diverse healthcare datasets using six distinct machine learning algorithms, demonstrating its robustness and superiority over conventional feature selection techniques. The results underscore the transformative potential of AFE in smart healthcare, enabling personalized and transparent patient care. Notably, the AFE algorithm, when combined with a Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting its capability to improve clinical decision-making processes in real-world healthcare applications.","['Prasenjit Maji', 'Amit Kumar Mondal', 'Hemanta Kumar Mondal', 'Saraju P. Mohanty']",2024-10-01T03:28:56Z,http://arxiv.org/abs/2410.00366v1,Healthcare & Biomedical AI,Predictive Healthcare,the Adaptive Feature Evaluator (AFE) algorithm improves feature selection . the algorithm optimizes clinical decision support systems (CDSS) the method is validated across three diverse healthcare datasets using six different machine learning algorithms .
The contribution of machine learning to the prevention of burnout among   healthcare workers in Morocco,"In recent years, and particularly during the Covid-19 pandemic, Morocco has experienced significant pressure from user demand, leading to a significant workload in public hospitals. This situation raises major questions regarding the occupational health of healthcare staff. While previous studies have focused on the role of AI in the safety and resilience of military personnel, no research has investigated its role in protecting healthcare personnel from psychosocial risks. This inadequacy leads us to formulate the following central question:What is the contribution of machine learning to the prevention of emotional exhaustion (burnout) among healthcare staff in Morocco? This work is part of a modeling approach aimed at developing a predictive model of the risks of emotional exhaustion (burn-out), the parameters of which will be estimated using supervised learning. From a scientific perspective, this work aims to contribute to the development of systems for preventing psychosocial risks affecting staff in healthcare establishments. From a managerial perspective, this research aims to equip decision-makers in healthcare establishments so that they can anticipate psychosocial disorders linked to emotional exhaustion (burn-out) and implement appropriate preventive measures.",['Mohammed Eddaou'],2025-06-25T16:56:06Z,http://arxiv.org/abs/2507.05264v1,Healthcare & Biomedical AI,Predictive Healthcare,"Morocco has experienced pressure from user demand, leading to a significant workload in public hospitals . no research has investigated its role in protecting healthcare personnel from psychosocial risks . from a scientific perspective, this work aims to contribute to the development of systems ."
"Leveraging Big Data Analytics in Healthcare Enhancement: Trends,   Challenges and Opportunities","Clinicians decisions are becoming more and more evidence-based meaning in no other field the big data analytics so promising as in healthcare. Due to the sheer size and availability of healthcare data, big data analytics has revolutionized this industry and promises us a world of opportunities. It promises us the power of early detection, prediction, prevention and helps us to improve the quality of life. Researchers and clinicians are working to inhibit big data from having a positive impact on health in the future. Different tools and techniques are being used to analyze, process, accumulate, assimilate and manage large amount of healthcare data either in structured or unstructured form. In this paper, we would like to address the need of big data analytics in healthcare: why and how can it help to improve life?. We present the emerging landscape of big data and analytical techniques in the five sub-disciplines of healthcare i.e.medical image analysis and imaging informatics, bioinformatics, clinical informatics, public health informatics and medical signal analytics. We presents different architectures, advantages and repositories of each discipline that draws an integrated depiction of how distinct healthcare activities are accomplished in the pipeline to facilitate individual patients from multiple perspectives. Finally the paper ends with the notable applications and challenges in adoption of big data analytics in healthcare.","['Arshia Rehman', 'Saeeda Naz', 'Imran Razzak']",2020-04-05T06:46:58Z,http://arxiv.org/abs/2004.09010v1,Healthcare & Biomedical AI,Predictive Healthcare,big data analytics has revolutionized healthcare due to the sheer size and availability of healthcare data . researchers and clinicians are working to inhibit big data from having a positive impact on health in the future .
A Secure Healthcare 5.0 System Based on Blockchain Technology Entangled   with Federated Learning Technique,"In recent years, the global Internet of Medical Things (IoMT) industry has evolved at a tremendous speed. Security and privacy are key concerns on the IoMT, owing to the huge scale and deployment of IoMT networks. Machine learning (ML) and blockchain (BC) technologies have significantly enhanced the capabilities and facilities of healthcare 5.0, spawning a new area known as ""Smart Healthcare."" By identifying concerns early, a smart healthcare system can help avoid long-term damage. This will enhance the quality of life for patients while reducing their stress and healthcare costs. The IoMT enables a range of functionalities in the field of information technology, one of which is smart and interactive health care. However, combining medical data into a single storage location to train a powerful machine learning model raises concerns about privacy, ownership, and compliance with greater concentration. Federated learning (FL) overcomes the preceding difficulties by utilizing a centralized aggregate server to disseminate a global learning model. Simultaneously, the local participant keeps control of patient information, assuring data confidentiality and security. This article conducts a comprehensive analysis of the findings on blockchain technology entangled with federated learning in healthcare. 5.0. The purpose of this study is to construct a secure health monitoring system in healthcare 5.0 by utilizing a blockchain technology and Intrusion Detection System (IDS) to detect any malicious activity in a healthcare network and enables physicians to monitor patients through medical sensors and take necessary measures periodically by predicting diseases.","['Abdur Rehman', 'Sagheer Abbas', 'M. A. Khan', 'Taher M. Ghazal', 'Khan Muhammad Adnan', 'Amir Mosavi']",2022-09-16T23:25:42Z,http://arxiv.org/abs/2209.09642v1,Healthcare & Biomedical AI,Predictive Healthcare,"the global Internet of Medical Things (IoMT) industry has evolved at a tremendous speed . security and privacy are key concerns on the IoMT, owing to huge scale and deployment . machine learning (ML) and blockchain (BC) technologies have significantly enhanced capabilities ."
Pathways to Good Healthcare Services and Patient Satisfaction: An   Evolutionary Game Theoretical Approach,"Spending by the UK's National Health Service (NHS) on independent healthcare treatment has been increased in recent years and is predicted to sustain its upward trend with the forecast of population growth. Some have viewed this increase as an attempt not to expand the patients' choices but to privatize public healthcare. This debate poses a social dilemma whether the NHS should stop cooperating with Private providers. This paper contributes to healthcare economic modelling by investigating the evolution of cooperation among three proposed populations: Public Healthcare Providers, Private Healthcare Providers and Patients. The Patient population is included as a main player in the decision-making process by expanding patient's choices of treatment. We develop a generic basic model that measures the cost of healthcare provision based on given parameters, such as NHS and private healthcare providers' cost of investments in both sectors, cost of treatments and gained benefits. A patient's costly punishment is introduced as a mechanism to enhance cooperation among the three populations. Our findings show that cooperation can be improved with the introduction of punishment (patient's punishment) against defecting providers. Although punishment increases cooperation, it is very costly considering the small improvement in cooperation in comparison to the basic model.","['Zainab Alalawi', 'The Anh Han', 'Yifeng Zeng', 'Aiman Elragig']",2019-07-06T15:38:33Z,http://arxiv.org/abs/1907.07132v1,Healthcare & Biomedical AI,Predictive Healthcare,spending by the UK's national health service (NHS) on independent healthcare treatment has been increased in recent years . some have viewed this increase as an attempt not to expand the patients' choices but to privatize public healthcare . this debate poses a social dilemma whether the NHS should stop cooperating with Private providers .
Integration of Federated Learning and Blockchain in Healthcare: A   Tutorial,"Wearable devices and medical sensors revolutionize health monitoring, raising concerns about data privacy in ML for healthcare. This tutorial explores FL and BC integration, offering a secure and privacy-preserving approach to healthcare analytics. FL enables decentralized model training on local devices at healthcare institutions, keeping patient data localized. This facilitates collaborative model development without compromising privacy. However, FL introduces vulnerabilities. BC, with its tamper-proof ledger and smart contracts, provides a robust framework for secure collaborative learning in FL. After presenting a taxonomy for the various types of data used in ML in medical applications, and a concise review of ML techniques for healthcare use cases, this tutorial explores three integration architectures for balancing decentralization, scalability, and reliability in healthcare data. Furthermore, it investigates how BCFL enhances data security and collaboration in disease prediction, medical image analysis, patient monitoring, and drug discovery. By providing a tutorial on FL, blockchain, and their integration, along with a review of BCFL applications, this paper serves as a valuable resource for researchers and practitioners seeking to leverage these technologies for secure and privacy-preserving healthcare ML. It aims to accelerate advancements in secure and collaborative healthcare analytics, ultimately improving patient outcomes.","['Yahya Shahsavari', 'Oussama A. Dambri', 'Yaser Baseri', 'Abdelhakim Senhaji Hafid', 'Dimitrios Makrakis']",2024-04-15T19:00:09Z,http://arxiv.org/abs/2404.10092v1,Healthcare & Biomedical AI,Predictive Healthcare,"this tutorial explores FL and BC integration, offering a secure and privacy-preserving approach to healthcare analytics . FL enables decentralized model training on local devices at healthcare institutions, keeping patient data localized . BC, with its tamper-proof ledger and smart contracts, provides a robust framework for secure collaborative learning ."
Data science in public health: building next generation capacity,"Rapidly evolving technology, data and analytic landscapes are permeating many fields and professions. In public health, the need for data science skills including data literacy is particularly prominent given both the potential of novel data types and analysis methods to fill gaps in existing public health research and intervention practices, as well as the potential of such data or methods to perpetuate or augment health disparities. Through a review of public health courses and programs at the top 10 U.S. and globally ranked schools of public health, this article summarizes existing educational efforts in public health data science. These existing practices serve to inform efforts for broadening such curricula to further schools and populations. Data science ethics course offerings are also examined in context of assessing how population health principles can be blended into training across levels of data involvement to augment the traditional core of public health curricula. Parallel findings from domestic and international 'outside the classroom' training programs are also synthesized to advance approaches for increasing diversity in public health data science. Based on these program reviews and their synthesis, a four-point formula is distilled for furthering public health data science education efforts, toward development of a critical and inclusive mass of practitioners with fluency to leverage data to advance goals of public health and improve quality of life in the digital age.","['Nicholas Mirin', 'Heather Mattie', 'Latifa Jackson', 'Zainab Samad', 'Rumi Chunara']",2022-08-06T08:09:42Z,http://arxiv.org/abs/2208.03461v1,Healthcare & Biomedical AI,Public Health Analytics,top 10 u.s. and globally ranked schools of public health reviewed . current practices serve to inform efforts for broadening such curricula to further schools and populations . data science ethics course offerings also examined .
Predicting health inspection results from online restaurant reviews,"Informatics around public health are increasingly shifting from the professional to the public spheres. In this work, we apply linguistic analytics to restaurant reviews, from Yelp, in order to automatically predict official health inspection reports. We consider two types of feature sets, i.e., keyword detection and topic model features, and use these in several classification methods. Our empirical analysis shows that these extracted features can predict public health inspection reports with over 90% accuracy using simple support vector machines.","['Samantha Wong', 'Hamidreza Chinaei', 'Frank Rudzicz']",2016-03-17T20:20:32Z,http://arxiv.org/abs/1603.05673v1,Healthcare & Biomedical AI,Public Health Analytics,linguistic analytics is applied to Yelp reviews to predict health inspection reports . features extracted from the reviews can predict inspection reports with 90% accuracy .
Hiding in plain sight: insights about health-care trends gained through   open health data,"The open data movement constitutes an approach to achieving accountability for government organizations, and is aligned with one of the sustainable development goals outlined by the United Nations. In the area of health care, government agencies at the Federal and State levels have released open health data consisting of de-identified patient outcomes, costs and ratings. We have applied big data analytics to understand patterns and trends in open health data. We envision the use of this data by concerned citizens to understand both national and local trends in health expenditures. We have built an open-source tool, BOAT (Big Data Open Source Analytics Tool, https://github.com/fdudatamining) to facilitate analytical exploration of open health data sets. We used BOAT to analyze data from the New York Statewide Planning and Research Cooperative System and determined that there has been a significant increase (40 percent) in the incidences of mental health issues amongst adolescents from 2009-2014. Using BOAT we analyzed costs for hip replacement surgery for 168,676 patients is in New York State, and showed that 88% of these patients had surgery costs of less than \$30,000. This figure provides a basis to understand the decision by The California Public Employees' Retirement System to cap hip replacement reimbursements at $30,000, resulting in significant savings. Our tool could enable researchers, hospitals, insurers and citizens to obtain an unbiased view on health-care expenditures, costs and emerging trends. Our tool is especially valuable in the current economic environment, where a significant amount of reporting is controlled by special interests groups and lobbies.","['A. Ravishankar Rao', 'Daniel Clarke']",2017-10-30T16:26:36Z,http://arxiv.org/abs/1710.11047v1,Healthcare & Biomedical AI,Public Health Analytics,"the open data movement is aligned with one of the sustainable development goals outlined by the united nations . we have built an open-source tool, BOAT (Big Data Open Source Analytics Tool), to facilitate analysis of open health data ."
Transitioning towards fit-for-purpose Public Health Surveillance Systems,"The COVID-19 pandemic has exposed several weaknesses in the public health infrastructure, including supply chain mechanisms and public health ICT systems. The expansion of testing and contact tracing has been key to identifying and isolating infected individuals, as well as tracking and containing the spread of the virus. Digital technologies, such as telemedicine and virtual consultations, have experienced a surge in demand to provide medical support while minimizing the risk of transmission and infection. The pandemic has made it clear that cooperation, information sharing, and communication among stakeholders are crucial in making the right decisions and preventing future outbreaks. Redesigning public health systems for effective management of outbreaks should include five key elements: disease surveillance and early warning systems, contact tracing and case management, data analytics and visualization, communication and education, and telemedicine. As the world navigates the COVID-19 pandemic, healthcare ICT systems will play an increasingly important role in the future of healthcare delivery. In a post COVID-19 world, several ICT strategies should be implemented to improve the quality, efficiency, and accessibility of healthcare services, including the expansion of telemedicine, data analytics and population health management, interoperability, and cybersecurity. Overall, this report summarises the importance of early detection and rapid response, international cooperation and coordination, clear and consistent communication, investing in public health systems and emergency preparedness, digital technology and telemedicine, and equity and social determinants of health. These lessons demonstrate the need for better preparedness and planning for future crises and the importance of addressing underlying issues to create a more resilient and accessible digital infrastructure.","['Maria N. Anastasiadou', 'Philippos Isaia', 'Panayiotis Kolios', 'Christos Charalambous']",2023-05-26T11:04:41Z,http://arxiv.org/abs/2305.16821v1,Healthcare & Biomedical AI,Public Health Analytics,the pandemic has exposed several weaknesses in the public health infrastructure . expansion of testing and contact tracing has been key to identifying and isolating infected individuals . digital technologies have experienced a surge in demand to provide medical support .
Estimating spatially varying health effects of wildland fire smoke using   mobile health data,"Wildland fire smoke exposures are an increasing threat to public health, and thus there is a growing need for studying the effects of protective behaviors on reducing health outcomes. Emerging smartphone applications provide unprecedented opportunities to deliver health risk communication messages to a large number of individuals when and where they experience the exposure and subsequently study the effectiveness, but also pose novel methodological challenges. Smoke Sense, a citizen science project, provides an interactive smartphone app platform for participants to engage with information about air quality and ways to protect their health and record their own health symptoms and actions taken to reduce smoke exposure. We propose a new, doubly robust estimator of the structural nested mean model parameter that accounts for spatially- and time-varying effects via a local estimating equation approach with geographical kernel weighting. Moreover, our analytical framework is flexible enough to handle informative missingness by inverse probability weighting of estimating functions. We evaluate the new method using extensive simulation studies and apply it to Smoke Sense data reported by the citizen scientists to increase the knowledge base about the relationship between health preventive measures and improved health outcomes. Our results estimate how the protective behaviors effects vary over space and time and find that protective behaviors have more significant effects on reducing health symptoms in the Southwest than the Northwest region of the USA.","['Lili Wu', 'Chenyin Gao', 'Shu Yang', 'Brian J. Reich', 'Ana G. Rappold']",2020-05-25T10:30:17Z,http://arxiv.org/abs/2005.12017v3,Healthcare & Biomedical AI,Public Health Analytics,smartphone applications provide opportunities to deliver health risk communication messages . smoke Sense provides an interactive smartphone app platform for participants to record health symptoms . the results estimate how the protective behaviors effects vary over space and time .
Causally Linking Health Application Data and Personal Information   Management Tools,"The proliferation of consumer health devices such as smart watches, sleep monitors, smart scales, etc, in many countries, has not only led to growing interest in health monitoring, but also to the development of a countless number of ``smart'' applications to support the exploration of such data by members of the general public, sometimes with integration into professional health services. While a variety of health data streams has been made available by such devices to users, these streams are often presented as separate time-series visualizations, in which the potential relationships between health variables are not explicitly made visible. Furthermore, despite the fact that other aspects of life, such as work and social connectivity, have become increasingly digitised, health and well-being applications make little use of the potentially useful contextual information provided by widely used personal information management tools, such as shared calendar and email systems. This paper presents a framework for the integration of these diverse data sources, analytic and visualization tools, with inference methods and graphical user interfaces to help users by highlighting causal connections among such time-series.","['Saturnino Luz', 'Masood Masoodian']",2023-08-11T19:22:11Z,http://arxiv.org/abs/2308.08556v1,Healthcare & Biomedical AI,Public Health Analytics,a variety of health data streams has been made available by smart watches . but these streams are often presented as separate time-series visualizations . health and well-being applications make little use of potentially useful contextual information .
Web data mining for public health purposes,"For a long time, public health events, such as disease incidence or vaccination activity, have been monitored to keep track of the health status of the population, allowing to evaluate the effect of public health initiatives and to decide where resources for improving public health are best spent. This thesis investigates the use of web data mining for public health monitoring, and makes contributions in the following two areas: New approaches for predicting public health events from web mined data, and novel applications of web mined data for public health monitoring.",['Niels Dalum Hansen'],2019-05-02T16:04:21Z,http://arxiv.org/abs/1905.00829v1,Healthcare & Biomedical AI,Public Health Analytics,this thesis investigates the use of web data mining for public health monitoring . it makes contributions in two areas: new approaches for predicting public health events .
Enabling Cost-Effective Population Health Monitoring By Exploiting   Spatiotemporal Correlation: An Empirical Study,"Because of its important role in health policy-shaping, population health monitoring (PHM) is considered a fundamental block for public health services. However, traditional public health data collection approaches, such as clinic-visit-based data integration or health surveys, could be very costly and time-consuming. To address this challenge, this paper proposes a cost-effective approach called Compressive Population Health (CPH), where a subset of a given area is selected in terms of regions within the area for data collection in the traditional way, while leveraging inherent spatial correlations of neighboring regions to perform data inference for the rest of the area. By alternating selected regions longitudinally, this approach can validate and correct previously assessed spatial correlations. To verify whether the idea of CPH is feasible, we conduct an in-depth study based on spatiotemporal morbidity rates of chronic diseases in more than 500 regions around London for over ten years. We introduce our CPH approach and present three extensive analytical studies. The first confirms that significant spatiotemporal correlations do exist. In the second study, by deploying multiple state-of-the-art data recovery algorithms, we verify that these spatiotemporal correlations can be leveraged to do data inference accurately using only a small number of samples. Finally, we compare different methods for region selection for traditional data collection and show how such methods can further reduce the overall cost while maintaining high PHM quality.","['Dawei Chen', 'Jiangtao Wang', 'Wenjie Ruan', 'Qiang Ni', 'Sumi Helal']",2020-04-25T19:30:39Z,http://arxiv.org/abs/2005.01423v1,Healthcare & Biomedical AI,Public Health Analytics,population health monitoring (PHM) is considered a fundamental block for public health services . traditional data collection approaches could be costly and time-consuming . this paper proposes a cost-effective approach called Compressive Population Health .
HealthPrism: A Visual Analytics System for Exploring Children's Physical   and Mental Health Profiles with Multimodal Data,"The correlation between children's personal and family characteristics (e.g., demographics and socioeconomic status) and their physical and mental health status has been extensively studied across various research domains, such as public health, medicine, and data science. Such studies can provide insights into the underlying factors affecting children's health and aid in the development of targeted interventions to improve their health outcomes. However, with the availability of multiple data sources, including context data (i.e., the background information of children) and motion data (i.e., sensor data measuring activities of children), new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data. Existing statistical hypothesis-based and learning model-based approaches have been inadequate for comprehensively analyzing the complex correlation between multimodal features and multi-dimensional health outcomes due to the limited information revealed. In this work, we first distill a set of design requirements from multiple levels through conducting a literature review and iteratively interviewing 11 experts from multiple domains (e.g., public health and medicine). Then, we propose HealthPrism, an interactive visual and analytics system for assisting researchers in exploring the importance and influence of various context and motion features on children's health status from multi-level perspectives. Within HealthPrism, a multimodal learning model with a gate mechanism is proposed for health profiling and cross-modality feature importance comparison. A set of visualization components is designed for experts to explore and understand multimodal data freely. We demonstrate the effectiveness and usability of HealthPrism through quantitative evaluation of the model performance, case studies, and expert interviews in associated domains.","['Zhihan Jiang', 'Handi Chen', 'Rui Zhou', 'Jing Deng', 'Xinchen Zhang', 'Running Zhao', 'Cong Xie', 'Yifang Wang', 'Edith C. H. Ngai']",2023-07-23T06:41:27Z,http://arxiv.org/abs/2307.12242v2,Healthcare & Biomedical AI,Public Health Analytics,"the correlation between children's personal and family characteristics and their health status has been extensively studied . new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data . a multimodal learning model with a gate mechanism is proposed ."
BCEA: An R Package for Cost-Effectiveness Analysis,"We describe in detail how to perform health economic cost-effectiveness analyses (CEA) using the R package $\textbf{BCEA}$ (Bayesian Cost-Effectiveness Analysis). CEA consist of analytic approaches for combining costs and health consequences of intervention(s). These help to understand how much an intervention may cost (per unit of health gained) compared to an alternative intervention, such as a control or status quo. For resource allocation, a decision maker may wish to know if an intervention is cost saving, and if not then how much more would it cost to implement it compared to a less effective intervention.   Current guidance for cost-effectiveness analyses advocates the quantification of uncertainties which can be represented by random samples obtained from a probability sensitivity analysis or, more efficiently, a Bayesian model. $\textbf{BCEA}$ can be used to post-process the sampled costs and health impacts to perform advanced analyses producing standardised and highly customisable outputs. We present the features of the package, including its many functions and their practical application. $\textbf{BCEA}$ is valuable for statisticians and practitioners working in the field of health economic modelling wanting to simplify and standardise their workflow, for example in the preparation of dossiers in support of marketing authorisation, or academic and scientific publications.","['Nathan Green', 'Anna Heath', 'Gianluca Baio']",2022-03-18T12:21:52Z,http://arxiv.org/abs/2203.09901v1,Healthcare & Biomedical AI,Public Health Analytics,$textbfBCEA$ can be used to post-process the sampled costs and health impacts . the package is useful for statisticians and practitioners working in the field of health economic modelling .
Digital Health and Indoor Air Quality: An IoT-Driven Human-Centred   Visualisation Platform for Behavioural Change and Technology Acceptance,"The detrimental effects of air pollutants on human health have prompted increasing concerns regarding indoor air quality (IAQ). The emergence of digital health interventions and citizen science initiatives has provided new avenues for raising awareness, improving IAQ, and promoting behavioural changes. The Technology Acceptance Model (TAM) offers a theoretical framework to understand user acceptance and adoption of IAQ technology. This paper presents a case study using the COM-B model and Internet of Things (IoT) technology to design a human-centred digital visualisation platform, leading to behavioural changes and improved IAQ. The study also investigates users' acceptance and adoption of the technology, focusing on their experiences, expectations, and the impact on IAQ. Integrating IAQ sensing, digital health-related interventions, citizen science, and the TAM model offers opportunities to address IAQ challenges, enhance public health, and foster sustainable indoor environments. The analytical results show that factors such as human behaviour, indoor activities, and awareness play crucial roles in shaping IAQ.","['Rameez Raja Kureshi', 'Bhupesh Kumar Mishra', 'Dhavalkumar Thakker', 'Suvodeep Mazumdar', 'Xiao Li']",2024-05-20T15:13:25Z,http://arxiv.org/abs/2405.13064v1,Healthcare & Biomedical AI,Public Health Analytics,"the technology acceptance model (TAM) offers a theoretical framework to understand user acceptance and adoption of IAQ technology . the case study uses the COM-B model and Internet of Things (ioT) technology to design a human-centred digital visualisation platform, leading to behavioural changes and improved IAQ ."
Intelligent Surveillance of World Health Organization (WHO) Integrated   Disease Surveillance and Response (IDSR) Data in Cameroon Using Multivariate   Cross-Correlation,"As developing countries continue to face challenges associated with infectious diseases, the need to improve infrastructure to systematically collect data which can be used to understand their outbreak patterns becomes more critical. The World Health Organization (WHO) Integrated Disease Surveillance and Response (IDSR) strategy seeks to drive the systematic collection of surveillance data to strengthen district-level reporting and to translate them into public health actions. Since the analysis of this surveillance data at the central levels of government in many developing nations has traditionally not included advanced analytics, there are opportunities for the development and exploration of computational approaches that can provide proactive insights and improve general health outcomes of infectious disease outbreaks. We propose and demonstrate a multivariate time series cross-correlation analysis as a foundational step towards gaining insight on infectious disease patterns via the pairwise computation of weighted cross-correlation scores for a specified disease across different health districts using surveillance data from Cameroon. Following the computation of weighted cross-correlation scores, we apply an anomaly detection algorithm to assess how outbreak alarm patterns align in highly correlated health districts. We demonstrate how multivariate cross-correlation analysis of weekly surveillance data can provide insight into infectious disease incidence patterns in Cameroon by identifying highly correlated health districts for a given disease. We further demonstrate scenarios in which identification of highly correlated districts aligns with alarms flagged using a standard anomaly detection algorithm, hinting at the potential of end to end solutions combining anomaly detection algorithms for flagging alarms in combination with multivariate cross-correlation analysis.","['Jianzhi Liu', 'Ziming Yang', 'Jesse E. Engelberg', 'Frankline S. Nsai', 'Serge Bataliack', 'Vikash Singh']",2019-10-17T07:13:22Z,http://arxiv.org/abs/1910.07741v1,Healthcare & Biomedical AI,Public Health Analytics,the world health organization (WHO) Integrated Disease Surveillance and Response (IDSR) strategy seeks to drive the systematic collection of surveillance data . a multivariate time series cross-correlation analysis is a foundational step towards gaining insight on infectious disease patterns .
Adaptive dynamics for individual payoff game-theoretic models of   vaccination,"Vaccination is widely recognised as one of the most effective forms of public health interventions. Individuals decisions regarding vaccination creates a complex social dilemma between individual and collective interests, where each person's decision affects the overall public health outcome. In this paper, we study the adaptive dynamics for the evolutionary dynamics of strategies in a fundamental game-theoretic model of vaticination. We show the existence of an (Nash) equilibrium and analyse the stability and bifurcations when varying the relevant parameters. We also demonstrate our analytical results by several concrete examples.","['Nataliya A. Balabanova', 'Manh Hong Duong']",2024-11-14T15:29:44Z,http://arxiv.org/abs/2411.09519v3,Healthcare & Biomedical AI,Public Health Analytics,Vaccination is widely recognised as one of the most effective forms of public health interventions . individuals decisions regarding vaccination creates a complex social dilemma between individual and collective interests .
Efficient Social Distancing for COVID-19: An Integration of Economic   Health and Public Health,"Social distancing has been the only effective way to contain the spread of an infectious disease prior to the availability of the pharmaceutical treatment. It can lower the infection rate of the disease at the economic cost. A pandemic crisis like COVID-19, however, has posed a dilemma to the policymakers since a long-term restrictive social distancing or even lockdown will keep economic cost rising. This paper investigates an efficient social distancing policy to manage the integrated risk from economic health and public health issues for COVID-19 using a stochastic epidemic modeling with mobility controls. The social distancing is to restrict the community mobility, which was recently accessible with big data analytics. This paper takes advantage of the community mobility data to model the COVID-19 processes and infer the COVID-19 driven economic values from major market index price, which allow us to formulate the search of the efficient social distancing policy as a stochastic control problem. We propose to solve the problem with a deep-learning approach. By applying our framework to the US data, we empirically examine the efficiency of the US social distancing policy and offer recommendations generated from the algorithm.","['Kexin Chen', 'Chi Seng Pun', 'Hoi Ying Wong']",2020-12-04T04:20:34Z,http://arxiv.org/abs/2012.02397v1,Healthcare & Biomedical AI,Public Health Analytics,social distancing has been the only effective way to contain the spread of infectious disease . but a long-term restrictive social policy or even lockdown will keep economic cost rising . this paper investigates an efficient policy to manage the risk of COVID-19 .
SoA-Fog: Secure Service-Oriented Edge Computing Architecture for Smart   Health Big Data Analytics,"The smart health paradigms employ Internet-connected wearables for telemonitoring, diagnosis for providing inexpensive healthcare solutions. Fog computing reduces latency and increases throughput by processing data near the body sensor network. In this paper, we proposed a secure serviceorientated edge computing architecture that is validated on recently released public dataset. Results and discussions support the applicability of proposed architecture for smart health applications. We proposed SoA-Fog i.e. a three-tier secure framework for efficient management of health data using fog devices. It discuss the security aspects in client layer, fog layer and the cloud layer. We design the prototype by using win-win spiral model with use case and sequence diagram. Overlay analysis was performed using proposed framework on malaria vector borne disease positive maps of Maharastra state in India from 2011 to 2014. The mobile clients were taken as test case. We performed comparative analysis between proposed secure fog framework and state-of-the art cloud-based framework.","['Rabindra K. Barik', 'Harishchandra Dubey', 'Kunal Mankodiya']",2017-12-25T16:29:45Z,http://arxiv.org/abs/1712.09098v1,Healthcare & Biomedical AI,Public Health Analytics,fog computing reduces latency and increases throughput by processing data near body sensor network . paper proposes a three-tier secure framework for efficient management of health data using fog devices .
Cost-Sensitive Diagnosis and Learning Leveraging Public Health Data,"Traditionally, machine learning algorithms rely on the assumption that all features of a given dataset are available for free. However, there are many concerns such as monetary data collection costs, patient discomfort in medical procedures, and privacy impacts of data collection that require careful consideration in any real-world health analytics system. An efficient solution would only acquire a subset of features based on the value it provides while considering acquisition costs. Moreover, datasets that provide feature costs are very limited, especially in healthcare. In this paper, we provide a health dataset as well as a method for assigning feature costs based on the total level of inconvenience asking for each feature entails. Furthermore, based on the suggested dataset, we provide a comparison of recent and state-of-the-art approaches to cost-sensitive feature acquisition and learning. Specifically, we analyze the performance of major sensitivity-based and reinforcement learning based methods in the literature on three different problems in the health domain, including diabetes, heart disease, and hypertension classification.","['Mohammad Kachuee', 'Kimmo Karkkainen', 'Orpaz Goldstein', 'Davina Zamanzadeh', 'Majid Sarrafzadeh']",2019-02-19T15:37:13Z,http://arxiv.org/abs/1902.07102v2,Healthcare & Biomedical AI,Public Health Analytics,"there are concerns such as monetary data collection costs and patient discomfort . datasets that provide feature costs are very limited, especially in healthcare . we compare recent and state-of-the-art approaches to cost-sensitive feature acquisition ."
Advancing Mental Health Pre-Screening: A New Custom GPT for   Psychological Distress Assessment,"This study introduces 'Psycho Analyst', a custom GPT model based on OpenAI's GPT-4, optimized for pre-screening mental health disorders. Enhanced with DSM-5, PHQ-8, detailed data descriptions, and extensive training data, the model adeptly decodes nuanced linguistic indicators of mental health disorders. It utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment, showcasing refined analytic capabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1 scores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of 2.89 and 3.69 in PHQ-8 scoring. These results highlight the model's precision and transformative potential in enhancing public mental health support, improving accessibility, cost-effectiveness, and serving as a second opinion for professionals.","['Jinwen Tang', 'Yi Shang']",2024-08-03T00:38:30Z,http://arxiv.org/abs/2408.01614v2,Healthcare & Biomedical AI,Public Health Analytics,"'Psycho Analyst' is a custom GPT model optimized for pre-screening mental health disorders . it utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment ."
From Personalized Medicine to Population Health: A Survey of mHealth   Sensing Techniques,"Mobile Sensing Apps have been widely used as a practical approach to collect behavioral and health-related information from individuals and provide timely intervention to promote health and well-beings, such as mental health and chronic cares. As the objectives of mobile sensing could be either \emph{(a) personalized medicine for individuals} or \emph{(b) public health for populations}, in this work we review the design of these mobile sensing apps, and propose to categorize the design of these apps/systems in two paradigms -- \emph{(i) Personal Sensing} and \emph{(ii) Crowd Sensing} paradigms. While both sensing paradigms might incorporate with common ubiquitous sensing technologies, such as wearable sensors, mobility monitoring, mobile data offloading, and/or cloud-based data analytics to collect and process sensing data from individuals, we present a novel taxonomy system with two major components that can specify and classify apps/systems from aspects of the life-cycle of mHealth Sensing: \emph{(1) Sensing Task Creation \& Participation}, \emph{(2) Health Surveillance \& Data Collection}, and \emph{(3) Data Analysis \& Knowledge Discovery}. With respect to different goals of the two paradigms, this work systematically reviews this field, and summarizes the design of typical apps/systems in the view of the configurations and interactions between these two components. In addition to summarization, the proposed taxonomy system also helps figure out the potential directions of mobile sensing for health from both personalized medicines and population health perspectives.","['Zhiyuan Wang', 'Haoyi Xiong', 'Jie Zhang', 'Sijia Yang', 'Mehdi Boukhechba', 'Laura E. Barnes', 'Daqing Zhang', 'Dejing Dou']",2021-07-02T10:16:21Z,http://arxiv.org/abs/2107.00948v3,Healthcare & Biomedical AI,Public Health Analytics,mobile sensing apps have been widely used to collect behavioral and health-related information from individuals . a proposed taxonomy system can specify and classify apps/systems from aspects of the life-cycle of mHealth Sensing . the objectives could be either personalized medicine for individuals or public health for populations .
Integrating Zero-Shot Classification to Advance Long COVID Literature: A   Systematic Social Media-Centered Review,"Long COVID continues to challenge public health by affecting a significant segment of individuals who have recovered from acute SARS-CoV-2 infection yet endure prolonged and often debilitating symptoms. Social media has emerged as a vital resource for those seeking real-time information, peer support, and validating their health concerns related to Long COVID. This paper examines recent works focusing on mining, analyzing, and interpreting user-generated content on social media platforms such as Twitter, Reddit, Facebook, and YouTube to capture the broader discourse on persistent post-COVID conditions. A novel transformer-based zero-shot learning approach serves as the foundation for classifying research papers in this area into four primary categories: Clinical or Symptom Characterization, Advanced NLP or Computational Methods, Policy, Advocacy, or Public Health Communication, and Online Communities and Social Support. This methodology showcases the adaptability of advanced language models in categorizing research papers without predefined training labels, thus enabling a more rapid and scalable assessment of existing literature. This review highlights the multifaceted nature of Long COVID research, where computational techniques applied to social media data reveal insights into narratives of individuals suffering from Long COVID. This review also demonstrates the capacity of social media analytics to inform clinical practice and contribute to policy-making related to Long COVID.",['Nirmalya Thakur'],2024-12-25T05:01:17Z,http://arxiv.org/abs/2412.18779v1,Healthcare & Biomedical AI,Public Health Analytics,social media has emerged as a vital resource for those seeking real-time information . this review highlights the multifaceted nature of Long COVID research . it demonstrates the capacity of social media analytics to inform clinical practice .
What's unusual in online disease outbreak news?,"Background: Accurate and timely detection of public health events of international concern is necessary to help support risk assessment and response and save lives. Novel event-based methods that use the World Wide Web as a signal source offer potential to extend health surveillance into areas where traditional indicator networks are lacking. In this paper we address the issue of systematically evaluating online health news to support automatic alerting using daily disease-country counts text mined from real world data using BioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration detection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance against expert moderated ProMED-mail postings. Results: We report sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), mean alerts/100 days and F1, at 95% confidence interval (CI) for 287 ProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period. Results indicate that W2 had the best F1 with a slight benefit for day of week effect over C2. In drill down analysis we indicate issues arising from the granular choice of country-level modeling, sudden drops in reporting due to day of week effects and reporting bias. Automatic alerting has been implemented in BioCaster available from http://born.nii.ac.jp. Conclusions: Online health news alerts have the potential to enhance manual analytical methods by increasing throughput, timeliness and detection rates. Systematic evaluation of health news aberrations is necessary to push forward our understanding of the complex relationship between news report volumes and case numbers and to select the best performing features and algorithms.",['Nigel Collier'],2011-10-13T23:22:43Z,http://arxiv.org/abs/1110.3091v1,Healthcare & Biomedical AI,Public Health Analytics,novel event-based methods that use the world wide web as a signal source offer potential to extend health surveillance into areas where traditional indicator networks are lacking . systematically evaluating online health news to support automatic alerting .
Formal Verification of Access Control Model for My Health Record System,"My Health Record system is the Australian Government's digital health record system that holds My Health Record. My Health Record is a secure online health record containing consumers' health information. The system aims to provide health care professionals with access to key health information, e.g. listing medicines, allergies and key diagnoses; radiology and pathology test results. The system (previously named Personally Controlled Electronic Health Record) enables consumers to decide how to share information with any of their health care providers who are registered and connected to the system. The My Health Record system operates under the Australian legislative framework My Health Records Act 2012. The Act establishes, inter alia, a privacy framework specifying which entities can collect, use and disclose certain information in the system and the penalties that can be imposed on improper collection, use and disclosure of this information. This paper presents the formal specification (from the legislation) and verification of the My Health Record regarding how consumers can control who access the information, and how the system adheres to such access. We rely on the correct-by-construction Event-B method to prove control and access properties of the system.",['Victor Rivera'],2020-06-12T03:47:59Z,http://arxiv.org/abs/2006.06933v1,Healthcare & Biomedical AI,Electronic Health Records,my Health Record is the Australian Government's digital health record system . the system aims to provide health care professionals with access to key health information . it enables consumers to decide how to share information with any of their health care providers .
Health Information Standardisation as a basis for Learning Health   Systems,"Standardisation of healthcare has been the focus of hospital management and clinicians since the 1990's. Electronic health records were already intended to provide clinicians with real-time access to clinical knowledge and care plans while also recording and storing vast amounts of patient data. It took more than three decades for electronic health records to start to become ubiquitous in all aspects of healthcare. Learning health systems are the next stage in health information systems whose potential benefits have been promoted for more than a decade - yet few are seen in clinical practice. Clinical care process specifications are a primary form of clinical documentation used in all aspects of healthcare, but they lack standardisation. This thesis contends that this lack of standardisation was inherited by electronic health records and that this is a significant issue holding back the development and adoption of learning health systems. Standardisation of clinical documents is used to mitigate issues in electronic health records as a basis for enabling learning health systems. One type of clinical document, the caremap, is standardised in order to achieve an effective approach to containing resources and ensuring consistency and quality. This led not only to improved clinicians' comprehension and acceptance of the clinical document, but also to reduced time expended in developing complicated learning health systems built using the input of clinical experts.",['Scott McLachlan'],2020-03-30T12:42:29Z,http://arxiv.org/abs/2004.04811v1,Healthcare & Biomedical AI,Electronic Health Records,it took more than three decades for electronic health records to become ubiquitous in all aspects of healthcare . learning health systems are the next stage in health information systems whose potential benefits have been promoted .
A Bayesian Approach to Modelling Longitudinal Data in Electronic Health   Records,"Analyzing electronic health records (EHR) poses significant challenges because often few samples are available describing a patient's health and, when available, their information content is highly diverse. The problem we consider is how to integrate sparsely sampled longitudinal data, missing measurements informative of the underlying health status and fixed demographic information to produce estimated survival distributions updated through a patient's follow up. We propose a nonparametric probabilistic model that generates survival trajectories from an ensemble of Bayesian trees that learns variable interactions over time without specifying beforehand the longitudinal process. We show performance improvements on Primary Biliary Cirrhosis patient data.","['Alexis Bellot', 'Mihaela van der Schaar']",2019-12-19T09:42:27Z,http://arxiv.org/abs/1912.09086v1,Healthcare & Biomedical AI,Electronic Health Records,a nonparametric probabilistic model generates survival trajectories from an ensemble of Bayesian trees . the model learns variable interactions over time without specifying beforehand the longitudinal process .
Exploiting Multimodal Biometrics in E-Privacy Scheme for Electronic   Health Records,"Existing approaches to protect the privacy of Electronic Health Records are either insufficient for existing medical laws or they are too restrictive in their usage. For example, smart card-based encryption systems require the patient to be always present to authorize access to medical records. Questionnaires were administered by 50 medical practitioners to identify and categorize different Electronic Health Records attributes. The system was implemented using multi biometrics of patients to access patient record in pre-hospital care.The software development tools employed were JAVA and MySQL database. The system provides applicable security when patients records are shared either with other practitioners, employers, organizations or research institutes. The result of the system evaluation shows that the average response time of 6 seconds and 11.1 seconds for fingerprint and iris respectively after ten different simulations. The system protects privacy and confidentiality by limiting the amount of data exposed to users.The system also enables emergency medical technicians to gain easy and reliable access to necessary attributes of patients Electronic Health Records while still maintaining the privacy and confidentiality of the data using the patients fingerprint and iris.","['Adebayo Omotosho', 'Omotanwa Adegbola', 'Barakat Adelakin', 'Adeyemi Adelakun', 'Justice Emuoyibofarhe']",2015-02-04T15:37:19Z,http://arxiv.org/abs/1502.01233v1,Healthcare & Biomedical AI,Electronic Health Records,a smart card-based encryption system requires the patient to be always present . the system provides applicable security when patients records are shared . it protects privacy by limiting the amount of data exposed to users .
Identifying Mentions of Pain in Mental Health Records Text: A Natural   Language Processing Approach,"Pain is a common reason for accessing healthcare resources and is a growing area of research, especially in its overlap with mental health. Mental health electronic health records are a good data source to study this overlap. However, much information on pain is held in the free text of these records, where mentions of pain present a unique natural language processing problem due to its ambiguous nature. This project uses data from an anonymised mental health electronic health records database. The data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not. This will facilitate the extraction of relevant pain information from large databases, and the use of such outputs for further studies on pain and mental health. 1,985 documents were manually triple-annotated for creation of gold standard training data, which was used to train three commonly used classification algorithms. The best performing model achieved an F1-score of 0.98 (95% CI 0.98-0.99).","['Jaya Chaturvedi', 'Sumithra Velupillai', 'Robert Stewart', 'Angus Roberts']",2023-04-03T11:56:11Z,http://arxiv.org/abs/2304.01240v2,Healthcare & Biomedical AI,Electronic Health Records,this project uses data from an anonymised mental health electronic health records database . the data are used to train a machine learning based classification algorithm to classify sentences as discussing patient pain or not .
Improving information retrieval from electronic health records using   dynamic and multi-collaborative filtering,"Due to the rapid growth of information available about individual patients, most physicians suffer from information overload when they review patient information in health information technology systems. In this manuscript, we present a novel hybrid dynamic and multi-collaborative filtering method to improve information retrieval from electronic health records. This method recommends relevant information from electronic health records for physicians during patient visits. It models information search dynamics using a Markov model. It also leverages the key idea of collaborative filtering, originating from Recommender Systems, to prioritize information based on various similarities among physicians, patients and information items. We tested this new method using real electronic health record data from the Indiana Network for Patient Care. Our experimental results demonstrated that for 46.7% of testing cases, this new method is able to correctly prioritize relevant information among top-5 recommendations that physicians are truly interested in.","['Ziwei Fan', 'Evan Burgun', 'Zhiyun Ren', 'Titus Schleyer', 'Xia Ning']",2020-08-12T15:46:33Z,http://arxiv.org/abs/2008.05399v1,Healthcare & Biomedical AI,Electronic Health Records,"a new filtering method recommends relevant information from electronic health records . it leverages collaborative filtering to prioritize information based on similarities among physicians, patients and information items . the method is able to correctly prioritize relevant information among top-5 recommendations ."
Privacy Preserving Machine Learning for Electronic Health Records using   Federated Learning and Differential Privacy,"An Electronic Health Record (EHR) is an electronic database used by healthcare providers to store patients' medical records which may include diagnoses, treatments, costs, and other personal information. Machine learning (ML) algorithms can be used to extract and analyze patient data to improve patient care. Patient records contain highly sensitive information, such as social security numbers (SSNs) and residential addresses, which introduces a need to apply privacy-preserving techniques for these ML models using federated learning and differential privacy.","['Naif A. Ganadily', 'Han J. Xia']",2024-06-23T00:01:03Z,http://arxiv.org/abs/2406.15962v1,Healthcare & Biomedical AI,Electronic Health Records,"machine learning (ML) algorithms can be used to extract and analyze patient data . patient records contain highly sensitive information, such as social security numbers ."
Predicting Stroke from Electronic Health Records,"Studies have identified various risk factors associated with the onset of stroke in an individual. Data mining techniques have been used to predict the occurrence of stroke based on these factors by using patients' medical records. However, there has been limited use of electronic health records to study the inter-dependency of different risk factors of stroke. In this paper, we perform an analysis of patients' electronic health records to identify the impact of risk factors on stroke prediction. We also provide benchmark performance of the state-of-art machine learning algorithms for predicting stroke using electronic health records.","['Chidozie Shamrock Nwosu', 'Soumyabrata Dev', 'Peru Bhardwaj', 'Bharadwaj Veeravalli', 'Deepu John']",2019-04-25T12:02:16Z,http://arxiv.org/abs/1904.11280v1,Healthcare & Biomedical AI,Electronic Health Records,data mining techniques have been used to predict the occurrence of stroke . there has been limited use of electronic health records to study the inter-dependency of different risk factors . we provide benchmark performance of state-of-art machine learning algorithms .
An Ontology for the Social Determinants of Health Domain,"Social determinants of health are societal factors, such as where a person was born, grew up, works, lives, etc, along with socioeconomic and community factors that affect individual health. Social Determinants of Health are correlated with many clinical outcomes, hence it is desirable to record SDOH data in Electronic Health Records (EHRs). Besides storing images, text, etc., EHRs rely on coded terms available in standard ontologies and terminologies to record observations and analyses. There is a substantial amount of research on understanding the clinical impact of SDOH, ranging from screening tools to practice based interventions. However, there is no comprehensive collection of terms for recording SDOH observations in EHRs. Our research goal is to develop an ontology that covers the terms describing SDOH. We present a prototype ontology called Social Determinant of Health Ontology (SOHO) that covers relevant concepts and IS--A relationships describing impacts and associations of social determinants. We describe the evaluation techniques that we applied to SOHO, including human experts review and algorithmic evaluation.","['Navya Martin Kollapally', 'Yan Chen', 'Julia Xu', 'James Geller']",2022-11-15T01:36:40Z,http://arxiv.org/abs/2211.07837v1,Healthcare & Biomedical AI,Electronic Health Records,social determinants of health are correlated with many clinical outcomes . there is a substantial amount of research on understanding the clinical impact of SDOH . but there is no comprehensive collection of terms for recording SDOH observations in EHRs .
Deep learning for prediction of population health costs,"Accurate prediction of healthcare costs is important for optimally managing health costs. However, methods leveraging the medical richness from data such as health insurance claims or electronic health records are missing. Here, we developed a deep neural network to predict future cost from health insurance claims records. We applied the deep network and a ridge regression model to a sample of 1.4 million German insurants to predict total one-year health care costs. Both methods were compared to Morbi-RSA models with various performance measures and were also used to predict patients with a change in costs and to identify relevant codes for this prediction. We showed that the neural network outperformed the ridge regression as well as all Morbi-RSA models for cost prediction. Further, the neural network was superior to ridge regression in predicting patients with cost change and identified more specific codes. In summary, we showed that our deep neural network can leverage the full complexity of the patient records and outperforms standard approaches. We suggest that the better performance is due to the ability to incorporate complex interactions in the model and that the model might also be used for predicting other health phenotypes.","['Philipp Drewe-Boss', 'Dirk Enders', 'Jochen Walker', 'Uwe Ohler']",2020-03-06T23:33:39Z,http://arxiv.org/abs/2003.03466v1,Healthcare & Biomedical AI,Electronic Health Records,a deep neural network predicts future cost from health insurance claims records . the model outperforms standard methods for cost prediction . it might also be used to predict other health phenotypes .
Demographical Priors for Health Conditions Diagnosis Using Medicare Data,"This paper presents an example of how demographical characteristics of patients influence their susceptibility to certain medical conditions. In this paper, we investigate the association of health conditions to age of patients in a heterogeneous population. We show that besides the symptoms a patients is having, the age has the potential of aiding the diagnostic process in hospitals. Working with Electronic Health Records (EHR), we show that medical conditions group into clusters that share distinctive population age densities. We use Electronic Health Records from Brazil for a period of 15 months from March of 2013 to July of 2014. The number of patients in the data is 1.7 million patients and the number of records is 47 million records. The findings has the potential of helping in a setting where an automated system undergoes the task of predicting the condition of a patient given their symptoms and demographical information.","['Fahad Alhasoun', 'May Alhazzani', 'Marta C. González']",2016-12-07T21:27:36Z,http://arxiv.org/abs/1612.02460v2,Healthcare & Biomedical AI,Electronic Health Records,this paper investigates the association of health conditions to age of patients . medical conditions group into clusters that share distinctive population age densities . the findings has the potential of helping in a setting where an automated system is .
Health Analytics: a systematic review of approaches to detect phenotype   cohorts using electronic health records,"The paper presents a systematic review of state-of-the-art approaches to identify patient cohorts using electronic health records. It gives a comprehensive overview of the most commonly de-tected phenotypes and its underlying data sets. Special attention is given to preprocessing of in-put data and the different modeling approaches. The literature review confirms natural language processing to be a promising approach for electronic phenotyping. However, accessibility and lack of natural language process standards for medical texts remain a challenge. Future research should develop such standards and further investigate which machine learning approaches are best suited to which type of medical data.","['Norman Hiob', 'Stefan Lessmann']",2017-07-24T07:19:57Z,http://arxiv.org/abs/1707.07425v1,Healthcare & Biomedical AI,Electronic Health Records,the paper presents a systematic review of state-of-the-art approaches to identify patient cohorts using electronic health records . it gives a comprehensive overview of the most commonly de-tected phenotypes and its underlying data sets .
Redesigning Electronic Health Record Systems to Support Developing   Countries,"Electronic Health Record (EHR) has become an essential tool in the healthcare ecosystem, providing authorized clinicians with patients' health-related information for better treatment. While most developed countries are taking advantage of EHRs to improve their healthcare system, it remains challenging in developing countries to support clinical decision-making and public health using a computerized patient healthcare information system. This paper proposes a novel EHR architecture suitable for developing countries--an architecture that fosters inclusion and provides solutions tailored to all social classes and socioeconomic statuses. Our architecture foresees an internet-free (offline) solution to allow medical transactions between healthcare organizations, and the storage of EHRs in geographically underserved and rural areas. Moreover, we discuss how artificial intelligence can leverage anonymous health-related information to enable better public health policy and surveillance.","['Jean Marie Tshimula', ""D'Jeff K. Nkashama"", 'Kalonji Kalala', 'Maximilien V. Dialufuma', 'Mbuyi Mukendi Didier', 'Hugues Kanda', 'Jean Tshibangu Muabila', 'Christian N. Mayemba']",2023-01-31T19:16:38Z,http://arxiv.org/abs/2302.01281v1,Healthcare & Biomedical AI,Electronic Health Records,most developed countries are taking advantage of EHRs to improve their healthcare system . but it remains challenging in developing countries to support clinical decision-making . our architecture foresees an internet-free (offline) solution to allow medical transactions .
Electronical Health Record's Systems. Interoperability,"Understanding the importance that the electronic medical health records system has, with its various structural types and grades, has led to the elaboration of a series of standards and quality control methods, meant to control its functioning. In time, the electronic health records system has evolved along with the medical data change of structure. Romania has not yet managed to fully clarify this concept, various definitions still being encountered, such as ""Patient's electronic chart"", ""Electronic health file"". A slow change from functional interoperability (OSI level 6) to semantic interoperability (level 7) is being aimed at the moment. This current article will try to present the main electronic files models, from a functional interoperability system's possibility to be created perspective.","['Simona Angela Apostol', 'Cosmin Catu', 'Corina Vernic']",2009-02-26T09:06:29Z,http://arxiv.org/abs/0902.4535v1,Healthcare & Biomedical AI,Electronic Health Records,the electronic medical health records system has evolved along with the medical data change of structure . a slow change from functional interoperability (OSI level 6) to semantic interoperability (level 7) is being aimed at the moment .
"Cloud-based Electronic Health Records for Real-time, Region-specific   Influenza Surveillance","Accurate real-time monitoring systems of influenza outbreaks help public health officials make informed decisions that may help save lives. We show that information extracted from cloud-based electronic health records databases, in combination with machine learning techniques and historical epidemiological information, have the potential to accurately and reliably provide near real-time regional predictions of flu outbreaks in the United States.","['Mauricio Santillana', 'Andre Nguyen', 'Tamara Louie', 'Anna Zink', 'Josh Gray', 'Iyue Sung', 'John S. Brownstein']",2015-12-13T02:51:36Z,http://arxiv.org/abs/1512.03990v1,Healthcare & Biomedical AI,Electronic Health Records,we show that cloud-based data can provide near real-time regional predictions of flu outbreaks . machine learning and historical epidemiological information can also be used to make predictions .
Predicting Severe Sepsis Using Text from the Electronic Health Record,"Employing a machine learning approach we predict, up to 24 hours prior, a diagnosis of severe sepsis. Strongly predictive models are possible that use only text reports from the Electronic Health Record (EHR), and omit structured numerical data. Unstructured text alone gives slightly better performance than structured data alone, and the combination further improves performance. We also discuss advantages of using unstructured EHR text for modeling, as compared to structured EHR data.","['Phil Culliton', 'Michael Levinson', 'Alice Ehresman', 'Joshua Wherry', 'Jay S. Steingrub', 'Stephen I. Gallant']",2017-11-30T17:52:07Z,http://arxiv.org/abs/1711.11536v1,Healthcare & Biomedical AI,Electronic Health Records,machine learning predicts a diagnosis of severe sepsis up to 24 hours prior . text reports from the electronic health record (EHR) and omit structured numerical data . strong predictive models are possible that use only text reports .
Bidirectional Recurrent Neural Networks for Medical Event Detection in   Electronic Health Records,"Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored various recurrent neural network frameworks and show that they significantly outperformed the CRF models.","['Abhyuday Jagannatha', 'Hong Yu']",2016-06-25T19:46:28Z,http://arxiv.org/abs/1606.07953v2,Healthcare & Biomedical AI,Electronic Health Records,the state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows .
Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic   Health Records,"Extractive summarization is very useful for physicians to better manage and digest Electronic Health Records (EHRs). However, the training of a supervised model requires disease-specific medical background and is thus very expensive. We studied how to utilize the intrinsic correlation between multiple EHRs to generate pseudo-labels and train a supervised model with no external annotation. Experiments on real-patient data validate that our model is effective in summarizing crucial disease-specific information for patients.","['Xiangan Liu', 'Keyang Xu', 'Pengtao Xie', 'Eric Xing']",2018-11-20T01:08:09Z,http://arxiv.org/abs/1811.08040v3,Healthcare & Biomedical AI,Electronic Health Records,extractive summarization is useful for physicians to better manage and digest electronic health records . training of a supervised model requires disease-specific medical background and is thus very expensive .
Identifying Health Risks from Family History: A Survey of Natural   Language Processing Techniques,"Electronic health records include information on patients' status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs.   We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on large-scale pre-trained language models. In addition to the areas where NLP has successfully been utilised, we also identify the areas where more research is needed to unlock the value of patients' records regarding data collection, task formulation and downstream applications.","['Xiang Dai', 'Sarvnaz Karimi', ""Nathan O'Callaghan""]",2024-03-15T03:43:07Z,http://arxiv.org/abs/2403.09997v1,Healthcare & Biomedical AI,Electronic Health Records,electronic health records include information on patients' status and medical history . this could cover the history of diseases and disorders that could be hereditary . natural language processing (NLP) and machine learning techniques can assist with identifying health risks .
Leveraging Technology for Healthcare and Retaining Access to Personal   Health Data to Enhance Personal Health and Well-being,"Health data is a sensitive category of personal data. It might result in a high risk to individual and health information handling rights and opportunities unless there is a palatable defense. Reasonable security standards are needed to protect electronic health records (EHR). All personal data handling needs adequate explanation. Maintaining access to medical data even in the developing world would favor health and well-being across the world. Unfortunately, there are still countries that hinder the portability of medical records. Numerous occurrences have shown that it still takes weeks for the medical data to be ported from one general physician (GP) to another. Cross border portability is nearly impossible due to the lack of technical infrastructure and standardization. We demonstrate the difficulty of the portability of medical records with some example case studies as a collaborative engagement exercise through a data mapping process to describe how different people and datapoints interact and evaluate EHR portability techniques. We then propose a blockchain-based EHR system that allows secure, and cross border sharing of medical data. The ethical and technical challenges around having such a system have also been discussed in this study.","['Ayan Chatterjee', 'Ali Shahaab', 'Martin W. Gerdes', 'Santiago Martinez', 'Pankaj Khatiwada']",2020-10-20T13:56:15Z,http://arxiv.org/abs/2010.10285v1,Healthcare & Biomedical AI,Electronic Health Records,"there are still countries that hinder the portability of medical records . cross border portability is nearly impossible due to the lack of technical infrastructure . we propose a blockchain-based system that allows secure, cross border sharing of medical data ."
HealthAdvisor: Recommendation System for Wearable Technologies enabling   Proactive Health Monitoring,"Proactive monitoring of one's health could avoid serious diseases as well as better maintain the individual's well-being. In today's IoT world, there has been numerous wearable technological devices to monitor/measure different health attributes. However, with that increasing number of attributes and wearables, it becomes unclear to the individual which ones they should be using. The aim of this paper is to provide a recommendation engine for personalized recommended wearables for any given individual. The way the engine works is through first identifying the diseases that this person is at risk of, given his/her attributes and medical history. We built a machine learning classification model for this task. Second, these diseases are mapped to the attributes that need to be measured in order to monitor such diseases. Third, we map these measurements to the appropriate wearable technologies. This is done via a textual analytics model that we developed that uses available information of different wearables to map the aforementioned measurements to these wearables. The output can be used to recommend the wearables to individuals as well as provide a feedback to wearable developers for common measurements that do not have corresponding wearables today.","['Shubhi Asthana', 'Ray Strong', 'Aly Megahed']",2016-12-02T19:28:58Z,http://arxiv.org/abs/1612.00800v1,Healthcare & Biomedical AI,Wearable Health Devices,"the aim of this paper is to provide a recommendation engine for personalized recommended wearables . the engine works by first identifying the diseases that this person is at risk of . then, these diseases are mapped to the attributes that need to"
Functional Data Analysis on Wearable Sensor Data: A Systematic Review,"Wearable devices and sensors have recently become a popular way to collect data, especially in the health sciences. The use of sensors allows patients to be monitored over a period of time with a high observation frequency. Due to the continuous-on-time structure of the data, novel statistical methods are recommended for the analysis of sensor data. One of the popular approaches in the analysis of wearable sensor data is functional data analysis. The main objective of this paper is to review functional data analysis methods applied to wearable device data according to the type of sensor. In addition, we introduce several freely available software packages and open databases of wearable device data to facilitate access to sensor data in different fields.","['Nihan Acar-Denizli', 'Pedro Delicado']",2024-10-15T12:51:11Z,http://arxiv.org/abs/2410.11562v1,Healthcare & Biomedical AI,Wearable Health Devices,wearable devices and sensors have become a popular way to collect data . the use of sensors allows patients to be monitored over a period of time . functional data analysis is one of the popular approaches in the analysis of wearable sensor data 
Investigating Opportunities to Support Kids' Agency and Well-being: A   Review of Kids' Wearables,"Wearable devices hold great potential for promoting children's health and well-being. However, research on kids' wearables is sparse and often focuses on their use in the context of parental surveillance. To gain insight into the current landscape of kids' wearables, we surveyed 47 wearable devices marketed for children. We collected rich data on the functionality of these devices and assessed how different features satisfy parents' information needs, and identified opportunities for wearables to support children's needs and interests. We found that many kids' wearables are technologically sophisticated devices that focus on parents' ability to communicate with their children and keep them safe, as well as encourage physical activity and nurture good habits. We discuss how our findings could inform the design of wearables that serve as more than monitoring devices, and instead support children and parents as equal stakeholders, providing implications for kids' agency, long-term development, and overall well-being. Finally, we identify future research efforts related to designing for kids' self-tracking and collaborative tracking with parents.","['Rachael Zehrung', 'Lily Huang', 'Bongshin Lee', 'Eun Kyoung Choe']",2021-04-13T07:21:12Z,http://arxiv.org/abs/2104.05979v1,Healthcare & Biomedical AI,Wearable Health Devices,wearable devices hold great potential for promoting children's health and well-being . research on kids' wearables is sparse and often focuses on their use in context of parental surveillance . cnn surveyed 47
Human Electromagnetic Field Exposure in Wearable Communications: A   Review,"The concern on human health is often overseen while wearable technologies attract exploding interests. Mainly due to the extreme proximity or a direct physical contact to the human skin, wearable communications devices are acknowledged to cause higher levels of specific absorption rate (SAR) at the skin surface. Unfortunately, so far, we have found no study encompassing all the aspects that the general public needs to understand about wearable technologies--i.e., the analytical and experimental backgrounds, and report of SAR levels generated from commercial wearable devices. In this context, this paper provides an extensive review on SAR from various commercial wearable devices that are currently sold in the market, as well as the analytical framework and the current measurement methodologies for standard compliance tests. Moreover, considering the present interest in millimeter wave (mmW), this paper sheds light on the SAR evaluated at 60 GHz and also compares the SAR to that measured at 2.4 GHz. We expect that this paper will be of value in informing the general public of the safety in using the currently sold wearable devices, and in igniting further study of the exact biological consequences from electromagnetic field (EMF) exposure due to wearable devices.","['Seungmo Kim', 'Yakub Sharif', 'Imtiaz Nasim']",2019-12-11T13:16:00Z,http://arxiv.org/abs/1912.05282v1,Healthcare & Biomedical AI,Wearable Health Devices,"wearable devices are acknowledged to cause higher levels of specific absorption rate (SAR) at the skin surface . so far, we have found no study encompassing all aspects of wearable technologies . this paper provides an extensive review on SAR"
On Reliability of Android Wearable Health Devices,"Wearable devices are increasingly being used for monitoring health signals and for fitness purposes with typical uses being calorie tracker, workout assistant, and step counter. Even though these wearables can measure many health signals (e.g. heart rate), they are still not perceived as highly accurate, relative to clinical monitoring devices. In this paper, we investigate the accuracy of heart monitor as included in two popular wearables Motorola Moto 360 and the Apple Watch. We analyze the accuracy from a hardware and a software perspective and show the effects of body motion on the heart rate monitors based on the use of photoplethysmography (PPG) signals used in Android wearables. We then do a software reliability study of the Android Wear OS, on which many wearables are based, using fuzz testing.","['Naixing Wang', 'Edgardo Barsallo Yi', 'Saurabh Bagchi']",2017-06-20T16:34:05Z,http://arxiv.org/abs/1706.09247v1,Healthcare & Biomedical AI,Wearable Health Devices,"wearable devices are increasingly being used for monitoring health signals . they are still not perceived as highly accurate, relative to clinical monitoring devices . we analyze the accuracy of heart rate monitors as included in two popular wearables ."
OpenHealth: Open Source Platform for Wearable Health Monitoring,"Movement disorders are becoming one of the leading causes of functional disability due to aging populations and extended life expectancy. Wearable health monitoring is emerging as an effective way to augment clinical care for movement disorders. However, wearable devices face a number of adaptation and technical challenges that hinder their widespread adoption. To address these challenges, we introduce OpenHealth, an open source platform for wearable health monitoring. OpenHealth aims to design a standard set of hardware/software and wearable devices that can enable autonomous collection of clinically relevant data. The OpenHealth platform includes a wearable device, standard software interfaces and reference implementations of human activity and gesture recognition applications.","['Ganapati Bhat', 'Ranadeep Deb', 'Umit Y. Ogras']",2019-02-19T01:53:52Z,http://arxiv.org/abs/1903.03168v2,Healthcare & Biomedical AI,Wearable Health Devices,"movement disorders are becoming one of the leading causes of functional disability . wearable devices face a number of adaptation and technical challenges that hinder their widespread adoption . openHealth platform includes a wearable device, standard software interfaces and reference implementations"
Research Focused Software Development Kits and Wearable Devices in   Physical Activity Research,"Introduction: The Canadian Guidelines recommend physical activity for overall health benefits, including cognitive, emotional, functional, and physical health. However, traditional research methods are inefficient and outdated. This paper aims to guide researchers in enhancing their research methods using software development kits and wearable smart devices. Methods: A generic model application was transformed into a research-based mobile application based on the UCLA researchers who collaborated with Apple. First, the research question and goals were identified. Then, three open-source software development kits (SDKs) were used to modify the generic model into the desired application. ResearchKit was used for informed consent, surveys, and active tasks. CareKit was the protocol manager to create participant protocols and track progress. Finally, HealthKit was used to access and share health-related data. The content expert evaluated the application, and the participant experience was optimized for easy use. The collected health-related data were analyzed to identify any significant findings. Results: Wearable health devices offer a convenient and non-invasive way to monitor and track health-related information. Conclusion: Leveraging the data provided by wearable devices, researchers can gain insights into the effectiveness of interventions and inform the development of evidence-based physical activity guidelines. The use of software development kits and wearable devices can enhance research methods and provide valuable insights into overall health benefits.","['Jason Tsang', 'Harry Prapavessis']",2023-05-12T20:07:44Z,http://arxiv.org/abs/2305.07744v1,Healthcare & Biomedical AI,Wearable Health Devices,paper aims to guide researchers in enhancing their research methods . use of software development kits and wearable devices can provide valuable insights . researchers can gain insights into effectiveness of interventions and inform guidelines .
Human EMF Exposure in Wearable Networks for Internet of Battlefield   Things,"Numerous antenna design approaches for wearable applications have been investigated in the literature. As on-body wearable communications become more ingrained in our daily activities, the necessity to investigate the impacts of these networks burgeons as a major requirement. In this study, we investigate the human electromagnetic field (EMF) exposure effect from on-body wearable devices at 2.4 GHz and 60 GHz, and compare the results to illustrate how the technology evolution to higher frequencies from wearable communications can impact our health. Our results suggest the average specific absorption rate (SAR) at 60 GHz can exceed the regulatory guidelines within a certain separation distance between a wearable device and the human skin surface. To the best of authors' knowledge, this is the first work that explicitly compares the human EMF exposure at different operating frequencies for on-body wearable communications, which provides a direct roadmap in design of wearable devices to be deployed in the Internet of Battlefield Things (IoBT).","['Imtiaz Nasim', 'Seungmo Kim']",2020-03-01T19:36:07Z,http://arxiv.org/abs/2003.01552v1,Healthcare & Biomedical AI,Wearable Health Devices,study investigates human electromagnetic field exposure effect from on-body wearable devices . results suggest average specific absorption rate (SAR) at 60 GHz can exceed regulatory guidelines . this is the first work that explicitly compares the human EMF
Adaptive Energy Management for Self-Sustainable Wearables in Mobile   Health,"Wearable devices that integrate multiple sensors, processors, and communication technologies have the potential to transform mobile health for remote monitoring of health parameters. However, the small form factor of the wearable devices limits the battery size and operating lifetime. As a result, the devices require frequent recharging, which has limited their widespread adoption. Energy harvesting has emerged as an effective method towards sustainable operation of wearable devices. Unfortunately, energy harvesting alone is not sufficient to fulfill the energy requirements of wearable devices. This paper studies the novel problem of adaptive energy management towards the goal of self-sustainable wearables by using harvested energy to supplement the battery energy and to reduce manual recharging by users. To solve this problem, we propose a principled algorithm referred as AdaEM. There are two key ideas behind AdaEM. First, it uses machine learning (ML) methods to learn predictive models of user activity and energy usage patterns. These models allow us to estimate the potential of energy harvesting in a day as a function of the user activities. Second, it reasons about the uncertainty in predictions and estimations from the ML models to optimize the energy management decisions using a dynamic robust optimization (DyRO) formulation. We propose a light-weight solution for DyRO to meet the practical needs of deployment. We validate the AdaEM approach on a wearable device prototype consisting of solar and motion energy harvesting using real-world data of user activities. Experiments show that AdaEM achieves solutions that are within 5% of the optimal with less than 0.005% execution time and energy overhead.","['Dina Hussein', 'Ganapati Bhat', 'Janardhan Rao Doppa']",2022-01-16T23:49:20Z,http://arxiv.org/abs/2201.07888v1,Healthcare & Biomedical AI,Wearable Health Devices,the small form factor of wearable devices limits the battery size and operating lifetime . energy harvesting alone is not sufficient to fulfill the energy requirements . this paper proposes a principled algorithm referred to as AdaEM .
"PAL: A Wearable Platform for Real-time, Personalized and Context-Aware   Health and Cognition Support","Personalized Active Learner (PAL) is a wearable system for real-time, personalized, and context-aware health and cognition support. PAL's system consists of a wearable device, mobile app, cloud database, data visualization web app, and machine learning server. PAL's wearable device uses multi-modal sensors (camera, microphone, heart-rate) with on-device machine learning and open-ear audio output to provide real-time and context-aware cognitive, behavioral and psychological interventions. PAL also allows users to track the long-term correlations between their activities and physiological states to make well-informed lifestyle decisions. In this paper, we present and open-source PAL's system so that people can use it for health and cognition support applications. We also open-source three fully-developed example applications using PAL for face-based memory augmentation, contextual language learning, and heart-rate-based psychological support. PAL's flexible, modular and extensible platform combines trends in data-driven medicine, mobile psychology, and cognitive enhancement to support data-driven and empowering health and cognition applications.","['Mina Khan', 'Glenn Fernandes', 'Utkarsh Sarawgi', 'Prudhvi Rampey', 'Pattie Maes']",2019-05-03T19:54:24Z,http://arxiv.org/abs/1905.01352v1,Healthcare & Biomedical AI,Wearable Health Devices,"Personalized Active Learner (PAL) is a wearable system for real-time, personalized, and context-aware health and cognition support . PAL's wearable device uses multi-modal sensors with on-device machine"
Mitigation of Human RF Exposure in Wearable Communications,"A major concern regarding wearable communications is human biological safety under exposure to radio frequency (RF) radiation generated by wearable devices. The biggest challenge in the implementation of wearable devices is to reduce the usage of energy to minimize the harmful impacts of exposure to RF on human health. Power management is one of the key energy-saving strategies used in wearable networks. Signals enter the receiver (Rx) from a transmitter (Tx) through the human body in the form of electromagnetic field (EMF) radiation produced during the transmission of the packet. It may have a negative effect on human health as a result of specific absorption rate (SAR). SAR is the amount of radio frequency energy consumed by human tissue in mass units. The higher the body's absorption rate, the more radio frequency radiation. Therefore, SAR can be reduced by distributing the power over a greater mass or tissue volume equivalently larger. The Institute of Electrical and Electronics Engineers (IEEE) 802.15.6-supported multi-hop topology is particularly useful for low-power embedded devices that can reduce consumption of energy by communicating to the receiver (Rx) through nearby transmitted devices. In this paper, we suggest a relaying mechanism to minimize the transmitted power and, as a consequence, the power density (PD), a measure of SAR.","['Yakub Ahmed Sharif', 'Seungmo Kim']",2020-05-26T02:39:27Z,http://arxiv.org/abs/2005.12487v1,Healthcare & Biomedical AI,Wearable Health Devices,"power management is one of the key energy-saving strategies used in wearable networks . the higher the body's absorption rate, the more radio frequency radiation . this paper suggests a relaying mechanism to minimize the transmitted power ."
Privacy is All You Need: Revolutionizing Wearable Health Data with   Advanced PETs,"In a world where data is the new currency, wearable health devices offer unprecedented insights into daily life, continuously monitoring vital signs and metrics. However, this convenience raises privacy concerns, as these devices collect sensitive data that can be misused or breached. Traditional measures often fail due to real-time data processing needs and limited device power. Users also lack awareness and control over data sharing and usage. We propose a Privacy-Enhancing Technology (PET) framework for wearable devices, integrating federated learning, lightweight cryptographic methods, and selectively deployed blockchain technology. The blockchain acts as a secure ledger triggered only upon data transfer requests, granting users real-time notifications and control. By dismantling data monopolies, this approach returns data sovereignty to individuals. Through real-world applications like secure medical data sharing, privacy-preserving fitness tracking, and continuous health monitoring, our framework reduces privacy risks by up to 70 percent while preserving data utility and performance. This innovation sets a new benchmark for wearable privacy and can scale to broader IoT ecosystems, including smart homes and industry. As data continues to shape our digital landscape, our research underscores the critical need to maintain privacy and user control at the forefront of technological progress.","['Karthik Barma', 'Seshu Babu Barma']",2025-03-05T12:01:22Z,http://arxiv.org/abs/2503.03428v1,Healthcare & Biomedical AI,Wearable Health Devices,a privacy-enhancing technology (PET) framework for wearable devices is proposed . the blockchain acts as a secure ledger triggered only upon data transfer requests . our framework reduces privacy risks by up to 70 percent while
Wearable Health Monitoring Using Capacitive Voltage-Mode Human Body   Communication,"Rapid miniaturization and cost reduction of computing, along with the availability of wearable and implantable physiological sensors have led to the growth of human Body Area Network (BAN) formed by a network of such sensors and computing devices. One promising application of such a network is wearable health monitoring where the collected data from the sensors would be transmitted and analyzed to assess the health of a person. Typically, the devices in a BAN are connected through wireless (WBAN), which suffers from energy inefficiency due to the high-energy consumption of wireless transmission. Human Body Communication (HBC) uses the relatively low loss human body as the communication medium to connect these devices, promising order(s) of magnitude better energy-efficiency and built-in security compared to WBAN. In this paper, we demonstrate a health monitoring device and system built using Commercial-Off-The- Shelf (COTS) sensors and components, that can collect data from physiological sensors and transmit it through a) intra-body HBC to another device (hub) worn on the body or b) upload health data through HBC-based human-machine interaction to an HBC capable machine. The system design constraints and signal transfer characteristics for the implemented HBC-based wearable health monitoring system are measured and analyzed, showing reliable connectivity with >8x power savings compared to Bluetooth lowenergy (BTLE).","['Shovan Maity', 'Debayan Das', 'Shreyas Sen']",2017-05-14T04:18:18Z,http://arxiv.org/abs/1705.06674v1,Healthcare & Biomedical AI,Wearable Health Devices,human Body Area Network (BAN) is a network of sensors and computing devices . data collected from sensors would be transmitted and analyzed to assess the health of a person . human body communication (hBC) uses the relatively low loss human body as the communication medium .
Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables   Improve Health Predictions,"Wearable devices record physiological and behavioral signals that can improve health predictions. While foundation models are increasingly used for such predictions, they have been primarily applied to low-level sensor data, despite behavioral data often being more informative due to their alignment with physiologically relevant timescales and quantities. We develop foundation models of such behavioral signals using over 2.5B hours of wearable data from 162K individuals, systematically optimizing architectures and tokenization strategies for this unique dataset. Evaluated on 57 health-related tasks, our model shows strong performance across diverse real-world applications including individual-level classification and time-varying health state prediction. The model excels in behavior-driven tasks like sleep prediction, and improves further when combined with representations of raw sensor data. These results underscore the importance of tailoring foundation model design to wearables and demonstrate the potential to enable new health applications.","['Eray Erturk', 'Fahad Kamran', 'Salar Abbaspourazad', 'Sean Jewell', 'Harsh Sharma', 'Yujie Li', 'Sinead Williamson', 'Nicholas J Foti', 'Joseph Futoma']",2025-06-30T19:01:00Z,http://arxiv.org/abs/2507.00191v1,Healthcare & Biomedical AI,Wearable Health Devices,wearable devices record physiological and behavioral signals that can improve health predictions . foundation models have been primarily applied to low-level sensor data . the model excels in behavior-driven tasks like sleep prediction .
Wearable Computing for Health and Fitness: Exploring the Relationship   between Data and Human Behaviour,"Health and fitness wearable technology has recently advanced, making it easier for an individual to monitor their behaviours. Previously self generated data interacts with the user to motivate positive behaviour change, but issues arise when relating this to long term mention of wearable devices. Previous studies within this area are discussed. We also consider a new approach where data is used to support instead of motivate, through monitoring and logging to encourage reflection. Based on issues highlighted, we then make recommendations on the direction in which future work could be most beneficial.","['Katrin Hänsel', 'Natalie Wilde', 'Hamed Haddadi', 'Akram Alomainy']",2015-09-17T12:59:24Z,http://arxiv.org/abs/1509.05238v2,Healthcare & Biomedical AI,Wearable Health Devices,"wearable technology has recently advanced, making it easier for an individual to monitor their behaviours . previously self generated data interacts with the user to motivate positive behaviour change . but issues arise when relating this to long term mention of wearable devices ."
Summary: Multi-modal Biometric-based Implicit Authentication of Wearable   Device Users,"The Internet of Things (IoT) is increasingly empowering people with an interconnected world of physical objects ranging from smart buildings to portable smart devices such as wearables. With recent advances in mobile sensing, wearables have become a rich collection of portable sensors and are able to provide various types of services including tracking of health and fitness, making financial transactions, and unlocking smart locks and vehicles. Most of these services are delivered based on users' confidential and personal data, which are stored on these wearables. Existing explicit authentication approaches (i.e., PINs or pattern locks) for wearables suffer from several limitations, including small or no displays, risk of shoulder surfing, and users' recall burden. Oftentimes, users completely disable security features out of convenience. Therefore, there is a need for a burden-free (implicit) authentication mechanism for wearable device users based on easily obtainable biometric data. In this paper, we present an implicit wearable device user authentication mechanism using combinations of three types of coarse-grain minute-level biometrics: behavioral (step counts), physiological (heart rate), and hybrid (calorie burn and metabolic equivalent of task). From our analysis of over 400 Fitbit users from a 17-month long health study, we are able to authenticate subjects with average accuracy values of around .93 (sedentary) and .90 (non-sedentary) with equal error rates of .05 using binary SVM classifiers. Our findings also show that the hybrid biometrics perform better than other biometrics and behavioral biometrics do not have a significant impact, even during non-sedentary periods.","['Sudip Vhaduri', 'Christian Poellabauer']",2019-07-15T16:06:50Z,http://arxiv.org/abs/1907.06563v1,Healthcare & Biomedical AI,Wearable Health Devices,"the internet of things (IoT) is increasingly empowering people with an interconnected world of physical objects . with recent advances in mobile sensing, wearables have become a rich collection of portable sensors ."
"Addressing Data Quality Challenges in Observational Ambulatory Studies:   Analysis, Methodologies and Practical Solutions for Wrist-worn Wearable   Monitoring","Chronic disease management and follow-up are vital for realizing sustained patient well-being and optimal health outcomes. Recent advancements in wearable sensing technologies, particularly wrist-worn devices, offer promising solutions for longitudinal patient follow-up by shifting from subjective, intermittent self-reporting to objective, continuous monitoring. However, collecting and analyzing wearable data presents unique challenges, such as data entry errors, non-wear periods, missing wearable data, and wearable artifacts. We therefore present an in-depth exploration of data analysis challenges tied to wrist-worn wearables and ambulatory label acquisition, using two real-world datasets (i.e., mBrain21 and ETRI lifelog2020). We introduce novel practical countermeasures, including participant compliance visualizations, interaction-triggered questionnaires to assess personal bias, and an optimized wearable non-wear detection pipeline. Further, we propose a visual analytics approach to validate processing pipelines using scalable tools such as tsflex and Plotly-Resampler. Lastly, we investigate the impact of missing wearable data on ""window-of-interest"" analysis methodologies. Prioritizing transparency and reproducibility, we offer open access to our detailed code examples, facilitating adaptation in future wearable research. In conclusion, our contributions provide actionable approaches for wearable data collection and analysis in chronic disease management.","['Jonas Van Der Donckt', 'Nicolas Vandenbussche', 'Jeroen Van Der Donckt', 'Stephanie Chen', 'Marija Stojchevska', 'Mathias De Brouwer', 'Bram Steenwinckel', 'Koen Paemeleire', 'Femke Ongenae', 'Sofie Van Hoecke']",2024-01-24T15:15:03Z,http://arxiv.org/abs/2401.13518v1,Healthcare & Biomedical AI,Wearable Health Devices,chronic disease management and follow-up are vital for realizing optimal health outcomes . collecting and analyzing wearable data presents unique challenges . mBrain21 and ETRI lifelog2020 are two real-world datasets .
IoT-based Wearables: A comprehensive Survey,"A substantial amount of growth is being achieved by businesses through IoT-based services. The emergent of small electronic devices capable of computing, which are commonly known as wearables in IoT domain has proven to have huge impact in people's life. Theses wearables are capable of collecting vital information about a person's activities and behaviours regularly. This makes them suitable for many applications in health monitoring, fitness, sports, education and some industry related applications. To this end, in this paper, we aim to provide a general review on IoT-based wearables, the sensors adopted for several categorized wearables, the communication technologies adopted and the most widely adopted data processing techniques for wearables. Furthermore, we present the challenges faced for wide adoption of wearables and the future research directions.","['Yahuza Bello', 'Emanuel Figetakis']",2023-04-12T13:50:06Z,http://arxiv.org/abs/2304.09861v1,Healthcare & Biomedical AI,Wearable Health Devices,"wearables can collect vital information about a person's activities and behaviours . they are suitable for many applications in health monitoring, fitness, sports, education and some industry related applications ."
Invited: Human-Inspired Distributed Wearable AI,"The explosive surge in Human-AI interactions, fused with a soaring fascination in wearable technology, has ignited a frenzy of innovation and the emergence of a myriad of Wearable AI devices, each wielding diverse form factors, tackling tasks from health surveillance to turbocharging productivity. This paper delves into the vision for wearable AI technology, addressing the technical bottlenecks that stand in the way of its promised advancements.   Embracing a paradigm shift, we introduce a Human-Inspired Distributed Network for Wearable AI, enabled by high-speed ultra-low-power secure connectivity via the emerging 'Body as a Wire' (Wi-R) technology. This breakthrough acts as the missing link: the artificial nervous system, seamlessly interconnecting all wearables and implantables, ushering in a new era of interconnected intelligence, where featherweight, perpetually operating wearable AI nodes redefine the boundaries of possibility.","['Shreyas Sen', 'Arunashish Datta']",2024-06-26T23:28:41Z,http://arxiv.org/abs/2406.18791v2,Healthcare & Biomedical AI,Wearable Health Devices,"this paper delves into the vision for wearable AI technology, addressing the technical bottlenecks that stand in the way of its promised advancements . we introduce a human-inspired distributed network for Wearable AI, enabled by high-speed ultra-low-power secure connectivity via the emerging 'Body as a Wire' (wi-R) technology ."
The Development of Wearable Polymer-Based Sensors: Perspectives,"The development of smart polymer materials is reviewed and illustrated. Important examples of these polymers include conducting polymers, ionic gels, stimulus-response be used polymers, liquid crystalline polymers and piezoelectric materials, which have desirable properties for use in wearable sensors. This review outlines the mode of action in these types of smart polymers systems for utilisation as wearable sensors. Categories of wearable sensors are considered as tattoo-like designs, patch-like, textile-based, and contact lens-based sensors. The advantages and disadvantages of each sensor types are considered together with information on the typical performance. The research gap linking smart polymer materials to wearable sensors with integrated power systems is highlighted. Smart polymer systems may be used as part of a holistic approach to improve wearable devices and accelerate the integration of wearable sensors and power systems, particularly in health care.","['Christian Harito', 'Listya Utari', 'Budi Riza Putra', 'Brian Yuliarto', 'Setyo Purwanto', 'Syed S. J. Zaidi', 'Dmitry V. Bavykin', 'Frank Marken', 'Frank C. Walsh']",2020-03-02T15:00:57Z,http://arxiv.org/abs/2003.00956v1,Healthcare & Biomedical AI,Wearable Health Devices,"important examples of these polymers include conducting polymers, ionic gels, stimulus-response be used polymers and liquid crystalline polymers . categories of wearable sensors are considered as tattoo-like designs, patch-like, textile-based, and contact lens-based sensors ."
"Harnessing Large Language Models for Mental Health: Opportunities,   Challenges, and Ethical Considerations","Large Language Models (LLMs) are transforming mental health care by enhancing accessibility, personalization, and efficiency in therapeutic interventions. These AI-driven tools empower mental health professionals with real-time support, improved data integration, and the ability to encourage care-seeking behaviors, particularly in underserved communities. By harnessing LLMs, practitioners can deliver more empathetic, tailored, and effective support, addressing longstanding gaps in mental health service provision. However, their implementation comes with significant challenges and ethical concerns. Performance limitations, data privacy risks, biased outputs, and the potential for generating misleading information underscore the critical need for stringent ethical guidelines and robust evaluation mechanisms. The sensitive nature of mental health data further necessitates meticulous safeguards to protect patient rights and ensure equitable access to AI-driven care. Proponents argue that LLMs have the potential to democratize mental health resources, while critics warn of risks such as misuse and the diminishment of human connection in therapy. Achieving a balance between innovation and ethical responsibility is imperative. This paper examines the transformative potential of LLMs in mental health care, highlights the associated technical and ethical complexities, and advocates for a collaborative, multidisciplinary approach to ensure these advancements align with the goal of providing compassionate, equitable, and effective mental health support.",['Hari Mohan Pandey'],2024-12-13T13:18:51Z,http://arxiv.org/abs/2501.10370v1,Healthcare & Biomedical AI,Mental Health AI,large language models (LLMs) are transforming mental health care . authors argue they have the potential to democratize mental health resources . critics warn of risks such as misuse and diminishment of human connection in therapy .
Enhancing Mental Health Support through Human-AI Collaboration: Toward   Secure and Empathetic AI-enabled chatbots,"Access to mental health support remains limited, particularly in marginalized communities where structural and cultural barriers hinder timely care. This paper explores the potential of AI-enabled chatbots as a scalable solution, focusing on advanced large language models (LLMs)-GPT v4, Mistral Large, and LLama V3.1-and assessing their ability to deliver empathetic, meaningful responses in mental health contexts. While these models show promise in generating structured responses, they fall short in replicating the emotional depth and adaptability of human therapists. Additionally, trustworthiness, bias, and privacy challenges persist due to unreliable datasets and limited collaboration with mental health professionals. To address these limitations, we propose a federated learning framework that ensures data privacy, reduces bias, and integrates continuous validation from clinicians to enhance response quality. This approach aims to develop a secure, evidence-based AI chatbot capable of offering trustworthy, empathetic, and bias-reduced mental health support, advancing AI's role in digital mental health care.","['Rawan AlMakinah', 'Andrea Norcini-Pala', 'Lindsey Disney', 'M. Abdullah Canbaz']",2024-09-17T20:49:13Z,http://arxiv.org/abs/2410.02783v1,Healthcare & Biomedical AI,Mental Health AI,"this paper explores the potential of AI-enabled chatbots as a scalable solution . it focuses on advanced large language models (LLMs)-GPT v4, Mistral Large, and LLama V3.1 . trustworthiness, bias, and privacy challenges persist due to unreliable datasets ."
"""It Listens Better Than My Therapist"": Exploring Social Media Discourse   on LLMs as Mental Health Tool","The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.",['Anna-Carolina Haensch'],2025-04-14T17:37:32Z,http://arxiv.org/abs/2504.12337v1,Healthcare & Biomedical AI,Mental Health AI,"chatbots such as ChatGPT have prompted growing interest in their role as informal mental health tools . large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability . results show that nearly 20% of comments reflect personal use ."
Challenges of Large Language Models for Mental Health Counseling,"The global mental health crisis is looming with a rapid increase in mental disorders, limited resources, and the social stigma of seeking treatment. As the field of artificial intelligence (AI) has witnessed significant advancements in recent years, large language models (LLMs) capable of understanding and generating human-like text may be used in supporting or providing psychological counseling. However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness. We explore potential solutions to these challenges that are practical and applicable to the current paradigm of AI. From our experience in developing and deploying LLMs for mental health, AI holds a great promise for improving mental health care, if we can carefully navigate and overcome pitfalls of LLMs.","['Neo Christopher Chung', 'George Dyer', 'Lennart Brocki']",2023-11-23T08:56:41Z,http://arxiv.org/abs/2311.13857v1,Healthcare & Biomedical AI,Mental Health AI,the global mental health crisis is looming with a rapid increase in mental disorders . large language models (LLMs) can understand and generate human-like text . the application of LLMs in the mental health domain raises concerns .
"Artificial Intelligence in Mental Health and Well-Being: Evolution,   Current Applications, Future Challenges, and Emerging Evidence","Artificial Intelligence (AI) is a broad field that is upturning mental health care in many ways, from addressing anxiety, depression, and stress to increasing access, personalization of treatment, and real-time monitoring that enhances patient outcomes. The current paper discusses the evolution, present application, and future challenges in the field of AI for mental health and well-being. From the early chatbot models, such as ELIZA, to modern machine learning systems, the integration of AI in mental health has grown rapidly to augment traditional treatment and open innovative solutions. AI-driven tools provide continuous support, offering personalized interventions and addressing issues such as treatment access and patient stigma. AI also enables early diagnosis through the analysis of complex datasets, including speech patterns and social media behavior, to detect early signs of conditions like depression and Post-Traumatic Stress Disorder (PTSD). Ethical challenges persist, however, most notably around privacy, data security, and algorithmic bias. With AI at the core of mental health care, there is a dire need to develop strong ethical frameworks that ensure patient rights are protected, access is equitable, and transparency is maintained in AI applications. Going forward, the role of AI in mental health will continue to evolve, and continued research and policy development will be needed to meet the diverse needs of patients while mitigating associated risks.",['Hari Mohan Pandey'],2024-12-13T22:06:35Z,http://arxiv.org/abs/2501.10374v1,Healthcare & Biomedical AI,Mental Health AI,"the current paper discusses the evolution, present application, and future challenges in the field of AI for mental health and well-being . the integration of AI in mental health has grown rapidly to augment traditional treatment and open innovative solutions ."
Public sentiment analysis and topic modeling regarding ChatGPT in mental   health on Reddit: Negative sentiments increase over time,"In order to uncover users' attitudes towards ChatGPT in mental health, this study examines public opinions about ChatGPT in mental health discussions on Reddit. Researchers used the bert-base-multilingual-uncased-sentiment techniques for sentiment analysis and the BERTopic model for topic modeling. It was found that overall, negative sentiments prevail, followed by positive ones, with neutral sentiments being the least common. The prevalence of negative emotions has increased over time. Negative emotions encompass discussions on ChatGPT providing bad mental health advice, debates on machine vs. human value, the fear of AI, and concerns about Universal Basic Income (UBI). In contrast, positive emotions highlight ChatGPT's effectiveness in counseling, with mentions of keywords like ""time"" and ""wallet."" Neutral discussions center around private data concerns. These findings shed light on public attitudes toward ChatGPT in mental health, potentially contributing to the development of trustworthy AI in mental health from the public perspective.","['Yunna Cai', 'Fan Wang', 'Haowei Wang', 'Qianwen Qian']",2023-11-27T13:23:11Z,http://arxiv.org/abs/2311.15800v1,Healthcare & Biomedical AI,Mental Health AI,"study examines public opinions about ChatGPT in mental health discussions on reddit . overall, negative sentiments prevail, followed by positive ones . negative emotions include debates on machine vs. human value, fear of AI ."
The Transition from Centralized Machine Learning to Federated Learning   for Mental Health in Education: A Survey of Current Methods and Future   Directions,"Research has increasingly explored the application of artificial intelligence (AI) and machine learning (ML) within the mental health domain to enhance both patient care and healthcare provider efficiency. Given that mental health challenges frequently emerge during early adolescence -- the critical years of high school and college -- investigating AI/ML-driven mental health solutions within the education domain is of paramount importance. Nevertheless, conventional AI/ML techniques follow a centralized model training architecture, which poses privacy risks due to the need for transferring students' sensitive data from institutions, universities, and clinics to central servers. Federated learning (FL) has emerged as a solution to address these risks by enabling distributed model training while maintaining data privacy. Despite its potential, research on applying FL to analyze students' mental health remains limited. In this paper, we aim to address this limitation by proposing a roadmap for integrating FL into mental health data analysis within educational settings. We begin by providing an overview of mental health issues among students and reviewing existing studies where ML has been applied to address these challenges. Next, we examine broader applications of FL in the mental health domain to emphasize the lack of focus on educational contexts. Finally, we propose promising research directions focused on using FL to address mental health issues in the education sector, which entails discussing the synergies between the proposed directions with broader human-centered domains. By categorizing the proposed research directions into short- and long-term strategies and highlighting the unique challenges at each stage, we aim to encourage the development of privacy-conscious AI/ML-driven mental health solutions.","['Maryam Ebrahimi', 'Rajeev Sahay', 'Seyyedali Hosseinalipour', 'Bita Akram']",2025-01-20T19:54:51Z,http://arxiv.org/abs/2501.11714v1,Healthcare & Biomedical AI,Mental Health AI,research has explored the application of artificial intelligence (AI) and machine learning (ML) within the mental health domain . conventional AI/ML techniques follow a centralized model training architecture . federated learning (FL) has emerged as a solution to address these risks .
Toward Safe Evolution of Artificial Intelligence (AI) based   Conversational Agents to Support Adolescent Mental and Sexual Health   Knowledge Discovery,"Following the recent release of various Artificial Intelligence (AI) based Conversation Agents (CAs), adolescents are increasingly using CAs for interactive knowledge discovery on sensitive topics, including mental and sexual health topics. Exploring such sensitive topics through online search has been an essential part of adolescent development, and CAs can support their knowledge discovery on such topics through human-like dialogues. Yet, unintended risks have been documented with adolescents' interactions with AI-based CAs, such as being exposed to inappropriate content, false information, and/or being given advice that is detrimental to their mental and physical well-being (e.g., to self-harm). In this position paper, we discuss the current landscape and opportunities for CAs to support adolescents' mental and sexual health knowledge discovery. We also discuss some of the challenges related to ensuring the safety of adolescents when interacting with CAs regarding sexual and mental health topics. We call for a discourse on how to set guardrails for the safe evolution of AI-based CAs for adolescents.","['Jinkyung Park', 'Vivek Singh', 'Pamela Wisniewski']",2024-04-03T19:18:25Z,http://arxiv.org/abs/2404.03023v1,Healthcare & Biomedical AI,Mental Health AI,adolescents are increasingly using AI-based conversation agents (CAs) for knowledge discovery . unintended risks have been documented with adolescents' interactions with CAs .
When Testing AI Tests Us: Safeguarding Mental Health on the Digital   Frontlines,"Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniquely interactional mode of testing, one in which individuals on red teams actively interact with the system, leveraging natural language to simulate malicious actors and solicit harmful outputs. This interactional labor done by red teams can result in mental health harms that are uniquely tied to the adversarial engagement strategies necessary to effectively red team. The importance of ensuring that generative AI models do not propagate societal or individual harm is widely recognized -- one less visible foundation of end-to-end AI safety is also the protection of the mental health and wellbeing of those who work to keep model outputs safe. In this paper, we argue that the unmet mental health needs of AI red-teamers is a critical workplace safety concern. Through analyzing the unique mental health impacts associated with the labor done by red teams, we propose potential individual and organizational strategies that could be used to meet these needs, and safeguard the mental health of red-teamers. We develop our proposed strategies through drawing parallels between common red-teaming practices and interactional labor common to other professions (including actors, mental health professionals, conflict photographers, and content moderators), describing how individuals and organizations within these professional spaces safeguard their mental health given similar psychological demands. Drawing on these protective practices, we describe how safeguards could be adapted for the distinct mental health challenges experienced by red teaming organizations as they mitigate emerging technological risks on the new digital frontlines.","['Sachin R. Pendse', 'Darren Gergle', 'Rachel Kornfield', 'Jonah Meyerhoff', 'David Mohr', 'Jina Suh', 'Annie Wescott', 'Casey Williams', 'Jessica Schleider']",2025-04-29T16:27:20Z,http://arxiv.org/abs/2504.20910v1,Healthcare & Biomedical AI,Mental Health AI,red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content . authors propose potential strategies that could be used to meet the unmet mental health needs of AI red teamers . they draw parallels between common red teaming practices and interactional labor common to others .
Risks from Language Models for Automated Mental Healthcare: Ethics and   Structure for Implementation,"Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questionnaires designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The questionnaire design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.   Trigger warning: Contains and discusses examples of sensitive mental health topics, including suicide and self-harm.","['Declan Grabb', 'Max Lamparth', 'Nina Vasan']",2024-04-02T15:05:06Z,http://arxiv.org/abs/2406.11852v2,Healthcare & Biomedical AI,Mental Health AI,"paper proposes framework that delineates levels of autonomy, outlines ethical requirements . it also defines beneficial default behaviors for AI agents in the context of mental health care . existing language models are insufficient to match the standard provided by human professionals ."
AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large   Language Models,"LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation. Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs. Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain. In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences. We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for ""Affirmative AI,"" ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals. We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids. Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context.","['Yinru Long', 'Zilin Ma', 'Yiyang Mei', 'Zhaoyuan Su']",2024-05-07T20:21:17Z,http://arxiv.org/abs/2405.04652v1,Healthcare & Biomedical AI,Mental Health AI,"research shows that LGBTQ+ people have been using chatbots for mental health needs . but concerns remain about their ability to provide empathetic, accurate, and affirming responses . we propose a framework for evaluating the affirmativeness of LLMs ."
"SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health   Support and Assessment, Based on a Systematic Application Survey","Mental health issues significantly impact individuals' daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Suicide Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches to assess preliminary assessments and suicide risk detection, utilizing annotated real-life interview data and professionally labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance model performance and usability through context-sensitive response adjustments and semantic coherence evaluations. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.","['Qiming Guo', 'Jinwen Tang', 'Wenbo Sun', 'Haoteng Tang', 'Yi Shang', 'Wenlu Wang']",2024-10-06T17:11:29Z,http://arxiv.org/abs/2410.11859v1,Healthcare & Biomedical AI,Mental Health AI,"this study aims to provide accessible, stigma-free, personalized, and real-time mental health support . introducing souLLMate, an adaptive LLM-driven system that integrates LLM technologies . system offers advanced features such as Suicide Risk Detection and Proactive Guidance Dialogue ."
"""For an App Supposed to Make Its Users Feel Better, It Sure is a Joke""   -- An Analysis of User Reviews of Mobile Mental Health Applications","Mobile mental health applications are seen as a promising way to fulfill the growing need for mental health care. Although there are more than ten thousand mental health apps available on app marketplaces, such as Google Play and Apple App Store, many of them are not evidence-based, or have been minimally evaluated or regulated. The real-life experience and concerns of the app users are largely unknown. To address this knowledge gap, we analyzed 2159 user reviews from 117 Android apps and 2764 user reviews from 76 iOS apps. Our findings include the critiques around inconsistent moderation standards and lack of transparency. App-embedded social features and chatbots were criticized for providing little support during crises. We provide research and design implications for future mental health app developers, discuss the necessity of developing a comprehensive and centralized app development guideline, and the opportunities of incorporating existing AI technology in mental health chatbots.","['MD Romael Haque', 'Sabirat Rubya']",2022-09-16T08:53:26Z,http://arxiv.org/abs/2209.07796v1,Healthcare & Biomedical AI,Mental Health AI,"mobile mental health apps are seen as a promising way to fulfill the growing need for mental health care . many of the apps are not evidence-based, or have been minimally evaluated or regulated . the real-life experience and concerns of the app users are largely unknown ."
Artificial Empathy: AI based Mental Health,"Many people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a ""Five minute therapist"" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.","['Aditya Naik', 'Jovi Thomas', 'Teja Sree', 'Himavant Reddy']",2025-05-30T02:36:56Z,http://arxiv.org/abs/2506.00081v1,Healthcare & Biomedical AI,Mental Health AI,"study: chatbots were primarily utilized as a ""five minute therapist"" or a non-judgmental companion . there were concerns about privacy and the security of sensitive information . limitations include inconsistent tone, occasional inappropriate responses ."
Explainable AI for Mental Disorder Detection via Social Media: A survey   and outlook,"Mental health constitutes a complex and pervasive global challenge, affecting millions of lives and often leading to severe consequences. In this paper, we conduct a thorough survey to explore the intersection of data science, artificial intelligence, and mental healthcare, focusing on the recent developments of mental disorder detection through online social media (OSM). A significant portion of the population actively engages in OSM platforms, creating a vast repository of personal data that holds immense potential for mental health analytics. The paper navigates through traditional diagnostic methods, state-of-the-art data- and AI-driven research studies, and the emergence of explainable AI (XAI) models for mental healthcare. We review state-of-the-art machine learning methods, particularly those based on modern deep learning, while emphasising the need for explainability in healthcare AI models. The experimental design section provides insights into prevalent practices, including available datasets and evaluation approaches. We also identify key issues and challenges in the field and propose promising future research directions. As mental health decisions demand transparency, interpretability, and ethical considerations, this paper contributes to the ongoing discourse on advancing XAI in mental healthcare through social media. The comprehensive overview presented here aims to guide researchers, practitioners, and policymakers in developing the area of mental disorder detection.","['Yusif Ibrahimov', 'Tarique Anwar', 'Tommy Yuan']",2024-06-10T02:51:16Z,http://arxiv.org/abs/2406.05984v1,Healthcare & Biomedical AI,Mental Health AI,"mental health constitutes a complex and pervasive global challenge, affecting millions of lives . this paper explores the intersection of data science, artificial intelligence, and mental healthcare . a significant portion of the population actively engages in online social media platforms ."
MentalChat16K: A Benchmark Dataset for Conversational Mental Health   Assistance,"We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.","['Jia Xu', 'Tianyi Wei', 'Bojian Hou', 'Patryk Orzechowski', 'Shu Yang', 'Ruochen Jin', 'Rachael Paulbeck', 'Joost Wagenaar', 'George Demiris', 'Li Shen']",2025-03-13T20:25:10Z,http://arxiv.org/abs/2503.13509v2,Healthcare & Biomedical AI,Mental Health AI,"MentalChat16K is an english benchmark dataset . it combines a synthetic mental health counseling dataset and transcripts . the dataset prioritizes patient privacy, ethical considerations, and responsible data usage ."
EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental   Health Safety,"The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent","['Jiahao Qiu', 'Yinghui He', 'Xinzhe Juan', 'Yimin Wang', 'Yuhan Liu', 'Zixin Yao', 'Yue Wu', 'Xun Jiang', 'Ling Yang', 'Mengdi Wang']",2025-04-13T18:47:22Z,http://arxiv.org/abs/2504.09689v3,Healthcare & Biomedical AI,Mental Health AI,"the rise of LLM-driven AI characters raises safety concerns . we propose EmoAgent, a multi-agent AI framework to evaluate and mitigate mental health hazards . EmoGuard serves as an intermediary, monitoring users' mental status ."
Big Data Analytics and AI in Mental Healthcare,"Mental health conditions cause a great deal of distress or impairment; depression alone will affect 11% of the world's population. The application of Artificial Intelligence (AI) and big-data technologies to mental health has great potential for personalizing treatment selection, prognosticating, monitoring for relapse, detecting and helping to prevent mental health conditions before they reach clinical-level symptomatology, and even delivering some treatments. However, unlike similar applications in other fields of medicine, there are several unique challenges in mental health applications which currently pose barriers towards the implementation of these technologies. Specifically, there are very few widely used or validated biomarkers in mental health, leading to a heavy reliance on patient and clinician derived questionnaire data as well as interpretation of new signals such as digital phenotyping. In addition, diagnosis also lacks the same objective 'gold standard' as in other conditions such as oncology, where clinicians and researchers can often rely on pathological analysis for confirmation of diagnosis. In this chapter we discuss the major opportunities, limitations and techniques used for improving mental healthcare through AI and big-data. We explore both the computational, clinical and ethical considerations and best practices as well as lay out the major researcher directions for the near future.","['Ariel Rosenfeld', 'David Benrimoh', 'Caitrin Armstrong', 'Nykan Mirchi', 'Timothe Langlois-Therrien', 'Colleen Rollins', 'Myriam Tanguay-Sela', 'Joseph Mehltretter', 'Robert Fratila', 'Sonia Israel', 'Emily Snook', 'Kelly Perlman', 'Akiva Kleinerman', 'Bechara Saab', 'Mark Thoburn', 'Cheryl Gabbay', 'Amit Yaniv-Rosenfeld']",2019-03-12T20:47:29Z,http://arxiv.org/abs/1903.12071v1,Healthcare & Biomedical AI,Mental Health AI,depression alone will affect 11% of the world's population . there are very few widely used or validated biomarkers in mental health . diagnosis also lacks the same objective 'gold standard' as in other conditions such as oncology .
Factors That Influence the Adoption of AI-enabled Conversational Agents   (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers:   From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review,"Artificial intelligent (AI) conversational agents hold a promising future in the field of mental health, especially in helping marginalized communities that lack access to mental health support services. It is tempting to have a 24/7 mental health companion that can be accessed anywhere using mobile phones to provide therapist-like advice. Yet, caution should be taken, and studies around their feasibility need to be surveyed. Before adopting such a rapidly changing technology, studies on its feasibility should be explored, summarized, and synthesized to gain a solid understanding of the status quo and to enable us to build a framework that can guide us throughout the development and deployment processes. Different perspectives must be considered when investigating the feasibility of AI conversational agents, including the mental healthcare professional perspective. The literature can provide insights into their perspectives in terms of opportunities, concerns, and implications. Mental health professionals, the subject-matter experts in this field, have their points of view that should be understood and considered. This systematic literature review will explore mental health practitioners' attitudes toward AI conversational agents and the factors that affect their adoption and recommendation of the technology to augment their services and treatments. The TAM3 Framework will be the lens through which this systematic literature review will be conducted.",['Rawan AlMakinah'],2025-01-26T02:31:27Z,http://arxiv.org/abs/2504.13183v1,Healthcare & Biomedical AI,Mental Health AI,"artificial intelligence (AI) conversational agents hold a promising future in the field of mental health . it is tempting to have a 24/7 mental health companion that can be accessed anywhere . but, caution should be taken, and studies around their feasibility need to be surveyed ."
The Importance of Justified Patient Trust in unlocking AI's potential in   mental healthcare,"Without trust, patients may hesitate to engage with AI systems, significantly limiting the technology's potential in mental healthcare. This paper focuses specifically on the trust that mental health patients, as direct users, must have in AI systems, highlighting the most sensitive and direct relationship between AI systems and those whose mental healthcare is impacted by them. We explore the concept of justified trust, why it is important for patient positive care outcomes, and the strategies needed to foster and maintain this trust. By examining these aspects, we highlight how cultivating justified trust is key to unlocking AI's potential impact in mental healthcare.","['Tita Alissa Bach', 'Niko Mannikko']",2024-10-14T07:50:10Z,http://arxiv.org/abs/2410.10233v1,Healthcare & Biomedical AI,Mental Health AI,this paper focuses specifically on the trust that mental health patients must have in AI systems . cultivating justified trust is key to unlocking AI's potential impact in mental healthcare .
Meta-research on COVID-19: An overview of the early trends,"COVID-19 is having a dramatic impact on research and researchers. The pandemic has underlined the severity of known challenges in research and surfaced new ones, but also accelerated the adoption of innovations and manifested new opportunities. This review considers early trends emerging from meta-research on COVID-19. In particular, it focuses on the following topics: i) mapping COVID-19 research; ii) data and machine learning; iii) research practices including open access and open data, reviewing, publishing and funding; iv) communicating research to the public; v) the impact of COVID-19 on researchers, in particular with respect to gender and career trajectories. This overview finds that most early meta-research on COVID-19 has been reactive and focused on short-term questions, while more recently a shift to consider the long-term consequences of COVID-19 is taking place. Based on these findings, the author speculates that some aspects of doing research during COVID-19 are more likely to persist than others. These include: the shift to virtual for academic events such as conferences; the use of openly accessible pre-prints; the `datafication' of scholarly literature and consequent broader adoption of machine learning in science communication; the public visibility of research and researchers on social and online media.",['Giovanni Colavizza'],2021-06-05T20:50:43Z,http://arxiv.org/abs/2106.02961v1,Healthcare & Biomedical AI,COVID-19 Research,this review considers early trends emerging from meta-research on COVID-19 . it focuses on: i) data and machine learning; ii) research practices including open access and open data . more recently a shift to consider the long-term consequences is taking place .
Machine learning applications for COVID-19: A state-of-the-art review,"The COVID-19 pandemic has galvanized the machine learning community to create new solutions that can help in the fight against the virus. The body of literature related to applications of machine learning and artificial intelligence to COVID-19 is constantly growing. The goal of this article is to present the latest advances in machine learning research applied to COVID-19. We cover four major areas of research: forecasting, medical diagnostics, drug development, and contact tracing. We review and analyze the most successful state of the art studies. In contrast to other existing surveys on the subject, our article presents a high level overview of the current research that is sufficiently detailed to provide an informed insight.","['Firuz Kamalov', 'Aswani Cherukuri', 'Hana Sulieman', 'Fadi Thabtah', 'Akbar Hossain']",2021-01-19T19:12:45Z,http://arxiv.org/abs/2101.07824v1,Healthcare & Biomedical AI,COVID-19 Research,"the pandemic of COVID-19 has galvanized the machine learning community . the goal of this article is to present the latest advances in machine learning . we cover four major areas of research: forecasting, medical diagnostics, drug development ."
"Audio, Speech, Language, & Signal Processing for COVID-19: A   Comprehensive Overview","The Coronavirus (COVID-19) pandemic has been the research focus world-wide in the year 2020. Several efforts, from collection of COVID-19 patients' data to screening them for the virus's detection are taken with rigour. A major portion of COVID-19 symptoms are related to the functioning of the respiratory system, which in-turn critically influences the human speech production system. This drives the research focus towards identifying the markers of COVID-19 in speech and other human generated audio signals. In this paper, we give an overview of the speech and other audio signal, language and general signal processing-based work done using Artificial Intelligence techniques to screen, diagnose, monitor, and spread the awareness aboutCOVID-19. We also briefly describe the research related to detect accord-ing COVID-19 symptoms carried out so far. We aspire that this collective information will be useful in developing automated systems, which can help in the context of COVID-19 using non-obtrusive and easy to use modalities such as audio, speech, and language.","['Gauri Deshpande', 'Björn W. Schuller']",2020-11-29T21:33:59Z,http://arxiv.org/abs/2011.14445v1,Healthcare & Biomedical AI,COVID-19 Research,the Coronavirus (COVID-19) pandemic has been the research focus world-wide in the year 2020 . symptoms related to respiratory system critically influences human speech production system . aspires that collective information will be useful in developing automated systems .
A review on the mobile applications developed for COVID-19: An   exploratory analysis,"The objective of this research is to explore the existing mobile applications developed for the COVID-19 pandemic. To obtain this research objective, firstly the related applications were selected through the systematic search technique in the popular application stores. Secondly, data related to the app objectives, functionalities provided by the app, user ratings, and user reviews were extracted. Thirdly, the extracted data were analyzed through the affinity diagram, noticing-collecting-thinking, and descriptive analysis. As outcomes, the review provides a state-of-the-art view of mobile apps developed for COVID-19 by revealing nine functionalities or features. It revealed ten factors related to information systems design characteristics that can guide future app design. The review outcome highlights the need for new development and further refinement of the existing applications considering not only the revealed objectives and their associated functionalities, but also revealed design characteristics such as reliability, performance, usefulness, supportive, security, privacy, flexibility, responsiveness, ease of use, and cultural sensitivity.","['Muhammad Nazrul Islam', 'Iyolita Islam', 'Kazi MD. Munim', 'A. K. M. Najmul Islam']",2020-08-20T16:34:24Z,http://arxiv.org/abs/2008.09063v1,Healthcare & Biomedical AI,COVID-19 Research,"the aim of this research is to explore the existing mobile applications developed for the COVID-19 pandemic . data related to the app objectives, functionalities provided by the app, user ratings, and user reviews were extracted ."
Robots and COVID-19: Challenges in integrating robots for collaborative   automation,"Objective: The status of human-robot collaboration for assembly applications is reviewed and key current challenges for the research community and practitioners are presented. Background: As the pandemic of COVID-19 started to surface the manufacturers went under pressure to address demand challenges. Social distancing measures made fewer people available to work. In such situations, robots were pointed at to support humans to address a shortage in supply. An important activity where humans are needed in a manufacturing value chain is assembly. HRC assembly systems are supposed to safeguard coexisting humans, perform a range of actions, and often need to be reconfigured to handle product variety. This requires them to be resilient and adaptable to various configurations during their operational life. Besides the potential advantages of using robots the challenges of using them in an industrial assembly are enormous. Methods: This mini-review summarizes the challenges of industrial deployment of collaborative robots for assembly applications. Applications: The documented challenges highlight the future research directions in human-robot interaction for industrial applications.",['Ali Ahmad Malik'],2020-06-01T15:16:17Z,http://arxiv.org/abs/2006.15975v1,Healthcare & Biomedical AI,COVID-19 Research,status of human-robot collaboration for assembly applications is reviewed . challenges highlight the future research directions in robotics for industrial applications .
Biometrics in the Era of COVID-19: Challenges and Opportunities,"Since early 2020 the COVID-19 pandemic has had a considerable impact on many aspects of daily life. A range of different measures have been implemented worldwide to reduce the rate of new infections and to manage the pressure on national health services. A primary strategy has been to reduce gatherings and the potential for transmission through the prioritisation of remote working and education. Enhanced hand hygiene and the use of facial masks have decreased the spread of pathogens when gatherings are unavoidable. These particular measures present challenges for reliable biometric recognition, e.g. for facial-, voice- and hand-based biometrics. At the same time, new challenges create new opportunities and research directions, e.g. renewed interest in non-constrained iris or periocular recognition, touch-less fingerprint- and vein-based authentication and the use of biometric characteristics for disease detection. This article presents an overview of the research carried out to address those challenges and emerging opportunities.","['Marta Gomez-Barrero', 'Pawel Drozdowski', 'Christian Rathgeb', 'Jose Patino', 'Massimmiliano Todisco', 'Andras Nautsch', 'Naser Damer', 'Jannis Priesnitz', 'Nicholas Evans', 'Christoph Busch']",2021-02-18T10:32:59Z,http://arxiv.org/abs/2102.09258v2,Healthcare & Biomedical AI,COVID-19 Research,since early 2020 the COVID-19 pandemic has had a considerable impact on many aspects of daily life . primary strategy has been to reduce gatherings and the potential for transmission through the prioritisation of remote working and education . new challenges create new opportunities and research directions .
When the Open Source Community Meets COVID-19: Characterizing COVID-19   themed GitHub Repositories,"Ever since the beginning of the outbreak of the COVID-19 pandemic, researchers from interdisciplinary domains have worked together to fight against the crisis. The open source community, plays a vital role in coping with the pandemic which is inherently a collaborative process. Plenty of COVID-19 related datasets, tools, software, deep learning models, are created and shared in research communities with great efforts. However, COVID-19 themed open source projects have not been systematically studied, and we are still unaware how the open source community helps combat COVID-19 in practice. To fill this void, in this paper, we take the first step to study COVID-19 themed repositories in GitHub, one of the most popular collaborative platforms. We have collected over 67K COVID-19 themed GitHub repositories till July 2020. We then characterize them from a number of aspects and classify them into six categories. We further investigate the contribution patterns of the contributors, and development and maintenance patterns of the repositories. This study sheds light on the promising direction of adopting open source technologies and resources to rapidly tackle the worldwide public health emergency in practice, and reveals existing challenges for improvement.","['Liu Wang', 'Ruiqing Li', 'Jiaxin Zhu', 'Guangdong Bai', 'Haoyu Wang']",2020-10-23T07:56:25Z,http://arxiv.org/abs/2010.12218v1,Healthcare & Biomedical AI,COVID-19 Research,the open source community plays a vital role in coping with the pandemic . but COVID-19 themed open source projects have not been systematically studied . this study sheds light on the promising direction of adopting open source technologies .
The new science of COVID-19: A Bibliographic and Network Analysis,"Since the outbreak of the COVID-19, there have been many scientific publications studying the COVID-19. The purpose of this study is to identify the research trend, collaboration pattern, most influential elements, etc. from scientific publications related to COVID-19 in 2020, by using bibliographic analysis and network analysis. In Chapter 1 and Chapter 2, motivation behind this paper is introduced. Some previous similar studies are discussed. Comparisons are made in different aspects, such as data collection methods, data analysis tools and methods, etc. Their advantages and limitations compared to this paper are also addressed. In Chapter 3, important concepts used in this paper related to bibliographic analysis such as h-index and g-index, and network analysis such as centrality measures and assortativity are introduced. Networks with small-world property and scale-free property will also be studied. In Chapter 4 and Chapter 5, the way the data are obtained for the analysis of this paper is introduced step by step. Full result is shown. In Chapter 6, conclusions are arrived. A general growing trend of the number of the publications is observed, due to the efforts made by scientific researchers. Meanwhile, measures should also be taken to encourage future study in this field.",['Xuezhou Fan'],2024-07-17T20:50:06Z,http://arxiv.org/abs/2407.15867v1,Healthcare & Biomedical AI,COVID-19 Research,"the purpose of this study is to identify the research trend, collaboration pattern, etc. from scientific publications related to COVID-19 in 2020 ."
Balancing Personal Privacy and Public Safety during COVID-19: The Case   of South Korea,"There has been vigorous debate on how different countries responded to the COVID-19 pandemic. To secure public safety, South Korea actively used personal information at the risk of personal privacy whereas France encouraged voluntary cooperation at the risk of public safety. In this article, after a brief comparison of contextual differences with France, we focus on South Korea's approaches to epidemiological investigations. To evaluate the issues pertaining to personal privacy and public health, we examine the usage patterns of original data, de-identification data, and encrypted data. Our specific proposal discusses the COVID index, which considers collective infection, outbreak intensity, availability of medical infrastructure, and the death rate. Finally, we summarize the findings and lessons for future research and the policy implications.","['Na Young Ahn', 'Jun Eun Park', 'Dong Hoon Lee', 'Paul C. Hong']",2020-04-29T21:52:02Z,http://arxiv.org/abs/2004.14495v2,Healthcare & Biomedical AI,COVID-19 Research,"in this article, we focus on South Korea's approaches to epidemiological investigations . to secure public safety, South Korea actively used personal information . whereas France encouraged voluntary cooperation at the risk of public safety ."
A Systematic Review of the Digital Interventions for Fighting COVID-19:   The Bangladesh Perspective,"The objective of this paper is to synthesize the digital interventions initiatives to fight against COVID-19 in Bangladesh and compare with other countries. In order to obtain our research objective, we conducted a systematic review of the online content. We first reviewed the digital interventions that have been used to fight against COVID-19 across the globe. We then reviewed the initiatives that have been taken place in Bangladesh. Thereafter, we present a comparative analysis between the initiatives taken in Bangladesh and the other countries. Our findings show that while Bangladesh is capable to take benefits of the digital intervention approaches, tighter cooperation between government and private organizations as well as universities would be needed to get the most benefits. Furthermore, the government needs to make sure that the privacy of its citizens are protected.","['Muhammad Nazrul Islam', 'A. K. M. Najmul Islam']",2020-06-28T08:03:25Z,http://arxiv.org/abs/2006.16882v1,Healthcare & Biomedical AI,COVID-19 Research,the aim of this paper is to synthesize the digital interventions initiatives to fight COVID-19 in Bangladesh . we present a comparative analysis between the initiatives taken in Bangladesh and the other countries .
COVID-19: Detecting Depression Signals during Stay-At-Home Period,"The new coronavirus outbreak has been officially declared a global pandemic by the World Health Organization. To grapple with the rapid spread of this ongoing pandemic, most countries have banned indoor and outdoor gatherings and ordered their residents to stay home. Given the developing situation with coronavirus, mental health is an important challenge in our society today. In this paper, we discuss the investigation of social media postings to detect signals relevant to depression. To this end, we utilize topic modeling features and a collection of psycholinguistic and mental-well-being attributes to develop statistical models to characterize and facilitate representation of the more subtle aspects of depression. Furthermore, we predict whether signals relevant to depression are likely to grow significantly as time moves forward. Our best classifier yields F-1 scores as high as 0.8 and surpasses the utilized baseline by a considerable margin, 0.173. In closing, we propose several future research avenues.","['Jean Marie Tshimula', 'Belkacem Chikhaoui', 'Shengrui Wang']",2021-02-28T19:30:20Z,http://arxiv.org/abs/2103.00597v1,Healthcare & Biomedical AI,COVID-19 Research,"coronavirus has been declared a global pandemic by the world health organization . most countries have banned indoor and outdoor gatherings and ordered residents to stay home . in this paper, we discuss the investigation of social media postings to detect depression ."
Beyond COVID-19: Network science and sustainable exit strategies,"On May $28^{th}$ and $29^{th}$, a two day workshop was held virtually, facilitated by the Beyond Center at ASU and Moogsoft Inc. The aim was to bring together leading scientists with an interest in Network Science and Epidemiology to attempt to inform public policy in response to the COVID-19 pandemic. Epidemics are at their core a process that progresses dynamically upon a network, and are a key area of study in Network Science. In the course of the workshop a wide survey of the state of the subject was conducted. We summarize in this paper a series of perspectives of the subject, and where the authors believe fruitful areas for future research are to be found.","['James Bell', 'Ginestra Bianconi', 'David Butler', 'Jon Crowcroft', 'Paul C. W Davies', 'Chris Hicks', 'Hyunju Kim', 'Istvan Z. Kiss', 'Francesco Di Lauro', 'Carsten Maple', 'Ayan Paul', 'Mikhail Prokopenko', 'Philip Tee', 'Sara I. Walker']",2020-09-27T22:16:02Z,http://arxiv.org/abs/2009.12968v2,Healthcare & Biomedical AI,COVID-19 Research,a two-day workshop was held at the beyond center at aSU . the aim was to try to inform public policy in response to the COVID-19 pandemic .
"A hierarchical spatio-temporal model to analyze relative risk variations   of COVID-19: a focus on Spain, Italy and Germany","The novel coronavirus disease (COVID-19) has spread rapidly across the world in a short period of time and with a heterogeneous pattern. Understanding the underlying temporal and spatial dynamics in the spread of COVID-19 can result in informed and timely public health policies. In this paper, we use a spatio-temporal stochastic model to explain the temporal and spatial variations in the daily number of new confirmed cases in Spain, Italy and Germany from late February to mid September 2020. Using a hierarchical Bayesian framework, we found that the temporal trend of the epidemic in the three countries rapidly reached their peaks and slowly started to decline at the beginning of April and then increased and reached their second maximum in August. However decline and increase of the temporal trend seems to be sharper in Spain and smoother in Germany. The spatial heterogeneity of the relative risk of COVID-19 in Spain is also more pronounced than Italy and Germany.","['Abdollah Jalilian', 'Jorge Mateu']",2020-09-28T18:49:10Z,http://arxiv.org/abs/2009.13577v1,Healthcare & Biomedical AI,COVID-19 Research,the new coronavirus disease (COVID-19) has spread rapidly across the world . it has a heterogeneous pattern . understanding the underlying temporal and spatial dynamics in the spread can result in informed public health policies .
"Machine Learning Research Towards Combating COVID-19: Virus Detection,   Spread Prevention, and Medical Assistance","COVID-19 was first discovered in December 2019 and has continued to rapidly spread across countries worldwide infecting thousands and millions of people. The virus is deadly, and people who are suffering from prior illnesses or are older than the age of 60 are at a higher risk of mortality. Medicine and Healthcare industries have surged towards finding a cure, and different policies have been amended to mitigate the spread of the virus. While Machine Learning (ML) methods have been widely used in other domains, there is now a high demand for ML-aided diagnosis systems for screening, tracking, and predicting the spread of COVID-19 and finding a cure against it. In this paper, we present a journey of what role ML has played so far in combating the virus, mainly looking at it from a screening, forecasting, and vaccine perspectives. We present a comprehensive survey of the ML algorithms and models that can be used on this expedition and aid with battling the virus.","['Osama Shahid', 'Mohammad Nasajpour', 'Seyedamin Pouriyeh', 'Reza M. Parizi', 'Meng Han', 'Maria Valero', 'Fangyu Li', 'Mohammed Aledhari', 'Quan Z. Sheng']",2020-09-29T21:27:11Z,http://arxiv.org/abs/2010.07036v1,Healthcare & Biomedical AI,COVID-19 Research,"COVID-19 has spread rapidly across countries worldwide infecting thousands and millions of people . there is a high demand for ML-aided diagnosis systems for screening, tracking, and predicting the spread of the virus . we present a journey of what role ML has played so far in combating the virus, mainly from a screening, forecasting, and vaccine perspective ."
Contact Tracing Mobile Apps for COVID-19: Privacy Considerations and   Related Trade-offs,"Contact tracing is an essential tool for public health officials and local communities to fight the spread of novel diseases, such as for the COVID-19 pandemic. The Singaporean government just released a mobile phone app, TraceTogether, that is designed to assist health officials in tracking down exposures after an infected individual is identified. However, there are important privacy implications of the existence of such tracking apps. Here, we analyze some of those implications and discuss ways of ameliorating the privacy concerns without decreasing usefulness to public health. We hope in writing this document to ensure that privacy is a central feature of conversations surrounding mobile contact tracing apps and to encourage community efforts to develop alternative effective solutions with stronger privacy protection for the users. Importantly, though we discuss potential modifications, this document is not meant as a formal research paper, but instead is a response to some of the privacy characteristics of direct contact tracing apps like TraceTogether and an early-stage Request for Comments to the community.   Date written: 2020-03-24   Minor correction: 2020-03-30","['Hyunghoon Cho', 'Daphne Ippolito', 'Yun William Yu']",2020-03-25T17:31:37Z,http://arxiv.org/abs/2003.11511v2,Healthcare & Biomedical AI,COVID-19 Research,"the Singaporean government just released a mobile phone app, TraceTogether . there are important privacy implications of the existence of such tracking apps . here, we analyze some of those implications and discuss ways of ameliorating the privacy concerns ."
Digital Resistance during COVID-19: A Workflow Management System of   Contactless Purchasing and Its Empirical Study of Customer Acceptance,"The COVID-19 pandemic has stimulated the shift of work and life from the physical to a more digital format. To survive and thrive, companies have integrated more digital-enabled elements into their businesses to facilitate resilience, by avoiding potential close physical contact. Following Design Science Research Methodology (DSRM), this paper builds a workflow management system for contactless digital resilience when customers are purchasing in a store. Customer behavior, in coping with digital resilience against COVID-19, is illustrated and empirically tested, using a derivative model in which the constructs are from classical theories. Data was collected from individual customers via the Internet, and 247 completed questionnaires were examined.",['Yang Lu'],2021-04-29T02:25:30Z,http://arxiv.org/abs/2105.07838v2,Healthcare & Biomedical AI,COVID-19 Research,this paper builds a workflow management system for contactless digital resilience . data was collected from individual customers via the internet .
Use of Digital Technologies in Public Health Responses to Tackle   Covid-19: the Bangladesh Perspective,"This paper aims to study the fight against COVID-19 in Bangladesh and digital intervention initiatives. To achieve the purpose of our research, we conducted a methodical review of online content. We have reviewed the first digital intervention that COVID-19 has been used to fight against worldwide. Then we reviewed the initiatives that have been taken in Bangladesh. Our paper has shown that while Bangladesh can take advantage of the digital intervention approach, it will require rigorous collaboration between government organizations and universities to get the most out of it. Public health can become increasingly digital in the future, and we are reviewing international alignment requirements. This exploration also focused on the strategies for controlling, evaluating, and using digital technology to strengthen epidemic management and future preparations for COVID-19.","['Samrat Kumar Dey', 'Khaleda Mehrin', 'Lubana Akter', 'Mshura Akter']",2022-03-10T11:30:45Z,http://arxiv.org/abs/2203.05303v1,Healthcare & Biomedical AI,COVID-19 Research,this paper aims to study the fight against COVID-19 in Bangladesh . we reviewed the first digital intervention used to fight against worldwide . the paper also reviewed the initiatives that have been taken in the country .
"Multi-dimensional Racism Classification during COVID-19: Stigmatization,   Offensiveness, Blame, and Exclusion","Transcending the binary categorization of racist texts, our study takes cues from social science theories to develop a multi-dimensional model for racism detection, namely stigmatization, offensiveness, blame, and exclusion. With the aid of BERT and topic modeling, this categorical detection enables insights into the underlying subtlety of racist discussion on digital platforms during COVID-19. Our study contributes to enriching the scholarly discussion on deviant racist behaviours on social media. First, a stage-wise analysis is applied to capture the dynamics of the topic changes across the early stages of COVID-19 which transformed from a domestic epidemic to an international public health emergency and later to a global pandemic. Furthermore, mapping this trend enables a more accurate prediction of public opinion evolvement concerning racism in the offline world, and meanwhile, the enactment of specified intervention strategies to combat the upsurge of racism during the global public health crisis like COVID-19. In addition, this interdisciplinary research also points out a direction for future studies on social network analysis and mining. Integration of social science perspectives into the development of computational methods provides insights into more accurate data detection and analytics.","['Xin Pei', 'Deval Mehta']",2022-08-29T00:38:56Z,http://arxiv.org/abs/2208.13318v1,Healthcare & Biomedical AI,COVID-19 Research,our study takes cues from social science theories to develop a multi-dimensional model for racism detection . it enables insights into the underlying subtlety of racist discussion on digital platforms during COVID-19 .
Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and   Experiments,"The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.","['Ngoc C. Lê', 'Hai-Chung Nguyen-Phung', 'Thu-Huong Pham Thi', 'Hue Vu', 'Phuong-Thao Nguyen Thi', 'Thu-Thuy Tran', 'Hong-Nhung Le Thi', 'Thuy-Duong Nguyen-Thi', 'Thanh-Huy Nguyen']",2025-04-21T05:21:34Z,http://arxiv.org/abs/2504.21016v2,Healthcare & Biomedical AI,COVID-19 Research,"the COVID-19 pandemic caused great losses worldwide . efforts are taken place to prevent but many countries have failed . in Vietnam, the traceability, localization, and quarantine contribute to effective disease prevention . this is done by hand, and take a lot of work ."
ICT Intervention in the Containment of the Pandemic Spread of COVID-19:   An Exploratory Study,"The objective of this article is to explore the Information and Communication Technology (ICT) interventions and its strengths, weaknesses, opportunities and threats for the containment of the pandemic spread of novel Coronavirus. The research adopted a qualitative research approach, while the study data were collected through online content review and Focus Group Discussion (FGD). Starting with a preliminary set of about 1200 electronic resources or contents, 56 were selected for review study, applying an inclusion and exclusion criteria. The review study revealed ICT interventions that include websites and dashboards, mobile applications, robotics and drones, artificial intelligence (AI), data analytic, wearable and sensor technology, social media and learning tools, and interactive voice response (IVR) as well as explored their respective usages to combat the pandemic spread of COVID-19. Later, the FGD was replicated with 22 participants and explored the possible strengths, weaknesses, opportunities, and threats (SWOT) of deploying such technologies to fight against the COVID-19 pandemic. This research not only explores the exiting status of ICT interventions to fight with the COVID-19 pandemic but also provides a number of implications for the government, practitioners, doctors, policymakers and researchers for the effective utilization of the existing ICT interventions and for the future potential research and technological development to the containment of the pandemic spread of COVID-19 and future pandemics.","['Akib Zaman', 'Muhammad Nazrul Islam', 'Tarannum Zaki', 'Mohammad Sajjad Hossain']",2020-04-21T10:34:14Z,http://arxiv.org/abs/2004.09888v1,Healthcare & Biomedical AI,COVID-19 Research,this article explores the ICT interventions for the containment of the pandemic spread of novel Coronavirus . the research adopted a qualitative research approach .
Meta-Stock: Task-Difficulty-Adaptive Meta-learning for Sub-new Stock   Price Prediction,"Sub-new stock price prediction, forecasting the price trends of stocks listed less than one year, is crucial for effective quantitative trading. While deep learning methods have demonstrated effectiveness in predicting old stock prices, they require large training datasets unavailable for sub-new stocks. In this paper, we propose Meta-Stock: a task-difficulty-adaptive meta-learning approach for sub-new stock price prediction. Leveraging prediction tasks formulated by old stocks, our meta-learning method aims to acquire the fast generalization ability that can be further adapted to sub-new stock price prediction tasks, thereby solving the data scarcity of sub-new stocks. Moreover, we enhance the meta-learning process by incorporating an adaptive learning strategy sensitive to varying task difficulties. Through wavelet transform, we extract high-frequency coefficients to manifest stock price volatility. This allows the meta-learning model to assign gradient weights based on volatility-quantified task difficulty. Extensive experiments on datasets collected from three stock markets spanning twenty-two years prove that our Meta-Stock significantly outperforms previous methods and manifests strong applicability in real-world stock trading. Besides, we evaluate the reasonability of the task difficulty quantification and the effectiveness of the adaptive learning strategy.","['Linghao Wang', 'Zhen Liu', 'Peitian Ma', 'Qianli Ma']",2023-08-22T01:54:11Z,http://arxiv.org/abs/2308.11117v1,Finance & Economics,Stock Price Prediction,deep learning methods have demonstrated effectiveness in predicting old stock prices . but they require large training datasets unavailable for sub-new stocks . meta-Stock aims to acquire the fast generalization ability that can be further adapted .
A Rational Finance Explanation of the Stock Predictability Puzzle,"In this paper, we address one of the main puzzles in finance observed in the stock market by proponents of behavioral finance: the stock predictability puzzle. We offer a statistical model within the context of rational finance which can be used without relying on behavioral finance assumptions to model the predictability of stock returns. We incorporate the predictability of stock returns into the well-known Black-Scholes option pricing formula. Empirically, we analyze the option and spot trader's market predictability of stock prices by defining a forward-looking measure which we call ""implied excess predictability"". The empirical results indicate the effect of option trader's predictability of stock returns on the price of stock options is an increasing function of moneyness, while this effect is decreasing for spot traders. These empirical results indicate potential asymmetric predictability of stock prices by spot and option traders. We show in pricing options with the strike price significantly higher or lower than the stock price, the predictability of the underlying stock's return should be incorporated into the option pricing formula. In pricing options that have moneyness close to one, stock return predictability is not incorporated into the option pricing model because stock return predictability is the same for both types of traders. In other words, spot traders and option traders are equally informed about the future value of the stock market in this case. Comparing different volatility measures, we find that the difference between implied and realized variances or variance risk premium can potentially be used as a stock return predictor.","['Abootaleb Shirvani', 'Svetlozar T. Rachev', 'Frank J. Fabozzi']",2019-11-06T04:15:17Z,http://arxiv.org/abs/1911.02194v1,Finance & Economics,Stock Price Prediction,"in this paper, we address one of the main puzzles in finance observed in the stock market . we incorporate the predictability of stock returns into the well-known black-scholes option pricing formula . empirical results indicate potential asymmetric predictability by spot and option traders ."
Stock Prices Prediction using Deep Learning Models,"Financial markets have a vital role in the development of modern society. They allow the deployment of economic resources. Changes in stock prices reflect changes in the market. In this study, we focus on predicting stock prices by deep learning model. This is a challenge task, because there is much noise and uncertainty in information that is related to stock prices. So this work uses sparse autoencoders with one-dimension (1-D) residual convolutional networks which is a deep learning model, to de-noise the data. Long-short term memory (LSTM) is then used to predict the stock price. The prices, indices and macroeconomic variables in past are the features used to predict the next day's price. Experiment results show that 1-D residual convolutional networks can de-noise data and extract deep features better than a model that combines wavelet transforms (WT) and stacked autoencoders (SAEs). In addition, we compare the performances of model with two different forecast targets of stock price: absolute stock price and price rate of change. The results show that predicting stock price through price rate of change is better than predicting absolute prices directly.","['Jialin Liu', 'Fei Chao', 'Yu-Chen Lin', 'Chih-Min Lin']",2019-09-25T11:38:25Z,http://arxiv.org/abs/1909.12227v1,Finance & Economics,Stock Price Prediction,this study focuses on predicting stock prices by deep learning model . it uses sparse autoencoders with one-dimension (1-D) residual convolutional networks . long-short term memory is then used to predict the stock price .
Some Ideas for Improving Stock Price Prediction Based on Machine   Learning,"Stock price prediction is a complicated and interesting task. Noisy trends make stock pricing sensitive and complicated while the economical motivation behind, keeps it interesting for researchers and investors. In this paper we are to outline two novel ideas for stock pricing. We also test each of our suggested algorithms for predicting the price of 6 stocks from different sectors. To show the efficiency of our proposed algorithm, we compare the predicted prices with real values and also perform a backtest to verify that the annual returns based on real data and predicted price are almost the same.",['Negin Bagherpour'],2023-12-17T07:32:16Z,http://arxiv.org/abs/2312.10633v1,Finance & Economics,Stock Price Prediction,in this paper we are to outline two novel ideas for stock price prediction . we also test each of our suggested algorithms for predicting the price of 6 stocks .
The Time Function of Stock Price,"This paper tends to define the quantitative relationship between the stock price and time as a time function. Based on the empirical evidence that the log-return of a stock is the series of white noise, a mathematical model of the integral white noise is established to describe the phenomenon of stock price movement. A deductive approach is used to derive the auto-correlation function, displacement formula and power spectral density of the stock price movement, which reveals not only the characteristics and rules of the movement but also the predictability of the stock price. The deductive fundamental is provided for the price analysis, prediction and risk management of portfolio investment.","['Shengfeng Mei', 'Hong Gao']",2020-08-11T03:02:14Z,http://arxiv.org/abs/2008.11806v2,Finance & Economics,Stock Price Prediction,"this paper tends to define the quantitative relationship between the stock price and time as a time function . a deductive approach is used to derive the auto-correlation function, displacement formula and power spectral density of stock price movement ."
A Hybrid Deep Learning Framework for Stock Price Prediction Considering   the Investor Sentiment of Online Forum Enhanced by Popularity,"Stock price prediction has always been a difficult task for forecasters. Using cutting-edge deep learning techniques, stock price prediction based on investor sentiment extracted from online forums has become feasible. We propose a novel hybrid deep learning framework for predicting stock prices. The framework leverages the XLNET model to analyze the sentiment conveyed in user posts on online forums, combines these sentiments with the post popularity factor to compute daily group sentiments, and integrates this information with stock technical indicators into an improved BiLSTM-highway model for stock price prediction. Through a series of comparative experiments involving four stocks on the Chinese stock market, it is demonstrated that the hybrid framework effectively predicts stock prices. This study reveals the necessity of analyzing investors' textual views for stock price prediction.","['Huiyu Li', 'Junhua Hu']",2024-05-17T07:18:08Z,http://arxiv.org/abs/2405.10584v1,Finance & Economics,Stock Price Prediction,the framework leverages the XLNET model to analyze the sentiment conveyed in user posts on online forums . it combines these sentiments with the post popularity factor to compute daily group sentiments . the framework integrates this information with stock technical indicators into an improved biLSTM-highway model .
Cross-sectional Stock Price Prediction using Deep Learning for Actual   Investment Management,"Stock price prediction has been an important research theme both academically and practically. Various methods to predict stock prices have been studied until now. The feature that explains the stock price by a cross-section analysis is called a ""factor"" in the field of finance. Many empirical studies in finance have identified which stocks having features in the cross-section relatively increase and which decrease in terms of price. Recently, stock price prediction methods using machine learning, especially deep learning, have been proposed since the relationship between these factors and stock prices is complex and non-linear. However, there are no practical examples for actual investment management. In this paper, therefore, we present a cross-sectional daily stock price prediction framework using deep learning for actual investment management. For example, we build a portfolio with information available at the time of market closing and invest at the time of market opening the next day. We perform empirical analysis in the Japanese stock market and confirm the profitability of our framework.","['Masaya Abe', 'Kei Nakagawa']",2020-02-17T14:28:42Z,http://arxiv.org/abs/2002.06975v1,Finance & Economics,Stock Price Prediction,"the feature that explains the stock price by a cross-section analysis is called a ""factor"" in the field of finance . the relationship between these factors and stock prices is complex and non-linear . there are no practical examples for actual investment management ."
FinGAT: Financial Graph Attention Networks for Recommending Top-K   Profitable Stocks,"Financial technology (FinTech) has drawn much attention among investors and companies. While conventional stock analysis in FinTech targets at predicting stock prices, less effort is made for profitable stock recommendation. Besides, in existing approaches on modeling time series of stock prices, the relationships among stocks and sectors (i.e., categories of stocks) are either neglected or pre-defined. Ignoring stock relationships will miss the information shared between stocks while using pre-defined relationships cannot depict the latent interactions or influence of stock prices between stocks. In this work, we aim at recommending the top-K profitable stocks in terms of return ratio using time series of stock prices and sector information. We propose a novel deep learning-based model, Financial Graph Attention Networks (FinGAT), to tackle the task under the setting that no pre-defined relationships between stocks are given. The idea of FinGAT is three-fold. First, we devise a hierarchical learning component to learn short-term and long-term sequential patterns from stock time series. Second, a fully-connected graph between stocks and a fully-connected graph between sectors are constructed, along with graph attention networks, to learn the latent interactions among stocks and sectors. Third, a multi-task objective is devised to jointly recommend the profitable stocks and predict the stock movement. Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit remarkable recommendation performance of our FinGAT, comparing to state-of-the-art methods.","['Yi-Ling Hsu', 'Yu-Che Tsai', 'Cheng-Te Li']",2021-06-18T14:51:14Z,http://arxiv.org/abs/2106.10159v1,Finance & Economics,Stock Price Prediction,"financial technology (FinTech) has drawn much attention among investors and companies . we propose a novel deep learning-based model, Financial Graph Attention Networks . a multi-task objective is devised to jointly recommend the profitable stocks ."
Robust Analysis of Stock Price Time Series Using CNN and LSTM-Based Deep   Learning Models,"Prediction of stock price and stock price movement patterns has always been a critical area of research. While the well-known efficient market hypothesis rules out any possibility of accurate prediction of stock prices, there are formal propositions in the literature demonstrating accurate modeling of the predictive systems that can enable us to predict stock prices with a very high level of accuracy. In this paper, we present a suite of deep learning-based regression models that yields a very high level of accuracy in stock price prediction. To build our predictive models, we use the historical stock price data of a well-known company listed in the National Stock Exchange (NSE) of India during the period December 31, 2012 to January 9, 2015. The stock prices are recorded at five minutes intervals of time during each working day in a week. Using these extremely granular stock price data, we build four convolutional neural network (CNN) and five long- and short-term memory (LSTM)-based deep learning models for accurate forecasting of the future stock prices. We provide detailed results on the forecasting accuracies of all our proposed models based on their execution time and their root mean square error (RMSE) values.","['Sidra Mehtab', 'Jaydip Sen', 'Subhasis Dasgupta']",2020-11-07T16:07:10Z,http://arxiv.org/abs/2011.08011v2,Finance & Economics,Stock Price Prediction,"in this paper, we present a suite of deep learning-based regression models . the models are based on historical stock price data of a well-known company . we provide detailed results on the forecasting accuracies ."
The Potential of Quantum Techniques for Stock Price Prediction,"We explored the potential applications of various Quantum Algorithms for stock price prediction by conducting a series of experimental simulations using both Classical as well as Quantum Hardware. Firstly, we extracted various stock price indicators, such as Moving Averages (MA), Average True Range (ATR), and Aroon, to gain insights into market trends and stock price movements. Next, we employed Quantum Annealing (QA) for feature selection and Principal Component Analysis (PCA) for dimensionality reduction. Further, we transformed the stock price prediction task essentially into a classification problem. We trained the Quantum Support Vector Machine (QSVM) to predict price movements (whether up or down) contrasted their performance with classical models and analyzed their accuracy on a dataset formulated using Quantum Annealing and PCA individually. We focused on the stock price prediction and binary classification of stock prices for four different companies, namely Apple, Visa, Johnson and Jonson, and Honeywell. We primarily used the real-time stock data of the raw stock prices of these companies. We compared various Quantum Computing techniques with their classical counterparts in terms of accuracy and F-score of the prediction model. Through these experimental simulations, we shed light on the potential advantages and limitations of Quantum Algorithms in stock price prediction and contribute to the growing body of knowledge at the intersection of Quantum Computing and Finance.","['Naman S', 'Gaurang B', 'Neel S', 'Aswath Babu H']",2023-08-25T19:26:41Z,http://arxiv.org/abs/2308.13642v1,Finance & Economics,Stock Price Prediction,experimental simulations shed light on potential applications of Quantum Algorithms for stock price prediction . we used both classical and Quantum hardware to extract stock price indicators .
The Predictability of Stock Price: Empirical Study onTick Data in   Chinese Stock Market,"Whether or not stocks are predictable has been a topic of concern for decades.The efficient market hypothesis (EMH) says that it is difficult for investors to make extra profits by predicting stock prices, but this may not be true, especially for the Chinese stock market. Therefore, we explore the predictability of the Chinese stock market based on tick data, a widely studied high-frequency data. We obtain the predictability of 3, 834 Chinese stocks by adopting the concept of true entropy, which is calculated by Limpel-Ziv data compression method. The Markov chain model and the diffusion kernel model are used to compare the upper bounds on predictability, and it is concluded that there is still a significant performance gap between the forecasting models used and the theoretical upper bounds.Our work shows that more than 73% of stocks have prediction accuracy greater than 70% and RMSE less than 2 CNY under different quantification intervals with different models. We further take Spearman's correlation to reveal that the average stock price and price volatility may have a negative impact on prediction accuracy, which may be helpful for stock investors.","['Yueshan Chen', 'Xingyu Xu', 'Tian Lan', 'Sihai Zhang']",2023-07-05T08:19:24Z,http://arxiv.org/abs/2307.02099v2,Finance & Economics,Stock Price Prediction,the efficient market hypothesis says it is difficult for investors to make extra profits . but this may not be true for the Chinese stock market . more than 73% of stocks have prediction accuracy greater than 70% .
A Comparative Predicting Stock Prices using Heston and Geometric   Brownian Motion Models,"This paper presents a novel approach to predicting stock prices using technical analysis. By utilizing Ito's lemma and Euler-Maruyama methods, the researchers develop Heston and Geometric Brownian Motion models that take into account volatility, interest rate, and historical stock prices to generate predictions. The results of the study demonstrate that these models are effective in accurately predicting stock prices and outperform commonly used statistical indicators. The authors conclude that this technical analysis-based method offers a promising solution for stock market prediction.","['H. T. Shehzad', 'M. A. Anwar', 'M. Razzaq']",2023-02-15T17:31:31Z,http://arxiv.org/abs/2302.07796v1,Finance & Economics,Stock Price Prediction,"utilizing ito's lemma and Euler-Maruyama methods, researchers develop models . they take into account volatility, interest rate, and historical stock prices . the results of the study demonstrate that these models are effective ."
Hidden Markov Models for Stock Market Prediction,"The stock market presents a challenging environment for accurately predicting future stock prices due to its intricate and ever-changing nature. However, the utilization of advanced methodologies can significantly enhance the precision of stock price predictions. One such method is Hidden Markov Models (HMMs). HMMs are statistical models that can be used to model the behavior of a partially observable system, making them suitable for modeling stock prices based on historical data. Accurate stock price predictions can help traders make better investment decisions, leading to increased profits.   In this article, we trained and tested a Hidden Markov Model for the purpose of predicting a stock closing price based on its opening price and the preceding day's prices. The model's performance has been evaluated using two indicators: Mean Average Prediction Error (MAPE), which specifies the average accuracy of our model, and Directional Prediction Accuracy (DPA), a newly introduced indicator that accounts for the number of fractional change predictions that are correct in sign.","['Luigi Catello', 'Ludovica Ruggiero', 'Lucia Schiavone', 'Mario Valentino']",2023-10-05T08:14:24Z,http://arxiv.org/abs/2310.03775v1,Finance & Economics,Stock Price Prediction,the stock market presents a challenging environment for accurately predicting future stock prices . advanced methodologies can significantly enhance the precision of stock price predictions . one such method is the use of Hidden Markov Models (HMMs)
Stock Price Prediction using Dynamic Neural Networks,"This paper will analyze and implement a time series dynamic neural network to predict daily closing stock prices. Neural networks possess unsurpassed abilities in identifying underlying patterns in chaotic, non-linear, and seemingly random data, thus providing a mechanism to predict stock price movements much more precisely than many current techniques. Contemporary methods for stock analysis, including fundamental, technical, and regression techniques, are conversed and paralleled with the performance of neural networks. Also, the Efficient Market Hypothesis (EMH) is presented and contrasted with Chaos theory using neural networks. This paper will refute the EMH and support Chaos theory. Finally, recommendations for using neural networks in stock price prediction will be presented.",['David Noel'],2023-06-18T20:06:44Z,http://arxiv.org/abs/2306.12969v1,Finance & Economics,Stock Price Prediction,this paper will analyze and implement a time series dynamic neural network . it will be used to predict daily closing stock prices . the paper will refute the EMH and support Chaos theory .
Design and Analysis of Robust Deep Learning Models for Stock Price   Prediction,"Building predictive models for robust and accurate prediction of stock prices and stock price movement is a challenging research problem to solve. The well-known efficient market hypothesis believes in the impossibility of accurate prediction of future stock prices in an efficient stock market as the stock prices are assumed to be purely stochastic. However, numerous works proposed by researchers have demonstrated that it is possible to predict future stock prices with a high level of precision using sophisticated algorithms, model architectures, and the selection of appropriate variables in the models. This chapter proposes a collection of predictive regression models built on deep learning architecture for robust and precise prediction of the future prices of a stock listed in the diversified sectors in the National Stock Exchange (NSE) of India. The Metastock tool is used to download the historical stock prices over a period of two years (2013- 2014) at 5 minutes intervals. While the records for the first year are used to train the models, the testing is carried out using the remaining records. The design approaches of all the models and their performance results are presented in detail. The models are also compared based on their execution time and accuracy of prediction.","['Jaydip Sen', 'Sidra Mehtab']",2021-06-17T17:15:02Z,http://arxiv.org/abs/2106.09664v1,Finance & Economics,Stock Price Prediction,the efficient market hypothesis believes in the impossibility of accurate prediction of future stock prices in an efficient stock market . the stock prices are assumed to be purely stochastic . a metastock tool is used to download historical stock prices over a period of two years (2013-2014) at 5 minutes intervals .
GCNET: graph-based prediction of stock price movement using graph   convolutional network,"The importance of considering related stocks data for the prediction of stock price movement has been shown in many studies, however, advanced graphical techniques for modeling, embedding and analyzing the behavior of interrelated stocks have not been widely exploited for the prediction of stocks price movements yet. The main challenges in this domain are to find a way for modeling the existing relations among an arbitrary set of stocks and to exploit such a model for improving the prediction performance for those stocks. The most of existing methods in this domain rely on basic graph-analysis techniques, with limited prediction power, and suffer from a lack of generality and flexibility. In this paper, we introduce a novel framework, called GCNET that models the relations among an arbitrary set of stocks as a graph structure called influence network and uses a set of history-based prediction models to infer plausible initial labels for a subset of the stock nodes in the graph. Finally, GCNET uses the Graph Convolutional Network algorithm to analyze this partially labeled graph and predicts the next price direction of movement for each stock in the graph. GCNET is a general prediction framework that can be applied for the prediction of the price fluctuations of interacting stocks based on their historical data. Our experiments and evaluations on a set of stocks from the NASDAQ index demonstrate that GCNET significantly improves the performance of SOTA in terms of accuracy and MCC measures.","['Alireza Jafari', 'Saman Haratizadeh']",2022-02-19T16:13:44Z,http://arxiv.org/abs/2203.11091v2,Finance & Economics,Stock Price Prediction,GCNET is a framework that can be applied for the prediction of stock price fluctuations . it uses history-based prediction models to infer plausible initial labels for a subset of stock nodes . the framework predicts the next price direction of movement for each stock in the graph .
A comparative study of Different Machine Learning Regressors For Stock   Market Prediction,"For the development of successful share trading strategies, forecasting the course of action of the stock market index is important. Effective prediction of closing stock prices could guarantee investors attractive benefits. Machine learning algorithms have the ability to process and forecast almost reliable closing prices for historical stock patterns. In this article, we intensively studied NASDAQ stock market and targeted to choose the portfolio of ten different companies belongs to different sectors. The objective is to compute opening price of next day stock using historical data. To fulfill this task nine different Machine Learning regressor applied on this data and evaluated using MSE and R2 as performance metric.","['Nazish Ashfaq', 'Zubair Nawaz', 'Muhammad Ilyas']",2021-04-14T15:37:33Z,http://arxiv.org/abs/2104.07469v1,Finance & Economics,Stock Price Prediction,the objective is to compute opening price of next day stock using historical data . machine learning algorithms have the ability to process and forecast almost reliable closing prices .
Global Stock Market Prediction Based on Stock Chart Images Using Deep   Q-Network,"We applied Deep Q-Network with a Convolutional Neural Network function approximator, which takes stock chart images as input, for making global stock market predictions. Our model not only yields profit in the stock market of the country where it was trained but generally yields profit in global stock markets. We trained our model only in the US market and tested it in 31 different countries over 12 years. The portfolios constructed based on our model's output generally yield about 0.1 to 1.0 percent return per transaction prior to transaction costs in 31 countries. The results show that there are some patterns on stock chart image, that tend to predict the same future stock price movements across global stock markets. Moreover, the results show that future stock prices can be predicted even if the training and testing procedures are done in different countries. Training procedure could be done in relatively large and liquid markets (e.g., USA) and tested in small markets. This result demonstrates that artificial intelligence based stock price forecasting models can be used in relatively small markets (emerging countries) even though they do not have a sufficient amount of data for training.","['Jinho Lee', 'Raehyun Kim', 'Yookyung Koh', 'Jaewoo Kang']",2019-02-28T08:40:24Z,http://arxiv.org/abs/1902.10948v1,Finance & Economics,Stock Price Prediction,we applied Deep Q-Network with a Convolutional Neural Network function approximator . our model generally yields profit in global stock markets . we trained our model only in the us market and tested it in 31 different countries over 12 years .
Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement   Prediction,"With technological advancements and the exponential growth of data, we have been unfolding different capabilities of neural networks in different sectors. In this paper, I have tried to use a specific type of Neural Network known as Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words, I have tried to construct and train a convolutional neural network on past stock prices data and then tried to predict the movement of stock price i.e. whether the stock price would rise or fall, in the coming time.",['Kunal Bhardwaj'],2021-06-03T15:14:46Z,http://arxiv.org/abs/2106.01920v1,Finance & Economics,Stock Price Prediction,"in this paper, i have tried to use a specific type of neural network known as Convolutional Neural Network(CNN/ConvNet) in the stock market . i've tried to construct and train a convolutional neural network on past stock prices data ."
From Local Patterns to Global Understanding: Cross-Stock Trend   Integration for Enhanced Predictive Modeling,"Stock price prediction is a critical area of financial forecasting, traditionally approached by training models using the historical price data of individual stocks. While these models effectively capture single-stock patterns, they fail to leverage potential correlations among stock trends, which could improve predictive performance. Current single-stock learning methods are thus limited in their ability to provide a broader understanding of price dynamics across multiple stocks. To address this, we propose a novel method that merges local patterns into a global understanding through cross-stock pattern integration. Our strategy is inspired by Federated Learning (FL), a paradigm designed for decentralized model training. FL enables collaborative learning across distributed datasets without sharing raw data, facilitating the aggregation of global insights while preserving data privacy. In our adaptation, we train models on individual stock data and iteratively merge them to create a unified global model. This global model is subsequently fine-tuned on specific stock data to retain local relevance. The proposed strategy enables parallel training of individual stock models, facilitating efficient utilization of computational resources and reducing overall training time. We conducted extensive experiments to evaluate the proposed method, demonstrating that it outperforms benchmark models and enhances the predictive capabilities of state-of-the-art approaches. Our results highlight the efficacy of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction, offering a robust alternative to traditional single-stock learning methodologies.","['Yi Hu', 'Hanchi Ren', 'Jingjing Deng', 'Xianghua Xie']",2025-05-22T12:04:10Z,http://arxiv.org/abs/2505.16573v1,Finance & Economics,Stock Price Prediction,stock price prediction is a critical area of financial forecasting . current single-stock learning methods are limited in their ability to provide a broader understanding . proposed method merges local patterns into a global understanding through cross-stock pattern integration .
Proactive Fraud Defense: Machine Learning's Evolving Role in Protecting   Against Online Fraud,"As online fraud becomes more sophisticated and pervasive, traditional fraud detection methods are struggling to keep pace with the evolving tactics employed by fraudsters. This paper explores the transformative role of machine learning in addressing these challenges by offering more advanced, scalable, and adaptable solutions for fraud detection and prevention. By analyzing key models such as Random Forest, Neural Networks, and Gradient Boosting, this paper highlights the strengths of machine learning in processing vast datasets, identifying intricate fraud patterns, and providing real-time predictions that enable a proactive approach to fraud prevention. Unlike rule-based systems that react after fraud has occurred, machine learning models continuously learn from new data, adapting to emerging fraud schemes and reducing false positives, which ultimately minimizes financial losses. This research emphasizes the potential of machine learning to revolutionize fraud detection frameworks by making them more dynamic, efficient, and capable of handling the growing complexity of fraud across various industries. Future developments in machine learning, including deep learning and hybrid models, are expected to further enhance the predictive accuracy and applicability of these systems, ensuring that organizations remain resilient in the face of new and emerging fraud tactics.",['Md Kamrul Hasan Chy'],2024-10-26T21:34:28Z,http://arxiv.org/abs/2410.20281v1,Finance & Economics,Fraud Detection,"this paper explores the transformative role of machine learning in addressing fraud challenges . it highlights the strengths of the technology in processing vast datasets . machine learning models continuously learn from new data, adapting to emerging schemes ."
Survey of Insurance Fraud Detection Using Data Mining Techniques,"With an increase in financial accounting fraud in the current economic scenario experienced, financial accounting fraud detection has become an emerging topics of great importance for academics, research and industries. Financial fraud is a deliberate act that is contrary to law, rule or policy with intent to obtain unauthorized financial benefit and intentional misstatements or omission of amounts by deceiving users of financial statements, especially investors and creditors. Data mining techniques are providing great aid in financial accounting fraud detection, since dealing with the large data volumes and complexities of financial data are big challenges for forensic accounting. Financial fraud can be classified into four: bank fraud, insurance fraud, securities and commodities fraud. Fraud is nothing but wrongful or criminal trick planned to result in financial or personal gains. This paper describes the more details on insurance sector related frauds and related solutions. In finance, insurance sector is doing important role and also it is unavoidable sector of every human being.","['H. Lookman Sithic', 'T. Balasubramanian']",2013-09-03T06:49:43Z,http://arxiv.org/abs/1309.0806v1,Finance & Economics,Fraud Detection,"financial fraud is a deliberate act with intent to obtain unauthorized financial benefit . data mining techniques are providing great aid in financial accounting fraud detection . financial fraud can be classified into four: bank fraud, insurance fraud, securities and commodities fraud ."
Search Rank Fraud De-Anonymization in Online Systems,"We introduce the fraud de-anonymization problem, that goes beyond fraud detection, to unmask the human masterminds responsible for posting search rank fraud in online systems. We collect and study search rank fraud data from Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters recruited from 6 crowdsourcing sites. We propose Dolos, a fraud de-anonymization system that leverages traits and behaviors extracted from these studies, to attribute detected fraud to crowdsourcing site fraudsters, thus to real identities and bank accounts. We introduce MCDense, a min-cut dense component detection algorithm to uncover groups of user accounts controlled by different fraudsters, and leverage stylometry and deep learning to attribute them to crowdsourcing site profiles. Dolos correctly identified the owners of 95% of fraudster-controlled communities, and uncovered fraudsters who promoted as many as 97.5% of fraud apps we collected from Google Play. When evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6 months, Dolos identified 1,056 apps with suspicious reviewer groups. We report orthogonal evidence of their fraud, including fraud duplicates and fraud re-posts.","['Mizanur Rahman', 'Nestor Hernandez', 'Bogdan Carbunar', 'Duen Horng Chau']",2018-06-23T05:36:56Z,http://arxiv.org/abs/1806.08910v1,Finance & Economics,Fraud Detection,a fraud de-anonymization problem unmasks the human masterminds responsible for posting fraud . 58 search rank fraudsters were recruited from 6 crowdsourcing sites . dolos identifies the owners of 95% of fraudster-controlled communities .
FraudJudger: Real-World Data Oriented Fraud Detection on Digital Payment   Platforms,"Automated fraud behaviors detection on electronic payment platforms is a tough problem. Fraud users often exploit the vulnerability of payment platforms and the carelessness of users to defraud money, steal passwords, do money laundering, etc, which causes enormous losses to digital payment platforms and users. There are many challenges for fraud detection in practice. Traditional fraud detection methods require a large-scale manually labeled dataset, which is hard to obtain in reality. Manually labeled data cost tremendous human efforts. Besides, the continuous and rapid evolution of fraud users makes it hard to find new fraud patterns based on existing detection rules. In our work, we propose a real-world data oriented detection paradigm which can detect fraud users and upgrade its detection ability automatically. Based on the new paradigm, we design a novel fraud detection model, FraudJudger, to analyze users behaviors on digital payment platforms and detect fraud users with fewer labeled data in training. FraudJudger can learn the latent representations of users from unlabeled data with the help of Adversarial Autoencoder (AAE). Furthermore, FraudJudger can find new fraud patterns from unknown users by cluster analysis. Our experiment is based on a real-world electronic payment dataset. Comparing with other well-known fraud detection methods, FraudJudger can achieve better detection performance with only 10% labeled data.","['Ruoyu Deng', 'Na Ruan']",2019-09-05T13:31:52Z,http://arxiv.org/abs/1909.02398v1,Finance & Economics,Fraud Detection,automated fraud behaviors detection on electronic payment platforms is a tough problem . fraud users often exploit the vulnerability of payment platforms and carelessness of users . traditional fraud detection methods require a large-scale manually labeled dataset .
Deep Q-Network-based Adaptive Alert Threshold Selection Policy for   Payment Fraud Systems in Retail Banking,"Machine learning models have widely been used in fraud detection systems. Most of the research and development efforts have been concentrated on improving the performance of the fraud scoring models. Yet, the downstream fraud alert systems still have limited to no model adoption and rely on manual steps. Alert systems are pervasively used across all payment channels in retail banking and play an important role in the overall fraud detection process. Current fraud detection systems end up with large numbers of dropped alerts due to their inability to account for the alert processing capacity. Ideally, alert threshold selection enables the system to maximize the fraud detection while balancing the upstream fraud scores and the available bandwidth of the alert processing teams. However, in practice, fixed thresholds that are used for their simplicity do not have this ability. In this paper, we propose an enhanced threshold selection policy for fraud alert systems. The proposed approach formulates the threshold selection as a sequential decision making problem and uses Deep Q-Network based reinforcement learning. Experimental results show that this adaptive approach outperforms the current static solutions by reducing the fraud losses as well as improving the operational efficiency of the alert system.","['Hongda Shen', 'Eren Kurshan']",2020-10-21T15:10:57Z,http://arxiv.org/abs/2010.11062v1,Finance & Economics,Fraud Detection,machine learning models have been widely used in fraud detection systems . downstream fraud alert systems still have limited to no model adoption . current systems end up with large numbers of dropped alerts due to inability to account for alert processing capacity .
Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM   Against Augmented Fraud and Phishing Inducements,"We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in dynamic, real-world scenarios. Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job postings, social media, and news, categorized into 5 major fraud types. Unlike previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to assess LLMs' resistance to fraud at different stages, including credibility building, urgency creation, and emotional manipulation. Furthermore, we evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM provides general decision-making assistance, and 2. Role-play, where the model assumes a specific persona, widely used in real-world agent-based interactions. Our evaluation reveals the significant challenges in defending against fraud and phishing inducement, especially in role-play settings and fake job postings. Additionally, we observe a substantial performance gap between Chinese and English, underscoring the need for improved multilingual fraud detection capabilities.","['Shu Yang', 'Shenzhe Zhu', 'Zeyu Wu', 'Keyu Wang', 'Junchi Yao', 'Junchao Wu', 'Lijie Hu', 'Mengdi Li', 'Derek F. Wong', 'Di Wang']",2025-02-18T14:47:02Z,http://arxiv.org/abs/2502.12904v2,Finance & Economics,Fraud Detection,"Fraud-R1 is a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing . it comprises 8,564 fraud cases sourced from scams, fake job postings, social media, and news . a performance gap exists between Chinese and English, under"
A Review of Financial Accounting Fraud Detection based on Data Mining   Techniques,"With an upsurge in financial accounting fraud in the current economic scenario experienced, financial accounting fraud detection (FAFD) has become an emerging topic of great importance for academic, research and industries. The failure of internal auditing system of the organization in identifying the accounting frauds has lead to use of specialized procedures to detect financial accounting fraud, collective known as forensic accounting. Data mining techniques are providing great aid in financial accounting fraud detection, since dealing with the large data volumes and complexities of financial data are big challenges for forensic accounting. This paper presents a comprehensive review of the literature on the application of data mining techniques for the detection of financial accounting fraud and proposes a framework for data mining techniques based accounting fraud detection. The systematic and comprehensive literature review of the data mining techniques applicable to financial accounting fraud detection may provide a foundation to future research in this field. The findings of this review show that data mining techniques like logistic models, neural networks, Bayesian belief network, and decision trees have been applied most extensively to provide primary solutions to the problems inherent in the detection and classification of fraudulent data.","['Anuj Sharma', 'Prabin Kumar Panigrahi']",2013-09-16T13:20:10Z,http://arxiv.org/abs/1309.3944v1,Finance & Economics,Fraud Detection,failure of internal auditing system has led to use of specialized procedures . data mining techniques are providing great aid in financial accounting fraud detection . dealing with large data volumes and complexities of financial data are big challenges .
detectGNN: Harnessing Graph Neural Networks for Enhanced Fraud Detection   in Credit Card Transactions,"Credit card fraud is a major issue nowadays, costing huge money and affecting trust in financial systems. Traditional fraud detection methods often fail to detect advanced and growing fraud techniques. This study focuses on using Graph Neural Networks (GNNs) to improve fraud detection by analyzing transactions as a network of connected data points, such as accounts, traders, and devices. The proposed ""detectGNN"" model uses advanced features like time-based patterns and dynamic updates to expose hidden fraud and improve detection accuracy. Tests show that GNNs perform better than traditional methods in finding complex and multi-layered fraud. The model also addresses real-time processing, data imbalance, and privacy concerns, making it practical for real-world use. This research shows that GNNs can provide a powerful, accurate, and a scalable solution for detecting fraud. Future work will focus on making the models easier to understand, privacy-friendly, and adaptable to new types of fraud, ensuring safer financial transactions in the digital world.","['Irin Sultana', 'Syed Mustavi Maheen', 'Naresh Kshetri', 'Md Nasim Fardous Zim']",2025-02-09T00:01:36Z,http://arxiv.org/abs/2503.22681v1,Finance & Economics,Fraud Detection,"credit card fraud is a major issue nowadays, costing huge money and affecting trust in financial systems . this study focuses on using Graph Neural Networks (GNNs) to improve fraud detection . the proposed ""detectGNN"" model uses advanced features like time-based patterns"
FraudDroid: Automated Ad Fraud Detection for Android Apps,"Although mobile ad frauds have been widespread, state-of-the-art approaches in the literature have mainly focused on detecting the so-called static placement frauds, where only a single UI state is involved and can be identified based on static information such as the size or location of ad views. Other types of fraud exist that involve multiple UI states and are performed dynamically while users interact with the app. Such dynamic interaction frauds, although now widely spread in apps, have not yet been explored nor addressed in the literature. In this work, we investigate a wide range of mobile ad frauds to provide a comprehensive taxonomy to the research community. We then propose, FraudDroid, a novel hybrid approach to detect ad frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI state transition graphs and collects their associated runtime network traffics, which are then leveraged to check against a set of heuristic-based rules for identifying ad fraudulent behaviours. We show empirically that FraudDroid detects ad frauds with a high precision (93%) and recall (92%). Experimental results further show that FraudDroid is capable of detecting ad frauds across the spectrum of fraud types. By analysing 12,000 ad-supported Android apps, FraudDroid identified 335 cases of fraud associated with 20 ad networks that are further confirmed to be true positive results and are shared with our fellow researchers to promote advanced ad fraud detection","['Feng Dong', 'Haoyu Wang', 'Li Li', 'Yao Guo', 'Tegawende F. Bissyande', 'Tianming Liu', 'Guoai Xu', 'Jacques Klein']",2017-09-05T01:58:05Z,http://arxiv.org/abs/1709.01213v4,Finance & Economics,Fraud Detection,"mobile ad frauds have been widespread, but the literature has mainly focused on static placement frauds . other types of fraud exist that involve multiple UI states and are performed dynamically while users interact with the app . we propose, FraudDroid, a novel hybrid approach to detect "
Deep Learning Methods for Credit Card Fraud Detection,"Credit card frauds are at an ever-increasing rate and have become a major problem in the financial sector. Because of these frauds, card users are hesitant in making purchases and both the merchants and financial institutions bear heavy losses. Some major challenges in credit card frauds involve the availability of public data, high class imbalance in data, changing nature of frauds and the high number of false alarms. Machine learning techniques have been used to detect credit card frauds but no fraud detection systems have been able to offer great efficiency to date. Recent development of deep learning has been applied to solve complex problems in various areas. This paper presents a thorough study of deep learning methods for the credit card fraud detection problem and compare their performance with various machine learning algorithms on three different financial datasets. Experimental results show great performance of the proposed deep learning methods against traditional machine learning models and imply that the proposed approaches can be implemented effectively for real-world credit card fraud detection systems.","['Thanh Thi Nguyen', 'Hammad Tahir', 'Mohamed Abdelrazek', 'Ali Babar']",2020-12-07T14:48:58Z,http://arxiv.org/abs/2012.03754v1,Finance & Economics,Fraud Detection,credit card frauds are at an ever-increasing rate and have become a major problem in the financial sector . card users are hesitant in making purchases and both the merchants and financial institutions bear heavy losses . this paper presents a thorough study of deep learning methods for the credit card
AI-based Identity Fraud Detection: A Systematic Review,"With the rapid development of digital services, a large volume of personally identifiable information (PII) is stored online and is subject to cyberattacks such as Identity fraud. Most recently, the use of Artificial Intelligence (AI) enabled deep fake technologies has significantly increased the complexity of identity fraud. Fraudsters may use these technologies to create highly sophisticated counterfeit personal identification documents, photos and videos. These advancements in the identity fraud landscape pose challenges for identity fraud detection and society at large. There is a pressing need to review and understand identity fraud detection methods, their limitations and potential solutions. This research aims to address this important need by using the well-known systematic literature review method. This paper reviewed a selected set of 43 papers across 4 major academic literature databases. In particular, the review results highlight the two types of identity fraud prevention and detection methods, in-depth and open challenges. The results were also consolidated into a taxonomy of AI-based identity fraud detection and prevention methods including key insights and trends. Overall, this paper provides a foundational knowledge base to researchers and practitioners for further research and development in this important area of digital identity fraud.","['Chuo Jun Zhang', 'Asif Q. Gill', 'Bo Liu', 'Memoona J. Anwar']",2025-01-16T01:52:30Z,http://arxiv.org/abs/2501.09239v1,Finance & Economics,Fraud Detection,"the use of artificial intelligence (AI) enabled deep fake technologies has significantly increased the complexity of identity fraud . there is a pressing need to review and understand identity fraud detection methods, their limitations and potential solutions ."
On some studies of Fraud Detection Pipeline and related issues from the   scope of Ensemble Learning and Graph-based Learning,"The UK anti-fraud charity Fraud Advisory Panel (FAP) in their review of 2016 estimates business costs of fraud at 144 billion, and its individual counterpart at 9.7 billion. Banking, insurance, manufacturing, and government are the most common industries affected by fraud activities. Designing an efficient fraud detection system could avoid losing the money; however, building this system is challenging due to many difficult problems, e.g.imbalanced data, computing costs, etc. Over the last three decades, there are various research relates to fraud detection but no agreement on what is the best approach to build the fraud detection system. In this thesis, we aim to answer some questions such as i) how to build a simplified and effective Fraud Detection System that not only easy to implement but also providing reliable results and our proposed Fraud Detection Pipeline is a potential backbone of the system and is easy to be extended or upgraded, ii) when to update models in our system (and keep the accuracy stable) in order to reduce the cost of updating process, iii) how to deal with an extreme imbalance in big data classification problem, e.g. fraud detection, since this is the gap between two difficult problems, iv) further, how to apply graph-based semi-supervised learning to detect fraudulent transactions.",['Tuan Tran'],2022-05-10T02:13:58Z,http://arxiv.org/abs/2205.04626v1,Finance & Economics,Fraud Detection,"the fraud Advisory Panel (FAP) estimates business costs of fraud at 144 billion . banking, insurance, manufacturing, and government are the most common industries affected by fraud activities . building an efficient fraud detection system could avoid losing the money ."
Challenges and Complexities in Machine Learning based Credit Card Fraud   Detection,"Credit cards play an exploding role in modern economies. Its popularity and ubiquity have created a fertile ground for fraud, assisted by the cross boarder reach and instantaneous confirmation. While transactions are growing, the fraud percentages are also on the rise as well as the true cost of a dollar fraud. Volume of transactions, uniqueness of frauds and ingenuity of the fraudster are main challenges in detecting frauds. The advent of machine learning, artificial intelligence and big data has opened up new tools in the fight against frauds. Given past transactions, a machine learning algorithm has the ability to 'learn' infinitely complex characteristics in order to identify frauds in real-time, surpassing the best human investigators. However, the developments in fraud detection algorithms has been challenging and slow due the massively unbalanced nature of fraud data, absence of benchmarks and standard evaluation metrics to identify better performing classifiers, lack of sharing and disclosure of research findings and the difficulties in getting access to confidential transaction data for research. This work investigates the properties of typical massively imbalanced fraud data sets, their availability, suitability for research use while exploring the widely varying nature of fraud distributions. Furthermore, we show how human annotation errors compound with machine classification errors. We also carry out experiments to determine the effect of PCA obfuscation (as a means of disseminating sensitive transaction data for research and machine learning) on algorithmic performance of classifiers and show that while PCA does not significantly degrade performance, care should be taken to use the appropriate principle component size (dimensions) to avoid overfitting.",['Gayan K. Kulatilleke'],2022-08-20T07:53:51Z,http://arxiv.org/abs/2208.10943v1,Finance & Economics,Fraud Detection,credit cards play an exploding role in modern economies . its popularity and ubiquity have created a fertile ground for fraud . the fraud percentages are also on the rise as well as the true cost of a dollar fraud. the advent of machine learning and big data has opened up new tools in the fight against frauds.
A novel approach to increase scalability while training machine learning   algorithms using Bfloat 16 in credit card fraud detection,"The use of credit cards has become quite common these days as digital banking has become the norm. With this increase, fraud in credit cards also has a huge problem and loss to the banks and customers alike. Normal fraud detection systems, are not able to detect the fraud since fraudsters emerge with new techniques to commit fraud. This creates the need to use machine learning-based software to detect frauds. Currently, the machine learning softwares that are available focuses only on the accuracy of detecting frauds but does not focus on the cost or time factors to detect. This research focuses on machine learning scalability for banks' credit card fraud detection systems. We have compared the existing machine learning algorithms and methods that are available with the newly proposed technique. The goal is to prove that using fewer bits for training a machine learning algorithm will result in a more scalable system, that will reduce the time and will also be less costly to implement.","['Bushra Yousuf', 'Rejwan Bin Sulaiman', 'Musarrat Saberin Nipun']",2022-06-24T01:22:17Z,http://arxiv.org/abs/2206.12415v1,Finance & Economics,Fraud Detection,the use of credit cards has become quite common these days as digital banking has become the norm . fraudsters emerge with new techniques to commit fraud . this creates the need to use machine learning-based software to detect frauds .
Customs Fraud Detection in the Presence of Concept Drift,"Capturing the changing trade pattern is critical in customs fraud detection. As new goods are imported and novel frauds arise, a drift-aware fraud detection system is needed to detect both known frauds and unknown frauds within a limited budget. The current paper proposes ADAPT, an adaptive selection method that controls the balance between exploitation and exploration strategies used for customs fraud detection. ADAPT makes use of the model performance trends and the amount of concept drift to determine the best exploration ratio at every time. Experiments on data from four countries over several years show that each country requires a different amount of exploration for maintaining its fraud detection system. We find the system with ADAPT can gradually adapt to the dataset and find the appropriate amount of exploration ratio with high performance.","['Tung-Duong Mai', 'Kien Hoang', 'Aitolkyn Baigutanova', 'Gaukhartas Alina', 'Sundong Kim']",2021-09-29T02:52:19Z,http://arxiv.org/abs/2109.14155v1,Finance & Economics,Fraud Detection,customs fraud detection needs a drift-aware system to detect new goods . ADAPT is an adaptive selection method that controls the balance between exploitation and exploration . each country requires a different amount of exploration for maintaining its fraud detection system .
FRAUDability: Estimating Users' Susceptibility to Financial Fraud Using   Adversarial Machine Learning,"In recent years, financial fraud detection systems have become very efficient at detecting fraud, which is a major threat faced by e-commerce platforms. Such systems often include machine learning-based algorithms aimed at detecting and reporting fraudulent activity. In this paper, we examine the application of adversarial learning based ranking techniques in the fraud detection domain and propose FRAUDability, a method for the estimation of a financial fraud detection system's performance for every user. We are motivated by the assumption that ""not all users are created equal"" -- while some users are well protected by fraud detection algorithms, others tend to pose a challenge to such systems. The proposed method produces scores, namely ""fraudability scores,"" which are numerical estimations of a fraud detection system's ability to detect financial fraud for a specific user, given his/her unique activity in the financial system. Our fraudability scores enable those tasked with defending users in a financial platform to focus their attention and resources on users with high fraudability scores to better protect them. We validate our method using a real e-commerce platform's dataset and demonstrate the application of fraudability scores from the attacker's perspective, on the platform, and more specifically, on the fraud detection systems used by the e-commerce enterprise. We show that the scores can also help attackers increase their financial profit by 54%, by engaging solely with users with high fraudability scores, avoiding those users whose spending habits enable more accurate fraud detection.","['Chen Doytshman', 'Satoru Momiyama', 'Inderjeet Singh', 'Yuval Elovici', 'Asaf Shabtai']",2023-12-02T18:33:05Z,http://arxiv.org/abs/2312.01200v1,Finance & Economics,Fraud Detection,financial fraud detection systems have become very efficient at detecting fraud . we propose a method for the estimation of a fraud detection system's performance for every user . fraudability scores can help attackers increase their financial profit by 54% .
Applying support vector data description for fraud detection,"Fraud detection is an important topic that applies to various enterprises such as banking and financial sectors, insurance, government agencies, law enforcement, and more. Fraud attempts have been risen remarkably in current years, shaping fraud detection an essential topic for research. One of the main challenges in fraud detection is acquiring fraud samples which is a complex and challenging task. In order to deal with this challenge, we apply one-class classification methods such as SVDD which does not need the fraud samples for training. Also, we present our algorithm REDBSCAN which is an extension of DBSCAN to reduce the number of samples and select those that keep the shape of data. The results obtained by the implementation of the proposed method indicated that the fraud detection process was improved in both performance and speed.","['Mohamad Khedmati', 'Masoud Erfani', 'Mohammad GhasemiGol']",2020-05-31T21:31:32Z,http://arxiv.org/abs/2006.00618v1,Finance & Economics,Fraud Detection,fraud detection is an important topic that applies to various enterprises . one-class classification methods such as SVDD do not need the fraud samples for training . the results obtained by the implementation of the proposed method indicated improvements .
Open ERP System Data For Occupational Fraud Detection,"Recent estimates report that companies lose 5% of their revenue to occupational fraud. Since most medium-sized and large companies employ Enterprise Resource Planning (ERP) systems to track vast amounts of information regarding their business process, researchers have in the past shown interest in automatically detecting fraud through ERP system data. Current research in this area, however, is hindered by the fact that ERP system data is not publicly available for the development and comparison of fraud detection methods. We therefore endeavour to generate public ERP system data that includes both normal business operation and fraud. We propose a strategy for generating ERP system data through a serious game, model a variety of fraud scenarios in cooperation with auditing experts, and generate data from a simulated make-to-stock production company with multiple research participants. We aggregate the generated data into ready to used datasets for fraud detection in ERP systems, and supply both the raw and aggregated data to the general public to allow for open development and comparison of fraud detection approaches on ERP system data.","['Julian Tritscher', 'Fabian Gwinner', 'Daniel Schlör', 'Anna Krause', 'Andreas Hotho']",2022-06-09T12:38:29Z,http://arxiv.org/abs/2206.04460v3,Finance & Economics,Fraud Detection,recent estimates report that companies lose 5% of their revenue to occupational fraud . researchers have shown interest in automatically detecting fraud through ERP system data . current research in this area is hindered by the fact that the data is not publicly available .
Social network analytics for supervised fraud detection in insurance,"Insurance fraud occurs when policyholders file claims that are exaggerated or based on intentional damages. This contribution develops a fraud detection strategy by extracting insightful information from the social network of a claim. First, we construct a network by linking claims with all their involved parties, including the policyholders, brokers, experts, and garages. Next, we establish fraud as a social phenomenon in the network and use the BiRank algorithm with a fraud specific query vector to compute a fraud score for each claim. From the network, we extract features related to the fraud scores as well as the claims' neighborhood structure. Finally, we combine these network features with the claim-specific features and build a supervised model with fraud in motor insurance as the target variable. Although we build a model for only motor insurance, the network includes claims from all available lines of business. Our results show that models with features derived from the network perform well when detecting fraud and even outperform the models using only the classical claim-specific features. Combining network and claim-specific features further improves the performance of supervised learning models to detect fraud. The resulting model flags highly suspicions claims that need to be further investigated. Our approach provides a guided and intelligent selection of claims and contributes to a more effective fraud investigation process.","['María Óskarsdóttir', 'Waqas Ahmed', 'Katrien Antonio', 'Bart Baesens', 'Rémi Dendievel', 'Tom Donas', 'Tom Reynkens']",2020-09-15T21:40:15Z,http://arxiv.org/abs/2009.08313v1,Finance & Economics,Fraud Detection,insurance fraud occurs when policyholders file claims that are exaggerated or based on intentional damages . this contribution develops a fraud detection strategy by extracting insightful information from the social network of a claim . the model flags highly suspicions claims that need to be further investigated .
"""Auntie, Please Don't Fall for Those Smooth Talkers"": How Chinese   Younger Family Members Safeguard Seniors from Online Fraud","Online fraud substantially harms individuals and seniors are disproportionately targeted. While family is crucial for seniors, little research has empirically examined how they protect seniors against fraud. To address this gap, we employed an inductive thematic analysis of 124 posts and 16,872 comments on RedNote (Xiaohongshu), exploring the family support ecosystem for senior-targeted online fraud in China. We develop a taxonomy of senior-targeted online fraud from a familial perspective, revealing younger members often spot frauds hard for seniors to detect, such as unusual charges. Younger family members fulfill multiple safeguarding roles, including preventative measures, fraud identification, fraud persuasion, loss recovery, and education. They also encounter numerous challenges, such as seniors' refusal of help and considerable mental and financial stress. Drawing on these, we develop a conceptual framework to characterize family support in senior-targeted fraud, and outline implications for researchers and practitioners to consider the broader stakeholder ecosystem and cultural aspects.","['Yue Deng', 'Changyang He', 'Yixin Zou', 'Bo Li']",2025-01-18T15:55:31Z,http://arxiv.org/abs/2501.10803v1,Finance & Economics,Fraud Detection,"online fraud substantially harms individuals and seniors are disproportionately targeted . little research has empirically examined how they protect seniors against fraud . younger family members fulfill multiple safeguarding roles, authors say ."
An overall view of key problems in algorithmic trading and recent   progress,"We summarize the fundamental issues at stake in algorithmic trading, and the progress made in this field over the last twenty years. We first present the key problems of algorithmic trading, describing the concepts of optimal execution, optimal placement, and price impact. We then discuss the most recent advances in algorithmic trading through the use of Machine Learning, discussing the use of Deep Learning, Reinforcement Learning, and Generative Adversarial Networks.",['Michaël Karpe'],2020-06-09T21:22:32Z,http://arxiv.org/abs/2006.05515v1,Finance & Economics,Algorithmic Trading,"we first present the key problems of algorithmic trading, describing the concepts of optimal execution, optimal placement, and price impact . we then discuss the most recent advances in algorithmic trade through the use of machine learning ."
ESG driven pairs algorithm for sustainable trading: Analysis from the   Indian market,"This paper proposes an algorithmic trading framework integrating Environmental, Social, and Governance (ESG) ratings with a pairs trading strategy. It addresses the demand for socially responsible investment solutions by developing a unique algorithm blending ESG data with methods for identifying co-integrated stocks. This allows selecting profitable pairs adhering to ESG principles. Further, it incorporates technical indicators for optimal trade execution within this sustainability framework. Extensive back-testing provides evidence of the model's effectiveness, consistently generating positive returns exceeding conventional pairs trading strategies, while upholding ESG principles. This paves the way for a transformative approach to algorithmic trading, offering insights for investors, policymakers, and academics.","['Eeshaan Dutta', 'Sarthak Diwan', 'Siddhartha P. Chakrabarty']",2024-01-26T10:37:23Z,http://arxiv.org/abs/2401.14761v1,Finance & Economics,Algorithmic Trading,this paper proposes an algorithmic trading framework integrating ESG ratings with a pairs trading strategy . it allows selecting profitable pairs adhering to ESG principles .
Adversarial Attacks on Deep Algorithmic Trading Policies,"Deep Reinforcement Learning (DRL) has become an appealing solution to algorithmic trading such as high frequency trading of stocks and cyptocurrencies. However, DRL have been shown to be susceptible to adversarial attacks. It follows that algorithmic trading DRL agents may also be compromised by such adversarial techniques, leading to policy manipulation. In this paper, we develop a threat model for deep trading policies, and propose two attack techniques for manipulating the performance of such policies at test-time. Furthermore, we demonstrate the effectiveness of the proposed attacks against benchmark and real-world DQN trading agents.","['Yaser Faghan', 'Nancirose Piazza', 'Vahid Behzadan', 'Ali Fathi']",2020-10-22T02:26:29Z,http://arxiv.org/abs/2010.11388v1,Finance & Economics,Algorithmic Trading,"deep learning (DRL) has become an appealing solution to algorithmic trading . however, it has been shown to be susceptible to adversarial attacks . we propose two attack techniques for manipulating the performance of deep trading policies ."
TradAO: A Visual Analytics System for Trading Algorithm Optimization,"With the wide applications of algorithmic trading, it has become critical for traders to build a winning trading algorithm to beat the market. However, due to the lack of efficient tools, traders mainly rely on their memory to manually compare the algorithm instances of a trading algorithm and further select the best trading algorithm instance for the real trading deployment. We work closely with industry practitioners to discover and consolidate user requirements and develop an interactive visual analytics system for trading algorithm optimization. Structured expert interviews are conducted to evaluateTradAOand a representative case study is documented for illustrating the system effectiveness. To the best of our knowledge, previous financial data visual analyses have mainly aimed to assist investment managers in investment portfolio analysis but have neglected the need of traders in developing trading algorithms for portfolio execution.TradAOis the first visual analytics system that assists users in comprehensively exploring the performances of a trading algorithm with different parameter settings.","['Ka Wing Tsang', 'Haotian Li', 'Fuk Ming Lam', 'Yifan Mu', 'Yong Wang', 'Huamin Qu']",2020-08-26T00:49:22Z,http://arxiv.org/abs/2008.11319v1,Finance & Economics,Algorithmic Trading,traders rely on memory to manually compare the instances of a trading algorithm . a representative case study is documented for illustrating the system effectiveness .
Effects of Regional Trade Agreement to Local and Global Trade Purity   Relationships,"In contrast to the rapid integration of the world economy, many regional trade agreements (RTAs) have also emerged since the early 1990s. This seeming contradiction has encouraged scholars and policy makers to explore the true effects of RTAs, including both regional and global trade relationships. This paper defines synthesized trade resistance and decomposes it into natural and artificial factors. Here, we separate the influence of geographical distance, economic volume, overall increases in transportation and labor costs and use the expectation maximization algorithm to optimize the parameters and quantify the trade purity indicator, which describes the true global trade environment and relationships among countries. This indicates that although global and most regional trade relations gradually deteriorated during the period 2007-2017, RTAs generate trade relations among members, especially contributing to the relative prosperity of EU and NAFTA countries. In addition, we apply the network to reflect the purity of the trade relations among countries. The effects of RTAs can be analyzed by comparing typical trade unions and trade communities, which are presented using an empirical network structure. This analysis shows that the community structure is quite consistent with some trade unions, and the representative RTAs constitute the core structure of international trade network. However, the role of trade unions has weakened, and multilateral trade liberalization has accelerated in the past decade. This means that more countries have recently tended to expand their trading partners outside of these unions rather than limit their trading activities to RTAs.","['Siyu Huang', 'Wensha Gou', 'Hongbo Cai', 'Xiaomeng Li', 'Qinghua Chen']",2020-05-29T07:53:39Z,http://arxiv.org/abs/2006.07329v1,Finance & Economics,Algorithmic Trading,this paper defines synthesized trade resistance and decomposes it into natural and artificial factors . trade purity indicator describes true global trade environment and relationships among countries . effects of RTAs can be analyzed by comparing typical trade unions and trade communities .
Quantitative Trading using Deep Q Learning,"Reinforcement learning (RL) is a subfield of machine learning that has been used in many fields, such as robotics, gaming, and autonomous systems. There has been growing interest in using RL for quantitative trading, where the goal is to make trades that generate profits in financial markets. This paper presents the use of RL for quantitative trading and reports a case study based on an RL-based trading algorithm. The results show that RL can be a useful tool for quantitative trading and can perform better than traditional trading algorithms. The use of reinforcement learning for quantitative trading is a promising area of research that can help develop more sophisticated and efficient trading systems. Future research can explore the use of other reinforcement learning techniques, the use of other data sources, and the testing of the system on a range of asset classes. Together, our work shows the potential in the use of reinforcement learning for quantitative trading and the need for further research and development in this area. By developing the sophistication and efficiency of trading systems, it may be possible to make financial markets more efficient and generate higher returns for investors.",['Soumyadip Sarkar'],2023-04-03T11:57:36Z,http://arxiv.org/abs/2304.06037v2,Finance & Economics,Algorithmic Trading,there has been growing interest in using RL for quantitative trading . the goal is to make trades that generate profits in financial markets . future research can explore the use of other reinforcement learning techniques .
An Application of Deep Reinforcement Learning to Algorithmic Trading,"This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in stock markets. It proposes a novel DRL trading strategy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new trading strategy is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN strategy.","['Thibaut Théate', 'Damien Ernst']",2020-04-07T14:57:23Z,http://arxiv.org/abs/2004.06627v3,Finance & Economics,Algorithmic Trading,it proposes a novel trading strategy to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets . the trading strategy is inspired from the popular DQN algorithm .
Calculating Profits and Losses for Algorithmic Trading Strategies: A   Short Guide,"We present a series of equations that track the total realized and unrealized profits and losses at any time, incorporating the spread. The resulting formalism is ideally suited to evaluate the performance of trading model algorithms.","['James B. Glattfelder', 'Thomas Houweling']",2024-11-21T12:31:14Z,http://arxiv.org/abs/2411.14068v1,Finance & Economics,Algorithmic Trading,we present a series of equations that track the total realized and unrealized profits and losses . the resulting formalism is ideally suited to evaluate the performance of trading model algorithms .
Using Macroeconomic Forecasts to Improve Mean Reverting Trading   Strategies,"A large class of trading strategies focus on opportunities offered by the yield curve. In particular, a set of yield curve trading strategies are based on the view that the yield curve mean-reverts. Based on these strategies' positive performance, a multiple pairs trading strategy on major currency pairs was implemented. To improve the algorithm's performance, machine learning forecasts of a series of pertinent macroeconomic variables were factored in, by optimizing the weights of the trading signals. This resulted in a clear improvement in the APR over the evaluation period, demonstrating that macroeconomic indicators, not only technical indicators, should be considered in trading strategies.",['Yash Sharma'],2017-05-22T22:10:56Z,http://arxiv.org/abs/1705.08022v1,Finance & Economics,Algorithmic Trading,a class of trading strategies focus on opportunities offered by the yield curve . a multiple pairs trading strategy on major currency pairs was implemented . machine learning forecasts of pertinent macroeconomic variables were factored in .
Generating Trading Signals by ML algorithms or time series ones?,"This research investigates efficiency on-line learning Algorithms to generate trading signals.I employed technical indicators based on high frequency stock prices and generated trading signals through ensemble of Random Forests. Similarly, Kalman Filter was used for signaling trading positions. Comparing Time Series methods with Machine Learning methods, results spurious of Kalman Filter to Random Forests in case of on-line learning predictions of stock prices",['Omid Safarzadeh'],2020-07-14T12:41:22Z,http://arxiv.org/abs/2007.11098v1,Finance & Economics,Algorithmic Trading,this research investigates efficiency on-line learning Algorithms to generate trading signals . I employed technical indicators based on high frequency stock prices .
Newly Developed Flexible Grid Trading Model Combined ANN and SSO   algorithm,"In modern society, the trading methods and strategies used in financial market have gradually changed from traditional on-site trading to electronic remote trading, and even online automatic trading performed by a pre-programmed computer programs because the continuous development of network and computer computing technology. The quantitative trading, which the main purpose is to automatically formulate people's investment decisions into a fixed and quantifiable operation logic that eliminates all emotional interference and the influence of subjective thoughts and applies this logic to financial market activities in order to obtain excess profits above average returns, has led a lot of attentions in financial market. The development of self-adjustment programming algorithms for automatically trading in financial market has transformed a top priority for academic research and financial practice. Thus, a new flexible grid trading model combined with the Simplified Swarm Optimization (SSO) algorithm for optimizing parameters for various market situations as input values and the fully connected neural network (FNN) and Long Short-Term Memory (LSTM) model for training a quantitative trading model to automatically calculate and adjust the optimal trading parameters for trading after inputting the existing market situation is developed and studied in this work. The proposed model provides a self-adjust model to reduce investors' effort in the trading market, obtains outperformed investment return rate and model robustness, and can properly control the balance between risk and return.","['Wei-Chang Yeh', 'Yu-Hsin Hsieh', 'Chia-Ling Huang']",2022-09-05T12:05:05Z,http://arxiv.org/abs/2211.12839v1,Finance & Economics,Algorithmic Trading,the quantitative trading has led a lot of attentions in financial market . the development of self-adjustment programming algorithms has transformed a top priority . a new flexible grid trading model is developed and studied .
Financial Trading with Feature Preprocessing and Recurrent Reinforcement   Learning,"Financial trading aims to build profitable strategies to make wise investment decisions in the financial market. It has attracted interests in the machine learning community for a long time. This paper proposes to trade financial assets automatically using feature preprocessing skills and Recurrent Reinforcement Learning (RRL) algorithm. The strategy starts from technical indicators extracted from assets' market information. Then these technical indicators are preprocessed by Principal Component Analysis (PCA) and Discrete Wavelet Transform (DWT) and eventually inputted to the RRL algorithm to do the trading. The extensive empirical evidence shows that the proposed strategy is not only effective and robust in its performance, but also can mitigate the drawbacks underlying the initial trading using RRL.",['Lin Li'],2021-09-11T13:57:31Z,http://arxiv.org/abs/2109.05283v1,Finance & Economics,Algorithmic Trading,financial trading aims to build profitable strategies to make wise investments . this paper proposes to trade financial assets automatically using feature preprocessing skills and Recurrent Reinforcement Learning (RRL) algorithm .
Reinforcement Learning Pair Trading: A Dynamic Scaling approach,"Cryptocurrency is a cryptography-based digital asset with extremely volatile prices. Around USD 70 billion worth of cryptocurrency is traded daily on exchanges. Trading cryptocurrency is difficult due to the inherent volatility of the crypto market. This study investigates whether Reinforcement Learning (RL) can enhance decision-making in cryptocurrency algorithmic trading compared to traditional methods. In order to address this question, we combined reinforcement learning with a statistical arbitrage trading technique, pair trading, which exploits the price difference between statistically correlated assets. We constructed RL environments and trained RL agents to determine when and how to trade pairs of cryptocurrencies. We developed new reward shaping and observation/action spaces for reinforcement learning. We performed experiments with the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data separated by 1 min intervals (n=263,520). The traditional non-RL pair trading technique achieved an annualized profit of 8.33%, while the proposed RL-based pair trading technique achieved annualized profits from 9.94% to 31.53%, depending upon the RL learner. Our results show that RL can significantly outperform manual and traditional pair trading techniques when applied to volatile markets such as~cryptocurrencies.","['Hongshen Yang', 'Avinash Malik']",2024-07-23T00:16:27Z,http://arxiv.org/abs/2407.16103v2,Finance & Economics,Algorithmic Trading,cryptocurrency is a cryptography-based digital asset with extremely volatile prices . around USD 70 billion worth of cryptocurrency is traded daily on exchanges . RL can significantly outperform manual and traditional pair trading techniques .
Efficient and fair trading algorithms in market design environments,"We propose a new method to define trading algorithms in market design environments. Dropping the traditional idea of clearing cycles in generated graphs, we use parameterized linear equations to define trading algorithms. Our method has two advantages. First, our method avoids discussing the details of who trades with whom and how, which can be a difficult question in complex environments. Second, by controlling parameter values in our equations, our method is flexible and transparent to satisfy various fairness criteria. We apply our method to several models and obtain new trading algorithms that are efficient and fair.","['Jingsheng Yu', 'Jun Zhang']",2020-05-14T11:36:11Z,http://arxiv.org/abs/2005.06878v3,Finance & Economics,Algorithmic Trading,"we use parameterized linear equations to define trading algorithms . our method avoids discussing details of who trades with whom and how . by controlling parameter values, our method is flexible and transparent ."
An Automated Portfolio Trading System with Feature Preprocessing and   Recurrent Reinforcement Learning,"We propose a novel portfolio trading system, which contains a feature preprocessing module and a trading module. The feature preprocessing module consists of various data processing operations, while in the trading part, we integrate the portfolio weight rebalance function with the trading algorithm and make the trading system fully automated and suitable for individual investors, holding a handful of stocks. The data preprocessing procedures are applied to remove the white noise in the raw data set and uncover the general pattern underlying the data set before the processed feature set is inputted into the trading algorithm. Our empirical results reveal that the proposed portfolio trading system can efficiently earn high profit and maintain a relatively low drawdown, which clearly outperforms other portfolio trading strategies.",['Lin Li'],2021-10-11T14:15:07Z,http://arxiv.org/abs/2110.05299v2,Finance & Economics,Algorithmic Trading,the proposed portfolio trading system contains a feature preprocessing module and a trading module . our empirical results reveal that the proposed system can efficiently earn high profit and maintain a relatively low drawdown .
Analysis of frequent trading effects of various machine learning models,"In recent years, high-frequency trading has emerged as a crucial strategy in stock trading. This study aims to develop an advanced high-frequency trading algorithm and compare the performance of three different mathematical models: the combination of the cross-entropy loss function and the quasi-Newton algorithm, the FCNN model, and the vector machine. The proposed algorithm employs neural network predictions to generate trading signals and execute buy and sell operations based on specific conditions. By harnessing the power of neural networks, the algorithm enhances the accuracy and reliability of the trading strategy. To assess the effectiveness of the algorithm, the study evaluates the performance of the three mathematical models. The combination of the cross-entropy loss function and the quasi-Newton algorithm is a widely utilized logistic regression approach. The FCNN model, on the other hand, is a deep learning algorithm that can extract and classify features from stock data. Meanwhile, the vector machine is a supervised learning algorithm recognized for achieving improved classification results by mapping data into high-dimensional spaces. By comparing the performance of these three models, the study aims to determine the most effective approach for high-frequency trading. This research makes a valuable contribution by introducing a novel methodology for high-frequency trading, thereby providing investors with a more accurate and reliable stock trading strategy.","['Jiahao Chen', 'Xiaofei Li']",2023-09-14T05:17:09Z,http://arxiv.org/abs/2311.10719v1,Finance & Economics,Algorithmic Trading,study aims to develop an advanced high-frequency trading algorithm . it compares the performance of three different mathematical models . the proposed algorithm employs neural network predictions to generate trading signals .
Algorithmic Trading Using Continuous Action Space Deep Reinforcement   Learning,"Price movement prediction has always been one of the traders' concerns in financial market trading. In order to increase their profit, they can analyze the historical data and predict the price movement. The large size of the data and complex relations between them lead us to use algorithmic trading and artificial intelligence. This paper aims to offer an approach using Twin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading strategy in the stock and cryptocurrency markets. Unlike previous studies using a discrete action space reinforcement learning algorithm, the TD3 is continuous, offering both position and the number of trading shares. Both the stock (Amazon) and cryptocurrency (Bitcoin) markets are addressed in this research to evaluate the performance of the proposed algorithm. The achieved strategy using the TD3 is compared with some algorithms using technical analysis, reinforcement learning, stochastic, and deterministic strategies through two standard metrics, Return and Sharpe ratio. The results indicate that employing both position and the number of trading shares can improve the performance of a trading system based on the mentioned metrics.","['Naseh Majidi', 'Mahdi Shamsi', 'Farokh Marvasti']",2022-10-07T11:42:31Z,http://arxiv.org/abs/2210.03469v1,Finance & Economics,Algorithmic Trading,"this paper aims to offer an approach using Twin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading strategy in the stock and cryptocurrency markets . the TD3 is continuous, offering both position and the number of trading shares ."
Impact of Tariff Wars on Global Economy,"The Ricardian model of world trade based on comparative advantage is not sufficient to justify equal trade relations.The existing model of trade relations does not explain the distribution of income among trading countries. This paper presents a method for building equitable trade relations. Its essence is to present an algorithm for building such trade relations, based on the previously proposed model of world trade, that the trade balance of each country would be equal to zero. Under such conditions, tariff wars would become impossible. It is proved that, provided that the supply structure is consistent with the demand structure, it is always possible to build an equilibrium price vector for which the trade balance of each country is zero. This state of economic equilibrium is called ideal. The article presents an algorithm to build an export structure based on the structure of imports. This algorithm is quite simple and allows for a wide range of applications. Under fairly simple realistic assumptions about the behaviour of countries trading with each other that are subject to tariff restrictions, it is proved that this leads to an increase in the prices of the goods traded by these countries. Among the equilibrium states, there are also those called oversupply states. The latter describes the phenomenon of recession. This contributes to a fall in stock market indices.","['N. S. Gonchar', 'O. P. Dovzhyk', 'A. S. Zhokhin', 'W. H. Kozyrsky', 'A. P. Makhort']",2025-05-08T18:13:31Z,http://arxiv.org/abs/2505.05576v1,Finance & Economics,Algorithmic Trading,this paper presents a method for building equitable trade relations . the article presents an algorithm to build an export structure based on the structure of imports . this algorithm is quite simple and allows for a wide range of applications .
Universal trading under proportional transaction costs,"The theory of optimal trading under proportional transaction costs has been considered from a variety of perspectives. In this paper, we show that all the results can be interpreted using a universal law, illustrating the results in trading algorithm design.",['Richard J Martin'],2016-03-21T19:46:19Z,http://arxiv.org/abs/1603.06558v1,Finance & Economics,Algorithmic Trading,the theory of optimal trading under proportional transaction costs has been considered . all the results can be interpreted using a universal law .
Multicurrency advisor based on the NSW model. Detailed description and   perspectives,Flexible algorithm of multicurrency trade on Forex market has been built on the grounds of non-linear stochastic wavelets (NSW) model. Probability of the loss-free trade has been evaluated. Results of the algorithm's real-time testing and issues of the algorithm's development are discussed.,['A. M. Avdeenko'],2011-11-24T11:23:43Z,http://arxiv.org/abs/1111.5726v1,Finance & Economics,Algorithmic Trading,flexible algorithm of multicurrency trade on Forex market has been built on the grounds of non-linear stochastic wavelets (NSW) model . Probability of the loss-free trade has been evaluated .
"Counterparty Risk FAQ: Credit VaR, PFE, CVA, DVA, Closeout, Netting,   Collateral, Re-hypothecation, WWR, Basel, Funding, CCDS and Margin Lending","We present a dialogue on Counterparty Credit Risk touching on Credit Value at Risk (Credit VaR), Potential Future Exposure (PFE), Expected Exposure (EE), Expected Positive Exposure (EPE), Credit Valuation Adjustment (CVA), Debit Valuation Adjustment (DVA), DVA Hedging, Closeout conventions, Netting clauses, Collateral modeling, Gap Risk, Re-hypothecation, Wrong Way Risk, Basel III, inclusion of Funding costs, First to Default risk, Contingent Credit Default Swaps (CCDS) and CVA restructuring possibilities through margin lending. The dialogue is in the form of a Q&A between a CVA expert and a newly hired colleague.",['Damiano Brigo'],2011-11-05T17:01:24Z,http://arxiv.org/abs/1111.1331v3,Finance & Economics,Credit Risk Modeling,the dialogue is in the form of a Q&A between a CVA expert and a newly hired colleague . the dialogue touches on Credit Valuation at Risk (Credit VaR)
Bivariate Semi-Markov Process for Counterparty Credit Risk,"We consider the problem of constructing an appropriate multivariate model for the study of the counterparty credit risk in credit rating migration problem. For this financial problem different multivariate Markov chain models were proposed. However the markovian assumption may be inappropriate for the study of the dynamic of credit ratings which typically show non markovian like behaviour. In this paper we develop a semi-Markov approach to the study of the counterparty credit risk by defining a new multivariate semi-Markov chain model. Methods are given for computing the transition probabilities, reliability functions and the price of a risky Credit Default Swap.","[""Guglielmo D'Amico"", 'Raimondo Manca', 'Giovanni Salvi']",2011-12-01T16:11:57Z,http://arxiv.org/abs/1112.0226v2,Finance & Economics,Credit Risk Modeling,the markovian assumption may be inappropriate for the study of the dynamic of credit ratings . in this paper we define a new multivariate semi-Markov chain model .
Pricing Financial Derivatives Subject to Counterparty Risk and Credit   Value Adjustment,"This article presents a generic model for pricing financial derivatives subject to counterparty credit risk. Both unilateral and bilateral types of credit risks are considered. Our study shows that credit risk should be modeled as American style options in most cases, which require a backward induction valuation. To correct a common mistake in the literature, we emphasize that the market value of a defaultable derivative is actually a risky value rather than a risk-free value. Credit value adjustment (CVA) is also elaborated. A practical framework is developed for pricing defaultable derivatives and calculating their CVAs at a portfolio level.",['David Lee'],2018-04-05T15:05:56Z,http://arxiv.org/abs/1804.02289v1,Finance & Economics,Credit Risk Modeling,study shows credit risk should be modeled as american style options . defaultable derivatives are risky rather than risk-free . credit value adjustment (CVA) also elaborated .
Credit Risk Assessment Model for UAE Commercial Banks: A Machine   Learning Approach,"Credit ratings are becoming one of the primary references for financial institutions of the country to assess credit risk in order to accurately predict the likelihood of business failure of an individual or an enterprise. Financial institutions, therefore, depend on credit rating tools and services to help them predict the ability of creditors to meet financial persuasions. Conventional credit rating is broadly categorized into two classes namely: good credit and bad credit. This approach lacks adequate precision to perform credit risk analysis in practice. Related studies have shown that data-driven machine learning algorithms outperform many conventional statistical approaches in solving this type of problem, both in terms of accuracy and efficiency. The purpose of this paper is to construct and validate a credit risk assessment model using Linear Discriminant Analysis as a dimensionality reduction technique to discriminate good creditors from bad ones and identify the best classifier for credit assessment of commercial banks based on real-world data. This will help commercial banks to avoid monetary losses and prevent financial crisis","['Aditya Saxena', 'Dr Parizad Dungore']",2024-07-02T08:06:42Z,http://arxiv.org/abs/2407.12044v1,Finance & Economics,Credit Risk Modeling,credit ratings are becoming a primary reference for financial institutions to assess credit risk . the purpose of this paper is to construct and validate a credit risk assessment model . it will help commercial banks avoid monetary losses and prevent financial crisis .
Risk-Minimizing Hedging of Counterparty Risk,"We study dynamic hedging of counterparty risk for a portfolio of credit derivatives. Our empirically driven credit model consists of interacting default intensities which ramp up and then decay after the occurrence of credit events. Using the Galtchouk-Kunita-Watanabe decomposition of the counterparty risk price payment stream, we recover a closed-form representation for the risk minimizing strategy in terms of classical solutions to nonlinear recursive systems of Cauchy problems. We discuss applications of our framework to the most prominent class of credit derivatives, including credit swap and risky bond portfolios, as well as first-to-default claims.","['Lijun Bo', 'Agostino Capponi', 'Claudia Ceci']",2017-09-04T18:53:01Z,http://arxiv.org/abs/1709.01115v1,Finance & Economics,Credit Risk Modeling,credit model consists of interacting default intensities which ramp up and decay . we recover a closed-form representation for the risk minimizing strategy . applications of our framework to credit swap and risky bond portfolios .
Applicability of Large Corporate Credit Models to Small Business Risk   Assessment,"There is a massive underserved market for small business lending in the US with the Federal Reserve estimating over \$650B in unmet annual financing needs. Assessing the credit risk of a small business is key to making good decisions whether to lend and at what terms. Large corporations have a well-established credit assessment ecosystem, but small businesses suffer from limited publicly available data and few (if any) credit analysts who cover them closely. We explore the applicability of (DL-based) large corporate credit risk models to small business credit rating.",['Khalid El-Awady'],2021-12-14T23:06:17Z,http://arxiv.org/abs/2201.08276v1,Finance & Economics,Credit Risk Modeling,the federal reserve estimates over $650B in unmet annual financing needs . small businesses suffer from limited publicly available data and few (if any) credit analysts .
"Credit Cycles, Securitization, and Credit Default Swaps","We present a limits-to-arbitrage model to study the impact of securitization, leverage and credit risk protection on the cyclicity of bank credit. In a stable bank credit situation, no cycles of credit expansion or contraction appear. Unlevered securitization together with mis-pricing of securitized assets increases lending cyclicality, favoring credit booms and busts. Leverage changes the state of affairs with respect to the simple securitization. First, the volume of real activity and banking profits increases. Second, banks sell securities when markets decline. This selling puts further pressure on falling prices. The mis-pricing of credit risk protection or securitized assets influences the real economy. Trading in these contracts reduces the amount of funding available to entrepreneurs, particularly to high-credit-risk borrowers. This trading decreases the liquidity of the securitized assets, and especially those based on investments with high credit risk.",['Juan Ignacio Peña'],2019-01-01T16:46:59Z,http://arxiv.org/abs/1901.00177v1,Finance & Economics,Credit Risk Modeling,"in a stable bank credit situation, no cycles of credit expansion or contraction appear . unlevered securitization and mis-pricing of assets increases lending cyclicality . banks sell securities when markets decline, putting further pressure on falling prices "
Analysing the Influence of Macroeconomic Factors on Credit Risk in the   UK Banking Sector,"Macroeconomic factors have a critical impact on banking credit risk, which cannot be directly controlled by banks, and therefore, there is a need for an early credit risk warning system based on the macroeconomy. By comparing different predictive models (traditional statistical and machine learning algorithms), this study aims to examine the macroeconomic determinants impact on the UK banking credit risk and assess the most accurate credit risk estimate using predictive analytics. This study found that the variance-based multi-split decision tree algorithm is the most precise predictive model with interpretable, reliable, and robust results. Our model performance achieved 95% accuracy and evidenced that unemployment and inflation rate are significant credit risk predictors in the UK banking context. Our findings provided valuable insights such as a positive association between credit risk and inflation, the unemployment rate, and national savings, as well as a negative relationship between credit risk and national debt, total trade deficit, and national income. In addition, we empirically showed the relationship between national savings and non-performing loans, thus proving the paradox of thrift. These findings benefit the credit risk management team in monitoring the macroeconomic factors thresholds and implementing critical reforms to mitigate credit risk.","['Hemlata Sharma', 'Aparna Andhalkar', 'Oluwaseun Ajao', 'Bayode Ogunleye']",2024-01-26T15:25:27Z,http://arxiv.org/abs/2401.14943v1,Finance & Economics,Credit Risk Modeling,this study aims to examine the macroeconomic determinants impact on banking credit risk . the variance-based decision tree algorithm is the most precise predictive model . unemployment and inflation rate are significant credit risk predictors in the UK .
Cross-Domain Behavioral Credit Modeling: transferability from private to   central data,"This paper introduces a credit risk rating model for credit risk assessment in quantitative finance, aiming to categorize borrowers based on their behavioral data. The model is trained on data from Experian, a widely recognized credit bureau, to effectively identify instances of loan defaults among bank customers. Employing state-of-the-art statistical and machine learning techniques ensures the model's predictive accuracy. Furthermore, we assess the model's transferability by testing it on behavioral data from the Bank of Italy, demonstrating its potential applicability across diverse datasets during prediction. This study highlights the benefits of incorporating external behavioral data to improve credit risk assessment in financial institutions.","['O. Didkovskyi', 'N. Jean', 'G. Le Pera', 'C. Nordio']",2024-01-18T07:54:41Z,http://arxiv.org/abs/2401.09778v1,Finance & Economics,Credit Risk Modeling,"this paper introduces a credit risk rating model for credit risk assessment in quantitative finance . the model is trained on data from Experian, a widely recognized credit bureau . employing state-of-the-art statistical and machine learning techniques ensures the model"
Modelling China's Credit System with Complex Network Theory for   Systematic Credit Risk Control,"The insufficient understanding of the credit network structure was recognized as a key factor for regulators' underestimation of the destructive systematic risk during the financial crisis that started in 2007. The existing credit network research either took a macro perspective to clarify the topological properties of financial systems at a descriptive level or analyzed the risk transmission path and characteristics of individual entities with much pre-assumptions of the network. Here, we used the theory of complex network to model China's credit system from 2000 to 2014 based on actual financial data. A bipartite financial institution-firm network and its projected sub-networks were constructed for an integrated analysis from both macro and micro perspectives, and the relationship between typological properties and systematic credit risk control was also explored. The typological analysis of the networks suggested that the financial institutions and firms were highly but asymmetrically connected, and the credit network structure made local idiosyncratic shocks possible to proliferate through the whole economy. In addition, the Chinese credit market was still dominated by state-owned financial institutions with firms competing fiercely for financial resources in the past fifteen years. Furthermore, the credit risk score (CRS) was introduced by simulation to identify the systematically important vertices in terms of systematic risk control. The results indicated that the vertices with more access to the credit market or less likelihood to be a bridge in the network were the ones with higher systematically importance. The empirical results from this study would provide specific policy suggestions to financial regulators on supervisory approaches and optimizing the allocation of regulatory resources to enhance the robustness of credit systems in China and in other countries.","['Xuan Lu', 'Li Huang', 'Kangjuan Lyu']",2018-12-04T11:16:54Z,http://arxiv.org/abs/1812.01341v1,Finance & Economics,Credit Risk Modeling,"china's credit system was modelled from 2000 to 2014 based on actual financial data . the credit network structure made local idiosyncratic shocks possible, authors say . they say the credit risk score (CRS) was introduced by simulation to"
Conditional Generative Modeling for Enhanced Credit Risk Management in   Supply Chain Finance,"The rapid expansion of cross-border e-commerce (CBEC) has created significant opportunities for small and medium-sized enterprises (SMEs), yet financing remains a critical challenge due to SMEs' limited credit histories. Third-party logistics (3PL)-led supply chain finance (SCF) has emerged as a promising solution, leveraging in-transit inventory as collateral. We propose an advanced credit risk management framework tailored for 3PL-led SCF, addressing the dual challenges of credit risk assessment and loan size determination. Specifically, we leverage conditional generative modeling of sales distributions through Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for risk estimation. We propose a unified framework that enables flexible estimation of multiple risk measures while introducing a functional risk measure formulation that systematically captures the relationship between these risk measures and varying loan levels, supported by theoretical guarantees. To capture complex covariate interactions in e-commerce sales data, we integrate QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on synthetic and real-world data validate the efficacy of our model for credit risk assessment and loan size determination. This study represents a pioneering application of generative AI in CBEC SCF risk management, offering a solid foundation for enhanced credit practices and improved SME access to capital.","['Qingkai Zhang', 'L. Jeff Hong', 'Houmin Yan']",2025-06-18T09:35:50Z,http://arxiv.org/abs/2506.15305v1,Finance & Economics,Credit Risk Modeling,the rapid expansion of cross-border e-commerce (CBEC) has created significant opportunities for SMEs . but financing remains a critical challenge due to SMEs' limited credit histories . we propose an advanced credit risk management framework tailored for 3PL-led
Long range Ising model for credit risk modeling in homogeneous   portfolios,Within the framework of maximum entropy principle we show that the finite-size long-range Ising model is the adequate model for the description of homogeneous credit portfolios and the computation of credit risk when default correlations between the borrowers are included. The exact analysis of the model suggest that when the correlation increases a first-order-like transition may occur inducing a sudden risk increase. Such a feature is not reproduced by the standard models used in credit risk modeling.,"['Jordi Molins', 'Eduard Vives']",2004-01-21T07:29:14Z,http://arxiv.org/abs/cond-mat/0401378v1,Finance & Economics,Credit Risk Modeling,the finite-size long-range Ising model is adequate for credit risk modeling . when the correlation increases a first-order-like transition may occur .
A Simplified Approach to modeling the credit-risk of CMO,"The credit crisis of 2007 and 2008 has thrown much focus on the models used to price mortgage backed securities. Many institutions have relied heavily on the credit ratings provided by credit agency. The relationships between management of credit agencies and debt issuers may have resulted in conflict of interest when pricing these securities which has lead to incorrect risk assumptions and value expectations from institutional buyers. Despite the existence of sophisticated models, institutional buyers have relied on these ratings when considering the risks involved with these products. Institutional investors interested in non-agency MBS are particularly vulnerable due to both the credit risks as well as prepayment risks. This paper describes a simple simulation model that model non-agency MBS and CMO. The simulation model builds on existing models for agency MBS. It incorporates credit risks of mortgage buyers using existing models used in capital requirements as specified by the Basel II Accord.",['K. Rajaratnam'],2009-03-09T20:12:34Z,http://arxiv.org/abs/0903.1643v2,Finance & Economics,Credit Risk Modeling,the credit crisis of 2007 and 2008 has thrown much focus on the models used to price mortgage backed securities . many institutions have relied heavily on the credit ratings provided by credit agency . this paper describes a simple simulation model that model non-agency MBS
Identification of Credit Risk Based on Cluster Analysis of Account   Behaviours,"Assessment of risk levels for existing credit accounts is important to the implementation of bank policies and offering financial products. This paper uses cluster analysis of behaviour of credit card accounts to help assess credit risk level. Account behaviour is modelled parametrically and we then implement the behavioural cluster analysis using a recently proposed dissimilarity measure of statistical model parameters. The advantage of this new measure is the explicit exploitation of uncertainty associated with parameters estimated from statistical models. Interesting clusters of real credit card behaviours data are obtained, in addition to superior prediction and forecasting of account default based on the clustering outcomes.","['Maha Bakoben', 'Tony Bellotti', 'Niall Adams']",2017-05-31T09:45:41Z,http://arxiv.org/abs/1706.07466v1,Finance & Economics,Credit Risk Modeling,this paper uses cluster analysis of behaviour of credit card accounts to help assess credit risk level . account behaviour is modelled parametrically and we then implement the behavioural cluster analysis using a recently proposed dissimilarity measure .
Model risk on credit risk,"This paper develops the Jungle model in a credit portfolio framework. The Jungle model is able to model credit contagion, produce doubly-peaked probability distributions for the total default loss and endogenously generate quasi phase transitions, potentially leading to systemic credit events which happen unexpectedly and without an underlying single cause. We show the Jungle model provides the optimal probability distribution for credit losses, under some reasonable empirical constraints. The Dandelion model, a particular case of the Jungle model, is presented, motivated and exactly solved. The Dandelion model provides an explicit example of doubly-peaked probability distribution for the credit losses. The Diamond model, another instance of the Jungle model, experiences the so called quasi phase transitions; in particular, both the U.S. subprime and the European sovereign crises are shown to be potential examples of quasi phase transitions. We argue the three known sources of default clustering (contagion, macroeconomic risk factors and frailty) can be understood under the unifying framework of contagion. We suggest how the Jungle model is able to explain a series of empirical stylized facts in credit portfolios, hard to reconcile by some standard credit portfolio models. We show the Jungle model can handle inhomogeneous portfolios with state-dependent recovery rates. We look at model risk in a credit risk framework under the Jungle model, especially in relation to systemic risks posed by doubly-peaked distributions and quasi phase transitions.","['J. Molins', 'E. Vives']",2015-02-17T06:06:52Z,http://arxiv.org/abs/1502.06984v3,Finance & Economics,Credit Risk Modeling,"this paper develops the Jungle model in a credit portfolio framework . the model is able to model credit contagion, produce doubly-peaked probability distributions . it also generates quasi phase transitions, potentially leading to systemic credit events "
The Network Effect in Credit Concentration Risk,"Measurement and management of credit concentration risk is critical for banks and relevant for micro-prudential requirements. While several methods exist for measuring credit concentration risk within institutions, the systemic effect of different institutions' exposures to the same counterparties has been less explored so far. In this paper, we propose a measure of the systemic credit concentration risk that arises because of common exposures between different institutions within a financial system. This approach is based on a network model that describes the effect of overlapping portfolios. This network metric is applied to synthetic and real world data to illustrate that the effect of common exposures is not fully reflected in single portfolio concentration measures. It also allows to quantify several aspects of the interplay between interconnectedness and credit risk. Using this network measure, we formulate an analytical approximation for the additional capital requirement corresponding to the systemic risk arising from credit concentration interconnectedness. Our methodology also avoids double counting between the granularity adjustment and the common exposure adjustment. Although approximated, this common exposure adjustment is able to capture, with only two parameters, an aspect of systemic risk that can extend single portfolios view to a system-wide one.","['Davide Cellai', 'Trevor Fitzpatrick']",2019-05-31T16:41:20Z,http://arxiv.org/abs/1905.13711v2,Finance & Economics,Credit Risk Modeling,"measure of credit concentration risk is based on a network model that describes the effect of overlapping portfolios . it allows to quantify several aspects of the interplay between interconnectedness and credit risk . the common exposure adjustment is able to capture, with only two"
An Integrated Machine Learning and Deep Learning Framework for Credit   Card Approval Prediction,"Credit scoring is vital in the financial industry, assessing the risk of lending to credit card applicants. Traditional credit scoring methods face challenges with large datasets and data imbalance between creditworthy and non-creditworthy applicants. This paper introduces an advanced machine learning and deep learning framework to improve the accuracy and reliability of credit card approval predictions. We utilized extensive datasets of user application records and credit history, implementing a comprehensive preprocessing strategy, feature engineering, and model integration. Our methodology combines neural networks with an ensemble of base models, including logistic regression, support vector machines, k-nearest neighbors, decision trees, random forests, and gradient boosting. The ensemble approach addresses data imbalance using Synthetic Minority Over-sampling Technique (SMOTE) and mitigates overfitting risks. Experimental results show that our integrated model surpasses traditional single-model approaches in precision, recall, F1-score, AUC, and Kappa, providing a robust and scalable solution for credit card approval predictions. This research underscores the potential of advanced machine learning techniques to transform credit risk assessment and financial decision-making.","['Kejian Tong', 'Zonglin Han', 'Yanxin Shen', 'Yujian Long', 'Yijing Wei']",2024-09-25T07:07:19Z,http://arxiv.org/abs/2409.16676v1,Finance & Economics,Credit Risk Modeling,"credit scoring is vital in the financial industry, assessing the risk of lending to credit card applicants . traditional credit scoring methods face challenges with large datasets and data imbalance . paper introduces advanced machine learning and deep learning framework to improve credit card approval predictions ."
The Climate Extended Risk Model (CERM),This paper addresses estimates of climate risk embedded within a bank credit portfolio. The proposed Climate Extended Risk Model (CERM) adapts well known credit risk models and makes it possible to calculate incremental credit losses on a loan portfolio that are rooted into physical and transition risks. The paper provides detailed description of the model hypotheses and steps.,"['Josselin Garnier', 'Jean-Baptiste Gaudemet', 'Anne Gruz']",2021-03-04T19:23:59Z,http://arxiv.org/abs/2103.03275v2,Finance & Economics,Credit Risk Modeling,paper addresses estimates of climate risk embedded within bank credit portfolio . climate extended risk model (CERM) adapts well known credit risk models . paper provides detailed description of model hypotheses and steps .
Tail Risk Alert Based on Conditional Autoregressive VaR by Regression   Quantiles and Machine Learning Algorithms,"As the increasing application of AI in finance, this paper will leverage AI algorithms to examine tail risk and develop a model to alter tail risk to promote the stability of US financial markets, and enhance the resilience of the US economy. Specifically, the paper constructs a multivariate multilevel CAViaR model, optimized by gradient descent and genetic algorithm, to study the tail risk spillover between the US stock market, foreign exchange market and credit market. The model is used to provide early warning of related risks in US stocks, US credit bonds, etc. The results show that, by analyzing the direction, magnitude, and pseudo-impulse response of the risk spillover, it is found that the credit market's spillover effect on the stock market and its duration are both greater than the spillover effect of the stock market and the other two markets on credit market, placing credit market in a central position for warning of extreme risks. Its historical information on extreme risks can serve as a predictor of the VaR of other markets.","['Zong Ke', 'Yuchen Yin']",2024-12-09T04:21:24Z,http://arxiv.org/abs/2412.06193v1,Finance & Economics,Credit Risk Modeling,"paper examines tail risk spillover between stock market, foreign exchange market and credit market . credit market's spillover effect on stock market and duration greater than stock market spillover . paper: credit market in a central position for warning of extreme risks ."
"Empowering Many, Biasing a Few: Generalist Credit Scoring through Large   Language Models","In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various financial risk assessment tasks. We evaluate CALM, existing state-of-art (SOTA) methods, open source and closed source LLMs on the build benchmark. Our empirical results illuminate the capability of LLMs to not only match but surpass conventional models, pointing towards a future where credit scoring can be more inclusive, comprehensive, and unbiased. We contribute to the industry's transformation by sharing our pioneering instruction-tuning datasets, credit and risk assessment LLM, and benchmarks with the research community and the financial industry.","['Duanyu Feng', 'Yongfu Dai', 'Jimin Huang', 'Yifang Zhang', 'Qianqian Xie', 'Weiguang Han', 'Zhengyu Chen', 'Alejandro Lopez-Lira', 'Hao Wang']",2023-10-01T03:50:34Z,http://arxiv.org/abs/2310.00566v3,Finance & Economics,Credit Risk Modeling,credit scoring is a fundamental element in the financial industry . traditional credit scoring methods often grapple with narrow knowledge scope . large language models (LLMs) have great potential for credit scoring tasks .
Towards Financial Sentiment Analysis in a South African Landscape,"Sentiment analysis as a sub-field of natural language processing has received increased attention in the past decade enabling organisations to more effectively manage their reputation through online media monitoring. Many drivers impact reputation, however, this thesis focuses only the aspect of financial performance and explores the gap with regards to financial sentiment analysis in a South African context. Results showed that pre-trained sentiment analysers are least effective for this task and that traditional lexicon-based and machine learning approaches are best suited to predict financial sentiment of news articles. The evaluated methods produced accuracies of 84\%-94\%. The predicted sentiments correlated quite well with share price and highlighted the potential use of sentiment as an indicator of financial performance. A main contribution of the study was updating an existing sentiment dictionary for financial sentiment analysis. Model generalisation was less acceptable due to the limited amount of training data used. Future work includes expanding the data set to improve general usability and contribute to an open-source financial sentiment analyser for South African data.","['Michelle Terblanche', 'Vukosi Marivate']",2021-06-18T08:48:47Z,http://arxiv.org/abs/2106.10004v1,Finance & Economics,Financial Sentiment Analysis,pre-trained sentiment analysers are least effective for this task . traditional lexicon-based and machine learning approaches are best suited to predict financial sentiment of news articles .
Financial Sentiment Analysis on News and Reports Using Large Language   Models and FinBERT,"Financial sentiment analysis (FSA) is crucial for evaluating market sentiment and making well-informed financial decisions. The advent of large language models (LLMs) such as BERT and its financial variant, FinBERT, has notably enhanced sentiment analysis capabilities. This paper investigates the application of LLMs and FinBERT for FSA, comparing their performance on news articles, financial reports and company announcements. The study emphasizes the advantages of prompt engineering with zero-shot and few-shot strategy to improve sentiment classification accuracy. Experimental results indicate that GPT-4o, with few-shot examples of financial texts, can be as competent as a well fine-tuned FinBERT in this specialized field.","['Yanxin Shen', 'Pulin Kirin Zhang']",2024-10-02T19:48:17Z,http://arxiv.org/abs/2410.01987v1,Finance & Economics,Financial Sentiment Analysis,"financial sentiment analysis (FSA) is crucial for evaluating market sentiment . large language models (LLMs) such as BERT and its financial variant, FinBERT, have enhanced sentiment analysis capabilities . study emphasizes the advantages of prompt engineering with zero-shot and few-shot strategy ."
Large language models in finance : what is financial sentiment?,"Financial sentiment has become a crucial yet complex concept in finance, increasingly used in market forecasting and investment strategies. Despite its growing importance, there remains a need to define and understand what financial sentiment truly represents and how it can be effectively measured. We explore the nature of financial sentiment and investigate how large language models (LLMs) contribute to its estimation. We trace the evolution of sentiment measurement in finance, from market-based and lexicon-based methods to advanced natural language processing techniques. The emergence of LLMs has significantly enhanced sentiment analysis, providing deeper contextual understanding and greater accuracy in extracting sentiment from financial text. We examine how BERT-based models, such as RoBERTa and FinBERT, are optimized for structured sentiment classification, while GPT-based models, including GPT-4, OPT, and LLaMA, excel in financial text generation and real-time sentiment interpretation. A comparative analysis of bidirectional and autoregressive transformer architectures highlights their respective roles in investor sentiment analysis, algorithmic trading, and financial decision-making. By exploring what financial sentiment is and how it is estimated within LLMs, we provide insights into the growing role of AI-driven sentiment analysis in finance.","['Kemal Kirtac', 'Guido Germano']",2025-03-05T15:51:25Z,http://arxiv.org/abs/2503.03612v4,Finance & Economics,Financial Sentiment Analysis,financial sentiment has become a crucial yet complex concept in finance . there remains a need to define and understand what financial sentiment truly represents . we investigate how large language models (LLMs) contribute to its estimation .
Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News   Headlines,"In this study, we explore the application of sentiment analysis on financial news headlines to understand investor sentiment. By leveraging Natural Language Processing (NLP) and Large Language Models (LLM), we analyze sentiment from the perspective of retail investors. The FinancialPhraseBank dataset, which contains categorized sentiments of financial news headlines, serves as the basis for our analysis. We fine-tuned several models, including distilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness in sentiment classification. Our experiments demonstrate that the fine-tuned gemma-7b model outperforms others, achieving the highest precision, recall, and F1 score. Specifically, the gemma-7b model showed significant improvements in accuracy after fine-tuning, indicating its robustness in capturing the nuances of financial sentiment. This model can be instrumental in providing market insights, risk management, and aiding investment decisions by accurately predicting the sentiment of financial news. The results highlight the potential of advanced LLMs in transforming how we analyze and interpret financial information, offering a powerful tool for stakeholders in the financial industry.","['Kangtong Mo', 'Wenyan Liu', 'Xuanzhen Xu', 'Chang Yu', 'Yuelin Zou', 'Fangqing Xia']",2024-06-19T15:20:19Z,http://arxiv.org/abs/2406.13626v1,Finance & Economics,Financial Sentiment Analysis,sentiment analysis can be applied to financial news headlines to understand investor sentiment . the financialPhraseBank dataset contains categorized sentiments of financial headlines . gemma-7b model shows significant improvements in accuracy after fine-tuning .
Sentiment Analysis of Financial News Articles using Performance   Indicators,"Mining financial text documents and understanding the sentiments of individual investors, institutions and markets is an important and challenging problem in the literature. Current approaches to mine sentiments from financial texts largely rely on domain specific dictionaries. However, dictionary based methods often fail to accurately predict the polarity of financial texts. This paper aims to improve the state-of-the-art and introduces a novel sentiment analysis approach that employs the concept of financial and non-financial performance indicators. It presents an association rule mining based hierarchical sentiment classifier model to predict the polarity of financial texts as positive, neutral or negative. The performance of the proposed model is evaluated on a benchmark financial dataset. The model is also compared against other state-of-the-art dictionary and machine learning based approaches and the results are found to be quite promising. The novel use of performance indicators for financial sentiment analysis offers interesting and useful insights.",['Srikumar Krishnamoorthy'],2018-11-25T01:36:12Z,http://arxiv.org/abs/1811.11008v1,Finance & Economics,Financial Sentiment Analysis,current methods to mine sentiments from financial documents rely on domain specific dictionaries . dictionary based methods often fail to accurately predict the polarity of financial texts . paper presents an association rule mining based hierarchical sentiment classifier model .
Automatic Construction of Context-Aware Sentiment Lexicon in the   Financial Domain Using Direction-Dependent Words,"Increasing attention has been drawn to the sentiment analysis of financial documents. The most popular examples of such documents include analyst reports and economic news, the analysis of which is frequently used to capture the trends in market sentiments. On the other hand, the significance of the role sentiment analysis plays in the financial domain has given rise to the efforts to construct a financial domain-specific sentiment lexicon. Sentiment lexicons lend a hand for solving various text mining tasks, such as unsupervised classification of text data, while alleviating the arduous human labor required for manual labeling. One of the challenges in the construction of an effective sentiment lexicon is that the semantic orientation of a word may change depending on the context in which it appears. For instance, the word ``profit"" usually conveys positive sentiments; however, when the word is juxtaposed with another word ``decrease,"" the sentiment associated with the phrase ``profit decreases"" now becomes negative. Hence, the sentiment of a given word may shift as one begins to consider the context surrounding the word. In this paper, we address this issue by incorporating context when building sentiment lexicon from a given corpus. Specifically, we construct a lexicon named Senti-DD for the Sentiment lexicon composed of Direction-Dependent words, which expresses each term a pair of a directional word and a direction-dependent word. Experiment results show that higher classification performance is achieved with Senti-DD, proving the effectiveness of our method for automatically constructing a context-aware sentiment lexicon in the financial domain.","['Jihye Park', 'Hye Jin Lee', 'Sungzoon Cho']",2021-06-10T13:08:00Z,http://arxiv.org/abs/2106.05723v1,Finance & Economics,Financial Sentiment Analysis,"sentiment analysis has led to efforts to construct a financial domain-specific sentiment lexicon . the semantic orientation of a word may change depending on the context in which it appears . in this paper, we address this issue by incorporating context ."
FinEAS: Financial Embedding Analysis of Sentiment,"We introduce a new language representation model in finance called Financial Embedding Analysis of Sentiment (FinEAS). In financial markets, news and investor sentiment are significant drivers of security prices. Thus, leveraging the capabilities of modern NLP approaches for financial sentiment analysis is a crucial component in identifying patterns and trends that are useful for market participants and regulators. In recent years, methods that use transfer learning from large Transformer-based language models like BERT, have achieved state-of-the-art results in text classification tasks, including sentiment analysis using labelled datasets. Researchers have quickly adopted these approaches to financial texts, but best practices in this domain are not well-established. In this work, we propose a new model for financial sentiment analysis based on supervised fine-tuned sentence embeddings from a standard BERT model. We demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific BERT.","['Asier Gutiérrez-Fandiño', 'Miquel Noguer i Alonso', 'Petter Kolm', 'Jordi Armengol-Estapé']",2021-10-31T15:41:56Z,http://arxiv.org/abs/2111.00526v2,Finance & Economics,Financial Sentiment Analysis,"we introduce a new language representation model called financial Embedding Analysis of Sentiment (FinEAS) we demonstrate our approach achieves significant improvements in comparison to vanilla BERT, LSTM, and FinBERT ."
Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of   General-Purpose Large Language Models,"Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital.","['Boyu Zhang', 'Hongyang Yang', 'Xiao-Yang Liu']",2023-06-22T03:56:38Z,http://arxiv.org/abs/2306.12659v1,Finance & Economics,Financial Sentiment Analysis,"sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media . authors introduce a simple yet effective instruction tuning approach to address these issues . a small portion of supervised financial sentiment analysis data is transformed into instruction data ."
Predict the Future from the Past? On the Temporal Data Distribution   Shift in Financial Sentiment Classifications,"Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model's capability to adapt to evolving temporal shifts in a volatile financial market.","['Yue Guo', 'Chenxi Hu', 'Yi Yang']",2023-10-19T09:59:52Z,http://arxiv.org/abs/2310.12620v1,Finance & Economics,Financial Sentiment Analysis,temporal data distribution shifts are prevalent in the financial text . how can a financial sentiment analysis system be trained in a volatile market environment? we propose a method that combines out-of-distribution detection with time series modeling .
A Multi-Source Entity-Level Sentiment Corpus for the Financial Domain:   The FinLin Corpus,"We introduce FinLin, a novel corpus containing investor reports, company reports, news articles, and microblogs from StockTwits, targeting multiple entities stemming from the automobile industry and covering a 3-month period. FinLin was annotated with a sentiment score and a relevance score in the range [-1.0, 1.0] and [0.0, 1.0], respectively. The annotations also include the text spans selected for the sentiment, thus, providing additional insight into the annotators' reasoning. Overall, FinLin aims to complement the current knowledge by providing a novel and publicly available financial sentiment corpus and to foster research on the topic of financial sentiment analysis and potential applications in behavioural science.",['Tobias Daudert'],2020-03-09T12:34:48Z,http://arxiv.org/abs/2003.04073v1,Finance & Economics,Financial Sentiment Analysis,"FinLin contains investor reports, company reports, news articles, and microblogs . it was annotated with a sentiment score and relevance score in the range [-1.0, 1.0] . the annotations also include the text spans selected for the sentiment ."
Transforming Sentiment Analysis in the Financial Domain with ChatGPT,"Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment classification and a 36\% higher correlation with market returns. By underlining the significance of prompt engineering, particularly in zero-shot contexts, this study spotlights ChatGPT's potential to substantially boost sentiment analysis in financial applications. By sharing the utilized dataset, our intention is to stimulate further research and advancements in the field of financial services.","['Georgios Fatouros', 'John Soldatos', 'Kalliopi Kouroumali', 'Georgios Makridis', 'Dimosthenis Kyriazis']",2023-08-13T09:20:47Z,http://arxiv.org/abs/2308.07935v1,Finance & Economics,Financial Sentiment Analysis,"this study examines the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis . using a zero-shot prompting approach, we examine multiple chatGPT prompts on a dataset of forex-related news headlines . we probe the correlation between predicted sentiment and market returns ."
Financial Sentiment Analysis: Leveraging Actual and Synthetic Data for   Supervised Fine-tuning,"The Efficient Market Hypothesis (EMH) highlights the essence of financial news in stock price movement. Financial news comes in the form of corporate announcements, news titles, and other forms of digital text. The generation of insights from financial news can be done with sentiment analysis. General-purpose language models are too general for sentiment analysis in finance. Curated labeled data for fine-tuning general-purpose language models are scare, and existing fine-tuned models for sentiment analysis in finance do not capture the maximum context width. We hypothesize that using actual and synthetic data can improve performance. We introduce BertNSP-finance to concatenate shorter financial sentences into longer financial sentences, and finbert-lc to determine sentiment from digital text. The results show improved performance on the accuracy and the f1 score for the financial phrasebank data with $50\%$ and $100\%$ agreement levels.",['Abraham Atsiwo'],2024-12-13T04:59:50Z,http://arxiv.org/abs/2412.09859v1,Finance & Economics,Financial Sentiment Analysis,"the Efficient Market Hypothesis (EMH) highlights the essence of financial news . financial news comes in the form of corporate announcements, news titles . general-purpose language models are too general for sentiment analysis in finance ."
Evaluating Financial Sentiment Analysis with Annotators Instruction   Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction   Accuracy,"Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods.","['A M Muntasir Rahman', 'Ajim Uddin', 'Guiling ""Grace"" Wang']",2025-05-09T19:44:04Z,http://arxiv.org/abs/2505.07871v1,Finance & Economics,Financial Sentiment Analysis,financial sentiment analysis (FSA) presents unique challenges to LLMs . the prowess of these models is often undermined by the subjectivity of sentiment classes . annotators' instruction Assisted Prompt aims to standardize understanding of sentiment .
FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis,"This paper presents a novel approach for explainability in financial analysis by deriving financially-explainable statistical relationships through aspect-based sentiment analysis, Pearson correlation, Granger causality & uncertainty coefficient. The proposed methodology involves constructing an aspect list from financial literature and applying aspect-based sentiment analysis on social media text to compute sentiment scores for each aspect. Pearson correlation is then applied to uncover financially explainable relationships between aspect sentiment scores and stock prices. Findings for derived relationships are made robust by applying Granger causality to determine the forecasting ability of each aspect sentiment score for stock prices. Finally, an added layer of interpretability is added by evaluating uncertainty coefficient scores between aspect sentiment scores and stock prices. This allows us to determine the aspects whose sentiment scores are most statistically significant for stock prices. Relative to other methods, our approach provides a more informative and accurate understanding of the relationship between sentiment analysis and stock prices. Specifically, this methodology enables an interpretation of the statistical relationship between aspect-based sentiment scores and stock prices, which offers explainability to AI-driven financial decision-making.","['Keane Ong', 'Wihan van der Heever', 'Ranjan Satapathy', 'Erik Cambria', 'Gianmarco Mengaldo']",2023-03-05T03:18:56Z,http://arxiv.org/abs/2303.02563v4,Finance & Economics,Financial Sentiment Analysis,"the paper presents a novel approach for explainability in financial analysis . it uses aspect-based sentiment analysis, Pearson correlation, Granger causality & uncertainty coefficient . the paper provides a more informative and accurate understanding of the relationship between sentiment analysis and stock prices ."
Contextual Sentence Analysis for the Sentiment Prediction on Financial   Data,"Newsletters and social networks can reflect the opinion about the market and specific stocks from the perspective of analysts and the general public on products and/or services provided by a company. Therefore, sentiment analysis of these texts can provide useful information to help investors trade in the market. In this paper, a hierarchical stack of Transformers model is proposed to identify the sentiment associated with companies and stocks, by predicting a score (of data type real) in a range between -1 and +1. Specifically, we fine-tuned a RoBERTa model to process headlines and microblogs and combined it with additional Transformer layers to process the sentence analysis with sentiment dictionaries to improve the sentiment analysis. We evaluated it on financial data released by SemEval-2017 task 5 and our proposition outperformed the best systems of SemEval-2017 task 5 and strong baselines. Indeed, the combination of contextual sentence analysis with the financial and general sentiment dictionaries provided useful information to our model and allowed it to generate more reliable sentiment scores.","['Elvys Linhares Pontes', 'Mohamed Benjannet']",2021-12-27T17:12:57Z,http://arxiv.org/abs/2112.13790v1,Finance & Economics,Financial Sentiment Analysis,sentiment analysis of newsletters and social networks can help investors trade in the market . a model is proposed to identify the sentiment associated with companies and stocks . we fine-tuned a RoBERTa model to process headlines and microblogs .
A Multi-Level Sentiment Analysis Framework for Financial Texts,"Existing financial sentiment analysis methods often fail to capture the multi-faceted nature of risk in bond markets due to their single-level approach and neglect of temporal dynamics. We propose Multi-Level Sentiment Analysis based on pre-trained language models (PLMs) and large language models (LLMs), a novel framework that systematically integrates firm-specific micro-level sentiment, industry-specific meso-level sentiment, and duration-aware smoothing to model the latency and persistence of textual impact. Applying our framework to the comprehensive Chinese bond market corpus constructed by us (2013-2023, 1.39M texts), we extracted a daily composite sentiment index. Empirical results show statistically measurable improvements in credit spread forecasting when incorporating sentiment (3.25% MAE and 10.96% MAPE reduction), with sentiment shifts closely correlating with major social risk events and firm-specific crises. This framework provides a more nuanced understanding of sentiment across different market levels while accounting for the temporal evolution of sentiment effects.","['Yiwei Liu', 'Junbo Wang', 'Lei Long', 'Xin Li', 'Ruiting Ma', 'Yuankai Wu', 'Xuebin Chen']",2025-04-03T09:35:07Z,http://arxiv.org/abs/2504.02429v1,Finance & Economics,Financial Sentiment Analysis,existing financial sentiment analysis methods often fail to capture multi-faceted nature of risk in bond markets . existing methods are single-level approach and neglect of temporal dynamics . this framework provides a more nuanced understanding of sentiment across different market levels while accounting for the temporal evolution of sentiment effects .
Evaluating Company-specific Biases in Financial Sentiment Analysis using   Large Language Models,"This study aims to evaluate the sentiment of financial texts using large language models~(LLMs) and to empirically determine whether LLMs exhibit company-specific biases in sentiment analysis. Specifically, we examine the impact of general knowledge about firms on the sentiment measurement of texts by LLMs. Firstly, we compare the sentiment scores of financial texts by LLMs when the company name is explicitly included in the prompt versus when it is not. We define and quantify company-specific bias as the difference between these scores. Next, we construct an economic model to theoretically evaluate the impact of sentiment bias on investor behavior. This model helps us understand how biased LLM investments, when widespread, can distort stock prices. This implies the potential impact on stock prices if investments driven by biased LLMs become dominant in the future. Finally, we conduct an empirical analysis using Japanese financial text data to examine the relationship between firm-specific sentiment bias, corporate characteristics, and stock performance.","['Kei Nakagawa', 'Masanori Hirano', 'Yugo Fujimoto']",2024-11-01T07:37:24Z,http://arxiv.org/abs/2411.00420v1,Finance & Economics,Financial Sentiment Analysis,this study aims to evaluate the sentiment of financial texts using large language models(LLMs) we define and quantify company-specific bias as the difference between these scores .
EFSA: Towards Event-Level Financial Sentiment Analysis,"In this paper, we extend financial sentiment analysis~(FSA) to event-level since events usually serve as the subject of the sentiment in financial text. Though extracting events from the financial text may be conducive to accurate sentiment predictions, it has specialized challenges due to the lengthy and discontinuity of events in a financial text. To this end, we reconceptualize the event extraction as a classification task by designing a categorization comprising coarse-grained and fine-grained event categories. Under this setting, we formulate the \textbf{E}vent-Level \textbf{F}inancial \textbf{S}entiment \textbf{A}nalysis~(\textbf{EFSA} for short) task that outputs quintuples consisting of (company, industry, coarse-grained event, fine-grained event, sentiment) from financial text. A large-scale Chinese dataset containing $12,160$ news articles and $13,725$ quintuples is publicized as a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based approach is devised for this task. Systematically investigations are conducted on our dataset, and the empirical results demonstrate the benchmarking scores of existing methods and our proposed method can reach the current state-of-the-art. Our dataset and framework implementation are available at https://anonymous.4open.science/r/EFSA-645E","['Tianyu Chen', 'Yiming Zhang', 'Guoxin Yu', 'Dapeng Zhang', 'Li Zeng', 'Qing He', 'Xiang Ao']",2024-04-08T07:36:26Z,http://arxiv.org/abs/2404.08681v2,Finance & Economics,Financial Sentiment Analysis,"in this paper, we extend financial sentiment analysis(FSA) to event-level . extracting events from financial text may be conducive to accurate sentiment predictions . but it has specialized challenges due to the lengthy and discontinuity of events ."
BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights,"This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextual aspects of market sentiment. The integration of advanced NLP techniques like BERTopic with traditional financial analysis methods marks a step forward in developing more sophisticated tools for understanding and predicting market behaviors.","['Enmin Zhu', 'Jerome Yen']",2024-04-02T15:50:10Z,http://arxiv.org/abs/2404.02053v2,Finance & Economics,Financial Sentiment Analysis,"this paper explores the intersection of natural language processing and financial analysis . we employ BERTopic, an advanced NLP technique, to analyze sentiment of stock market topics . results indicate topics in stock market comments provide valuable insights ."
FinEntity: Entity-level Sentiment Classification for Financial Texts,"In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called \textbf{FinEntity}, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in the paper. Additionally, we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on entity-level sentiment classification. In a case study, we demonstrate the practical utility of using FinEntity in monitoring cryptocurrency markets. The data and code of FinEntity is available at \url{https://github.com/yixuantt/FinEntity}","['Yixuan Tang', 'Yi Yang', 'Allen H Huang', 'Andy Tam', 'Justin Z Tang']",2023-10-19T01:38:40Z,http://arxiv.org/abs/2310.12406v1,Finance & Economics,Financial Sentiment Analysis,"we introduce an entity-level sentiment classification dataset, called textbfFinEntity . it annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news . we document the dataset construction process in the paper ."
"AI, insurance, discrimination and unfair differentiation. An overview   and research agenda","Insurers underwrite risks: they calculate risks and decide on the insurance price. Insurers seem captivated by two trends enabled by Artificial Intelligence (AI). First, insurers could use AI for analysing more and new types of data to assess risks more precisely: data-intensive underwriting. Second, insurers could use AI to monitor the behaviour of individual consumers in real-time: behaviour-based insurance. For example, some car insurers offer a discount if the consumer agrees to being tracked by the insurer and drives safely. While the two trends bring many advantages, they may also have discriminatory effects on society. This paper focuses on the following question. Which effects related to discrimination and unfair differentiation may occur if insurers use data-intensive underwriting and behaviour-based insurance?","['Marvin S. L. van Bekkum', 'Frederik Zuiderveen Borgesius', 'Tom Heskes']",2024-01-22T12:39:36Z,http://arxiv.org/abs/2401.11892v5,Finance & Economics,AI in Insurance,insurers underwrite risks and decide on the insurance price . insurers could use AI for analysing more and new types of data . they could also use AI to monitor the behaviour of individual consumers .
AI Liability Insurance With an Example in AI-Powered E-diagnosis System,"Artificial Intelligence (AI) has received an increasing amount of attention in multiple areas. The uncertainties and risks in AI-powered systems have created reluctance in their wild adoption. As an economic solution to compensate for potential damages, AI liability insurance is a promising market to enhance the integration of AI into daily life. In this work, we use an AI-powered E-diagnosis system as an example to study AI liability insurance. We provide a quantitative risk assessment model with evidence-based numerical analysis. We discuss the insurability criteria for AI technologies and suggest necessary adjustments to accommodate the features of AI products. We show that AI liability insurance can act as a regulatory mechanism to incentivize compliant behaviors and serve as a certificate of high-quality AI systems. Furthermore, we suggest premium adjustment to reflect the dynamic evolution of the inherent uncertainty in AI. Moral hazard problems are discussed and suggestions for AI liability insurance are provided.","['Yunfei Ge', 'Quanyan Zhu']",2023-06-01T21:03:47Z,http://arxiv.org/abs/2306.01149v1,Finance & Economics,AI in Insurance,"the uncertainties and risks in AI-powered systems have created reluctance in their wild adoption . as an economic solution to compensate for potential damages, AI liability insurance is a promising market . we discuss the insurability criteria for AI technologies and suggest necessary adjustments ."
Discrimination and AI in insurance: what do people find fair? Results   from a survey,"Two modern trends in insurance are data-intensive underwriting and behavior-based insurance. Data-intensive underwriting means that insurers use and analyze more data for estimating the chance that a consumer files a claim and calculating the premium based on that estimation. Insurers analyze the new datasets with artificial intelligence (AI) to discover new correlations, with which they can estimate the policyholder's expected claims cost more precisely. Insurers also offer behavior-based insurance. For example, some car insurers use AI to follow the driving behavior of an individual policyholder in real-time and decide whether to offer that policyholder a discount. Similarly, a life insurer could track a policyholder's activity with a smart watch and offer a discount for an active lifestyle.   In this paper, we report on a survey of the Dutch population (N=999) in which we asked people's opinions about examples of data-intensive underwriting and behavior-based insurance. The main results include the following. First, if survey respondents find an insurance practice unfair, they also find the practice unacceptable. Second, respondents find almost all modern insurance practices that we described unfair. Third, respondents find practices fairer if they can influence the premium. For example, respondents find behavior-based car insurance with a car tracker relatively fair. Fourth, if respondents do not see the logic of using a certain consumer characteristic, then respondents find it unfair if an insurer calculates the premium based on the characteristic. Fifth, respondents find it unfair if an insurer offers an insurance product only to a specific group, such as car insurance specifically for family doctors. Sixth, respondents find it unfair if an insurance practice leads to higher prices for poorer people. We reflect on the policy implications of the findings.","['Frederik Zuiderveen Borgesius', 'Marvin van Bekkum', 'Iris van Ooijen', 'Gabi Schaap', 'Maaike Harbers', 'Tjerk Timan']",2025-01-22T14:18:47Z,http://arxiv.org/abs/2501.12897v1,Finance & Economics,AI in Insurance,"two trends in insurance are data-intensive underwriting and behavior-based insurance . some car insurers use AI to follow the driving behavior of an individual policyholder . if survey respondents find an insurance practice unfair, they also find it unacceptable ."
Liability and Insurance for Catastrophic Losses: the Nuclear Power   Precedent and Lessons for AI,"As AI systems become more autonomous and capable, experts warn of them potentially causing catastrophic losses. Drawing on the successful precedent set by the nuclear power industry, this paper argues that developers of frontier AI models should be assigned limited, strict, and exclusive third party liability for harms resulting from Critical AI Occurrences (CAIOs) - events that cause or easily could have caused catastrophic losses. Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities. Based on theoretical arguments and observations from the analogous nuclear power context, insurers are expected to engage in a mix of causal risk-modeling, monitoring, lobbying for stricter regulation, and providing loss prevention guidance in the context of insuring against heavy-tail risks from AI. While not a substitute for regulation, clear liability assignment and mandatory insurance can help efficiently allocate resources to risk-modeling and safe design, facilitating future regulatory efforts.",['Cristian Trout'],2024-09-10T17:41:31Z,http://arxiv.org/abs/2409.06673v2,Finance & Economics,AI in Insurance,"Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities."
RISC: Generating Realistic Synthetic Bilingual Insurance Contract,"This paper presents RISC, an open-source Python package data generator (https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form in French and English. Insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson. Hence, they are a much more complex class of documents than those in traditional NLP corpora. Therefore, we introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile Contract dataset based on the mandatory Quebec car insurance contract. The dataset comprises 10,000 French and English unannotated insurance contracts. RISCBAC enables NLP research for unsupervised automatic summarisation, question answering, text simplification, machine translation and more. Moreover, it can be further automatically annotated as a dataset for supervised tasks such as NER","['David Beauchemin', 'Richard Khoury']",2023-04-09T10:42:18Z,http://arxiv.org/abs/2304.04212v1,Finance & Economics,AI in Insurance,"RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form . insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson . the dataset comprises 10,000 french and english unannotated insurance contracts."
An Attack Method for Medical Insurance Claim Fraud Detection based on   Generative Adversarial Network,"Insurance fraud detection represents a pivotal advancement in modern insurance service, providing intelligent and digitalized monitoring to enhance management and prevent fraud. It is crucial for ensuring the security and efficiency of insurance systems. Although AI and machine learning algorithms have demonstrated strong performance in detecting fraudulent claims, the absence of standardized defense mechanisms renders current systems vulnerable to emerging adversarial threats. In this paper, we propose a GAN-based approach to conduct adversarial attacks on fraud detection systems. Our results indicate that an attacker, without knowledge of the training data or internal model details, can generate fraudulent cases that are classified as legitimate with a 99\% attack success rate (ASR). By subtly modifying real insurance records and claims, adversaries can significantly increase the fraud risk, potentially bypassing compromised detection systems. These findings underscore the urgent need to enhance the robustness of insurance fraud detection models against adversarial manipulation, thereby ensuring the stability and reliability of different insurance systems.","['Yining Pang', 'Chenghan Li']",2025-06-22T05:02:45Z,http://arxiv.org/abs/2506.19871v1,Finance & Economics,AI in Insurance,insurance fraud detection is a key advancement in modern insurance service . lack of standardized defense mechanisms renders current systems vulnerable . authors propose a GAN-based approach to conduct adversarial attacks on fraud detection systems .
Adversarial AI in Insurance: Pervasiveness and Resilience,"The rapid and dynamic pace of Artificial Intelligence (AI) and Machine Learning (ML) is revolutionizing the insurance sector. AI offers significant, very much welcome advantages to insurance companies, and is fundamental to their customer-centricity strategy. It also poses challenges, in the project and implementation phase. Among those, we study Adversarial Attacks, which consist of the creation of modified input data to deceive an AI system and produce false outputs. We provide examples of attacks on insurance AI applications, categorize them, and argue on defence methods and precautionary systems, considering that they can involve few-shot and zero-shot multilabelling. A related topic, with growing interest, is the validation and verification of systems incorporating AI and ML components. These topics are discussed in various sections of this paper.","['Elisa Luciano', 'Matteo Cattaneo', 'Ron Kenett']",2023-01-17T08:49:54Z,http://arxiv.org/abs/2301.07520v1,Finance & Economics,AI in Insurance,"the rapid and dynamic pace of Artificial Intelligence (AI) and Machine Learning (ML) is revolutionizing the insurance sector . it also poses challenges, in the project and implementation phase . we provide examples of attacks on insurance AI applications ."
A Checklist for Explainable AI in the Insurance Domain,"Artificial intelligence (AI) is a powerful tool to accomplish a great many tasks. This exciting branch of technology is being adopted increasingly across varying sectors, including the insurance domain. With that power arise several complications. One of which is a lack of transparency and explainability of an algorithm for experts and non-experts alike. This brings into question both the usefulness as well as the accuracy of the algorithm, coupled with an added difficulty to assess potential biases within the data or the model. In this paper, we investigate the current usage of AI algorithms in the Dutch insurance industry and the adoption of explainable artificial intelligence (XAI) techniques. Armed with this knowledge we design a checklist for insurance companies that should help assure quality standards regarding XAI and a solid foundation for cooperation between organisations. This checklist extends an existing checklist of SIVI, the standardisation institute for digital cooperation and innovation in Dutch insurance.","['Olivier Koster', 'Ruud Kosman', 'Joost Visser']",2021-07-18T10:19:04Z,http://arxiv.org/abs/2107.14039v1,Finance & Economics,AI in Insurance,"artificial intelligence (AI) is a powerful tool to accomplish a great many tasks . it is being adopted increasingly across varying sectors, including the insurance domain . lack of transparency and explainability of an algorithm brings into question its usefulness ."
Evaluating if trust and personal information privacy concerns are   barriers to using health insurance that explicitly utilizes AI,"Trust and privacy have emerged as significant concerns in online transactions. Sharing information on health is especially sensitive but it is necessary for purchasing and utilizing health insurance. Evidence shows that consumers are increasingly comfortable with technology in place of humans, but the expanding use of AI potentially changes this. This research explores whether trust and privacy concern are barriers to the adoption of AI in health insurance. Two scenarios are compared: The first scenario has limited AI that is not in the interface and its presence is not explicitly revealed to the consumer. In the second scenario there is an AI interface and AI evaluation, and this is explicitly revealed to the consumer. The two scenarios were modeled and compared using SEM PLS-MGA. The findings show that trust is significantly lower in the second scenario where AI is visible. Privacy concerns are higher with AI but the difference is not statistically significant within the model.","['Alex Zarifis', 'Peter Kawalek', 'Aida Azadegan']",2024-01-20T15:02:56Z,http://arxiv.org/abs/2401.11249v1,Finance & Economics,AI in Insurance,research explores whether trust and privacy concerns are barriers to AI in health insurance . trust is significantly lower in the second scenario where AI is visible . privacy concerns higher with AI but the difference is not statistically significant .
POSGen: Personalized Opening Sentence Generation for Online Insurance   Sales,"The insurance industry is shifting their sales mode from offline to online, in expectation to reach massive potential customers in the digitization era. Due to the complexity and the nature of insurance products, a cost-effective online sales solution is to exploit chatbot AI to raise customers' attention and pass those with interests to human agents for further sales. For high response and conversion rates of customers, it is crucial for the chatbot to initiate a conversation with personalized opening sentences, which are generated with user-specific topic selection and ordering. Such personalized opening sentence generation is challenging because (i) there are limited historical samples for conversation topic recommendation in online insurance sales and (ii) existing text generation schemes often fail to support customized topic ordering based on user preferences. We design POSGen, a personalized opening sentence generation scheme dedicated for online insurance sales. It transfers user embeddings learned from auxiliary online user behaviours to enhance conversation topic recommendation, and exploits a context management unit to arrange the recommended topics in user-specific ordering for opening sentence generation. POSGen is deployed on a real-world online insurance platform. It achieves 2.33x total insurance premium improvement through a two-month global test.","['Yu Li', 'Yi Zhang', 'Weijia Wu', 'Zimu Zhou', 'Qiang Li']",2023-02-10T01:40:03Z,http://arxiv.org/abs/2302.06470v1,Finance & Economics,AI in Insurance,"the insurance industry is shifting their sales mode from offline to online . a cost-effective online sales solution is to exploit chatbot AI to raise customers' attention . for high response and conversion rates of customers, it is crucial for the chatbot to initiate a conversation ."
"Reinsuring AI: Energy, Agriculture, Finance & Medicine as Precedents for   Scalable Governance of Frontier Artificial Intelligence","The governance of frontier artificial intelligence (AI) systems--particularly those capable of catastrophic misuse or systemic failure--requires institutional structures that are robust, adaptive, and innovation-preserving. This paper proposes a novel framework for governing such high-stakes models through a three-tiered insurance architecture: (1) mandatory private liability insurance for frontier model developers; (2) an industry-administered risk pool to absorb recurring, non-catastrophic losses; and (3) federally backed reinsurance for tail-risk events. Drawing from historical precedents in nuclear energy (Price-Anderson), terrorism risk (TRIA), agricultural crop insurance, flood reinsurance, and medical malpractice, the proposal shows how the federal government can stabilize private AI insurance markets without resorting to brittle regulation or predictive licensing regimes. The structure aligns incentives between AI developers and downstream stakeholders, transforms safety practices into insurable standards, and enables modular oversight through adaptive eligibility criteria. By focusing on risk-transfer mechanisms rather than prescriptive rules, this framework seeks to render AI safety a structural feature of the innovation ecosystem itself--integrated into capital markets, not external to them. The paper concludes with a legal and administrative feasibility analysis, proposing avenues for statutory authorization and agency placement within existing federal structures.",['Nicholas Stetler'],2025-04-02T21:02:19Z,http://arxiv.org/abs/2504.02127v1,Finance & Economics,AI in Insurance,"proposal proposes a three-tiered insurance architecture for governing AI systems . mandatory private liability insurance for frontier model developers . industry-administered risk pool to absorb recurring, non-catastrophic losses ."
Creative Uses of AI Systems and their Explanations: A Case Study from   Insurance,"Recent works have recognized the need for human-centered perspectives when designing and evaluating human-AI interactions and explainable AI methods. Yet, current approaches fall short at intercepting and managing unexpected user behavior resulting from the interaction with AI systems and explainability methods of different stake-holder groups. In this work, we explore the use of AI and explainability methods in the insurance domain. In an qualitative case study with participants with different roles and professional backgrounds, we show that AI and explainability methods are used in creative ways in daily workflows, resulting in a divergence between their intended and actual use. Finally, we discuss some recommendations for the design of human-AI interactions and explainable AI methods to manage the risks and harness the potential of unexpected user behavior.","['Michaela Benk', 'Raphael Weibel', 'Andrea Ferrario']",2022-05-02T14:24:11Z,http://arxiv.org/abs/2205.00931v2,Finance & Economics,AI in Insurance,"we explore the use of AI and explainability methods in the insurance domain . we show that they are used in creative ways, resulting in a divergence between intended and actual use ."
Discrimination-free Insurance Pricing with Privatized Sensitive   Attributes,"Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints on insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that are tailored to the insurance domain, using only privatized sensitive attributes. Notably, our approach ensures statistical guarantees, does not require direct access to sensitive attributes, and adapts to varying transparency requirements, addressing regulatory demands while ensuring fairness in insurance pricing.","['Tianhe Zhang', 'Suhan Liu', 'Peng Shi']",2025-04-16T05:29:11Z,http://arxiv.org/abs/2504.11775v2,Finance & Economics,AI in Insurance,fairness has emerged as a critical consideration in the landscape of machine learning algorithms . the field of algorithmic bias has introduced various fairness concepts . not all sectors have embraced these fairness principles to the same extent .
The scope for AI-augmented interpretation of building blueprints in   commercial and industrial property insurance,"This report, commissioned by the WTW research network, investigates the use of AI in property risk assessment. It (i) reviews existing work on risk assessment in commercial and industrial properties and automated information extraction from building blueprints; and (ii) presents an exploratory 'proof-of concept-solution' exploring the feasibility of using machine learning for the automated extraction of information from building blueprints to support insurance risk assessment.","['Long Chen', 'Mao Ye', 'Alistair Milne', 'John Hillier', 'Frances Oglesby']",2022-04-29T16:52:04Z,http://arxiv.org/abs/2205.01671v2,Finance & Economics,AI in Insurance,report investigates the use of AI in property risk assessment . it reviews existing work on risk assessment in commercial and industrial properties . and presents an exploratory 'proof-of concept-solution' exploring the feasibility of using machine learning for the automated extraction of information from building blueprints .
Machine Learning Recommendation System For Health Insurance Decision   Making In Nigeria,"The uptake of health insurance has been poor in Nigeria, a significant step to improving this includes improved awareness, access to information and tools to support decision making. Artificial intelligence (AI) based recommender systems have gained popularity in helping individuals find movies, books, music, and different types of products on the internet including diverse applications in healthcare. The content-based methodology (item-based approach) was employed in the recommender system. We applied both the K-Nearest Neighbor (KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our chosen algorithm after several evaluations based of their outcomes in comparison with domain knowledge. The recommender system takes into consideration the choices entered by the user, filters the health management organization (HMO) data by location and chosen prices. It then recommends the top 3 HMOs with closest similarity in services offered. A recommendation tool to help people find and select the best health insurance plan for them is useful in reducing the barrier of accessing health insurance. Users are empowered to easily find appropriate information on available plans, reduce cognitive overload in dealing with over 100 options available in the market and easily see what matches their financial capacity.","['Ayomide Owoyemi', 'Emmanuel Nnaemeka', 'Temitope O. Benson', 'Ronald Ikpe', 'Blessing Nwachukwu', 'Temitope Isedowo']",2023-05-18T04:54:23Z,http://arxiv.org/abs/2305.10708v1,Finance & Economics,AI in Insurance,the uptake of health insurance has been poor in Nigeria . a recommendation tool is useful in reducing the barrier of accessing health insurance . users are empowered to easily find appropriate information on available plans .
Blockchain-based Immutable Evidence and Decentralized Loss Adjustment   for Autonomous Vehicle Accidents in Insurance,"In case of an accident between two autonomous vehicles equipped with emerging technologies, how do we apportion liability among the various players? A special liability regime has not even yet been established for damages that may arise due to the accidents of autonomous vehicles. Would the immutable, time-stamped sensor records of vehicles on distributed ledger help define the intertwined relations of liability subjects right through the accident? What if the synthetic media created through deepfake gets involved in the insurance claims? While integrating AI-powered anomaly or deepfake detection into automated insurance claims processing helps to prevent insurance fraud, it is only a matter of time before deepfake becomes nearly undetectable even to elaborate forensic tools. This paper proposes a blockchain-based insurtech decentralized application to check the authenticity and provenance of the accident footage and also to decentralize the loss-adjusting process through a hybrid of decentralized and centralized databases using smart contracts.",['Mehmet Parlak'],2023-03-29T21:50:13Z,http://arxiv.org/abs/2303.18130v1,Finance & Economics,AI in Insurance,the paper proposes a blockchain-based insurtech decentralized application to check the authenticity of the accident footage . it also proposes to decentralize the loss-adjusting process through a hybrid of decentralized and centralized databases .
AutoFraudNet: A Multimodal Network to Detect Fraud in the Auto Insurance   Industry,"In the insurance industry detecting fraudulent claims is a critical task with a significant financial impact. A common strategy to identify fraudulent claims is looking for inconsistencies in the supporting evidence. However, this is a laborious and cognitively heavy task for human experts as insurance claims typically come with a plethora of data from different modalities (e.g. images, text and metadata). To overcome this challenge, the research community has focused on multimodal machine learning frameworks that can efficiently reason through multiple data sources. Despite recent advances in multimodal learning, these frameworks still suffer from (i) challenges of joint-training caused by the different characteristics of different modalities and (ii) overfitting tendencies due to high model complexity. In this work, we address these challenges by introducing a multimodal reasoning framework, AutoFraudNet (Automobile Insurance Fraud Detection Network), for detecting fraudulent auto-insurance claims. AutoFraudNet utilizes a cascaded slow fusion framework and state-of-the-art fusion block, BLOCK Tucker, to alleviate the challenges of joint-training. Furthermore, it incorporates a light-weight architectural design along with additional losses to prevent overfitting. Through extensive experiments conducted on a real-world dataset, we demonstrate: (i) the merits of multimodal approaches, when compared to unimodal and bimodal methods, and (ii) the effectiveness of AutoFraudNet in fusing various modalities to boost performance (over 3\% in PR AUC).","['Azin Asgarian', 'Rohit Saha', 'Daniel Jakubovitz', 'Julia Peyre']",2023-01-15T13:50:32Z,http://arxiv.org/abs/2301.07526v1,Finance & Economics,AI in Insurance,"autofraudNet is a multimodal reasoning framework for detecting fraudulent auto-insurance claims . it uses a cascaded slow fusion framework and state-of-the-art fusion block, BLOCK Tucker . the framework incorporates a light-weight architectural design along with additional losses ."
AI-Enhanced Business Process Automation: A Case Study in the Insurance   Domain Using Object-Centric Process Mining,"Recent advancements in Artificial Intelligence (AI), particularly Large Language Models (LLMs), have enhanced organizations' ability to reengineer business processes by automating knowledge-intensive tasks. This automation drives digital transformation, often through gradual transitions that improve process efficiency and effectiveness. To fully assess the impact of such automation, a data-driven analysis approach is needed - one that examines how traditional and AI-enhanced process variants coexist during this transition. Object-Centric Process Mining (OCPM) has emerged as a valuable method that enables such analysis, yet real-world case studies are still needed to demonstrate its applicability. This paper presents a case study from the insurance sector, where an LLM was deployed in production to automate the identification of claim parts, a task previously performed manually and identified as a bottleneck for scalability. To evaluate this transformation, we apply OCPM to assess the impact of AI-driven automation on process scalability. Our findings indicate that while LLMs significantly enhance operational capacity, they also introduce new process dynamics that require further refinement. This study also demonstrates the practical application of OCPM in a real-world setting, highlighting its advantages and limitations.","['Shahrzad Khayatbashi', 'Viktor Sjölind', 'Anders Granåker', 'Amin Jalali']",2025-04-24T06:43:29Z,http://arxiv.org/abs/2504.17295v1,Finance & Economics,AI in Insurance,recent advancements in AI have enhanced organizations' ability to reengineer business processes by automating knowledge-intensive tasks . Object-Centric Process Mining (OCPM) has emerged as a valuable method that enables such analysis . but real-world case studies are still needed to demonstrate its applicability .
Insuring Uninsurable Risks from AI: Government as Insurer of Last Resort,"Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.",['Cristian Trout'],2024-09-10T17:41:24Z,http://arxiv.org/abs/2409.06672v3,Finance & Economics,AI in Insurance,"experts believe that AI systems will sooner or later pose uninsurable risks . this paper proposes a government-provided, mandatory indemnification program for AI developers . the program uses risk-priced indemnity fees to induce socially optimal levels of care ."
Analyzing the impact of two major factors on medical expenses paid by   health insurance organization in Iran,"In healthcare scope, the profound role of insurance companies is undeniable. Health insurance establishments main responsibility is to support public health financially and promote the quality of health services. Governments subsidies to healthcare insurance, insured payments and insurance companies costs must be specified in such a way that both people and insurers mutually benefit. In this research, we propose a model for determining healthcare costs paid by health insurance organization with regard to two major factors, the geographical regions where the patients live and the seasons when they receive service, using two-way ANOVA method. Since both effects are found to be significant, allocating different insurance costs to the people residing in different regions, and also changing the patterns of insurance extension in different seasons with regard to the results derived from the research, can detract healthcare costs and give more satisfaction to the lower income insured patients.","['Seyed Nasser Moosavi', 'Ashkan Khalifeh', 'Ali Shojaee', 'Masoud Abessi']",2019-11-10T16:51:29Z,http://arxiv.org/abs/1911.03961v1,Finance & Economics,AI in Insurance,the profound role of insurance companies is undeniable in healthcare scope . researchers propose a model for determining healthcare costs paid by health insurance organization . allocating different insurance costs to the people residing in different regions can detract healthcare costs .
Information Content of DSGE Forecasts,"This paper examines the question whether information is contained in forecasts from DSGE models beyond that contained in lagged values, which are extensively used in the models. Four sets of forecasts are examined. The results are encouraging for DSGE forecasts of real GDP. The results suggest that there is information in the DSGE forecasts not contained in forecasts based only on lagged values and that there is no information in the lagged-value forecasts not contained in the DSGE forecasts. The opposite is true for forecasts of the GDP deflator.   Keywords: DSGE forecasts, Lagged values   JEL Classification Codes: E10, E17, C53",['Ray Fair'],2018-08-08T18:29:54Z,http://arxiv.org/abs/1808.02910v1,Finance & Economics,Economic Forecasting,four sets of forecasts are examined . results are encouraging for DSGE forecasts of real GDP . findings suggest there is information not contained in forecasts based on lagged values .
Forecasting for monetary policy,"This paper discusses three key themes in forecasting for monetary policy highlighted in the Bernanke (2024) review: the challenges in economic forecasting, the conditional nature of central bank forecasts, and the importance of forecast evaluation. In addition, a formal evaluation of the Bank of England's inflation forecasts indicates that, despite the large forecast errors in recent years, they were still accurate relative to common benchmarks.",['Laura Coroneo'],2025-01-13T15:04:12Z,http://arxiv.org/abs/2501.07386v1,Finance & Economics,Economic Forecasting,a formal evaluation of the Bank of England's inflation forecasts indicates they were still accurate relative to common benchmarks . the paper discusses three key themes highlighted in the Bernanke (2024) review .
Flexible global forecast combinations,"Forecast combination -- the aggregation of individual forecasts from multiple experts or models -- is a proven approach to economic forecasting. To date, research on economic forecasting has concentrated on local combination methods, which handle separate but related forecasting tasks in isolation. Yet, it has been known for over two decades in the machine learning community that global methods, which exploit task-relatedness, can improve on local methods that ignore it. Motivated by the possibility for improvement, this paper introduces a framework for globally combining forecasts while being flexible to the level of task-relatedness. Through our framework, we develop global versions of several existing forecast combinations. To evaluate the efficacy of these new global forecast combinations, we conduct extensive comparisons using synthetic and real data. Our real data comparisons, which involve forecasts of core economic indicators in the Eurozone, provide empirical evidence that the accuracy of global combinations of economic forecasts can surpass local combinations.","['Ryan Thompson', 'Yilin Qian', 'Andrey L. Vasnev']",2022-07-15T07:23:06Z,http://arxiv.org/abs/2207.07318v3,Finance & Economics,Economic Forecasting,forecast combination is a proven approach to economic forecasting . global methods that exploit task-relatedness can improve on local methods . real data comparisons provide empirical evidence that global combinations can surpass local combinations .
Probabilistic Load Forecasting via Point Forecast Feature Integration,"Short-term load forecasting is a critical element of power systems energy management systems. In recent years, probabilistic load forecasting (PLF) has gained increased attention for its ability to provide uncertainty information that helps to improve the reliability and economics of system operation performances. This paper proposes a two-stage probabilistic load forecasting framework by integrating point forecast as a key probabilistic forecasting feature into PLF. In the first stage, all related features are utilized to train a point forecast model and also obtain the feature importance. In the second stage the forecasting model is trained, taking into consideration point forecast features, as well as selected feature subsets. During the testing period of the forecast model, the final probabilistic load forecast results are leveraged to obtain both point forecasting and probabilistic forecasting. Numerical results obtained from ISO New England demand data demonstrate the effectiveness of the proposed approach in the hour-ahead load forecasting, which uses the gradient boosting regression for the point forecasting and quantile regression neural networks for the probabilistic forecasting.","['Qicheng Chang', 'Yishen Wang', 'Xiao Lu', 'Di Shi', 'Haifeng Li', 'Jiajun Duan', 'Zhiwei Wang']",2019-03-26T05:35:46Z,http://arxiv.org/abs/1903.10684v1,Finance & Economics,Economic Forecasting,the paper proposes a two-stage probabilistic load forecasting framework . it integrates point forecast as a key probabilistic forecasting feature into PLF . results demonstrate the effectiveness of the proposed approach in the hour-ahead load forecast .
Forecasting skill of a crowd-prediction platform: A comparison of   exchange rate forecasts,"Open online crowd-prediction platforms are increasingly used to forecast trends and complex events. Despite the large body of research on crowd-prediction and forecasting tournaments, online crowd-prediction platforms have never been directly compared to other forecasting methods. In this analysis, exchange rate crowd-predictions made on Metaculus are compared to predictions made by the random-walk, a statistical model considered extremely hard-to-beat. The random-walk provides less erroneous forecasts, but the crowd-prediction does very well. By using the random-walk as a benchmark, this analysis provides a rare glimpse into the forecasting skill displayed on open online crowd-prediction platforms.",['Niklas Valentin Lehmann'],2023-12-14T16:15:16Z,http://arxiv.org/abs/2312.09081v2,Finance & Economics,Economic Forecasting,"open online crowd-prediction platforms are increasingly used to forecast trends . the random-walk provides less erroneous forecasts, but does very well . this analysis provides a rare glimpse into the forecasting skill displayed on open platforms ."
Machine learning and economic forecasting: the role of international   trade networks,"This study examines the effects of de-globalization trends on international trade networks and their role in improving forecasts for economic growth. Using section-level trade data from nearly 200 countries from 2010 to 2022, we identify significant shifts in the network topology driven by rising trade policy uncertainty. Our analysis highlights key global players through centrality rankings, with the United States, China, and Germany maintaining consistent dominance. Using a horse race of supervised regressors, we find that network topology descriptors evaluated from section-specific trade networks substantially enhance the quality of a country's GDP growth forecast. We also find that non-linear models, such as Random Forest, XGBoost, and LightGBM, outperform traditional linear models used in the economics literature. Using SHAP values to interpret these non-linear model's predictions, we find that about half of most important features originate from the network descriptors, underscoring their vital role in refining forecasts. Moreover, this study emphasizes the significance of recent economic performance, population growth, and the primary sector's influence in shaping economic growth predictions, offering novel insights into the intricacies of economic growth forecasting.","['Thiago C. Silva', 'Paulo V. B. Wilhelm', 'Diego R. Amancio']",2024-04-11T21:04:56Z,http://arxiv.org/abs/2404.08712v1,Finance & Economics,Economic Forecasting,study examines effects of de-globalization trends on international trade networks . section-level trade data from nearly 200 countries from 2010 to 2022 are used . analysis highlights key global players through centrality rankings .
An Initial Study on Load Forecasting Considering Economic Factors,"This paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting (LF). In LF, the positive forecasting errors often have different economic impact from the negative forecasting errors. Considering this difference, a new objective function is proposed to put different prices on the positive and negative forecasting errors. QR is used to find the optimal solution of the proposed objective function. Using normalized net energy load of New England network, the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method. The simulation results show that the proposed method is more effective in reducing the economic cost of the LF errors than the other three methods.","['Hossein Sangrody', 'Ning Zhou']",2017-03-19T00:53:48Z,http://arxiv.org/abs/1703.06375v1,Finance & Economics,Economic Forecasting,"the paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting . positive and negative forecasting errors often have different economic impact . the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method ."
The application of sub-seasonal to seasonal (S2S) predictions for   hydropower forecasting,"Inflow forecasts play an essential role in the management of hydropower reservoirs. Forecasts help operators schedule power generation in advance to maximise economic value, mitigate downstream flood risk, and meet environmental requirements. The horizon of operational inflow forecasts is often limited in range to ~2 weeks ahead, marking the predictability barrier of deterministic weather forecasts. Reliable inflow forecasts in the sub-seasonal to seasonal (S2S) range would allow operators to take proactive action to mitigate risks of adverse weather conditions, thereby improving water management and increasing revenue. This study outlines a method of deriving skilful S2S inflow forecasts using a case study reservoir in the Scottish Highlands. We generate ensemble inflow forecasts by training a linear regression model for the observed inflow onto S2S ensemble precipitation predictions from the European Centre for Medium-range Weather Forecasting (ECMWF). Subsequently, post-processing techniques from Ensemble Model Output Statistics are applied to derive calibrated S2S probabilistic inflow forecasts, without the application of a separate hydrological model. We find the S2S probabilistic inflow forecasts hold skill relative to climatological forecasts up to 6 weeks ahead. The inflow forecasts hold greater skill during winter compared with summer. The forecasts, however, struggle to predict high summer inflows, even at short lead-times. The potential for the S2S probabilistic inflow forecasts to improve water management and deliver increased economic value is confirmed using a stylised cost model. While applied to hydropower forecasting, the results and methods presented here are relevant to broader fields of water management and S2S forecasting applications.","['Robert M. Graham', 'Jethro Browell', 'Douglas Bertram', 'Christopher J. White']",2021-08-13T14:30:43Z,http://arxiv.org/abs/2108.06269v1,Finance & Economics,Economic Forecasting,forecasts help operators schedule power generation in advance to maximise economic value . the horizon of operational inflow forecasts is often limited in range to 2 weeks ahead . forecasts in the sub-seasonal to seasonal (S2S) range would allow operators to take proactive action .
Predicting Inflation: Professional Experts Versus No-Change Forecasts,"We compare forecasts of United States inflation from the Survey of Professional Forecasters (SPF) to predictions made by simple statistical techniques. In nowcasting, economic expertise is persuasive. When projecting beyond the current quarter, novel yet simplistic probabilistic no-change forecasts are equally competitive. We further interpret surveys as ensembles of forecasts, and show that they can be used similarly to the ways in which ensemble prediction systems have transformed weather forecasting. Then we borrow another idea from weather forecasting, in that we apply statistical techniques to postprocess the SPF forecast, based on experience from the recent past. The foregoing conclusions remain unchanged after survey postprocessing.","['Tilmann Gneiting', 'Thordis L. Thorarinsdottir']",2010-10-12T08:01:37Z,http://arxiv.org/abs/1010.2318v1,Finance & Economics,Economic Forecasting,the survey of professional forecasters (SPF) forecasts inflation in the u.s. compared to predictions made by simple statistical techniques . the results remain unchanged after survey postprocessing .
Prediction intervals for economic fixed-event forecasts,"The fixed-event forecasting setup is common in economic policy. It involves a sequence of forecasts of the same (`fixed') predictand, so that the difficulty of the forecasting problem decreases over time. Fixed-event point forecasts are typically published without a quantitative measure of uncertainty. To construct such a measure, we consider forecast postprocessing techniques tailored to the fixed-event case. We develop regression methods that impose constraints motivated by the problem at hand, and use these methods to construct prediction intervals for gross domestic product (GDP) growth in Germany and the US.","['Fabian Krüger', 'Hendrik Plett']",2022-10-24T19:35:24Z,http://arxiv.org/abs/2210.13562v3,Finance & Economics,Economic Forecasting,a fixed-event forecasting setup is common in economic policy . it involves a sequence of forecasts of the same (fixed') predictand . the difficulty of the forecasting problem decreases over time .
What Does ChatGPT Make of Historical Stock Returns? Extrapolation and   Miscalibration in LLM Stock Return Forecasts,"We examine how large language models (LLMs) interpret historical stock returns and compare their forecasts with estimates from a crowd-sourced platform for ranking stocks. While stock returns exhibit short-term reversals, LLM forecasts over-extrapolate, placing excessive weight on recent performance similar to humans. LLM forecasts appear optimistic relative to historical and future realized returns. When prompted for 80% confidence interval predictions, LLM responses are better calibrated than survey evidence but are pessimistic about outliers, leading to skewed forecast distributions. The findings suggest LLMs manifest common behavioral biases when forecasting expected returns but are better at gauging risks than humans.","['Shuaiyu Chen', 'T. Clifton Green', 'Huseyin Gulen', 'Dexin Zhou']",2024-09-17T20:23:36Z,http://arxiv.org/abs/2409.11540v1,Finance & Economics,Economic Forecasting,"large language models (LLMs) interpret historical stock returns . stock returns exhibit short-term reversals, but LLM forecasts over-extrapolate . results suggest LLMs manifest common behavioral biases ."
The Memorization Problem: Can We Trust LLMs' Economic Forecasts?,"Large language models (LLMs) cannot be trusted for economic forecasts during periods covered by their training data. We provide the first systematic evaluation of LLMs' memorization of economic and financial data, including major economic indicators, news headlines, stock returns, and conference calls. Our findings show that LLMs can perfectly recall the exact numerical values of key economic variables from before their knowledge cutoff dates. This recall appears to be randomly distributed across different dates and data types. This selective perfect memory creates a fundamental issue -- when testing forecasting capabilities before their knowledge cutoff dates, we cannot distinguish whether LLMs are forecasting or simply accessing memorized data. Explicit instructions to respect historical data boundaries fail to prevent LLMs from achieving recall-level accuracy in forecasting tasks. Further, LLMs seem exceptional at reconstructing masked entities from minimal contextual clues, suggesting that masking provides inadequate protection against motivated reasoning. Our findings raise concerns about using LLMs to forecast historical data or backtest trading strategies, as their apparent predictive success may merely reflect memorization rather than genuine economic insight. Any application where future knowledge would change LLMs' outputs can be affected by memorization. In contrast, consistent with the absence of data contamination, LLMs cannot recall data after their knowledge cutoff date.","['Alejandro Lopez-Lira', 'Yuehua Tang', 'Mingyin Zhu']",2025-04-20T23:36:27Z,http://arxiv.org/abs/2504.14765v1,Finance & Economics,Economic Forecasting,large language models (LLMs) cannot be trusted for economic forecasts . we provide the first systematic evaluation of LLMs' memorization of data . recall appears to be randomly distributed across different dates and data types .
LfEdNet: A Task-based Day-ahead Load Forecasting Model for Stochastic   Economic Dispatch,"Load forecasting is one of the most important and studied topics in modern power systems. Most of the existing researches on day-ahead load forecasting try to build a good model to improve the forecasting accuracy. The forecasted load is then used as the input to generation scheduling with the ultimate goal of minimizing the cost of generation schedules. However, existing day-ahead load forecasting models do not consider this ultimate goal at the training/forecasting stage. This paper proposes a task-based day-ahead load forecasting model labeled as LfEdNet that combines two individual layers in one model, including a load forecasting layer based on deep neural network (Lf layer) and a day-ahead stochastic economic dispatch (SED) layer (Ed layer). The training of LfEdNet aims to minimize the cost of the day-ahead SED in the Ed layer by updating the parameters of the Lf layer. Sequential quadratic programming (SQP) is used to solve the day-ahead SED in the Ed layer. The test results demonstrate that the forecasted results produced by LfEdNet can lead to lower cost of day-ahead SED while maintaining a relatively high forecasting accuracy.","['Jiayu Han', 'Lei Yan', 'Zuyi Li']",2020-08-16T23:09:30Z,http://arxiv.org/abs/2008.07025v1,Finance & Economics,Economic Forecasting,the paper proposes a task-based day-ahead load forecasting model labeled as LfEdNet . the model combines two individual layers in one model . it aims to minimize the cost of the day ahead stochastic economic dispatch (SED)
"Econophysics of Macroeconomics: ""Action-at-a-Distance"" and Waves","We present macroeconomic model that describes evolution of macroeconomic variables and macroeconomic waves on economic space. Risk ratings of economic agents play role of their coordinates on economic space. Aggregation of economic variables like Assets and Investment, Credits and Loans of economic agents at point x define corresponding macroeconomic variables as functions of time t and coordinates x on economic space. Evolution of macroeconomic variables is determined by economic and financial transactions between economic agents. Such transactions can occur between economic agents with any coordinates x and y and that reflect non-local ""action-at-a-distance"" character of internal macroeconomic interactions. For instance, Buy-Sell transactions between points x and y on economic space define dynamics of Assets at point x and Investment at point y. Aggregates of transactions between economic agents at point x and y on economic space define economic fields as functions of two coordinates. To describe dynamics of economic fields on economic space we derive hydrodynamic-like equations. For simple models of interactions between economic fields we derive hydrodynamic-like equations in a closed form and obtain wave equations for their perturbations. Economic field waves propagate on economic space and their amplitudes can grow up as exponent in time and may disturb economic stability. Diversities of macroeconomic and financial waves on economic space in simple models uncover importance of wave processes for macroeconomic modeling and forecasting.",['Victor Olkhov'],2017-02-09T09:36:46Z,http://arxiv.org/abs/1702.02763v1,Finance & Economics,Economic Forecasting,risk ratings of economic agents play role of their coordinates on economic space . evolution of macroeconomic variables is determined by economic and financial transactions . economic field waves propagate and their amplitudes can grow up as exponent in time .
Machine Learning for Economic Forecasting: An Application to China's GDP   Growth,"This paper aims to explore the application of machine learning in forecasting Chinese macroeconomic variables. Specifically, it employs various machine learning models to predict the quarterly real GDP growth of China, and analyzes the factors contributing to the performance differences among these models. Our findings indicate that the average forecast errors of machine learning models are generally lower than those of traditional econometric models or expert forecasts, particularly in periods of economic stability. However, during certain inflection points, although machine learning models still outperform traditional econometric models, expert forecasts may exhibit greater accuracy in some instances due to experts' more comprehensive understanding of the macroeconomic environment and real-time economic variables. In addition to macroeconomic forecasting, this paper employs interpretable machine learning methods to identify the key attributive variables from different machine learning models, aiming to enhance the understanding and evaluation of their contributions to macroeconomic fluctuations.","['Yanqing Yang', 'Xingcheng Xu', 'Jinfeng Ge', 'Yan Xu']",2024-07-04T03:04:55Z,http://arxiv.org/abs/2407.03595v1,Finance & Economics,Economic Forecasting,this paper aims to explore the application of machine learning in forecasting macroeconomic variables . it employs various machine learning models to predict the quarterly real GDP growth of china . the average forecast errors are generally lower than those of traditional econometric models .
Econometrics of Machine Learning Methods in Economic Forecasting,"This paper surveys the recent advances in machine learning method for economic forecasting. The survey covers the following topics: nowcasting, textual data, panel and tensor data, high-dimensional Granger causality tests, time series cross-validation, classification with economic losses.","['Andrii Babii', 'Eric Ghysels', 'Jonas Striaukas']",2023-08-21T19:19:34Z,http://arxiv.org/abs/2308.10993v1,Finance & Economics,Economic Forecasting,"this paper surveys the recent advances in machine learning method for economic forecasting . the survey covers the following topics: nowcasting, textual data, panel and tensor data ."
The value of forecasts: Quantifying the economic gains of accurate   quarter-hourly electricity price forecasts,"We propose a multivariate elastic net regression forecast model for German quarter-hourly electricity spot markets. While the literature is diverse on day-ahead prediction approaches, both the intraday continuous and intraday call-auction prices have not been studied intensively with a clear focus on predictive power. Besides electricity price forecasting, we check for the impact of early day-ahead (DA) EXAA prices on intraday forecasts. Another novelty of this paper is the complementary discussion of economic benefits. A precise estimation is worthless if it cannot be utilized. We elaborate possible trading decisions based upon our forecasting scheme and analyze their monetary effects. We find that even simple electricity trading strategies can lead to substantial economic impact if combined with a decent forecasting technique.","['Christopher Kath', 'Florian Ziel']",2018-11-21T06:05:38Z,http://arxiv.org/abs/1811.08604v1,Finance & Economics,Economic Forecasting,we propose a multivariate elastic net regression forecast model for german spot markets . we check for the impact of early day-ahead (DA) EXAA prices on intraday forecasts . even simple electricity trading strategies can lead to substantial economic impact .
Statistical and Economic Evaluation of Time Series Models for   Forecasting Arrivals at Call Centers,"Call centers' managers are interested in obtaining accurate point and distributional forecasts of call arrivals in order to achieve an optimal balance between service quality and operating costs. We present a strategy for selecting forecast models of call arrivals which is based on three pillars: (i) flexibility of the loss function; (ii) statistical evaluation of forecast accuracy; (iii) economic evaluation of forecast performance using money metrics. We implement fourteen time series models and seven forecast combination schemes on three series of daily call arrivals. Although we focus mainly on point forecasts, we also analyze density forecast evaluation. We show that second moments modeling is important both for point and density forecasting and that the simple Seasonal Random Walk model is always outperformed by more general specifications. Our results suggest that call center managers should invest in the use of forecast models which describe both first and second moments of call arrivals.","['Andrea Bastianin', 'Marzio Galeotti', 'Matteo Manera']",2018-04-23T09:57:42Z,http://arxiv.org/abs/1804.08315v1,Finance & Economics,Economic Forecasting,call center managers are interested in obtaining accurate point and distributional forecasts of call arrivals . they want to achieve an optimal balance between service quality and operating costs . we implement fourteen time series models and seven forecast combination schemes .
Forecasting with Feedback,"Systematically biased forecasts are typically interpreted as evidence of forecasters' irrationality and/or asymmetric loss. In this paper we propose an alternative explanation: when forecasts inform economic policy decisions, and the resulting actions affect the realization of the forecast target itself, forecasts may be optimally biased even under quadratic loss. The result arises in environments in which the forecaster is uncertain about the decision maker's reaction to the forecast, which is presumably the case in most applications. We illustrate the empirical relevance of our theory by reviewing some stylized properties of Green Book inflation forecasts and relating them to the predictions from our model. Our results point out that the presence of policy feedback poses a challenge to traditional tests of forecast rationality.","['Robert P. Lieli', 'Augusto Nieto-Barthaburu']",2023-08-29T06:50:13Z,http://arxiv.org/abs/2308.15062v3,Finance & Economics,Economic Forecasting,"systematically biased forecasts are typically interpreted as evidence of forecasters' irrationality and/or asymmetric loss . in environments in which the forecaster is uncertain about the decision maker's reaction to the forecast, it may be optimally biased even under quadratic loss"
Indexing Economic Fluctuation Narratives from Keiki Watchers Survey,"In this paper, we design indices of economic fluctuation narratives derived from economic surveys. Companies, governments, and investors rely on key metrics like GDP and industrial production indices to predict economic trends. However, they have yet to effectively leverage the wealth of information contained in economic text, such as causal relationships, in their economic forecasting. Therefore, we design indices of economic fluctuation from economic surveys by using our previously proposed narrative framework. From the evaluation results, it is observed that the proposed indices had a stronger correlation with cumulative lagging diffusion index than other types of diffusion indices.","['Eriko Shigetsugu', 'Hiroki Sakaji', 'Itsuki Noda']",2024-12-02T08:32:02Z,http://arxiv.org/abs/2412.01265v1,Finance & Economics,Economic Forecasting,"in this paper, we design indices of economic fluctuation narratives . companies, governments, and investors rely on key metrics like GDP and industrial production . but, they have yet to effectively leverage the wealth of information contained in economic text ."
A Risk-Sensitive Portfolio Optimization Problem with Fixed Incomes   Securities,"We discuss a class of risk-sensitive portfolio optimization problems. We consider the portfolio optimization model investigated by Nagai in 2003. The model by its nature can include fixed income securities as well in the portfolio. Under fairly general conditions, we prove the existence of optimal portfolio in both finite and infinite horizon problems.","['Mayank Goel', 'K. Suresh Kumar']",2007-11-17T05:46:12Z,http://arxiv.org/abs/0711.2718v1,Finance & Economics,Portfolio Optimization,we prove the existence of optimal portfolio in finite and infinite horizon problems . the model by its nature can include fixed income securities as well .
Portfolio Optimization in the Stochastic Portfolio Theory Framework,"I discuss some theoretical results with a view to motivate some practical choices in portfolio optimization. Even though the setting is not completely general (for example, the covariance matrix is assumed to be non-singular), I attempt to highlight the features that have practical relevance. The mathematical setting is Stochastic Portfolio Theory, which is flexible enough to describe most realistic assets, and it has been successfully employed for managing equity portfolios since 1987.",['Vassilios Papathanakos'],2016-01-28T02:40:54Z,http://arxiv.org/abs/1601.07628v1,Finance & Economics,Portfolio Optimization,the mathematical setting is Stochastic Portfolio Theory . it has been successfully employed for managing equity portfolios since 1987 .
The Feedback Effect of Hedging in Portfolio Optimization,"In this short note, we will show how to optimize the portfolio of a large trader whose hedging strategy affects the price of his assets.",['Pierre Henry-Labordere'],2004-04-21T22:07:00Z,http://arxiv.org/abs/cond-mat/0404520v1,Finance & Economics,Portfolio Optimization,"in this short note, we will show how to optimize the portfolio of a large trader . hedging strategy affects the price of his assets."
Fat Tailed Factors,"Standard, PCA-based factor analysis suffers from a number of well known problems due to the random nature of pairwise correlations of asset returns. We analyse an alternative based on ICA, where factors are identified based on their non-Gaussianity, instead of their variance. Generalizations of portfolio construction to the ICA framework leads to two semi-optimal portfolio construction methods: a fat-tailed portfolio, which maximises return per unit of non-Gaussianity, and the hybrid portfolio, which asymptotically reduces variance and non-Gaussianity in parallel. For fat-tailed portfolios, the portfolio weights scale like performance to the power of $1/3$, as opposed to linear scaling of Kelly portfolios; such portfolio construction significantly reduces portfolio concentration, and the winner-takes-all problem inherent in Kelly portfolios. For hybrid portfolios, the variance is diversified at the same rate as Kelly PCA-based portfolios, but excess kurtosis is diversified much faster than in Kelly, at the rate of $n^{-2}$ compared to Kelly portfolios' $n^{-1}$ for increasing number of components $n$.",['Jan Rosenzweig'],2020-11-27T10:16:44Z,http://arxiv.org/abs/2011.13637v6,Finance & Economics,Portfolio Optimization,"standard, PCA-based factor analysis suffers from a number of well known problems . factors are identified based on their non-Gaussianity, instead of their variance . fat-tailed and hybrid portfolios maximise return per unit of non-gaussiality ."
LoCoV: low dimension covariance voting algorithm for portfolio   optimization,"Minimum-variance portfolio optimizations rely on accurate covariance estimator to obtain optimal portfolios. However, it usually suffers from large error from sample covariance matrix when the sample size $n$ is not significantly larger than the number of assets $p$. We analyze the random matrix aspects of portfolio optimization and identify the order of errors in sample optimal portfolio weight and show portfolio risk are underestimated when using samples. We also provide LoCoV (low dimension covariance voting) algorithm to reduce error inherited from random samples. From various experiments, LoCoV is shown to outperform the classical method by a large margin.","['JunTao Duan', 'Ionel Popescu']",2022-04-01T04:42:56Z,http://arxiv.org/abs/2204.00204v1,Finance & Economics,Portfolio Optimization,"minimum-variance portfolio optimizations rely on accurate covariance estimator to obtain optimal portfolios . but, it usually suffers from large error when the sample size $n$ is not significantly larger than the number of assets $p$ ."
Dynamic optimization of a portfolio,"In this paper, we consider the problem of optimization of a portfolio consisting of securities. An investor with an initial capital, is interested in constructing a portfolio of securities. If the prices of securities change, the investor shall decide on reallocation of the portfolio. At each moment of time, the prices of securities change and the investor is interested in constructing a dynamic portfolio of securities. The investor wishes to maximize the value of his portfolio at the end of time $T$. We use a novel theoretical approach based on dynamic programming to solve the age old problem of dynamic programming. We consider two cases i.e. Deterministic and Stochastic to approach the problem and show how the portfolio is maximized using dynamic programming.","['Oleg Malafeyev', 'Achal Awasthi']",2017-12-02T10:25:46Z,http://arxiv.org/abs/1712.00585v1,Finance & Economics,Portfolio Optimization,"in this paper, we consider the problem of optimization of a portfolio consisting of securities . if the prices of securities change, the investor shall decide on reallocation of the portfolio . we use a novel theoretical approach based on dynamic programming to solve the age old problem"
Portfolio Optimization: A Comparative Study,"Portfolio optimization has been an area that has attracted considerable attention from the financial research community. Designing a profitable portfolio is a challenging task involving precise forecasting of future stock returns and risks. This chapter presents a comparative study of three portfolio design approaches, the mean-variance portfolio (MVP), hierarchical risk parity (HRP)-based portfolio, and autoencoder-based portfolio. These three approaches to portfolio design are applied to the historical prices of stocks chosen from ten thematic sectors listed on the National Stock Exchange (NSE) of India. The portfolios are designed using the stock price data from January 1, 2018, to December 31, 2021, and their performances are tested on the out-of-sample data from January 1, 2022, to December 31, 2022. Extensive results are analyzed on the performance of the portfolios. It is observed that the performance of the MVP portfolio is the best on the out-of-sample data for the risk-adjusted returns. However, the autoencoder portfolios outperformed their counterparts on annual returns.","['Jaydip Sen', 'Subhasis Dasgupta']",2023-07-11T06:56:06Z,http://arxiv.org/abs/2307.05048v1,Finance & Economics,Portfolio Optimization,portfolio optimization has attracted considerable attention from the financial research community . this chapter presents a comparative study of three portfolio design approaches . the MVP portfolio is the best on the out-of-sample data for the risk-adjusted returns .
"Neural Networks for Portfolio-Level Risk Management: Portfolio   Compression, Static Hedging, Counterparty Credit Risk Exposures and Impact on   Capital Requirement","In this paper, we present an artificial neural network framework for portfolio compression of a large portfolio of European options with varying maturities (target portfolio) by a significantly smaller portfolio of European options with shorter or same maturity (compressed portfolio), which also represents a self-replicating static hedge portfolio of the target portfolio. For the proposed machine learning architecture, which is consummately interpretable by choice of design, we also define the algorithm to learn model parameters by providing a parameter initialisation technique and leveraging the optimisation methodology proposed in Lokeshwar and Jain (2024), which was initially introduced to price Bermudan options. We demonstrate the convergence of errors and the iterative evolution of neural network parameters over the course of optimization process, using selected target portfolio samples for illustration. We demonstrate through numerical examples that the Exposure distributions and Exposure profiles (Expected Exposure and Potential Future Exposure) of the target portfolio and compressed portfolio align closely across future risk horizons under risk-neutral and real-world scenarios. Additionally, we benchmark the target portfolio's Financial Greeks (Delta, Gamma, and Vega) against the compressed portfolio at future time horizons across different market scenarios generated by Monte-Carlo simulations. Finally, we compare the regulatory capital requirement under the standardised approach for counterparty credit risk of the target portfolio against the compressed portfolio and highlight that the capital requirement for the compact portfolio substantially reduces.","['Vikranth Lokeshwar Dhandapani', 'Shashi Jain']",2024-02-27T23:43:31Z,http://arxiv.org/abs/2402.17941v1,Finance & Economics,Portfolio Optimization,"in this paper, we present an artificial neural network framework for portfolio compression . target portfolio represents a self-replicating static hedge portfolio of the target portfolio . the capital requirement for the compact portfolio substantially reduces ."
Wasserstein-Kelly Portfolios: A Robust Data-Driven Solution to Optimize   Portfolio Growth,"We introduce a robust variant of the Kelly portfolio optimization model, called the Wasserstein-Kelly portfolio optimization. Our model, taking a Wasserstein distributionally robust optimization (DRO) formulation, addresses the fundamental issue of estimation error in Kelly portfolio optimization by defining a ``ball"" of distributions close to the empirical return distribution using the Wasserstein metric and seeking a robust log-optimal portfolio against the worst-case distribution from the Wasserstein ball. Enhancing the Kelly portfolio using Wasserstein DRO is a natural step to take, given many successful applications of the latter in areas such as machine learning for generating robust data-driven solutions. However, naive application of Wasserstein DRO to the growth-optimal portfolio problem can lead to several issues, which we resolve through careful modelling. Our proposed model is both practically motivated and efficiently solvable as a convex program. Using empirical financial data, our numerical study demonstrates that the Wasserstein-Kelly portfolio can outperform the Kelly portfolio in out-of-sample testing across multiple performance metrics and exhibits greater stability.",['Jonathan Yu-Meng Li'],2023-02-27T17:17:33Z,http://arxiv.org/abs/2302.13979v1,Finance & Economics,Portfolio Optimization,we introduce a robust variant of the Kelly portfolio optimization model . called the Wasserstein-Kelly portfolio optimization . the model addresses the fundamental issue of estimation error in Kelly portfolio optimisation .
Risk management in multi-objective portfolio optimization under   uncertainty,"In portfolio optimization, decision makers face difficulties from uncertainties inherent in real-world scenarios. These uncertainties significantly influence portfolio outcomes in both classical and multi-objective Markowitz models. To address these challenges, our research explores the power of robust multi-objective optimization. Since portfolio managers frequently measure their solutions against benchmarks, we enhance the multi-objective min-regret robustness concept by incorporating these benchmark comparisons.   This approach bridges the gap between theoretical models and real-world investment scenarios, offering portfolio managers more reliable and adaptable strategies for navigating market uncertainties. Our framework provides a more nuanced and practical approach to portfolio optimization under real-world conditions.","['Yannick Becker', 'Pascal Halffmann', 'Anita Schöbel']",2024-07-29T12:17:11Z,http://arxiv.org/abs/2407.19936v1,Finance & Economics,Portfolio Optimization,research explores the power of robust multi-objective optimization . it bridges gap between theoretical models and real-world investment scenarios . our framework provides a more nuanced and practical approach .
Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using   Selected Stocks from the Indian Stock Market,"Designing an optimum portfolio that allocates weights to its constituent stocks in a way that achieves the best trade-off between the return and the risk is a challenging research problem. The classical mean-variance theory of portfolio proposed by Markowitz is found to perform sub-optimally on the real-world stock market data since the error in estimation for the expected returns adversely affects the performance of the portfolio. This paper presents three approaches to portfolio design, viz, the minimum risk portfolio, the optimum risk portfolio, and the Eigen portfolio, for seven important sectors of the Indian stock market. The daily historical prices of the stocks are scraped from Yahoo Finance website from January 1, 2016, to December 31, 2020. Three portfolios are built for each of the seven sectors chosen for this study, and the portfolios are analyzed on the training data based on several metrics such as annualized return and risk, weights assigned to the constituent stocks, the correlation heatmaps, and the principal components of the Eigen portfolios. Finally, the optimum risk portfolios and the Eigen portfolios for all sectors are tested on their return over a period of a six-month period. The performances of the portfolios are compared and the portfolio yielding the higher return for each sector is identified.","['Jaydip Sen', 'Sidra Mehtab']",2021-07-23T17:50:45Z,http://arxiv.org/abs/2107.11371v1,Finance & Economics,Portfolio Optimization,the classical mean-variance theory of portfolio is found to perform sub-optimally . the error in estimation for expected returns adversely affects the performance of the portfolio . three portfolios are built for each of the seven sectors chosen for this study .
Dynamic and static fund separations and their stability for long-term   optimal investments,"This paper investigates dynamic and static fund separations and their stability for long-term optimal investments under three model classes. An investor maximizes the expected utility with constant relative risk aversion under an incomplete market consisting of a safe asset, several risky assets, and a single state variable. The state variables in two of the model classes follow a 3/2 process and an inverse Bessel process, respectively. The other market model has the partially observed state variable modeled as an Ornstein-Uhlenbeck state process. We show that the dynamic optimal portfolio of this utility maximization consists of m+3 portfolios: the safe asset, the myopic portfolio, the m time-independent portfolios, and the intertemporal portfolio. Over time, the intertemporal portfolio eventually vanishes, leading the dynamic portfolio to converge to m+2 portfolios, referred to as the static portfolio. We also prove that the convergence is stable under model parameter perturbations. In addition, sensitivities of the intertemporal portfolio with respect to small parameters perturbations also vanish in the long run. The convergence rate for the intertemporal portfolio and its sensitivities are computed explicitly for the presented models.","['Hyungbin Park', 'Heejun Yeo']",2022-12-01T09:37:56Z,http://arxiv.org/abs/2212.00391v2,Finance & Economics,Portfolio Optimization,"an investor maximizes the expected utility with constant relative risk aversion . over time, the intertemporal portfolio eventually vanishes, leading the dynamic portfolio to converge to m+2 portfolios . the convergence is stable under model parameter perturbations ."
Optimal trend following portfolios,"This paper derives an optimal portfolio that is based on trend-following signal. Building on an earlier related article, it provides a unifying theoretical setting to introduce an autocorrelation model with the covariance matrix of trends and risk premia. We specify practically relevant models for the covariance matrix of trends. The optimal portfolio is decomposed into four basic components that yield four basic portfolios: Markowitz, risk parity, agnostic risk parity, and trend following on risk parity. The overperformance of the proposed optimal portfolio, applied to cross-asset trading universe, is confirmed by empirical backtests. We provide thus a unifying framework to describe and rationalize earlier developed portfolios.",['Sebastien Valeyre'],2022-01-17T21:22:33Z,http://arxiv.org/abs/2201.06635v1,Finance & Economics,Portfolio Optimization,this paper derives an optimal portfolio that is based on trend-following signal . it provides a unifying theoretical setting to introduce an autocorrelation model . the proposed optimal portfolio is decomposed into four basic components .
Portfolio Optimization in R,"We consider the problem of finding the efficient frontier associated with the risk-return portfolio optimization model. We derive the analytical expression of the efficient frontier for a portfolio of N risky assets, and for the case when a risk-free asset is added to the model. Also, we provide an R implementation, and we discuss in detail a numerical example of a portfolio of several risky common stocks.",['M. Andrecut'],2013-07-01T17:48:40Z,http://arxiv.org/abs/1307.0450v2,Finance & Economics,Portfolio Optimization,a risk-free asset is added to the risk-return portfolio optimization model . the efficient frontier is found for a portfolio of risky assets . we provide an R implementation and discuss a numerical example .
Mind the Cap! -- Constrained Portfolio Optimisation in Heston's   Stochastic Volatility Model,"We consider a portfolio optimisation problem for a utility-maximising investor who faces convex constraints on his portfolio allocation in Heston's stochastic volatility model. We apply the duality methods developed in previous work to obtain a closed-form expression for the optimal portfolio allocation. In doing so, we observe that allocation constraints impact the optimal constrained portfolio allocation in a fundamentally different way in Heston's stochastic volatility model than in the Black Scholes model. In particular, the optimal constrained portfolio may be different from the naive capped portfolio, which caps off the optimal unconstrained portfolio at the boundaries of the constraints. Despite this difference, we illustrate by way of a numerical analysis that in most realistic scenarios the capped portfolio leads to slim annual wealth equivalent losses compared to the optimal constrained portfolio. During a financial crisis, however, a capped solution might lead to compelling annual wealth equivalent losses.","['Marcos Escobar-Anel', 'Michel Kschonnek', 'Rudi Zagst']",2023-06-19T20:54:27Z,http://arxiv.org/abs/2306.11158v1,Finance & Economics,Portfolio Optimization,we consider a portfolio optimisation problem for a utility-maximising investor . we observe that allocation constraints impact the optimal constrained portfolio . a capped solution might lead to compelling annual wealth equivalent losses .
Portfolio optimization for heavy-tailed assets: Extreme Risk Index vs.   Markowitz,"Using daily returns of the S&P 500 stocks from 2001 to 2011, we perform a backtesting study of the portfolio optimization strategy based on the extreme risk index (ERI). This method uses multivariate extreme value theory to minimize the probability of large portfolio losses. With more than 400 stocks to choose from, our study seems to be the first application of extreme value techniques in portfolio management on a large scale. The primary aim of our investigation is the potential of ERI in practice. The performance of this strategy is benchmarked against the minimum variance portfolio and the equally weighted portfolio. These fundamental strategies are important benchmarks for large-scale applications. Our comparison includes annualized portfolio returns, maximal drawdowns, transaction costs, portfolio concentration, and asset diversity in the portfolio. In addition to that we study the impact of an alternative tail index estimator. Our results show that the ERI strategy significantly outperforms both the minimum-variance portfolio and the equally weighted portfolio on assets with heavy tails.","['Georg Mainik', 'Georgi Mitov', 'Ludger Rüschendorf']",2015-05-15T12:16:43Z,http://arxiv.org/abs/1505.04045v1,Finance & Economics,Portfolio Optimization,use daily returns of the S&P 500 stocks from 2001 to 2011 . primary aim of study is the potential of extreme value techniques in portfolio management .
RPS: Portfolio Asset Selection using Graph based Representation Learning,"Portfolio optimization is one of the essential fields of focus in finance. There has been an increasing demand for novel computational methods in this area to compute portfolios with better returns and lower risks in recent years. We present a novel computational method called Representation Portfolio Selection (RPS) by redefining the distance matrix of financial assets using Representation Learning and Clustering algorithms for portfolio selection to increase diversification. RPS proposes a heuristic for getting closer to the optimal subset of assets. Using empirical results in this paper, we demonstrate that widely used portfolio optimization algorithms, such as MVO, CLA, and HRP, can benefit from our asset subset selection.","['MohammadAmin Fazli', 'Parsa Alian', 'Ali Owfi', 'Erfan Loghmani']",2021-11-28T12:17:19Z,http://arxiv.org/abs/2111.15634v1,Finance & Economics,Portfolio Optimization,portfolio optimization is one of the essential fields of focus in finance . there has been an increasing demand for novel computational methods in this area . we present a new computational method called Representation Portfolio Selection .
Portfolio Optimization on Multivariate Regime Switching GARCH Model with   Normal Tempered Stable Innovation,"This paper uses simulation-based portfolio optimization to mitigate the left tail risk of the portfolio. The contribution is twofold. (i) We propose the Markov regime-switching GARCH model with multivariate normal tempered stable innovation (MRS-MNTS-GARCH) to accommodate fat tails, volatility clustering and regime switch. The volatility of each asset independently follows the regime-switch GARCH model, while the correlation of joint innovation of the GARCH models follows the Hidden Markov Model. (ii) We use tail risk measures, namely conditional value-at-risk (CVaR) and conditional drawdown-at-risk (CDaR), in the portfolio optimization. The optimization is performed with the sample paths simulated by the MRS-MNTS-GARCH model. We conduct an empirical study on the performance of optimal portfolios. Out-of-sample tests show that the optimal portfolios with tail measures outperform the optimal portfolio with standard deviation measure and the equally weighted portfolio in various performance measures. The out-of-sample performance of the optimal portfolios is also more robust to suboptimality on the efficient frontier.","['Cheng Peng', 'Young Shin Kim', 'Stefan Mittnik']",2020-09-23T20:25:14Z,http://arxiv.org/abs/2009.11367v3,Finance & Economics,Portfolio Optimization,this paper uses simulation-based portfolio optimization to mitigate the left tail risk of the portfolio . the volatility of each asset independently follows the regime-switch GARCH model . tail risk measures are used in the portfolio optimization .
A General Framework for Portfolio Construction Based on Generative   Models of Asset Returns,"In this paper, we present an integrated approach to portfolio construction and optimization, leveraging high-performance computing capabilities. We first explore diverse pairings of generative model forecasts and objective functions used for portfolio optimization, which are evaluated using performance-attribution models based on LASSO. We illustrate our approach using extensive simulations of crypto-currency portfolios, and we show that the portfolios constructed using the vine-copula generative model and the Sharpe-ratio objective function consistently outperform. To accommodate a wide array of investment strategies, we further investigate portfolio blending and propose a general framework for evaluating and combining investment strategies. We employ an extension of the multi-armed bandit framework and use value models and policy models to construct eclectic blended portfolios based on past performance. We consider similarity and optimality measures for value models and employ probability-matching (""blending"") and a greedy algorithm (""switching"") for policy models. The eclectic portfolios are also evaluated using LASSO models. We show that the value model utilizing cosine similarity and logit optimality consistently delivers robust superior performances. The extent of outperformance by eclectic portfolios over their benchmarks significantly surpasses that achieved by individual generative model-based portfolios over their respective benchmarks.","['Tuoyuan Cheng', 'Kan Chen']",2023-12-06T05:08:53Z,http://arxiv.org/abs/2312.03294v1,Finance & Economics,Portfolio Optimization,"in this paper, we present an integrated approach to portfolio construction and optimization . we show that the portfolios constructed using the vine-copula generative model consistently outperform . to accommodate a wide array of investment strategies, we further investigate portfolio blending ."
Signature Methods in Stochastic Portfolio Theory,"In the context of stochastic portfolio theory we introduce a novel class of portfolios which we call linear path-functional portfolios. These are portfolios which are determined by certain transformations of linear functions of a collections of feature maps that are non-anticipative path functionals of an underlying semimartingale. As main example for such feature maps we consider the signature of the (ranked) market weights. We prove that these portfolios are universal in the sense that every continuous, possibly path-dependent, portfolio function of the market weights can be uniformly approximated by signature portfolios. We also show that signature portfolios can approximate the growth-optimal portfolio in several classes of non-Markovian market models arbitrarily well and illustrate numerically that the trained signature portfolios are remarkably close to the theoretical growth-optimal portfolios. Besides these universality features, the main numerical advantage lies in the fact that several optimization tasks like maximizing (expected) logarithmic wealth or mean-variance optimization within the class of linear path-functional portfolios reduce to a convex quadratic optimization problem, thus making it computationally highly tractable. We apply our method also to real market data based on several indices. Our results point towards out-performance on the considered out-of-sample data, also in the presence of transaction costs.","['Christa Cuchiero', 'Janka Möller']",2023-10-03T18:00:37Z,http://arxiv.org/abs/2310.02322v3,Finance & Economics,Portfolio Optimization,linear path-functional portfolios are determined by certain transformations of linear functions . they are non-anticipative path functionals of an underlying semimartingale . our results point towards out-performance on the considered out-of-sample data .
Blockchain Data Analytics: Review and Challenges,"The integration of blockchain technology with data analytics is essential for extracting insights in the cryptocurrency space. Although academic literature on blockchain data analytics is limited, various industry solutions have emerged to address these needs. This paper provides a comprehensive literature review, drawing from both academic research and industry applications. We classify blockchain analytics tools into categories such as block explorers, on-chain data providers, research platforms, and crypto market data providers. Additionally, we discuss the challenges associated with blockchain data analytics, including data accessibility, scalability, accuracy, and interoperability. Our findings emphasize the importance of bridging academic research and industry innovations to advance blockchain data analytics.",['Rischan Mafrur'],2025-03-12T08:49:51Z,http://arxiv.org/abs/2503.09165v1,Finance & Economics,Blockchain & Crypto Analytics,academic literature on blockchain data analytics is limited . industry solutions have emerged to address these needs . this paper provides a comprehensive literature review .
Dissecting Ethereum Blockchain Analytics: What We Learn from Topology   and Geometry of Ethereum Graph,"Blockchain technology and, in particular, blockchain-based cryptocurrencies offer us information that has never been seen before in the financial world. In contrast to fiat currencies, all transactions of crypto-currencies and crypto-tokens are permanently recorded on distributed ledgers and are publicly available. As a result, this allows us to construct a transaction graph and to assess not only its organization but to glean relationships between transaction graph properties and crypto price dynamics. The ultimate goal of this paper is to facilitate our understanding on horizons and limitations of what can be learned on crypto-tokens from local topology and geometry of the Ethereum transaction network whose even global network properties remain scarcely explored. By introducing novel tools based on topological data analysis and functional data depth into Blockchain Data Analytics, we show that Ethereum network (one of the most popular blockchains for creating new crypto-tokens) can provide critical insights on price strikes of crypto-tokens that are otherwise largely inaccessible with conventional data sources and traditional analytic methods.","['Yitao Li', 'Umar Islambekov', 'Cuneyt Akcora', 'Ekaterina Smirnova', 'Yulia R. Gel', 'Murat Kantarcioglu']",2019-12-20T21:18:56Z,http://arxiv.org/abs/1912.10105v1,Finance & Economics,Blockchain & Crypto Analytics,all transactions of crypto-currencies and crypto-tokens are permanently recorded on ledgers . this allows us to construct a transaction graph and assess its organization . the ultimate goal of this paper is to facilitate our understanding on horizons and limitations of what can be learned on crypto-
A Survey on Applications of Game Theory in Blockchain,"In the past decades, the blockchain technology has attracted tremendous attention from both academia and industry. The popularity of blockchain networks was originated from a crypto-currency to serve as a decentralized and tamperproof transaction data ledger. Nowadays, blockchain as the key framework in the decentralized public data-ledger, has been applied to a wide range of scenarios far beyond crypto-currencies, such as Internet of Things (IoT), healthcare, and insurance. This survey aims to fill the gap between the large number of studies on blockchain network, where game theory emerges as an analytical tool, and the lack of a comprehensive survey on the game theoretical approaches applied in blockchain related issues. In this paper, we review game models proposed to address common issues in the blockchain network. The issues include security issues, e.g., selfish mining, majority attack and Denial of Service (DoS) attack, issues regard mining management, e.g., computational power allocation, reward allocation, and pool selection, as well as issues regarding blockchain economic and energy trading. Additionally, we discuss advantages and disadvantages of these selected game models and solutions. Finally, we highlight important challenges and future research directions of applying game theoretical approaches to incentive mechanism design, and the combination of blockchain with other technologies.","['Ziyao Liu', 'Nguyen Cong Luong', 'Wenbo Wang', 'Dusit Niyato', 'Ping Wang', 'Ying-Chang Liang', 'Dong In Kim']",2019-02-28T02:25:51Z,http://arxiv.org/abs/1902.10865v2,Finance & Economics,Blockchain & Crypto Analytics,"the popularity of the blockchain technology has attracted tremendous attention from both academia and industry . this survey aims to fill the gap between the large number of studies on blockchain network . issues include security issues, e.g., selfish mining, majority attack and DoS attack ."
Blockchain mechanism and distributional characteristics of cryptos,"We investigate the relationship between underlying blockchain mechanism of cryptocurrencies and its distributional characteristics. In addition to price, we emphasise on using actual block size and block time as the operational features of cryptos. We use distributional characteristics such as fourier power spectrum, moments, quantiles, global we optimums, as well as the measures for long term dependencies, risk and noise to summarise the information from crypto time series. With the hypothesis that the blockchain structure explains the distributional characteristics of cryptos, we use characteristic based spectral clustering to cluster the selected cryptos into five groups. We scrutinise these clusters and find that indeed, the clusters of cryptos share similar mechanism such as origin of fork, difficulty adjustment frequency, and the nature of block size. This paper provides crypto creators and users with a better understanding toward the connection between the blockchain protocol design and distributional characteristics of cryptos.","['Min-Bin Lin', 'Kainat Khowaja', 'Cathy Yi-Hsuan Chen', 'Wolfgang Karl Härdle']",2020-11-26T11:23:30Z,http://arxiv.org/abs/2011.13240v2,Finance & Economics,Blockchain & Crypto Analytics,"we focus on using actual block size and block time as operational features of cryptos . using characteristic based spectral clustering, we cluster cryptos into five groups . clusters share similar mechanism such as origin of fork, difficulty adjustment frequency ."
"Political, economic, and governance attitudes of blockchain users","We present a survey to evaluate crypto-political, crypto-economic, and crypto-governance sentiment in people who are part of a blockchain ecosystem. Based on 3710 survey responses, we describe their beliefs, attitudes, and modes of participation in crypto and investigate how self-reported political affiliation and blockchain ecosystem affiliation are associated with these. We observed polarization in questions on perceptions of the distribution of economic power, personal attitudes towards crypto, normative beliefs about the distribution of power in governance, and external regulation of blockchain technologies. Differences in political self-identification correlated with opinions on economic fairness, gender equity, decision-making power and how to obtain favorable regulation, while blockchain affiliation correlated with opinions on governance and regulation of crypto and respondents' semantic conception of crypto and personal goals for their involvement. We also find that a theory-driven constructed political axis is supported by the data and investigate the possibility of other groupings of respondents or beliefs arising from the data.","['Lucia M. Korpas', 'Seth Frey', 'Joshua Tan']",2023-01-06T22:30:22Z,http://arxiv.org/abs/2301.02734v1,Finance & Economics,Blockchain & Crypto Analytics,"a survey evaluates crypto-political, crypto-economic, and crypto-governance sentiment . results describe beliefs, attitudes, and modes of participation in crypto . political self-identification correlated with economic fairness, gender equity, decision-making power ."
The Application of Blockchain-Based Crypto Assets for Integrating the   Physical and Financial Supply Chains in the Construction & Engineering   Industry,"Supply chain integration remains an elusive goal for the construction and engineering industry. The high degree of fragmentation and the reliance on third-party financial institutions has pushed the physical and financial supply chains apart. The paper demonstrates how blockchain-based crypto assets (crypto currencies and crypto tokens) can address this limitation when used for conditioning the flow of funds based on the flow of products. The paper contrasts the integration between cash and product flows in supply chains that rely on fiat currencies and crypto assets for their payment settlement. Two facets of crypto asset-enabled integration, atomicity and granularity, are further introduced. The thesis is validated in the context of construction progress payments. The as-built data captured by unmanned aerial and ground vehicles was passed to an autonomous smart contract-based method that utilizes crypto-currencies and crypto tokens for payment settlement; the resulting payment datasets, written to the Ethereum blockchain, were analyzed in terms of their integration of product and cash flow. The work is concluded with a discussion of findings and their implications for the industry.","['Hesam Hamledari', 'Martin Fischer']",2020-12-03T18:27:16Z,http://arxiv.org/abs/2012.02147v1,Finance & Economics,Blockchain & Crypto Analytics,supply chain integration remains an elusive goal for the construction industry . high degree of fragmentation and reliance on third-party financial institutions has pushed supply chains apart . paper demonstrates how blockchain-based crypto assets can address this limitation .
Are Crypto Ecosystems (De)centralizing? A Framework for Longitudinal   Analysis,"Blockchain technology relies on decentralization to resist faults and attacks while operating without trusted intermediaries. Although industry experts have touted decentralization as central to their promise and disruptive potential, it is still unclear whether the crypto ecosystems built around blockchains are becoming more or less decentralized over time. As crypto plays an increasing role in facilitating economic transactions and peer-to-peer interactions, measuring their decentralization becomes even more essential. We thus propose a systematic framework for measuring the decentralization of crypto ecosystems over time and compare commonly used decentralization metrics. We applied this framework to seven prominent crypto ecosystems, across five distinct subsystems and across their lifetime for over 15 years. Our analysis revealed that while crypto has largely become more decentralized over time, recent trends show a shift toward centralization in the consensus layer, NFT marketplaces, and developers. Our framework and results inform researchers, policymakers, and practitioners about the design, regulation, and implementation of crypto ecosystems and provide a systematic, replicable foundation for future studies.","['Harang Ju', 'Ehsan Valavi', 'Madhav Kumar', 'Sinan Aral']",2025-06-02T23:45:33Z,http://arxiv.org/abs/2506.02324v1,Finance & Economics,Blockchain & Crypto Analytics,industry experts have touted decentralization as central to their promise and disruptive potential . but it is still unclear whether crypto ecosystems built around blockchains are becoming more or less decentralized over time . we propose a systematic framework for measuring the decentralizedization of crypto ecosystem .
"The Rise and Fall of Cryptocurrencies: Defining the Economic and Social   Values of Blockchain Technologies, assessing the Opportunities, and defining   the Financial and Cybersecurity Risks of the Metaverse","This paper contextualises the common queries of ""why is crypto crashing?"" and ""why is crypto down?"", the research transcends beyond the frequent market fluctuations to unravel how cryptocurrencies fundamentally work and the step-by-step process on how to create a cryptocurrency.   The study examines blockchain technologies and their pivotal role in the evolving Metaverse, shedding light on topics such as how to invest in cryptocurrency, the mechanics behind crypto mining, and strategies to effectively buy and trade cryptocurrencies. Through an interdisciplinary approach, the research transitions from the fundamental principles of fintech investment strategies to the overarching implications of blockchain within the Metaverse. Alongside exploring machine learning potentials in financial sectors and risk assessment methodologies, the study critically assesses whether developed or developing nations are poised to reap greater benefits from these technologies. Moreover, it probes into both enduring and dubious crypto projects, drawing a distinct line between genuine blockchain applications and Ponzi-like schemes. The conclusion resolutely affirms the continuing dominance of blockchain technologies, underlined by a profound exploration of their intrinsic value and a reflective commentary by the author on the potential risks confronting individual investors.",['Petar Radanliev'],2023-08-09T06:07:41Z,http://arxiv.org/abs/2309.12322v1,Finance & Economics,Blockchain & Crypto Analytics,"study examines how cryptocurrencies fundamentally work and the step-by-step process on how to create a cryptocurrency . it examines the mechanics behind crypto mining, and strategies to effectively buy and trade cryptocurrencies . the study critically assesses whether developed or developing nations are poised to reap"
Disorder Unleashes Panic in Bitcoin Dynamics,"The behaviour of Bitcoin owners is reflected in the structure and the number of bitcoin transactions encoded in the Blockchain. Likewise, the behaviour of Bitcoin traders is reflected in the formation of bullish and bearish trends in the crypto market. In light of these observations, we wonder if human behaviour underlies some relationship between the Blockchain and the crypto market. To address this question, we map the Blockchain to a spin-lattice problem, whose configurations form ordered and disordered patterns, representing the behaviour of Bitcoin owners. This novel approach allows us to obtain time series suitable to detect a causal relationship between the dynamics of the Blockchain and market trends of the Bitcoin and to find that disordered patterns in the Blockchain precede Bitcoin panic selling. Our results suggest that human behaviour underlying Blockchain evolution and the crypto market brings out a fascinating connection between disorder and panic in Bitcoin dynamics.","['Marco Alberto Javarone', 'Gabriele Di Antonio', 'Gianni Valerio Vinci', 'Raffaele Cristodaro', 'Claudio J. Tessone', 'Luciano Pietronero']",2022-09-20T16:24:18Z,http://arxiv.org/abs/2209.09832v1,Finance & Economics,Blockchain & Crypto Analytics,"we map the Blockchain to a spin-lattice problem, whose configurations form ordered and disordered patterns, representing the behaviour of Bitcoin owners . this novel approach allows us to detect a causal relationship between the dynamics of the Blockchain and market trends of the Bitcoin ."
An Empirical Study of Blockchain Repositories in GitHub,"Blockchain is a distributed ledger technique that guarantees the traceability of transactions. Blockchain is adopted in multiple domains like finance (e.g., cryptocurrency), healthcare, security, and supply chain. In the open-source software (OSS) portal GitHub, we observe a growing adoption of Blockchain-based solutions. Given the rapid emergence of Blockchain-based solutions in our daily life and the evolving cryptocurrency market, it is important to know the status quo, how developers generally interact in those repos, and how much freedom they have in applying code changes. We report an empirical study of 3,664 Blockchain software repositories from GitHub. We divide the Blockchain repositories into two categories: Tool (e.g., SDKs) and Applications (e.g., service/solutions developed using SDKs). The Application category is further divided into two sub-categories: Crypto and Non-Crypto applications. In all Blockchain repository categories, the contribution interactions on commits are the most common interaction type. We found that more organizations contributing to the Blockchain repos than individual users. The median numbers of internal and external users in tools are higher than the application repos. We observed a higher degree of collaboration (e.g., for maintenance efforts) among users in Blockchain tools than those in the application repos. Among the artifacts, issues have a greater number of interactions than commits and pull requests. Related to autonomy we found that less than half of total project contributions are autonomous. Our findings offer implications to Blockchain stakeholders, like developers to stay aware of OSS practices around Blockchain software.","['Ajoy Das', 'Gias Uddin', 'Guenther Ruhe']",2022-05-17T04:21:06Z,http://arxiv.org/abs/2205.08087v1,Finance & Economics,Blockchain & Crypto Analytics,blockchain is a distributed ledger technique that guarantees the traceability of transactions . more organizations contributing to the Blockchain repositories than individual users . less than half of total project contributions are autonomous .
The Kosmosis Use-Case of Crypto Rug Pull Detection and Prevention,"Current methods to prevent crypto asset fraud are based on the analysis of transaction graphs within blockchain networks. While effective for identifying transaction patterns indicative of fraud, it does not capture the semantics of transactions and is constrained to blockchain data. Consequently, preventive methods based on transaction graphs are inherently limited. In response to these limitations, we propose the Kosmosis approach, which aims to incrementally construct a knowledge graph as new blockchain and social media data become available. During construction, it aims to extract the semantics of transactions and connect blockchain addresses to their real-world entities by fusing blockchain and social media data in a knowledge graph. This enables novel preventive methods against rug pulls as a form of crypto asset fraud. To demonstrate the effectiveness and practical applicability of the Kosmosis approach, we examine a series of real-world rug pulls from 2021. Through this case, we illustrate how Kosmosis can aid in identifying and preventing such fraudulent activities by leveraging the insights from the constructed knowledge graph.","['Philipp Stangl', 'Christoph P. Neumann']",2024-05-30T07:17:57Z,http://arxiv.org/abs/2405.19762v1,Finance & Economics,Blockchain & Crypto Analytics,"current methods to prevent crypto asset fraud are based on transaction graphs . but they do not capture the semantics of transactions and are constrained to blockchain data . we propose the Kosmosis approach, which aims to incrementally construct a knowledge graph ."
Crypto Currency Regulation and Law Enforcement Perspectives,"This paper provides an overview of how crypto currency and blockchain engineering interacts with the law enforcement. We point out that a large proportion of crypto users are amateur investors and the dominant and the largest segment in crypto crime are simply investment scams (!). We look at various questions of criminal use and misuse of technology, especially in the areas of money laundering or cashing out the profits originating from illicit activities. The aim of the paper is to raise a set of concerns arising in the criminal justice and policing circles, based on the interviews with law enforcement practitioners, and to see how cryptos could be reconciled with public security and safety. We propose a simplified classification of crimes related to crypto currency. We study the development of blockchains in a broader context of applied cryptography and payment technology. Ransomware is a big threat but we also need protection against corporate misconduct or negligence, with untested financial services breaching customer trust or government regulations. Not paying taxes is illegal, but there is more at stake: exposing crypto holders to losing all their savings in scams or thefts. Interestingly, privacy helps to defend on multiple fronts: against social engineering, targeted crime, scams, and also against cybersecurity thefts and hacks.","['Nicolas T. Courtois', 'Kacper T. Gradon', 'Klaus Schmeh']",2021-09-01T09:56:28Z,http://arxiv.org/abs/2109.01047v1,Finance & Economics,Blockchain & Crypto Analytics,the dominant and largest segment in crypto crime are simply investment scams . the aim of the paper is to raise a set of concerns arising in the criminal justice and policing circles .
A theory of transaction parallelism in blockchains,"Decentralized blockchain platforms have enabled the secure exchange of crypto-assets without the intermediation of trusted authorities. To this purpose, these platforms rely on a peer-to-peer network of byzantine nodes, which collaboratively maintain an append-only ledger of transactions, called blockchain. Transactions represent the actions required by users, e.g. the transfer of some units of crypto-currency to another user, or the execution of a smart contract which distributes crypto-assets according to its internal logic. Part of the nodes of the peer-to-peer network compete to append transactions to the blockchain. To do so, they group the transactions sent by users into blocks, and update their view of the blockchain state by executing these transactions in the chosen order. Once a block of transactions is appended to the blockchain, the other nodes validate it, re-executing the transactions in the same order. The serial execution of transactions does not take advantage of the multi-core architecture of modern processors, so contributing to limit the throughput. In this paper we develop a theory of transaction parallelism for blockchains, which is based on static analysis of transactions and smart contracts. We illustrate how blockchain nodes can use our theory to parallelize the execution of transactions. Initial experiments on Ethereum show that our technique can improve the performance of nodes.","['Massimo Bartoletti', 'Letterio Galletta', 'Maurizio Murgia']",2020-11-27T17:06:11Z,http://arxiv.org/abs/2011.13837v4,Finance & Economics,Blockchain & Crypto Analytics,"decentralized blockchain platforms have enabled the secure exchange of crypto-assets . they rely on a peer-to-peer network of byzantine nodes, which maintain an append-only ledger of transactions . part of the nodes compete to append transactions to the blockchain"
Blockchain for the IoT: Opportunities and Challenges,"Blockchain technology has been transforming the financial industry and has created a new crypto-economy in the last decade. The foundational concepts such as decentralized trust and distributed ledger are promising for distributed, and large-scale Internet of Things (IoT) applications. However, the applications of Blockchain beyond cryptocurrencies in this domain are few and far between because of the lack of understanding and inherent architectural challenges. In this paper, we describe the opportunities for applications of blockchain for the IoT and examine the challenges involved in architecting Blockchain-based IoT applications.","['Gowri Sankar Ramachandran', 'Bhaskar Krishnamachari']",2018-05-08T03:20:48Z,http://arxiv.org/abs/1805.02818v1,Finance & Economics,Blockchain & Crypto Analytics,"the foundational concepts such as decentralized trust and distributed ledger are promising for distributed, and large-scale Internet of Things (IoT) applications . the applications of Blockchain beyond cryptocurrencies in this domain are few and far between ."
Blockchain Transaction Processing,"A blockchain is an append-only linked-list of blocks, which is maintained at each participating node. Each block records a set of transactions and their associated metadata. Blockchain transactions act on the identical ledger data stored at each node. Blockchain was first perceived by Satoshi Nakamoto as a peer-to-peer digital-commodity (also known as crypto-currency) exchange system. Blockchains received traction due to their inherent property of immutability-once a block is accepted, it cannot be reverted.","['Suyash Gupta', 'Mohammad Sadoghi']",2021-07-24T12:20:36Z,http://arxiv.org/abs/2107.11592v2,Finance & Economics,Blockchain & Crypto Analytics,a blockchain is an append-only linked-list of blocks maintained at each participating node . each block records a set of transactions and their associated metadata . blockchain transactions act on the identical ledger data stored at each node.
The Evolution of Embedding Metadata in Blockchain Transactions,"The use of blockchains is growing every day, and their utility has greatly expanded from sending and receiving crypto-coins to smart-contracts and decentralized autonomous organizations. Modern blockchains underpin a variety of applications: from designing a global identity to improving satellite connectivity. In our research we look at the ability of blockchains to store metadata in an increasing volume of transactions and with evolving focus of utilization. We further show that basic approaches to improving blockchain privacy also rely on embedding metadata. This paper identifies and classifies real-life blockchain transactions embedding metadata of a number of major protocols running essentially over the bitcoin blockchain. The empirical analysis here presents the evolution of metadata utilization in the recent years, and the discussion suggests steps towards preventing criminal use. Metadata are relevant to any blockchain, and our analysis considers primarily bitcoin as a case study. The paper concludes that simultaneously with both expanding legitimate utilization of embedded metadata and expanding blockchain functionality, the applied research on improving anonymity and security must also attempt to protect against blockchain abuse.","['Tooba Faisal', 'Nicolas Courtois', 'Antoaneta Serguieva']",2018-06-18T14:38:02Z,http://arxiv.org/abs/1806.06738v1,Finance & Economics,Blockchain & Crypto Analytics,research looks at the ability of blockchains to store metadata in an increasing volume of transactions . basic approaches to improving blockchain privacy also rely on embedding metadata . analysis considers primarily bitcoin as a case study .
Evolutionary Dynamics of Sustainable Blockchains,"The energy sustainability of blockchains, whose consensus protocol rests on the Proof-of-Work, nourishes a heated debate. The underlying issue lies in a highly energy-consuming process, defined as mining, required to validate crypto-asset transactions. Mining is the process of solving a cryptographic puzzle, incentivised by the possibility of gaining a reward. The higher the number of users performing mining, i.e. miners, the higher the overall electricity consumption of a blockchain. For that reason, mining constitutes a negative environmental externality. Here, we study whether miners' interests can meet the collective need to curb energy consumption. To this end, we introduce the Crypto-Asset Game, namely a model based on the framework of Evolutionary Game Theory devised for studying the dynamics of a population whose agents can play as crypto-asset users or as miners. The energy consumption of mining impacts the payoff of both strategies, representing a direct cost for miners and an environmental factor for crypto-asset users. The proposed model, studied via numerical simulations, shows that, in some conditions, the agent population can reach a strategy profile that optimises global energy consumption, i.e. composed of a low density of miners. To conclude, can a Proof-of-Work-based blockchain become energetically sustainable? Our results suggest that blockchain protocol parameters could have a relevant role in the global energy consumption of this technology.","['Marco Alberto Javarone', 'Gabriele Di Antonio', 'Gianni Valerio Vinci', 'Luciano Pietronero', 'Carlo Gola']",2022-09-26T16:00:42Z,http://arxiv.org/abs/2209.12809v1,Finance & Economics,Blockchain & Crypto Analytics,"mining is a process of solving a cryptographic puzzle, incentivised by the possibility of gaining a reward . the higher the number of users performing mining, i.e. miners, the higher overall electricity consumption of a blockchain . we introduce the Crypto-Asset Game, a model based on the framework of Evolutionary Game Theory ."
Max-min Fairness Based Faucet Design for Blockchains,"In order to have transactions executed and recorded on blockchains such as the Ethereum Mainnet, fees expressed in crypto-currency units of the blockchain must be paid. One can buy crypto-currency called Ether of the Ethereum blockchain from exchanges and pay for the transaction fees. In the case of test networks (such as Rinkeby) or scientific research blockchains (such as Bloxberg), free crypto-currency, Ether, is distributed to users via faucets. Since transaction slots on the blocks, storage and smart contract executions are consuming blockchain resources, Ethers are distributed by fixed small amounts to users. Users may have different amount of Ether requirements; some small amounts and some large amounts during different times. As a result, rather than allowing the user to get a fixed small amount of Ether, a more general distribution mechanism that allows a user to demand and claim arbitrary amounts of Ether, while satisfying fairness among users, is needed. For this end, Max-min Fairness based schemes have been used in centralized settings. Our work contributes a Max-min Fairness based algorithm and its Solidity smart contract implementation that requires low transaction costs independent of the number of users. This is important on the Ethereum blockchain, since a smart contract execution with transaction costs depending on the number of users would mean block gas limit exhaustion problem will eventually be met, making the smart contract ineffective. We report tests which confirm that the low transaction cost aims have been achieved by our algorithm.","['Serdar Metin', 'Can Özturan']",2021-08-19T12:38:44Z,http://arxiv.org/abs/2108.08656v1,Finance & Economics,Blockchain & Crypto Analytics,"in order to have transactions executed and recorded on blockchains, fees must be paid . free crypto-currency, Ether, is distributed to users via faucets . users may have different amount of Ether requirements; some small amounts and some large amounts ."
The merits of using Ethereum MainNet as a Coordination Blockchain for   Ethereum Private Sidechains,"A Coordination Blockchain is a blockchain with the task of coordinating activities of multiple private blockchains. This paper discusses the pros and cons of using Ethereum MainNet, the public Ethereum blockchain, as a Coordination Blockchain. The requirements Ethereum MainNet needs to fulfil to perform this role are discussed within the context of Ethereum Private Sidechains, a private blockchain technology which allows many blockchains to be operated in parallel, and allows atomic crosschain transactions to execute across blockchains. Ethereum MainNet is a permissionless network which aims to offer strong authenticity, integrity, and non-repudiation properties, that incentivises good behaviour using crypto economics. This paper demonstrates that Ethereum MainNet does deliver these properties. It then provides a comprehensive review of the features of Ethereum Private Sidechains, with a focus on the potential usage of Coordination Blockchains for these features. Finally, the merits of using Ethereum MainNet as a Coordination Blockchain are assessed. For Ethereum Private Sidechains, we found that Ethereum MainNet is best suited to storing long term static data that needs to be widely available, such as the Ethereum Registration Authority information. However, due to Ethereum MainNet's probabilistic finality, it is not well suited to information that needs to be available and acted upon immediately, such as the Sidechain Public Keys and Atomic Crosschain Transaction state information that need to be accessible prior to the first atomic crosschain transaction being issued on a sidechain. Although this paper examined the use of Ethereum MainNet as a Coordination Blockchain within reference to Ethereum Private Sidechains, the discussions and observations of the typical tasks a Coordination blockchain may be expected to perform are applicable more widely to any multi-blockchain system.",['Peter Robinson'],2019-06-11T07:49:37Z,http://arxiv.org/abs/1906.04421v2,Finance & Economics,Blockchain & Crypto Analytics,"a Coordination Blockchain is a blockchain with the task of coordinating activities of multiple private blockchains . this paper discusses the pros and cons of using Ethereum MainNet, the public Ethereum blockchain . it aims to offer strong authenticity, integrity, and non-repudiation properties ."
Bitcoin's Crypto Flow Network,"How crypto flows among Bitcoin users is an important question for understanding the structure and dynamics of the cryptoasset at a global scale. We compiled all the blockchain data of Bitcoin from its genesis to the year 2020, identified users from anonymous addresses of wallets, and constructed monthly snapshots of networks by focusing on regular users as big players. We apply the methods of bow-tie structure and Hodge decomposition in order to locate the users in the upstream, downstream, and core of the entire crypto flow. Additionally, we reveal principal components hidden in the flow by using non-negative matrix factorization, which we interpret as a probabilistic model. We show that the model is equivalent to a probabilistic latent semantic analysis in natural language processing, enabling us to estimate the number of such hidden components. Moreover, we find that the bow-tie structure and the principal components are quite stable among those big players. This study can be a solid basis on which one can further investigate the temporal change of crypto flow, entry and exit of big players, and so forth.","['Yoshi Fujiwara', 'Rubaiyat Islam']",2021-06-21T23:15:49Z,http://arxiv.org/abs/2106.11446v2,Finance & Economics,Blockchain & Crypto Analytics,how crypto flows among Bitcoin users is an important question for understanding . we compiled all the blockchain data of Bitcoin from its genesis to the year 2020 . the bow-tie structure and the principal components are quite stable among those big players .
"The New Anticipatory Governance Culture for Innovation: Regulatory   Foresight, Regulatory Experimentation and Regulatory Learning","With the rapid pace of technological innovation, traditional methods of policy formation and legislating are becoming conspicuously anachronistic. The need for regulatory choices to be made to counter the deadening effect of regulatory lag is more important to developing markets and fostering growth than achieving one off regulatory perfection. This article advances scholarship on innovation policy and the regulation of technological innovation in the European Union. It does so by considering what building an agile yet robust anticipatory governance regulatory culture involves. It systematically excavates a variety of tools and elements that are being put into use in inventive ways and argues that these need to be more cohesively and systemically integrated into the regulatory toolbox. Approaches covered include strategic foresight, the critical embrace of iterative policy development and regulatory learning in the face of uncertainty and the embrace of bottom up approaches to cocreation of policy such as Policy Labs and the testing and regulatory learning through pilot regulation and experimentation. The growing use of regulatory sandboxes as an EU policy tool to boost innovation and navigate regulatory complexity as seen in the EU AI Act is also probed",['Deirdre Ahern'],2025-01-10T12:26:38Z,http://arxiv.org/abs/2501.05921v1,Finance & Economics,Regulatory Technology,"this article examines what building an agile yet robust anticipatory governance regulatory culture involves . it systematically excavates a variety of tools and elements that are being put into use in inventive ways . approaches covered include strategic foresight, critical embrace of iterative policy development and regulatory learning ."
Uncovering Regulatory Affairs Complexity in Medical Products: A   Qualitative Assessment Utilizing Open Coding and Natural Language Processing   (NLP),"This study investigates the complexity of regulatory affairs in the medical device industry, a critical factor influencing market access and patient care. Through qualitative research, we sought expert insights to understand the factors contributing to this complexity. The study involved semi-structured interviews with 28 professionals from medical device companies, specializing in various aspects of regulatory affairs. These interviews were analyzed using open coding and Natural Language Processing (NLP) techniques. The findings reveal key sources of complexity within the regulatory landscape, divided into five domains: (A) Regulatory language complexity, (B) Intricacies within the regulatory process, (C) Global-level complexities, (D) Database-related considerations, and (E) Product-level issues. The participants highlighted the need for strategies to streamline regulatory compliance, enhance interactions between regulatory bodies and industry players, and develop adaptable frameworks for rapid technological advancements. Emphasizing interdisciplinary collaboration and increased transparency, the study concludes that these elements are vital for establishing coherent and effective regulatory procedures in the medical device sector.","['Yu Han', 'Aaron Ceross', 'Jeroen H. M. Bergmann']",2023-12-30T03:39:57Z,http://arxiv.org/abs/2401.02975v1,Finance & Economics,Regulatory Technology,the study involved semi-structured interviews with 28 professionals from medical device companies . the findings reveal key sources of complexity within the regulatory landscape . participants highlighted need for strategies to streamline regulatory compliance .
RIRAG: Regulatory Information Retrieval and Answer Generation,"Regulatory documents, issued by governmental regulatory bodies, establish rules, guidelines, and standards that organizations must adhere to for legal compliance. These documents, characterized by their length, complexity and frequent updates, are challenging to interpret, requiring significant allocation of time and expertise on the part of organizations to ensure ongoing compliance. Regulatory Natural Language Processing (RegNLP) is a multidisciplinary field aimed at simplifying access to and interpretation of regulatory rules and obligations. We introduce a task of generating question-passages pairs, where questions are automatically created and paired with relevant regulatory passages, facilitating the development of regulatory question-answering systems. We create the ObliQA dataset, containing 27,869 questions derived from the collection of Abu Dhabi Global Markets (ADGM) financial regulation documents, design a baseline Regulatory Information Retrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a novel evaluation metric that tests whether generated answers accurately capture all relevant obligations while avoiding contradictions.","['Tuba Gokhan', 'Kexin Wang', 'Iryna Gurevych', 'Ted Briscoe']",2024-09-09T14:44:19Z,http://arxiv.org/abs/2409.05677v2,Finance & Economics,Regulatory Technology,"Regulatory documents establish rules, guidelines, and standards that organizations must adhere to for legal compliance . Regulatory natural language processing (RegNLP) is a multidisciplinary field aimed at simplifying access to and interpretation of regulatory rules ."
Personalizing Smart Home Privacy Protection With Individuals' Regulatory   Focus: Would You Preserve or Enhance Your Information Privacy?,"In this study, we explore the effectiveness of persuasive messages endorsing the adoption of a privacy protection technology (IoT Inspector) tailored to individuals' regulatory focus (promotion or prevention). We explore if and how regulatory fit (i.e., tuning the goal-pursuit mechanism to individuals' internal regulatory focus) can increase persuasion and adoption. We conducted a between-subject experiment (N = 236) presenting participants with the IoT Inspector in gain (""Privacy Enhancing Technology"" -- PET) or loss (""Privacy Preserving Technology"" -- PPT) framing. Results show that the effect of regulatory fit on adoption is mediated by trust and privacy calculus processes: prevention-focused users who read the PPT message trust the tool more. Furthermore, privacy calculus favors using the tool when promotion-focused individuals read the PET message. We discuss the contribution of understanding the cognitive mechanisms behind regulatory fit in privacy decision-making to support privacy protection.","['Reza Ghaiumy Anaraky', 'Yao Li', 'Hichang Cho', 'Danny Yuxing Huang', 'Kaileigh A. Byrne', 'Bart Knijnenburg', 'Oded Nov']",2024-02-27T19:03:07Z,http://arxiv.org/abs/2402.17838v1,Finance & Economics,Regulatory Technology,we explore if and how regulatory fit can increase persuasion and adoption . prevention-focused users who read the PPT message trust the tool more . privacy calculus favors using the tool when promotion-focused individuals read the PET .
Developing RFID library systems in the direction of integration into the   global identification system EPC,"The possibility of modification of the regulatory framework of RFID library systems in the direction of integration into the EPCglobal Network, based on existing harmonized standards of RFID technology. It is shown that this approach to the formation of the regulatory framework will improve the availability of RFID technology for libraries and contribute to the overall development of library technologies.",['Igor Timoshenko'],2018-05-27T21:33:59Z,http://arxiv.org/abs/1805.10696v1,Finance & Economics,Regulatory Technology,the possibility of modification of the regulatory framework of RFID library systems . this will improve the availability of RFID technology for libraries . it will contribute to the overall development of library technologies .
On Mobile DNA in Artificial Regulatory Networks: Evolving Functional and   Structural Dynamism,"There is a growing body of work considering the use of representations based upon genetic regulatory networks. This paper uses a recently presented abstract, tunable Boolean regulatory network model to explore aspects of mobile DNA, such as transposons, within these dynamical systems. The significant role of mobile DNA in the evolution of natural systems is becoming increasingly clear. Whilst operators loosely based upon transposons have previously been used within evolutionary computation, their use within regulatory network representations enables the potential exploitation of numerous new mechanisms. This paper shows how dynamically controlling network node connectivity and function via transposon-inspired mechanisms can be selected for under non-stationary and coevolutionary scenarios, including when such changes are heritable.",['Larry Bull'],2013-03-26T09:03:35Z,http://arxiv.org/abs/1303.7220v1,Finance & Economics,Regulatory Technology,"paper explores aspects of mobile DNA, such as transposons, within dynamical systems . paper shows how dynamically controlling network node connectivity can be selected ."
"Market Misconduct in Decentralized Finance (DeFi): Analysis, Regulatory   Challenges and Policy Implications","Technological advancement drives financial innovation, reshaping the traditional finance landscape and redefining user-market interactions. The rise of blockchain and Decentralized Finance (DeFi) underscores this intertwined evolution of technology and finance. While DeFi has introduced exciting opportunities, it has also exposed the ecosystem to new forms of market misconduct. This paper aims to bridge the academic and regulatory gaps by addressing key research questions about market misconduct in DeFi. We begin by discussing how blockchain technology can potentially enable the emergence of novel forms of market misconduct. We then offer a comprehensive definition and taxonomy for understanding DeFi market misconduct. Through comparative analysis and empirical measurements, we examine the novel forms of misconduct in DeFi, shedding light on their characteristics and social impact. Subsequently, we investigate the challenges of building a tailored regulatory framework for DeFi. We identify key areas where existing regulatory frameworks may need enhancement. Finally, we discuss potential approaches that bring DeFi into the regulatory perimeter.","['Xihan Xiong', 'Zhipeng Wang', 'Tianxiang Cui', 'William Knottenbelt', 'Michael Huth']",2023-11-29T15:18:31Z,http://arxiv.org/abs/2311.17715v3,Finance & Economics,Regulatory Technology,the rise of blockchain and Decentralized Finance (DeFi) underscores this intertwined evolution of technology and finance . the paper aims to bridge the academic and regulatory gaps by addressing key research questions about market misconduct .
"Towards Adaptive AI Governance: Comparative Insights from the U.S., EU,   and Asia","Artificial intelligence (AI) trends vary significantly across global regions, shaping the trajectory of innovation, regulation, and societal impact. This variation influences how different regions approach AI development, balancing technological progress with ethical and regulatory considerations. This study conducts a comparative analysis of AI trends in the United States (US), the European Union (EU), and Asia, focusing on three key dimensions: generative AI, ethical oversight, and industrial applications. The US prioritizes market-driven innovation with minimal regulatory constraints, the EU enforces a precautionary risk-based framework emphasizing ethical safeguards, and Asia employs state-guided AI strategies that balance rapid deployment with regulatory oversight. Although these approaches reflect different economic models and policy priorities, their divergence poses challenges to international collaboration, regulatory harmonization, and the development of global AI standards. To address these challenges, this paper synthesizes regional strengths to propose an adaptive AI governance framework that integrates risk-tiered oversight, innovation accelerators, and strategic alignment mechanisms. By bridging governance gaps, this study offers actionable insights for fostering responsible AI development while ensuring a balance between technological progress, ethical imperatives, and regulatory coherence.","['Vikram Kulothungan', 'Deepti Gupta']",2025-04-01T11:05:47Z,http://arxiv.org/abs/2504.00652v1,Finance & Economics,Regulatory Technology,"this study conducts a comparative analysis of AI trends in the u.s., the european union (EU) and Asia . it focuses on three key dimensions: generative AI, ethical oversight, and industrial applications . the US prioritizes market-driven innovation with minimal regulatory constraints ."
Perspective on recent developments and challenges in regulatory and   systems genomics,"Predicting how genetic variation affects phenotypic outcomes at the organismal, cellular, and molecular levels requires deciphering the cis-regulatory code, the sequence rules by which non-coding regions regulate genes. In this perspective, we discuss recent computational progress and challenges towards solving this fundamental problem. We describe how cis-regulatory elements are mapped and how their sequence rules can be learned and interpreted with sequence-to-function neural networks, with the goal of identifying genetic variants in human disease. We also discuss how studies of the 3D chromatin organization could help identifying long-range regulatory effects and how current methods for mapping gene regulatory networks could better describe biological processes. We point out current gaps in knowledge along with technical limitations and benchmarking challenges of computational methods. Finally, we discuss newly emerging technologies, such as spatial transcriptomics, and outline strategies for creating a more general model of the cis-regulatory code that is more broadly applicable across cell types and individuals.","['Julia Zeiltinger', 'Sushmita Roy', 'Ferhat Ay', 'Anthony Mathelier', 'Alejandra Medina-Rivera', 'Shaun Mahony', 'Saurabh Sinha', 'Jason Ernst']",2024-11-07T01:44:17Z,http://arxiv.org/abs/2411.04363v1,Finance & Economics,Regulatory Technology,the cis-regulatory code is the sequence rules by which non-coding regions regulate genes . it can be learned and interpreted with sequence-to-function neural networks . studies of 3D chromatin organization could help identifying long-range regulatory effects .
Data-Driven Analysis of AI in Medical Device Software in China: Deep   Learning and General AI Trends Based on Regulatory Data,"Artificial intelligence (AI) in medical device software (MDSW) represents a transformative clinical technology, attracting increasing attention within both the medical community and the regulators. In this study, we leverage a data-driven approach to automatically extract and analyze AI-enabled medical devices (AIMD) from the National Medical Products Administration (NMPA) regulatory database. The continued increase in publicly available regulatory data requires scalable methods for analysis. Automation of regulatory information screening is essential to create reproducible insights that can be quickly updated in an ever changing medical device landscape. More than 4 million entries were assessed, identifying 2,174 MDSW registrations, including 531 standalone applications and 1,643 integrated within medical devices, of which 43 were AI-enabled. It was shown that the leading medical specialties utilizing AIMD include respiratory (20.5%), ophthalmology/endocrinology (12.8%), and orthopedics (10.3%). This approach greatly improves the speed of data extracting providing a greater ability to compare and contrast. This study provides the first extensive, data-driven exploration of AIMD in China, showcasing the potential of automated regulatory data analysis in understanding and advancing the landscape of AI in medical technology.","['Yu Han', 'Aaron Ceross', 'Sarim Ather', 'Jeroen H. M. Bergmann']",2024-11-11T21:28:50Z,http://arxiv.org/abs/2411.07378v2,Finance & Economics,Regulatory Technology,"more than 4 million entries were assessed, identifying 2,174 MDSW registrations . leading medical specialties utilizing AIMD include respiratory (20.5%), ophthalmology/endocrinology (12.8%) and orthopedics (10.3%)"
"Enhancing Financial Inclusion and Regulatory Challenges: A Critical   Analysis of Digital Banks and Alternative Lenders Through Digital Platforms,   Machine Learning, and Large Language Models Integration","This paper explores the dual impact of digital banks and alternative lenders on financial inclusion and the regulatory challenges posed by their business models. It discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) in enhancing financial services accessibility for underserved populations. Through a detailed analysis of operational frameworks and technological infrastructures, this research identifies key mechanisms that facilitate broader financial access and mitigate traditional barriers. Additionally, the paper addresses significant regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection. Employing a mixed-methods approach, which combines quantitative financial data analysis with qualitative insights from industry experts, this paper elucidates the complexities of leveraging digital technology to foster financial inclusivity. The findings underscore the necessity of evolving regulatory frameworks that harmonize innovation with comprehensive risk management. This paper concludes with policy recommendations for regulators, financial institutions, and technology providers, aiming to cultivate a more inclusive and stable financial ecosystem through prudent digital technology integration.",['Luke Lee'],2024-04-18T05:00:53Z,http://arxiv.org/abs/2404.11898v1,Finance & Economics,Regulatory Technology,"this paper explores the dual impact of digital banks and alternative lenders on financial inclusion . it discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) the paper addresses regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection ."
"Dynamic Spectrum Access: Signal Processing, Networking, and Regulatory   Policy","In this article, we first provide a taxonomy of dynamic spectrum access. We then focus on opportunistic spectrum access, the overlay approach under the hierarchical access model of dynamic spectrum access. we aim to provide an overview of challenges and recent developments in both technological and regulatory aspects of opportunistic spectrum access.","['Qing Zhao', 'Brian M. Sadler']",2006-09-27T08:05:39Z,http://arxiv.org/abs/cs/0609149v1,Finance & Economics,Regulatory Technology,"in this article, we first provide a taxonomy of dynamic spectrum access . then we focus on opportunistic spectrum access, the overlay approach . we aim to provide an overview of challenges and recent developments ."
ICT Support for Regulatory Compliance of Business Processes,"In this paper we propose an ITC (Information and Communication Technology) approach to support regulatory compliance for business processes, and we report on the development and evaluation of a business process compliance checker called Regorous, based on the compliance-by-design methodology proposed by Governatori and Sadiq",['Guido Governatori'],2014-03-26T21:26:46Z,http://arxiv.org/abs/1403.6865v1,Finance & Economics,Regulatory Technology,we propose an ITC approach to support regulatory compliance for business processes . we report on the development and evaluation of a business process compliance checker
Gene regulatory network in single cells based on the Poisson log-normal   model,"Gene regulatory network inference is crucial for understanding the complex molecular interactions in various genetic and environmental conditions. The rapid development of single-cell RNA sequencing (scRNA-seq) technologies unprecedentedly enables gene regulatory networks inference at the single cell resolution. However, traditional graphical models for continuous data, such as Gaussian graphical models, are inappropriate for network inference of scRNA-seq's count data. Here, we model the scRNA-seq data using the multivariate Poisson log-normal (PLN) distribution and represent the precision matrix of the latent normal distribution as the regulatory network. We propose to first estimate the latent covariance matrix using a moment estimator and then estimate the precision matrix by minimizing the lasso-penalized D-trace loss function. We establish the convergence rate of the covariance matrix estimator and further establish the convergence rates and the sign consistency of the proposed PLNet estimator of the precision matrix in the high dimensional setting. The performance of PLNet is evaluated and compared with available methods using simulation and gene regulatory network analysis of scRNA-seq data.","['Feiyi Xiao', 'Junjie Tang', 'Huaying Fang', 'Ruibin Xi']",2021-11-07T09:07:13Z,http://arxiv.org/abs/2111.04037v1,Finance & Economics,Regulatory Technology,rapid development of single-cell RNA sequencing enables gene regulatory networks inference . traditional graphical models for continuous data are inappropriate for network in
Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for   AI-Driven Cybersecurity,"This paper critically examines the evolving ethical and regulatory challenges posed by the integration of artificial intelligence (AI) in cybersecurity. We trace the historical development of AI regulation, highlighting major milestones from theoretical discussions in the 1940s to the implementation of recent global frameworks such as the European Union AI Act. The current regulatory landscape is analyzed, emphasizing risk-based approaches, sector-specific regulations, and the tension between fostering innovation and mitigating risks. Ethical concerns such as bias, transparency, accountability, privacy, and human oversight are explored in depth, along with their implications for AI-driven cybersecurity systems. Furthermore, we propose strategies for promoting AI literacy and public engagement, essential for shaping a future regulatory framework. Our findings underscore the need for a unified, globally harmonized regulatory approach that addresses the unique risks of AI in cybersecurity. We conclude by identifying future research opportunities and recommending pathways for collaboration between policymakers, industry leaders, and researchers to ensure the responsible deployment of AI technologies in cybersecurity.",['Vikram Kulothungan'],2025-01-15T18:17:37Z,http://arxiv.org/abs/2501.10467v1,Finance & Economics,Regulatory Technology,"the current regulatory landscape is analyzed, emphasizing risk-based approaches . findings underscore the need for a unified, globally harmon"
Defining AI in Policy versus Practice,"Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Karen Huang', 'Ghislain Bugingo']",2019-12-23T20:18:21Z,http://arxiv.org/abs/1912.11095v1,Finance & Economics,Regulatory Technology,"legal and regulatory interventions require agreed-upon definitions of AI . consensus around a definition of AI has been elusive, especially in policy"
"Abasy Atlas: A comprehensive inventory of systems, global network   properties and systems-level elements across bacteria","The availability of databases electronically encoding curated regulatory networks and of high-throughput technologies and methods to discover regulatory interactions provides an invaluable source of data to understand the principles underpinning the organization and evolution of these networks responsible for cellular regulation. Nevertheless, data on these sources never goes beyond the regulon level despite the fact that regulatory networks are complex hierarchical-modular structures still challenging our understanding. This brings the necessity for an inventory of systems across a large range of organisms, a key step to rendering feasible comparative systems biology approaches. In this work, we take the first step towards a global understanding of the regulatory networks organization by making a cartography of the functional architectures of diverse bacteria. Abasy (Across-bacteria systems) Atlas provides a comprehensive inventory of annotated functional systems, global network properties, and systems-level elements (global regulators, modular genes shaping functional systems, basal machinery genes, and intermodular genes) predicted by the natural decomposition approach for reconstructed and meta-curated regulatory networks across a large range of bacteria, including pathogenically and biotechnologically relevant organisms. The meta-curation of regulatory datasets provides the most complete and reliable set of regulatory interactions currently available. Abasy Atlas contains systems and system-level elements for 50 regulatory networks comprising 78,649 regulatory interactions covering 42 bacteria in nine taxa, containing 3,708 regulons and 1,776 systems. All this brings together a large corpus of data that will surely inspire studies to generate hypothesis regarding the principles governing the evolution and organization of systems and the functional architectures controlling them.","['Miguel A. Ibarra-Arellano', 'Adrián I. Campos-González', 'Luis G. Treviño-Quintanilla', 'Andreas Tauch', 'Julio A. Freyre-González']",2016-05-16T21:47:02Z,http://arxiv.org/abs/1605.04959v1,Finance & Economics,Regulatory Technology,the meta-curation of regulatory datasets provides the most reliable set of interactions currently available . data on these sources never goes beyond the regul
Regulating Artificial Intelligence: Proposal for a Global Solution,"With increasing ubiquity of artificial intelligence (AI) in modern societies, individual countries and the international community are working hard to create an innovation-friendly, yet safe, regulatory environment. Adequate regulation is key to maximize the benefits and minimize the risks stemming from AI technologies. Developing regulatory frameworks is, however, challenging due to AI's global reach and the existence of widespread misconceptions about the notion of regulation. We argue that AI-related challenges cannot be tackled effectively without sincere international coordination supported by robust, consistent domestic and international governance arrangements. Against this backdrop, we propose the establishment of an international AI governance framework organized around a new AI regulatory agency that -- drawing on interdisciplinary expertise -- could help creating uniform standards for the regulation of AI technologies and inform the development of AI policies around the world. We also believe that a fundamental change of mindset on what constitutes regulation is necessary to remove existing barriers that hamper contemporary efforts to develop AI regulatory regimes, and put forward some recommendations on how to achieve this, and what opportunities doing so would present.","['Olivia J. Erdélyi', 'Judy Goldsmith']",2020-05-22T09:24:07Z,http://arxiv.org/abs/2005.11072v1,Finance & Economics,Regulatory Technology,"the international community is working hard to create an innovation-friendly, yet safe, regulatory environment . a new AI governance framework could help create uniform"
Prediction of a Gene Regulatory Network from Gene Expression Profiles   With Linear Regression and Pearson Correlation Coefficient,"Reconstruction of gene regulatory networks is the process of identifying gene dependency from gene expression profile through some computation techniques. In our human body, though all cells pose similar genetic material but the activation state may vary. This variation in the activation of genes helps researchers to understand more about the function of the cells. Researchers get insight about diseases like mental illness, infectious disease, cancer disease and heart disease from microarray technology, etc. In this study, a cancer-specific gene regulatory network has been constructed using a simple and novel machine learning approach. In First Step, linear regression algorithm provided us the significant genes those expressed themselves differently. Next, regulatory relationships between the identified genes has been computed using Pearson correlation coefficient. Finally, the obtained results have been validated with the available databases and literatures. We can identify the hub genes and can be targeted for the cancer diagnosis.","['Md Mehedi Hassan Onik', 'Shakhawat Ahmmed Nobin', 'Adnan Ferdous Ashrafi', 'Tareque Mohmud Chowdhury']",2018-05-02T01:44:43Z,http://arxiv.org/abs/1805.01506v1,Finance & Economics,Regulatory Technology,"in our human body, though all cells pose similar genetic material but the activation state may vary . a cancer-specific gene regulatory network has"
AI Regulation in Europe: From the AI Act to Future Regulatory Challenges,"This chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to address the multifaceted challenges posed by AI, asserting that, while the Act is a step in the right direction, it has shortcomings that could hinder the advancement of AI technologies. The paper also anticipates upcoming regulatory challenges, such as the management of toxic content, environmental concerns, and hybrid threats. It advocates for immediate action to create protocols for regulated access to high-performance, potentially open-source AI systems. Although the AI Act is a significant legislative milestone, it needs additional refinement and global collaboration for the effective governance of rapidly evolving AI technologies.",['Philipp Hacker'],2023-10-06T07:52:56Z,http://arxiv.org/abs/2310.04072v1,Finance & Economics,Regulatory Technology,chapter provides a comprehensive discussion on AI regulation in the eu . it contrasts it with the more sectoral and self-regul
Legal Case Document Summarization: Extractive and Abstractive Methods   and their Evaluation,"Summarization of legal case judgement documents is a challenging problem in Legal NLP. However, not much analyses exist on how different families of summarization models (e.g., extractive vs. abstractive) perform when applied to legal case documents. This question is particularly important since many recent transformer-based abstractive summarization models have restrictions on the number of input tokens, and legal documents are known to be very long. Also, it is an open question on how best to evaluate legal case document summarization systems. In this paper, we carry out extensive experiments with several extractive and abstractive summarization methods (both supervised and unsupervised) over three legal summarization datasets that we have developed. Our analyses, that includes evaluation by law practitioners, lead to several interesting insights on legal summarization in specific and long document summarization in general.","['Abhay Shukla', 'Paheli Bhattacharya', 'Soham Poddar', 'Rajdeep Mukherjee', 'Kripabandhu Ghosh', 'Pawan Goyal', 'Saptarshi Ghosh']",2022-10-14T05:43:08Z,http://arxiv.org/abs/2210.07544v1,Law & Policy,Legal Document Summarization,extractive vs. abstractive summarization models can be used to summarize legal documents . many recent transformer-based abstractive
LexAbSumm: Aspect-based Summarization of Legal Decisions,"Legal professionals frequently encounter long legal judgments that hold critical insights for their work. While recent advances have led to automated summarization solutions for legal documents, they typically provide generic summaries, which may not meet the diverse information needs of users. To address this gap, we introduce LexAbSumm, a novel dataset designed for aspect-based summarization of legal case decisions, sourced from the European Court of Human Rights jurisdiction. We evaluate several abstractive summarization models tailored for longer documents on LexAbSumm, revealing a challenge in conditioning these models to produce aspect-specific summaries. We release LexAbSum to facilitate research in aspect-based summarization for legal domain.","['T. Y. S. S Santosh', 'Mahmoud Aly', 'Matthias Grabmair']",2024-03-31T08:00:40Z,http://arxiv.org/abs/2404.00594v1,Law & Policy,Legal Document Summarization,legal professionals often encounter long legal judgments that hold critical insights . recent advances have led to automated summarization solutions for legal documents . but
Incorporating Domain Knowledge for Extractive Summarization of Legal   Case Documents,"Automatic summarization of legal case documents is an important and practical challenge. Apart from many domain-independent text summarization algorithms that can be used for this purpose, several algorithms have been developed specifically for summarizing legal case documents. However, most of the existing algorithms do not systematically incorporate domain knowledge that specifies what information should ideally be present in a legal case document summary. To address this gap, we propose an unsupervised summarization algorithm DELSumm which is designed to systematically incorporate guidelines from legal experts into an optimization setup. We conduct detailed experiments over case documents from the Indian Supreme Court. The experiments show that our proposed unsupervised method outperforms several strong baselines in terms of ROUGE scores, including both general summarization algorithms and legal-specific ones. In fact, though our proposed algorithm is unsupervised, it outperforms several supervised summarization models that are trained over thousands of document-summary pairs.","['Paheli Bhattacharya', 'Soham Poddar', 'Koustav Rudra', 'Kripabandhu Ghosh', 'Saptarshi Ghosh']",2021-06-30T08:06:15Z,http://arxiv.org/abs/2106.15876v1,Law & Policy,Legal Document Summarization,delhi's proposed unsupervised summarization algorithm is unsupervised . it incorporates guidelines from legal experts into an optimization setup 
Structured Legal Document Generation in India: A Model-Agnostic Wrapper   Approach with VidhikDastaavej,"Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.","['Shubham Kumar Nigam', 'Balaramamahanthi Deepak Patnaik', 'Ajay Varghese Thomas', 'Noel Shallum', 'Kripabandhu Ghosh', 'Arnab Bhattacharya']",2025-04-04T14:41:50Z,http://arxiv.org/abs/2504.03486v1,Law & Policy,Legal Document Summarization,the structured generation of private legal documents in the Indian legal domain remains largely unaddressed . to ensure real-world applic
MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of   Indian Legal Case Judgments,"Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity -- Indian legal case judgments are mostly written in complex English, but a significant portion of India's population lacks command of the English language. Hence, it is crucial to summarize the legal documents in Indian languages to ensure equitable access to justice. While prior research primarily focuses on summarizing legal case judgments in their source languages, this study presents a pioneering effort toward cross-lingual summarization of English legal documents into Hindi, the most frequently spoken Indian language. We construct the first high-quality legal corpus comprising of 3,122 case judgments from prominent Indian courts in English, along with their summaries in both English and Hindi, drafted by legal practitioners. We benchmark the performance of several diverse summarization approaches on our corpus and demonstrate the need for further research in cross-lingual summarization in the legal domain.","['Debtanu Datta', 'Shubham Soni', 'Rajdeep Mukherjee', 'Saptarshi Ghosh']",2023-10-28T05:51:57Z,http://arxiv.org/abs/2310.18600v1,Law & Policy,Legal Document Summarization,a significant portion of india's population lacks command of the english language . this study presents a pioneering effort toward cross-lingual
ArgLegalSumm: Improving Abstractive Summarization of Legal Documents   with Argument Mining,A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines,"['Mohamed Elaraby', 'Diane Litman']",2022-09-04T15:55:56Z,http://arxiv.org/abs/2209.01650v2,Law & Policy,Legal Document Summarization,we introduce a technique to capture the argumentative structure of legal documents . we integrate argument role labeling into the summarization process .
Towards Argument-Aware Abstractive Summarization of Long Legal Opinions   with Summary Reranking,"We propose a simple approach for the abstractive summarization of long legal opinions that considers the argument structure of the document. Legal opinions often contain complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main points of the legal opinion. Our approach involves using argument role information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document's argument structure. We demonstrate the effectiveness of our approach on a dataset of long legal opinions and show that it outperforms several strong baselines.","['Mohamed Elaraby', 'Yang Zhong', 'Diane Litman']",2023-06-01T13:44:45Z,http://arxiv.org/abs/2306.00672v1,Law & Policy,Legal Document Summarization,we propose a simple approach for the abstractive summarization of long legal opinions . our approach uses argument role information to generate multiple candidate sum
Computing and Exploiting Document Structure to Improve Unsupervised   Extractive Summarization of Legal Case Decisions,"Though many algorithms can be used to automatically summarize legal case decisions, most fail to incorporate domain knowledge about how important sentences in a legal decision relate to a representation of its document structure. For example, analysis of a legal case summarization dataset demonstrates that sentences serving different types of argumentative roles in the decision appear in different sections of the document. In this work, we propose an unsupervised graph-based ranking model that uses a reweighting algorithm to exploit properties of the document structure of legal case decisions. We also explore the impact of using different methods to compute the document structure. Results on the Canadian Legal Case Law dataset show that our proposed method outperforms several strong baselines.","['Yang Zhong', 'Diane Litman']",2022-11-06T22:20:42Z,http://arxiv.org/abs/2211.03229v1,Law & Policy,Legal Document Summarization,many algorithms can be used to automatically summarize legal case decisions . but most fail to incorporate domain knowledge about how important sentences are . we propose an
A Deep Learning-Based System for Automatic Case Summarization,"This paper presents a deep learning-based system for efficient automatic case summarization. Leveraging state-of-the-art natural language processing techniques, the system offers both supervised and unsupervised methods to generate concise and relevant summaries of lengthy legal case documents. The user-friendly interface allows users to browse the system's database of legal case documents, select their desired case, and choose their preferred summarization method. The system generates comprehensive summaries for each subsection of the legal text as well as an overall summary. This demo streamlines legal case document analysis, potentially benefiting legal professionals by reducing workload and increasing efficiency. Future work will focus on refining summarization techniques and exploring the application of our methods to other types of legal texts.","['Minh Duong', 'Long Nguyen', 'Yen Vuong', 'Trong Le', 'Ha-Thanh Nguyen']",2023-12-13T01:18:10Z,http://arxiv.org/abs/2312.07824v1,Law & Policy,Legal Document Summarization,this paper presents a deep learning-based system for efficient automatic case summarization . the system offers both supervised and unsupervised methods . it generates concise and relevant summaries of lengthy legal case documents .
Corpus for Automatic Structuring of Legal Documents,"In populous countries, pending legal cases have been growing exponentially. There is a need for developing techniques for processing and organizing legal documents. In this paper, we introduce a new corpus for structuring legal documents. In particular, we introduce a corpus of legal judgment documents in English that are segmented into topical and coherent parts. Each of these parts is annotated with a label coming from a list of pre-defined Rhetorical Roles. We develop baseline models for automatically predicting rhetorical roles in a legal document based on the annotated corpus. Further, we show the application of rhetorical roles to improve performance on the tasks of summarization and legal judgment prediction. We release the corpus and baseline model code along with the paper.","['Prathamesh Kalamkar', 'Aman Tiwari', 'Astha Agarwal', 'Saurabh Karn', 'Smita Gupta', 'Vivek Raghavan', 'Ashutosh Modi']",2022-01-31T11:12:44Z,http://arxiv.org/abs/2201.13125v2,Law & Policy,Legal Document Summarization,"in populous countries, pending legal cases have been growing exponentially . there is a need for developing techniques for processing and organizing legal documents . we develop baseline models for automatically predicting rhetorical roles in a legal document ."
Unlocking Practical Applications in Legal Domain: Evaluation of GPT for   Zero-Shot Semantic Annotation of Legal Texts,"We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F1=.73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.",['Jaromir Savelka'],2023-05-08T01:55:53Z,http://arxiv.org/abs/2305.04417v1,Law & Policy,Legal Document Summarization,the ability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) comes from legal documents of various types . the model performs surprisingly well in zero-shot settings on diverse types of documents .
Indian Legal Text Summarization: A Text Normalisation-based Approach,"In the Indian court system, pending cases have long been a problem. There are more than 4 crore cases outstanding. Manually summarising hundreds of documents is a time-consuming and tedious task for legal stakeholders. Many state-of-the-art models for text summarization have emerged as machine learning has progressed. Domain-independent models don't do well with legal texts, and fine-tuning those models for the Indian Legal System is problematic due to a lack of publicly available datasets. To improve the performance of domain-independent models, the authors have proposed a methodology for normalising legal texts in the Indian context. The authors experimented with two state-of-the-art domain-independent models for legal text summarization, namely BART and PEGASUS. BART and PEGASUS are put through their paces in terms of extractive and abstractive summarization to understand the effectiveness of the text normalisation approach. Summarised texts are evaluated by domain experts on multiple parameters and using ROUGE metrics. It shows the proposed text normalisation approach is effective in legal texts with domain-independent models.","['Satyajit Ghosh', 'Mousumi Dutta', 'Tanaya Das']",2022-06-13T15:16:50Z,http://arxiv.org/abs/2206.06238v2,Law & Policy,Legal Document Summarization,"in the Indian court system, pending cases have long been a problem . there are more than 4 crore cases outstanding . to improve the performance of domain-independent models, authors propose a methodology ."
Robust Deep Reinforcement Learning for Extractive Legal Summarization,"Automatic summarization of legal texts is an important and still a challenging task since legal documents are often long and complicated with unusual structures and styles. Recent advances of deep models trained end-to-end with differentiable losses can well-summarize natural text, yet when applied to legal domain, they show limited results. In this paper, we propose to use reinforcement learning to train current deep summarization models to improve their performance on the legal domain. To this end, we adopt proximal policy optimization methods and introduce novel reward functions that encourage the generation of candidate summaries satisfying both lexical and semantic criteria. We apply our method to training different summarization backbones and observe a consistent and significant performance gain across 3 public legal datasets.","['Duy-Hung Nguyen', 'Bao-Sinh Nguyen', 'Nguyen Viet Dung Nghiem', 'Dung Tien Le', 'Mim Amina Khatun', 'Minh-Tien Nguyen', 'Hung Le']",2021-11-13T17:27:49Z,http://arxiv.org/abs/2111.07158v2,Law & Policy,Legal Document Summarization,"automatic summarization of legal documents is an important and challenging task . when applied to legal domain, they show limited results . this paper proposes to use reinforcement learning to train current deep summarisation models ."
"Multi-Task Deep Learning for Legal Document Translation, Summarization   and Multi-Label Classification","The digitalization of the legal domain has been ongoing for a couple of years. In that process, the application of different machine learning (ML) techniques is crucial. Tasks such as the classification of legal documents or contract clauses as well as the translation of those are highly relevant. On the other side, digitized documents are barely accessible in this field, particularly in Germany. Today, deep learning (DL) is one of the hot topics with many publications and various applications. Sometimes it provides results outperforming the human level. Hence this technique may be feasible for the legal domain as well. However, DL requires thousands of samples to provide decent results. A potential solution to this problem is multi-task DL to enable transfer learning. This approach may be able to overcome the data scarcity problem in the legal domain, specifically for the German language. We applied the state of the art multi-task model on three tasks: translation, summarization, and multi-label classification. The experiments were conducted on legal document corpora utilizing several task combinations as well as various model parameters. The goal was to find the optimal configuration for the tasks at hand within the legal domain. The multi-task DL approach outperformed the state of the art results in all three tasks. This opens a new direction to integrate DL technology more efficiently in the legal domain.","['Ahmed Elnaggar', 'Christoph Gebendorfer', 'Ingo Glaser', 'Florian Matthes']",2018-10-16T08:54:50Z,http://arxiv.org/abs/1810.07513v1,Law & Policy,Legal Document Summarization,the digitalization of the legal domain has been ongoing for a couple of years . the application of different machine learning (ML) techniques is crucial . a potential solution to this problem is multi-task DL to enable transfer learning .
CoPERLex: Content Planning with Event-based Representations for Legal   Case Summarization,"Legal professionals often struggle with lengthy judgments and require efficient summarization for quick comprehension. To address this challenge, we investigate the need for structured planning in legal case summarization, particularly through event-centric representations that reflect the narrative nature of legal case documents. We propose our framework, CoPERLex, which operates in three stages: first, it performs content selection to identify crucial information from the judgment; second, the selected content is utilized to generate intermediate plans through event-centric representations modeled as Subject-Verb-Object tuples; and finally, it generates coherent summaries based on both the content and the structured plan. Our experiments on four legal summarization datasets demonstrate the effectiveness of integrating content selection and planning components, highlighting the advantages of event-centric plans over traditional entity-centric approaches in the context of legal judgements.","['T. Y. S. S. Santosh', 'Youssef Farag', 'Matthias Grabmair']",2025-01-23T22:03:45Z,http://arxiv.org/abs/2501.14112v1,Law & Policy,Legal Document Summarization,"legal professionals often struggle with lengthy judgments and require efficient summarization . to address this challenge, we investigate the need for structured planning in legal case summaries . our experiments demonstrate the effectiveness of integrating content selection and planning components ."
Applicability of Large Language Models and Generative Models for Legal   Case Judgement Summarization,"Automatic summarization of legal case judgements, which are known to be long and complex, has traditionally been tried via extractive summarization models. In recent years, generative models including abstractive summarization models and Large language models (LLMs) have gained huge popularity. In this paper, we explore the applicability of such models for legal case judgement summarization. We applied various domain specific abstractive summarization models and general domain LLMs as well as extractive summarization models over two sets of legal case judgements from the United Kingdom (UK) Supreme Court and the Indian (IN) Supreme Court and evaluated the quality of the generated summaries. We also perform experiments on a third dataset of legal documents of a different type, Government reports from the United States (US). Results show that abstractive summarization models and LLMs generally perform better than the extractive methods as per traditional metrics for evaluating summary quality. However, detailed investigation shows the presence of inconsistencies and hallucinations in the outputs of the generative models, and we explore ways to reduce the hallucinations and inconsistencies in the summaries. Overall, the investigation suggests that further improvements are needed to enhance the reliability of abstractive models and LLMs for legal case judgement summarization. At present, a human-in-the-loop technique is more suitable for performing manual checks to identify inconsistencies in the generated summaries.","['Aniket Deroy', 'Kripabandhu Ghosh', 'Saptarshi Ghosh']",2024-07-06T04:49:40Z,http://arxiv.org/abs/2407.12848v2,Law & Policy,Legal Document Summarization,generative models have gained huge popularity for summarizing legal case judgements . abstractive summarization models and large language models (LLMs) generally perform better than extractive methods . detailed investigation shows the presence of inconsistencies and hallucinations in the summaries .
PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for   Legal Domain Adaptation?,"In this paper, we present Paramanu-Ayn, a collection of legal language models trained exclusively on Indian legal case documents. This 97-million-parameter Auto-Regressive (AR) decoder-only model was pretrained from scratch with a context size of 8192 on a single GPU for just 185 hours, achieving an efficient MFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We evaluated our model using perplexity and zero-shot tasks: case judgment prediction with explanation and abstractive case summarization. Paramanu-Ayn outperformed Llama-2 7B and Gemini-Pro in case judgment prediction with explanation task on test accuracy by nearly 2 percentage points, despite being 72 times smaller. In zero-shot abstractive summarization, it surpassed decoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10 percentage points in BLEU and METEOR metrics, and by nearly 4 percentage points in BERTScore. Further evaluations on zero-shot commonsense and mathematical benchmarks showed that Paramanu-Ayn excelled despite being trained exclusively on legal documents, outperforming Llama-1, Llama-2, and Falcon on AGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our model on 10,763 diverse legal tasks, including legal clause generation, legal drafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above 8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by GPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge and generalize to draft legal contracts and legal clauses with limited instruction-tuning. Hence, we conclude that for a strong domain-specialized generative language model (such as legal), domain specialized pretraining from scratch is more cost effective, environmentally friendly, and remains competitive with larger models or even better than adapting LLMs for legal domain tasks.","['Mitodru Niyogi', 'Arnab Bhattacharya']",2024-03-20T15:39:54Z,http://arxiv.org/abs/2403.13681v2,Law & Policy,Legal Document Summarization,paramanu-ayn is a collection of legal language models trained exclusively on legal documents . the 97-million-parameter auto-regressive (AR) decoder-only model was pretrained from scratch . we also developed a legal domain specialized BPE tokenizer .
Building Legal Case Retrieval Systems with Lexical Matching and   Summarization using A Pre-Trained Phrase Scoring Model,"We present our method for tackling the legal case retrieval task of the Competition on Legal Information Extraction/Entailment 2019. Our approach is based on the idea that summarization is important for retrieval. On one hand, we adopt a summarization based model called encoded summarization which encodes a given document into continuous vector space which embeds the summary properties of the document. We utilize the resource of COLIEE 2018 on which we train the document representation model. On the other hand, we extract lexical features on different parts of a given query and its candidates. We observe that by comparing different parts of the query and its candidates, we can achieve better performance. Furthermore, the combination of the lexical features with latent features by the summarization-based method achieves even better performance. We have achieved the state-of-the-art result for the task on the benchmark of the competition.","['Vu Tran', 'Minh Le Nguyen', 'Ken Satoh']",2020-09-29T15:10:59Z,http://arxiv.org/abs/2009.14083v1,Law & Policy,Legal Document Summarization,the method is based on the idea that summarization is important for retrieval . we extract lexical features on different parts of a given query and its candidates . the method achieves the state-of-the-art result on the benchmark of the competition .
Identification of Rhetorical Roles of Sentences in Indian Legal   Judgments,"Automatically understanding the rhetorical roles of sentences in a legal case judgement is an important problem to solve, since it can help in several downstream tasks like summarization of legal judgments, legal search, and so on. The task is challenging since legal case documents are usually not well-structured, and these rhetorical roles may be subjective (as evident from variation of opinions between legal experts). In this paper, we address this task for judgments from the Supreme Court of India. We label sentences in 50 documents using multiple human annotators, and perform an extensive analysis of the human-assigned labels. We also attempt automatic identification of the rhetorical roles of sentences. While prior approaches towards this task used Conditional Random Fields over manually handcrafted features, we explore the use of deep neural models which do not require hand-crafting of features. Experiments show that neural models perform much better in this task than baseline methods which use handcrafted features.","['Paheli Bhattacharya', 'Shounak Paul', 'Kripabandhu Ghosh', 'Saptarshi Ghosh', 'Adam Wyner']",2019-11-13T11:21:20Z,http://arxiv.org/abs/1911.05405v1,Law & Policy,Legal Document Summarization,automatic understanding of rhetorical roles of sentences in a legal case can help in downstream tasks . we label sentences in 50 documents using multiple human annotators . deep neural models perform much better in this task than baseline methods .
Hybrid Deep Learning for Legal Text Analysis: Predicting Punishment   Durations in Indonesian Court Rulings,"Limited public understanding of legal processes and inconsistent verdicts in the Indonesian court system led to widespread dissatisfaction and increased stress on judges. This study addresses these issues by developing a deep learning-based predictive system for court sentence lengths. Our hybrid model, combining CNN and BiLSTM with attention mechanism, achieved an R-squared score of 0.5893, effectively capturing both local patterns and long-term dependencies in legal texts. While document summarization proved ineffective, using only the top 30% most frequent tokens increased prediction performance, suggesting that focusing on core legal terminology balances information retention and computational efficiency. We also implemented a modified text normalization process, addressing common errors like misspellings and incorrectly merged words, which significantly improved the model's performance. These findings have important implications for automating legal document processing, aiding both professionals and the public in understanding court judgments. By leveraging advanced NLP techniques, this research contributes to enhancing transparency and accessibility in the Indonesian legal system, paving the way for more consistent and comprehensible legal decisions.","['Muhammad Amien Ibrahim', 'Alif Tri Handoyo', 'Maria Susan Anggreainy']",2024-10-26T07:07:48Z,http://arxiv.org/abs/2410.20104v1,Law & Policy,Legal Document Summarization,a deep learning-based predictive system for court sentence lengths was developed . the model captures both local patterns and long-term dependencies in legal texts . findings have important implications for automating legal document processing .
An Ontological AI-and-Law Framework for the Autonomous Levels of AI   Legal Reasoning,"A framework is proposed that seeks to identify and establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR). Doing so provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law, and can be utilized by scholars in academic pursuits of AI legal reasoning, along with being used by law practitioners and legal professionals in gauging how advances in AI are aiding the practice of law and the realization of aspirational versus achieved results. A set of seven levels of autonomy for AI and Legal Reasoning are meticulously proffered and mindfully discussed.",['Lance Eliot'],2020-08-04T16:12:30Z,http://arxiv.org/abs/2008.07328v1,Law & Policy,AI in Legal Reasoning,a framework is proposed that seeks to establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR) the framework provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law .
Robustness and Overcoming Brittleness of AI-Enabled Legal   Micro-Directives: The Role of Autonomous Levels of AI Legal Reasoning,"Recent research by legal scholars suggests that the law might inevitably be transformed into legal micro-directives consisting of legal rules that are derived from legal standards or that are otherwise produced automatically or via the consequent derivations of legal goals and then propagated via automation for everyday use as readily accessible lawful directives throughout society. This paper examines and extends the legal micro-directives theories in three crucial respects: (1) By indicating that legal micro-directives are likely to be AI-enabled and evolve over time in scope and velocity across the autonomous levels of AI Legal Reasoning, (2) By exploring the trade-offs between legal standards and legal rules as the imprinters of the micro-directives, and (3) By illuminating a set of brittleness exposures that can undermine legal micro-directives and proffering potential mitigating remedies to seek greater robustness in the instantiation and promulgation of such AI-powered lawful directives.",['Lance Eliot'],2020-08-31T05:09:03Z,http://arxiv.org/abs/2009.02243v1,Law & Policy,AI in Legal Reasoning,recent research suggests that the law might inevitably be transformed into legal micro-directives . these are legal rules that are derived from legal standards or otherwise produced automatically . they are then propagated via automation for everyday use as readily accessible lawful directives .
AI and Legal Argumentation: Aligning the Autonomous Levels of AI Legal   Reasoning,"Legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law, and extensive research has attempted to augment or undertake legal argumentation via the use of computer-based automation including Artificial Intelligence (AI). AI advances in Natural Language Processing (NLP) and Machine Learning (ML) have especially furthered the capabilities of leveraging AI for aiding legal professionals, doing so in ways that are modeled here as CARE, namely Crafting, Assessing, Refining, and Engaging in legal argumentation. In addition to AI-enabled legal argumentation serving to augment human-based lawyering, an aspirational goal of this multi-disciplinary field consists of ultimately achieving autonomously effected human-equivalent legal argumentation. As such, an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to the maturation of AI and Legal Argumentation (AILA), proffering a new means of gauging progress in this ever-evolving and rigorously sought domain.",['Lance Eliot'],2020-09-11T22:05:40Z,http://arxiv.org/abs/2009.11180v1,Law & Policy,AI in Legal Reasoning,"legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law . research has attempted to augment or undertake legal arguments via the use of AI . an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning ."
"Tasks and Roles in Legal AI: Data Curation, Annotation, and Verification","The application of AI tools to the legal field feels natural: large legal document collections could be used with specialized AI to improve workflow efficiency for lawyers and ameliorate the ""justice gap"" for underserved clients. However, legal documents differ from the web-based text that underlies most AI systems. The challenges of legal AI are both specific to the legal domain, and confounded with the expectation of AI's high performance in high-stakes settings. We identify three areas of special relevance to practitioners: data curation, data annotation, and output verification. First, it is difficult to obtain usable legal texts. Legal collections are inconsistent, analog, and scattered for reasons technical, economic, and jurisdictional. AI tools can assist document curation efforts, but the lack of existing data also limits AI performance. Second, legal data annotation typically requires significant expertise to identify complex phenomena such as modes of judicial reasoning or controlling precedents. We describe case studies of AI systems that have been developed to improve the efficiency of human annotation in legal contexts and identify areas of underperformance. Finally, AI-supported work in the law is valuable only if results are verifiable and trustworthy. We describe both the abilities of AI systems to support evaluation of their outputs, as well as new approaches to systematic evaluation of computational systems in complex domains. We call on both legal and AI practitioners to collaborate across disciplines and to release open access materials to support the development of novel, high-performing, and reliable AI tools for legal applications.","['Allison Koenecke', 'Jed Stiglitz', 'David Mimno', 'Matthew Wilkens']",2025-04-02T04:34:58Z,http://arxiv.org/abs/2504.01349v1,Law & Policy,AI in Legal Reasoning,legal documents differ from web-based text that underlies most AI systems . challenges of legal AI are both specific to the legal domain . lack of existing data limits AI performance .
Legal Judgment Prediction (LJP) Amid the Advent of Autonomous AI Legal   Reasoning,"Legal Judgment Prediction (LJP) is a longstanding and open topic in the theory and practice-of-law. Predicting the nature and outcomes of judicial matters is abundantly warranted, keenly sought, and vigorously pursued by those within the legal industry and also by society as a whole. The tenuous act of generating judicially laden predictions has been limited in utility and exactitude, requiring further advancement. Various methods and techniques to predict legal cases and judicial actions have emerged over time, especially arising via the advent of computer-based modeling. There has been a wide range of approaches attempted, including simple calculative methods to highly sophisticated and complex statistical models. Artificial Intelligence (AI) based approaches have also been increasingly utilized. In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy. Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu.",['Lance Eliot'],2020-09-29T00:12:42Z,http://arxiv.org/abs/2009.14620v1,Law & Policy,AI in Legal Reasoning,legal judgment prediction (LJP) is a longstanding and open topic in the theory and practice-of-law . the tenuous act of generating judicially laden predictions has been limited in utility and exactitude . a review of the literature encompassing Legal Judgment Prediction is undertaken .
Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal   Assistance,"Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well.","['Jatin Gupta', 'Akhil Sharma', 'Saransh Singhania', 'Ali Imam Abidi']",2025-05-28T06:06:53Z,http://arxiv.org/abs/2505.22003v1,Law & Policy,AI in Legal Reasoning,"Legal Assist AI is a transformer-based model designed to bridge this gap . the system retrieves relevant legal information from a curated database . it generates accurate responses, enabling effective assistance for diverse users ."
Multidimensionality of Legal Singularity: Parametric Analysis and the   Autonomous Levels of AI Legal Reasoning,"Legal scholars have in the last several years embarked upon an ongoing discussion and debate over a potential Legal Singularity that might someday occur, involving a variant or law-domain offshoot leveraged from the Artificial Intelligence (AI) realm amid its many decades of deliberations about an overarching and generalized technological singularity (referred to classically as The Singularity). This paper examines the postulated Legal Singularity and proffers that such AI and Law cogitations can be enriched by these three facets addressed herein: (1) dovetail additionally salient considerations of The Singularity into the Legal Singularity, (2) make use of an in-depth and innovative multidimensional parametric analysis of the Legal Singularity as posited in this paper, and (3) align and unify the Legal Singularity with the Levels of Autonomy (LoA) associated with AI Legal Reasoning (AILR) as propounded in this paper.",['Lance Eliot'],2020-08-24T17:28:35Z,http://arxiv.org/abs/2008.10575v1,Law & Policy,AI in Legal Reasoning,this paper examines the postulated Legal Singularity . it posits a variant or law-domain offshoot leveraged from the Artificial Intelligence realm . this paper proposes that such AI and Law cogitations can be enriched .
LeKUBE: A Legal Knowledge Update BEnchmark,"Recent advances in Large Language Models (LLMs) have significantly shaped the applications of AI in multiple fields, including the studies of legal intelligence. Trained on extensive legal texts, including statutes and legal documents, the legal LLMs can capture important legal knowledge/concepts effectively and provide important support for downstream legal applications such as legal consultancy. Yet, the dynamic nature of legal statutes and interpretations also poses new challenges to the use of LLMs in legal applications. Particularly, how to update the legal knowledge of LLMs effectively and efficiently has become an important research problem in practice. Existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain and cannot address the specific challenges of the legal domain, such as the nuanced application of new legal knowledge, the complexity and lengthiness of legal regulations, and the intricate nature of legal reasoning. To address this gap, we introduce the Legal Knowledge Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for legal LLMs across five dimensions. Specifically, we categorize the needs of knowledge updates in the legal domain with the help of legal professionals, and then hire annotators from law schools to create synthetic updates to the Chinese Criminal and Civil Code as well as sets of questions of which the answers would change after the updates. Through a comprehensive evaluation of state-of-the-art knowledge update methods, we reveal a notable gap between existing knowledge update methods and the unique needs of the legal domain, emphasizing the need for further research and development of knowledge update mechanisms tailored for legal LLMs.","['Changyue Wang', 'Weihang Su', 'Hu Yiran', 'Qingyao Ai', 'Yueyue Wu', 'Cheng Luo', 'Yiqun Liu', 'Min Zhang', 'Shaoping Ma']",2024-07-19T10:40:10Z,http://arxiv.org/abs/2407.14192v2,Law & Policy,AI in Legal Reasoning,how to update the legal knowledge of LLMs effectively and efficiently is an important research problem . existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain .
Legal Sentiment Analysis and Opinion Mining (LSAOM): Assimilating   Advances in Autonomous AI Legal Reasoning,"An expanding field of substantive interest for the theory of the law and the practice-of-law entails Legal Sentiment Analysis and Opinion Mining (LSAOM), consisting of two often intertwined phenomena and actions underlying legal discussions and narratives: (1) Sentiment Analysis (SA) for the detection of expressed or implied sentiment about a legal matter within the context of a legal milieu, and (2) Opinion Mining (OM) for the identification and illumination of explicit or implicit opinion accompaniments immersed within legal discourse. Efforts to undertake LSAOM have historically been performed by human hand and cognition, and only thinly aided in more recent times by the use of computer-based approaches. Advances in Artificial Intelligence (AI) involving especially Natural Language Processing (NLP) and Machine Learning (ML) are increasingly bolstering how automation can systematically perform either or both of Sentiment Analysis and Opinion Mining, all of which is being inexorably carried over into engagement within a legal context for improving LSAOM capabilities. This research paper examines the evolving infusion of AI into Legal Sentiment Analysis and Opinion Mining and proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR), plus provides additional insights regarding AI LSAOM in its mechanizations and potential impact to the study of law and the practicing of law.",['Lance Eliot'],2020-10-02T04:15:21Z,http://arxiv.org/abs/2010.02726v1,Law & Policy,AI in Legal Reasoning,a growing field of substantive interest entails legal Sentiment Analysis and Opinion Mining (LSAOM) advances in Artificial Intelligence (AI) are bolstering how automation can perform either or both of LSAOM . this research paper proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning .
Continual Pre-Training is (not) What You Need in Domain Adaption,"The recent advances in Legal Large Language Models (LLMs) have transformed the landscape of legal research and practice by automating tasks, enhancing research precision, and supporting complex decision-making processes. However, effectively adapting LLMs to the legal domain remains challenging due to the complexity of legal reasoning, the need for precise interpretation of specialized language, and the potential for hallucinations. This paper examines the efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the legal reasoning capabilities of LLMs. Through a series of experiments on legal reasoning tasks within the Taiwanese legal framework, we demonstrate that while DACP enhances domain-specific knowledge, it does not uniformly improve performance across all legal tasks. We discuss the trade-offs involved in DACP, particularly its impact on model generalization and performance in prompt-based tasks, and propose directions for future research to optimize domain adaptation strategies in legal AI.","['Pin-Er Chen', 'Da-Chen Lian', 'Shu-Kai Hsieh', 'Sieh-Chuen Huang', 'Hsuan-Lei Shao', 'Jun-Wei Chiu', 'Yang-Hsien Lin', 'Zih-Ching Chen', 'Cheng-Kuang', 'Eddie TC Huang', 'Simon See']",2025-04-18T10:14:51Z,http://arxiv.org/abs/2504.13603v1,Law & Policy,AI in Legal Reasoning,this paper examines the efficacy of Domain-Adaptive Continual Pre-Training . DACP does not uniformly improve performance across all legal tasks .
Legal Evalutions and Challenges of Large Language Models,"In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.","['Jiaqi Wang', 'Huan Zhao', 'Zhenyuan Yang', 'Peng Shu', 'Junhao Chen', 'Haobo Sun', 'Ruixi Liang', 'Shixin Li', 'Pengcheng Shi', 'Longjun Ma', 'Zongjia Liu', 'Zhengliang Liu', 'Tianyang Zhong', 'Yutong Zhang', 'Chong Ma', 'Xin Zhang', 'Tuo Zhang', 'Tianli Ding', 'Yudan Ren', 'Tianming Liu', 'Xi Jiang', 'Shu Zhang']",2024-11-15T12:23:12Z,http://arxiv.org/abs/2411.10137v1,Law & Policy,AI in Legal Reasoning,"in this paper, we review legal testing methods based on large language models . systematic tests are conducted on English and Chinese legal cases . the experimental results highlight both the potential and limitations of LLMs ."
Using LLMs to Discover Legal Factors,"Factors are a foundational component of legal analysis and computational models of legal reasoning. These factor-based representations enable lawyers, judges, and AI and Law researchers to reason about legal cases. In this paper, we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain. Our method takes as input raw court opinions and produces a set of factors and associated definitions. We demonstrate that a semi-automated approach, incorporating minimal human involvement, produces factor representations that can predict case outcomes with moderate success, if not yet as well as expert-defined factors can.","['Morgan Gray', 'Jaromir Savelka', 'Wesley Oliver', 'Kevin Ashley']",2024-10-10T00:42:10Z,http://arxiv.org/abs/2410.07504v1,Law & Policy,AI in Legal Reasoning,we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain . our method takes as input raw court opinions and produces a set of factors .
An Impact Model of AI on the Principles of Justice: Encompassing the   Autonomous Levels of AI Legal Reasoning,"Efforts furthering the advancement of Artificial Intelligence (AI) will increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the practice of law. It is argued in this research paper that the infusion of AI into existing and future legal activities and the judicial structure needs to be undertaken by mindfully observing an alignment with the core principles of justice. As such, the adoption of AI has a profound twofold possibility of either usurping the principles of justice, doing so in a Dystopian manner, and yet also capable to bolster the principles of justice, doing so in a Utopian way. By examining the principles of justice across the Levels of Autonomy (LoA) of AI Legal Reasoning, the case is made that there is an ongoing tension underlying the efforts to develop and deploy AI that can demonstrably determine the impacts and sway upon each core principle of justice and the collective set.",['Lance Eliot'],2020-08-26T22:56:41Z,http://arxiv.org/abs/2008.12615v1,Law & Policy,AI in Legal Reasoning,"eric liu: infusion of AI needs to be mindfully observing core principles of justice . he says adoption of AI has twofold possibility of usurping the justice principles . there is an ongoing tension underlying efforts to develop and deploy AI, he writes ."
Promises and pitfalls of artificial intelligence for legal applications,"Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.","['Sayash Kapoor', 'Peter Henderson', 'Arvind Narayanan']",2024-01-10T19:50:37Z,http://arxiv.org/abs/2402.01656v1,Law & Policy,AI in Legal Reasoning,we dive into AI's increasingly prevalent roles in three types of legal tasks . tasks that would lead to most significant changes to the legal profession are harder to evaluate . we recommend better evaluation and deployment of AI in legal contexts .
CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation,"Legal case documents play a critical role in judicial proceedings. As the number of cases continues to rise, the reliance on manual drafting of legal case documents is facing increasing pressure and challenges. The development of large language models (LLMs) offers a promising solution for automating document generation. However, existing benchmarks fail to fully capture the complexities involved in drafting legal case documents in real-world scenarios. To address this gap, we introduce CaseGen, the benchmark for multi-stage legal case documents generation in the Chinese legal domain. CaseGen is based on 500 real case samples annotated by legal experts and covers seven essential case sections. It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in the context of legal case document generation. To ensure an accurate and comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and validate its effectiveness through human annotations. We evaluate several widely used general-domain LLMs and legal-specific LLMs, highlighting their limitations in case document generation and pinpointing areas for potential improvement. This work marks a step toward a more effective framework for automating legal case documents drafting, paving the way for the reliable application of AI in the legal field. The dataset and code are publicly available at https://github.com/CSHaitao/CaseGen.","['Haitao Li', 'Jiaying Ye', 'Yiran Hu', 'Jia Chen', 'Qingyao Ai', 'Yueyue Wu', 'Junjie Chen', 'Yifan Chen', 'Cheng Luo', 'Quan Zhou', 'Yiqun Liu']",2025-02-25T08:03:32Z,http://arxiv.org/abs/2502.17943v1,Law & Policy,AI in Legal Reasoning,"caseGen is the benchmark for multi-stage legal case documents generation . it supports drafting defense statements, writing trial facts, composing legal reasoning . the benchmark is based on 500 real case samples annotated by legal experts ."
An Argumentation-Based Legal Reasoning Approach for DL-Ontology,"Ontology is a popular method for knowledge representation in different domains, including the legal domain, and description logics (DL) is commonly used as its description language. To handle reasoning based on inconsistent DL-based legal ontologies, the current paper presents a structured argumentation framework particularly for reasoning in legal contexts on the basis of ASPIC+, and translates the legal ontology into formulas and rules of an argumentation theory. With a particular focus on the design of autonomous vehicles from the perspective of legal AI, we show that using this combined theory of formal argumentation and DL-based legal ontology, acceptable assertions can be obtained based on inconsistent ontologies, and the traditional reasoning tasks of DL ontologies can also be accomplished. In addition, a formal definition of explanations for the result of reasoning is presented.","['Zhe Yu', 'Yiwei Lu']",2022-09-07T11:08:08Z,http://arxiv.org/abs/2209.03070v2,Law & Policy,AI in Legal Reasoning,"ontology is a popular method for knowledge representation in different domains . description logics (DL) is commonly used as its description language . acceptable assertions can be obtained based on inconsistent ontologies, paper shows ."
Artificial Intelligence and Legal Analysis: Implications for Legal   Education and the Profession,"This article reports the results of a study examining the ability of legal and non-legal Large Language Models to perform legal analysis using the Issue-Rule-Application-Conclusion framework. LLMs were tested on legal reasoning tasks involving rule analysis and analogical reasoning. The results show that LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail, an inability to commit to answers, false confidence, and hallucinations. The study compares legal and nonlegal LLMs, identifies shortcomings, and explores traits that may hinder their ability to think like a lawyer. It also discusses the implications for legal education and practice, highlighting the need for critical thinking skills in future lawyers and the potential pitfalls of overreliance on artificial intelligence AI resulting in a loss of logic, reasoning, and critical thinking skills.",['Lee Peoples'],2025-02-04T19:50:48Z,http://arxiv.org/abs/2502.03487v1,Law & Policy,AI in Legal Reasoning,"study examines ability of legal and non-legal Large Language Models to perform legal analysis . results show LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail . study highlights the need for critical thinking skills in future lawyers ."
Turing Test and the Practice of Law: The Role of Autonomous Levels of AI   Legal Reasoning,"Artificial Intelligence (AI) is increasingly being applied to law and a myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR) autonomous capabilities. A major question that has generally been unaddressed involves how we will know when AILR has achieved autonomous capacities. The field of AI has grappled with similar quandaries over how to assess the attainment of Artificial General Intelligence (AGI), a persistently discussed issue among scholars since the inception of AI, with the Turing Test communally being considered as the bellwether for ascertaining such matters. This paper proposes a variant of the Turing Test that is customized for specific use in the AILR realm, including depicting how this famous gold standard of AI fulfillment can be robustly applied across the autonomous levels of AI Legal Reasoning.",['Lance Eliot'],2020-08-18T04:50:23Z,http://arxiv.org/abs/2008.07743v1,Law & Policy,AI in Legal Reasoning,artificial intelligence (AI) is increasingly being applied to law and a myriad of legal tasks . a major question that has generally been unaddressed is how we will know when AILR has achieved autonomous capacities . the field of AI has grappled with similar quandaries over how to assess the attainment of Artificial General Intelligence (AGI)
JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning,"The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX","['Huanghai Liu', 'Quzhe Huang', 'Qingjing Chen', 'Yiran Hu', 'Jiayu Ma', 'Yun Liu', 'Weixing Shen', 'Yansong Feng']",2025-02-24T14:02:00Z,http://arxiv.org/abs/2502.17166v1,Law & Policy,AI in Legal Reasoning,"the Four-Element Theory is a fundamental framework in criminal law . many large language models (LLMs) attempt to incorporate this theory when handling legal tasks . to address this limitation, we introduce an expert-annotated knowledge base covering 155 criminal charges ."
RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with   Large Language Models,"Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.","['Yue Zhang', 'Zhiliang Tian', 'Shicheng Zhou', 'Haiyang Wang', 'Wenqing Hou', 'Yuying Liu', 'Xuechen Zhao', 'Minlie Huang', 'Ye Wang', 'Bin Zhou']",2025-05-27T14:50:21Z,http://arxiv.org/abs/2505.21281v1,Law & Policy,AI in Legal Reasoning,"legal Judgment Prediction (LJP) is a pivotal task in legal AI . existing semantic-enhanced models integrate judicial precedents and legal knowledge . but they neglect legal reasoning logic, a critical component of legal judgments . experimental results on two public datasets show superior performance ."
A Bytecode-based Approach for Smart Contract Classification,"With the development of blockchain technologies, the number of smart contracts deployed on blockchain platforms is growing exponentially, which makes it difficult for users to find desired services by manual screening. The automatic classification of smart contracts can provide blockchain users with keyword-based contract searching and helps to manage smart contracts effectively. Current research on smart contract classification focuses on Natural Language Processing (NLP) solutions which are based on contract source code. However, more than 94% of smart contracts are not open-source, so the application scenarios of NLP methods are very limited. Meanwhile, NLP models are vulnerable to adversarial attacks. This paper proposes a classification model based on features from contract bytecode instead of source code to solve these problems. We also use feature selection and ensemble learning to optimize the model. Our experimental studies on over 3,300 real-world Ethereum smart contracts show that our model can classify smart contracts without source code and has better performance than baseline models. Our model also has good resistance to adversarial attacks compared with NLP-based models. In addition, our analysis reveals that account features used in many smart contract classification models have little effect on classification and can be excluded.","['Chaochen Shi', 'Yong Xiang', 'Robin Ram Mohan Doss', 'Jiangshan Yu', 'Keshav Sood', 'Longxiang Gao']",2021-05-31T03:00:29Z,http://arxiv.org/abs/2106.15497v1,Law & Policy,Contract Analysis & NLP,the number of smart contracts deployed on blockchain platforms is growing exponentially . a new classification model can classify smart contracts without source code . the model also has good resistance to adversarial attacks .
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature,"Distributed Ledger Technology (DLT) faces increasing environmental scrutiny, particularly concerning the energy consumption of the Proof of Work (PoW) consensus mechanism and broader Environmental, Social, and Governance (ESG) issues. However, existing systematic literature reviews of DLT rely on limited analyses of citations, abstracts, and keywords, failing to fully capture the field's complexity and ESG concerns. We address these challenges by analyzing the full text of 24,539 publications using Natural Language Processing (NLP) with our manually labeled Named Entity Recognition (NER) dataset of 39,427 entities for DLT. This methodology identified 505 key publications at the DLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP and temporal graph analysis reveals critical trends in DLT evolution and ESG impacts, including cryptography and peer-to-peer networks research's foundational influence, Bitcoin's persistent impact on research and environmental concerns (a ""Lindy effect""), Ethereum's catalytic role on Proof of Stake (PoS) and smart contract adoption, and the industry's progressive shift toward energy-efficient consensus mechanisms. Our contributions include the first DLT-specific NER dataset addressing the scarcity of high-quality labeled NLP data in blockchain research, a methodology integrating NLP and temporal graph analysis for large-scale interdisciplinary literature reviews, and the first NLP-driven literature review focusing on DLT's ESG aspects.","['Walter Hernandez Cruz', 'Kamil Tylinski', 'Alastair Moore', 'Niall Roche', 'Nikhil Vadgama', 'Horst Treiblmaier', 'Jiangbo Shangguan', 'Paolo Tasca', 'Jiahua Xu']",2023-08-23T20:42:32Z,http://arxiv.org/abs/2308.12420v4,Law & Policy,Contract Analysis & NLP,"Distributed Ledger Technology (DLT) faces increasing environmental scrutiny . literature reviews of DLT rely on limited analyses of citations, abstracts, and keywords . authors analyze full text of 24,539 publications using natural language processing ."
AI-assisted German Employment Contract Review: A Benchmark Dataset,"Employment contracts are used to agree upon the working conditions between employers and employees all over the world. Understanding and reviewing contracts for void or unfair clauses requires extensive knowledge of the legal system and terminology. Recent advances in Natural Language Processing (NLP) hold promise for assisting in these reviews. However, applying NLP techniques on legal text is particularly difficult due to the scarcity of expert-annotated datasets. To address this issue and as a starting point for our effort in assisting lawyers with contract reviews using NLP, we release an anonymized and annotated benchmark dataset for legality and fairness review of German employment contract clauses, alongside with baseline model evaluations.","['Oliver Wardas', 'Florian Matthes']",2025-01-27T14:48:09Z,http://arxiv.org/abs/2501.17194v1,Law & Policy,Contract Analysis & NLP,recent advances in natural language processing (NLP) hold promise for assisting in contracts reviews . applying NLP techniques on legal text is particularly difficult due to the scarcity of expert-annotated datasets .
LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk   Prediction: A Clinical NLP,"Timely identification and accurate risk stratification of cardiovascular disease (CVD) remain essential for reducing global mortality. While existing prediction models primarily leverage structured data, unstructured clinical notes contain valuable early indicators. This study introduces a novel LLM-augmented clinical NLP pipeline that employs domain-adapted large language models for symptom extraction, contextual reasoning, and correlation from free-text reports. Our approach integrates cardiovascular-specific fine-tuning, prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III and CARDIO-NLP datasets demonstrate improved performance in precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by cardiologists. Challenges such as contextual hallucination, which occurs when plausible information contracts with provided source, and temporal ambiguity, which is related with models struggling with chronological ordering of events are addressed using prompt engineering and hybrid rule-based verification. This work underscores the potential of LLMs in clinical decision support systems (CDSS), advancing early warning systems and enhancing the translation of patient narratives into actionable risk assessments.","['Haowei Yang', 'Ziyu Shen', 'Junli Shao', 'Luyao Men', 'Xinyue Han', 'Jing Dong']",2025-07-15T07:32:16Z,http://arxiv.org/abs/2507.11052v1,Law & Policy,Contract Analysis & NLP,"this study introduces a novel LLM-augmented clinical NLP pipeline . it employs domain-adapted large language models for symptom extraction, contextual reasoning . challenges such as contextual hallucination are addressed using prompt engineering ."
An Interpretable Deep Learning System for Automatically Scoring Request   for Proposals,"The Managed Care system within Medicaid (US Healthcare) uses Request For Proposals (RFP) to award contracts for various healthcare and related services. RFP responses are very detailed documents (hundreds of pages) submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing (NLP) systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses. Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively asses the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems.","['Subhadip Maji', 'Anudeep Srivatsav Appe', 'Raghav Bali', 'Veera Raghavendra Chikka', 'Arijit Ghosh Chowdhury', 'Vamsi M Bhandaru']",2020-08-05T20:21:35Z,http://arxiv.org/abs/2008.02347v1,Law & Policy,Contract Analysis & NLP,the Managed Care system within Medicaid (US Healthcare) uses Request for Proposals (RFP) to award contracts . subject matter expertise and domain knowledge play an important role in preparing RFP responses . our work draws parallels between scoring RFPs and essay scoring models .
STAN: Towards Describing Bytecodes of Smart Contract,"More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users.","['Xiaoqi Li', 'Ting Chen', 'Xiapu Luo', 'Tao Zhang', 'Le Yu', 'Zhou Xu']",2020-07-19T15:48:35Z,http://arxiv.org/abs/2007.09696v1,Law & Policy,Contract Analysis & NLP,more than eight million smart contracts have been deployed into Ethereum . less than 1% of deployed smart contracts are open-source . a system named STAN can generate descriptions for the bytecodes of smart contracts .
RISC: Generating Realistic Synthetic Bilingual Insurance Contract,"This paper presents RISC, an open-source Python package data generator (https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form in French and English. Insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson. Hence, they are a much more complex class of documents than those in traditional NLP corpora. Therefore, we introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile Contract dataset based on the mandatory Quebec car insurance contract. The dataset comprises 10,000 French and English unannotated insurance contracts. RISCBAC enables NLP research for unsupervised automatic summarisation, question answering, text simplification, machine translation and more. Moreover, it can be further automatically annotated as a dataset for supervised tasks such as NER","['David Beauchemin', 'Richard Khoury']",2023-04-09T10:42:18Z,http://arxiv.org/abs/2304.04212v1,Law & Policy,Contract Analysis & NLP,"RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form . insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson . the dataset comprises 10,000 french and english unannotated insurance contracts."
"Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa   Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP   dalam bahasa Indonesia","This study provides an overview of the history of the development of Natural Language Processing (NLP) in the context of the Indonesian language, with a focus on the basic technologies, methods, and practical applications that have been developed. This review covers developments in basic NLP technologies such as stemming, part-of-speech tagging, and related methods; practical applications in cross-language information retrieval systems, information extraction, and sentiment analysis; and methods and techniques used in Indonesian language NLP research, such as machine learning, statistics-based machine translation, and conflict-based approaches. This study also explores the application of NLP in Indonesian language industry and research and identifies challenges and opportunities in Indonesian language NLP research and development. Recommendations for future Indonesian language NLP research and development include developing more efficient methods and technologies, expanding NLP applications, increasing sustainability, further research into the potential of NLP, and promoting interdisciplinary collaboration. It is hoped that this review will help researchers, practitioners, and the government to understand the development of Indonesian language NLP and identify opportunities for further research and development.",['Mukhlis Amien'],2023-03-28T02:31:47Z,http://arxiv.org/abs/2304.02746v1,Law & Policy,Contract Analysis & NLP,"this study provides an overview of the history of the development of natural language processing (NLP) in the context of the Indonesian language . it focuses on the basic technologies, methods, and practical applications that have been developed ."
CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review,"Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.","['Dan Hendrycks', 'Collin Burns', 'Anya Chen', 'Spencer Ball']",2021-03-10T18:59:34Z,http://arxiv.org/abs/2103.06268v2,Law & Policy,Contract Analysis & NLP,"the Contract Understanding Atticus Dataset (CUAD) consists of over 13,000 annotations . the dataset highlights salient portions of a contract that are important for a human to review . CUAD can serve as a challenging research benchmark for the broader NLP community ."
Construction contract risk identification based on knowledge-augmented   language model,"Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ logical thinking during the task and provide insights and recommendations for future research.","['Saika Wong', 'Chunmo Zheng', 'Xing Su', 'Yinqiu Tang']",2023-09-22T05:27:06Z,http://arxiv.org/abs/2309.12626v1,Law & Policy,Contract Analysis & NLP,large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks . but they struggle with domain-specific knowledge and addressing specialized issues . this paper presents a novel approach that leverages LLMs with construction contract knowledge .
Moving closer: contractive maps on discrete metric spaces and graphs,"We consider discrete metric spaces and we look for non-constant contractions. We introduce the notion of contractive map and we characterize the spaces with non-constant contractive maps. We provide some examples to discussion the possible relations between contractions, contractive maps and constant functions. Finally we apply the main result to the subgraphs of a non-oriented, connected graph.",['Fabio Zucca'],2010-11-18T09:54:15Z,http://arxiv.org/abs/1011.4163v1,Law & Policy,Contract Analysis & NLP,we consider discrete metric spaces and we look for non-constant contractions . we introduce the notion of contractive map and we characterize the spaces with non-concurrent contractive maps .
Two classes of Meir-Keeler contractions,"In the present paper, we prove that Z-contractions and weakly type contractions are actually Meir-Keeler contractions.","['L. Gavruta', 'P. Gavruta', 'F. Khojasteh']",2014-05-20T11:19:06Z,http://arxiv.org/abs/1405.5034v1,Law & Policy,Contract Analysis & NLP,"in the present paper, we prove that Z-contractions and weakly type contractions are actually Meir-Keeler contractions."
Quasi-Sectorial Contractions,We revise the notion of the quasi-sectorial contractions. Our main theorem establishes a relation between semigroups of quasi-sectorial contractions and a class of m-sectorial generators. We discuss a relevance of this kind of contractions to the theory of operator-norm approximations of strongly continuous semigroups.,['V. A. Zagrebnov'],2007-11-03T21:28:26Z,http://arxiv.org/abs/0711.0478v1,Law & Policy,Contract Analysis & NLP,our main theorem establishes a relation between semigroups of quasi-sectorial contractions . we discuss a relevance of this kind of contractions to the theory of operator-norm approximations .
On the equivalence of the Mizoguchi-Takahashi locally contractive map to   Nadler's locally contractive map,"In this article, we have proved the equivalence between the Mizoguchi-Takahashi uniformly~locally~contractive map to the multi-valued map satisfying the Nadler contractive condition uniformly~locally~on a metrically convex space.","['Asrifa Sultana', 'Xiaolong Qin']",2018-01-13T17:48:13Z,http://arxiv.org/abs/1801.04469v1,Law & Policy,Contract Analysis & NLP,"in this article, we have proved the equivalence between the Mizoguchi-Takahashi map and the multi-valued map satisfying the Nadler contractive condition uniformlylocallyon a metrically convex space."
Automating construction contract review using knowledge graph-enhanced   large language models,"An effective and efficient review of construction contracts is essential for minimizing construction projects losses, but current methods are time-consuming and error-prone. Studies using methods based on Natural Language Processing (NLP) exist, but their scope is often limited to text classification or segmented label prediction. This paper investigates whether integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and interpretability of automated contract risk identification. A tuning-free approach is proposed that integrates LLMs with a Nested Contract Knowledge Graph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework for contract knowledge retrieval and reasoning. Tested on international EPC contracts, the method achieves more accurate risk evaluation and interpretable risk summaries than baseline models. These findings demonstrate the potential of combining LLMs and KGs for reliable reasoning in tasks that are knowledge-intensive and specialized, such as contract review.","['Chunmo Zheng', 'Saika Wong', 'Xing Su', 'Yinqiu Tang', 'Ahsan Nawaz', 'Mohamad Kassem']",2023-09-21T14:53:36Z,http://arxiv.org/abs/2309.12132v2,Law & Policy,Contract Analysis & NLP,a tuning-free approach is proposed that integrates LLMs with a Nested Contract Knowledge Graph . the method achieves more accurate risk evaluation and interpretable risk summaries than baseline models .
ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting,"Information retrieval, specifically contract clause retrieval, is foundational to contract drafting because lawyers rarely draft contracts from scratch; instead, they locate and revise the most relevant precedent. We introduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval benchmark for contract drafting fully annotated by experts. ACORD focuses on complex contract clauses such as Limitation of Liability, Indemnification, Change of Control, and Most Favored Nation. It includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task is to find the most relevant precedent clauses to a query. The bi-encoder retriever paired with pointwise LLMs re-rankers shows promising results. However, substantial improvements are still needed to effectively manage the complex legal work typically undertaken by lawyers. As the first retrieval benchmark for contract drafting annotated by experts, ACORD can serve as a valuable IR benchmark for the NLP community.","['Steven H. Wang', 'Maksim Zubkov', 'Kexin Fan', 'Sarah Harrell', 'Yuyang Sun', 'Wei Chen', 'Andreas Plesner', 'Roger Wattenhofer']",2025-01-11T16:37:49Z,http://arxiv.org/abs/2501.06582v3,Law & Policy,Contract Analysis & NLP,"ACORD is the first retrieval benchmark for contract drafting fully annotated by experts . it includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars . ACORD can serve as valuable IR benchmark for the NLP community ."
Generalized notions of character amenability,"In this paper the concepts of character contractibility, approximate character amenability (contractibility) and uniform approximate character amenability (contractibility) are introduced. We are concerned with the relations among the generalized concepts of character amenability for Banach algebra. We prove that approximate character amenability and approximate character contractibility are the same properties, as are uniform approximate character amenability and character amenability, as are uniform approximate character contractibility and character contractibility. For commutative Banach algebra, we prove that character contractibility and contractibility are the same properties. Moreover, general theory for those concepts is developed.","['Luo Yi Shi', 'Yu Jing Wu', 'You Qing Ji']",2010-08-31T08:22:04Z,http://arxiv.org/abs/1008.5252v1,Law & Policy,Contract Analysis & NLP,"in this paper the concepts of character contractibility, approximate character amenability (contractibility) and uniform approximate character (contractability) are introduced . for commutative Banach algebra, we prove that character and contractibility are the same ."
Approximating fixed points of enriched contractions in Banach spaces,"We introduce a large class of mappings, called enriched contractions, which includes, amongst many other contractive type mappings, the Picard-Banach contractions and some nonexpansive mappings. We show that any enriched contraction has a unique fixed point and that this fixed point can be approximated by means of an appropriate Kransnoselskij iterative scheme. Several important results in fixed point theory are shown to be corollaries or consequences of the main results in this paper. We also study the fixed points of local enriched contractions, asymptotic enriched contractions and Maia type enriched contractions. Examples to illustrate the generality of our new concepts and the corresponding fixed point theorems are also given.","['Vasile Berinde', 'Mădălina Păcurar']",2019-09-05T13:14:23Z,http://arxiv.org/abs/1909.02382v1,Law & Policy,Contract Analysis & NLP,"we introduce a large class of mappings, called enriched contractions . we show that any enriched contractive type contraction has a unique fixed point . important results in fixed point theory are shown to be corollaries of main results ."
Approximating fixed points of enriched Chatterjea contractions by   Krasnoselskij iterative method in Banach spaces,"Using the technique of enrichment of contractive type mappings by Krasnoselskij averaging, introduced in [Berinde, V., {\it Approximating fixed points of enriched nonexpansive mappings by Krasnoselskij iteration in Hilbert spaces}, Carpathian J. Math. {\bf 35} (2019), no. 3, 277-288.], we introduce the class of enriched Chatterjea contractions and prove general fixed point theorems for such contractions in the setting of a Banach space. Examples to illustrate the richness of the new class of contractions and the relationship between enriched Banach contractions, enriched Kannan contractions and enriched Kannan contractions are also given.","['Vasile Berinde', 'Mădălina Păcurar']",2019-09-05T13:17:49Z,http://arxiv.org/abs/1909.03494v1,Law & Policy,Contract Analysis & NLP,we introduce the class of enriched Chatterjea contractions and prove general fixed point theorems for such contractions in the setting of a Banach space .
Output contraction analysis of nonlinear systems,"This paper introduce the notion of output contraction that expands the contraction notion to the time-varying nonlinear systems with output. It pertains to the systems' property that any pair of outputs from the system converge to each other exponentially. This concept exhibits a more expansive nature when contrasted with another generalized contraction framework known as partial contraction. The first result establishes a connection between the output contraction of a time-varying system and the output exponential stability of its variational system. Subsequently, we derive a sufficient condition for achieving output contraction in time-varying systems by applying the output contraction Lyapunov criterion. Finally, we apply the results to analyze the output exponential stability of nonlinear time-invariant systems.","['Hao Yin', 'Bayu Jayawardhana', 'Stephan Trenn']",2023-12-11T13:48:50Z,http://arxiv.org/abs/2312.06384v1,Law & Policy,Contract Analysis & NLP,this paper introduces the notion of output contraction that expands the contraction notion . it pertains to the properties that any pair of outputs from the system converge to each other exponentially . this concept exhibits a more expansive nature when contrasted
A Reasoning-Focused Legal Retrieval Benchmark,"As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs (""RAG"" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.","['Lucia Zheng', 'Neel Guha', 'Javokhir Arifov', 'Sarah Zhang', 'Michal Skreta', 'Christopher D. Manning', 'Peter Henderson', 'Daniel E. Ho']",2025-05-06T20:44:03Z,http://arxiv.org/abs/2505.03970v1,Law & Policy,Legal QA,legal AI developers have turned to retrieval-augmented LLMs (RAG) systems . lack of realistic legal RAG benchmarks captures complexity of legal retrieval . bar exam QA and housing Statute QA were produced through annotation processes .
"Legal Question-Answering in the Indian Context: Efficacy, Challenges,   and Potential of Modern AI Models","Legal QA platforms bear the promise to metamorphose the manner in which legal experts engage with jurisprudential documents. In this exposition, we embark on a comparative exploration of contemporary AI frameworks, gauging their adeptness in catering to the unique demands of the Indian legal milieu, with a keen emphasis on Indian Legal Question Answering (AILQA). Our discourse zeroes in on an array of retrieval and QA mechanisms, positioning the OpenAI GPT model as a reference point. The findings underscore the proficiency of prevailing AILQA paradigms in decoding natural language prompts and churning out precise responses. The ambit of this study is tethered to the Indian criminal legal landscape, distinguished by its intricate nature and associated logistical constraints. To ensure a holistic evaluation, we juxtapose empirical metrics with insights garnered from seasoned legal practitioners, thereby painting a comprehensive picture of AI's potential and challenges within the realm of Indian legal QA.","['Shubham Kumar Nigam', 'Shubham Kumar Mishra', 'Ayush Kumar Mishra', 'Noel Shallum', 'Arnab Bhattacharya']",2023-09-26T07:56:55Z,http://arxiv.org/abs/2309.14735v2,Law & Policy,Legal QA,"legal QA platforms bear the promise of metamorphosing the manner in which legal experts engage with jurisprudential documents . in this exposition, we embark on a comparative exploration of contemporary AI frameworks . our discourse zeroe"
GPTs and Language Barrier: A Cross-Lingual Legal QA Examination,"In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.","['Ha-Thanh Nguyen', 'Hiroaki Yamada', 'Ken Satoh']",2024-03-26T20:47:32Z,http://arxiv.org/abs/2403.18098v1,Law & Policy,Legal QA,"in this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems . using the COLIEE Task 4, we provide valuable insights into G"
Exploring the State of the Art in Legal QA Systems,"Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. Question answering (QA) systems are designed to generate answers to questions asked in human languages. QA uses natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, QA faces challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. At this time, there is a lack of surveys that discuss legal question answering. To address this problem, we provide a comprehensive survey that reviews 14 benchmark datasets for question-answering in the legal field as well as presents a comprehensive review of the state-of-the-art Legal Question Answering deep learning models. We cover the different architectures and techniques used in these studies and the performance and limitations of these models. Moreover, we have established a public GitHub repository where we regularly upload the most recent articles, open data, and source code. The repository is available at: \url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.","['Abdelrahman Abdallah', 'Bhawna Piryani', 'Adam Jatowt']",2023-04-13T15:48:01Z,http://arxiv.org/abs/2304.06623v3,Law & Policy,Legal QA,question answering systems are designed to generate answers to questions asked in human languages . QA faces challenges such as improving natural language understanding . survey reviews 14 benchmark datasets for question-answering in the legal field .
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for   Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people seeking legal advice, which aims to retrieve the most applicable answers from a large-scale database of question-answer pairs. Previous methods mainly use a dual-encoder architecture to learn dense representations of both questions and answers. However, these methods could suffer from lacking domain knowledge and sufficient labeled training data. In this paper, we propose a three-stage (\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking) framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes the fine-grained text representation learning and boosts the performance of dense retrieval with the dual-encoder architecture. Concretely, we first conduct domain-specific pre-training on legal questions and answers through a self-supervised training objective, allowing the pre-trained model to be adapted to the legal domain. Then, we perform task-specific fine-tuning of the dual-encoder on legal question-answer pairs by using the supervised learning objective, leading to a high-quality dual-encoder for the specific downstream QA task. Finally, we employ a contextual re-ranking objective to further refine the output representations of questions produced by the document encoder, which uses contextual similarity to increase the discrepancy between the anchor and hard negative samples for better question re-ranking. We conduct extensive experiments on a manually annotated legal QA dataset. Experimental results show that our PFR-LQA method achieves better performance than the strong competitors for legal question answering.","['Shiwen Ni', 'Hao Cheng', 'Min Yang']",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,Law & Policy,Legal QA,legal question answering (QA) has attracted increasing attention from people seeking legal advice . it aims to retrieve the most applicable answers from a large-scale database of question-answer pairs . previous methods could suffer from lacking domain knowledge and sufficient labele
SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning,"Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.","['Ojasw Upadhyay', 'Abishek Saravanakumar', 'Ayman Ismail']",2025-04-26T01:42:22Z,http://arxiv.org/abs/2504.18762v2,Law & Policy,Legal QA,"SynLexLM is a novel approach to efficiently pre-train a legal LLM . it employs curriculum learning, progressing from simple to complex legal texts and queries . we aim to achieve improved performance on legal benchmarks compared to traditional models"
Expert Finding in Legal Community Question Answering,"Expert finding has been well-studied in community question answering (QA) systems in various domains. However, none of these studies addresses expert finding in the legal domain, where the goal is for citizens to find lawyers based on their expertise. In the legal domain, there is a large knowledge gap between the experts and the searchers, and the content on the legal QA websites consist of a combination formal and informal communication. In this paper, we propose methods for generating query-dependent textual profiles for lawyers covering several aspects including sentiment, comments, and recency. We combine query-dependent profiles with existing expert finding methods. Our experiments are conducted on a novel dataset gathered from an online legal QA service. We discovered that taking into account different lawyer profile aspects improves the best baseline model. We make our dataset publicly available for future work.","['Arian Askari', 'Suzan Verberne', 'Gabriella Pasi']",2022-01-19T15:46:55Z,http://arxiv.org/abs/2201.07667v3,Law & Policy,Legal QA,experts have been well-studied in community question answering systems . but none of these studies addresses expert finding in the legal domain . authors propose methods for generating query-dependent profiles for lawyers .
LawInstruct: A Resource for Studying Language Model Adaptation to the   Legal Domain,"Instruction tuning is an important step in making language models useful for direct user interaction. However, the legal domain is underrepresented in typical instruction datasets (e.g., only 10 out of 1600+ tasks in Super-NaturalInstructions). To study whether instruction tuning on legal datasets is necessary for strong legal reasoning, we aggregate 58 annotated legal datasets and write instructions for each, creating LawInstruct. LawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M examples across diverse tasks such as legal QA, summarization of court cases, and legal argument mining. We evaluate our models on LegalBench, measuring legal reasoning across five categories in 162 challenging and realistic legal tasks, and MMLU, to measure potential drops in general reasoning capabilities. We find that legal-specific instruction tuning on Flan-T5 - yielding FLawN-T5 - improves performance on LegalBench across all model sizes, with an aggregate increase of 15 points or 50% over Flan-T5 for the base size. No model size shows performance drops in MMLU. We publish LawInstruct as a resource for further study of instruction tuning in the legal domain.","['Joel Niklaus', 'Lucia Zheng', 'Arya D. McCarthy', 'Christopher Hahn', 'Brian M. Rosen', 'Peter Henderson', 'Daniel E. Ho', 'Garrett Honke', 'Percy Liang', 'Christopher Manning']",2024-04-02T17:33:34Z,http://arxiv.org/abs/2404.02127v2,Law & Policy,Legal QA,"instruction tuning is an important step in making language models useful for direct user interaction . however, the legal domain is underrepresented in typical instruction datasets . we aggregate 58 annotated legal datasets and write instructions for each . LawInstruct covers 17"
JEC-QA: A Legal-Domain Question Answering Dataset,"We present JEC-QA, the largest question answering dataset in the legal domain, collected from the National Judicial Examination of China. The examination is a comprehensive evaluation of professional skills for legal practitioners. College students are required to pass the examination to be certified as a lawyer or a judge. The dataset is challenging for existing question answering methods, because both retrieving relevant materials and answering questions require the ability of logic reasoning. Due to the high demand of multiple reasoning abilities to answer legal questions, the state-of-the-art models can only achieve about 28% accuracy on JEC-QA, while skilled humans and unskilled humans can reach 81% and 64% accuracy respectively, which indicates a huge gap between humans and machines on this task. We will release JEC-QA and our baselines to help improve the reasoning ability of machine comprehension models. You can access the dataset from http://jecqa.thunlp.org/.","['Haoxi Zhong', 'Chaojun Xiao', 'Cunchao Tu', 'Tianyang Zhang', 'Zhiyuan Liu', 'Maosong Sun']",2019-11-27T08:13:27Z,http://arxiv.org/abs/1911.12011v1,Law & Policy,Legal QA,college students must pass the examination to be certified as a lawyer or a judge . the dataset is challenging for existing question answering methods . skilled humans and unskilled humans can reach 81% and 64% accuracy respectively .
Improving Vietnamese Legal Question--Answering System based on Automatic   Data Enrichment,"Question answering (QA) in law is a challenging problem because legal documents are much more complicated than normal texts in terms of terminology, structure, and temporal and logical relationships. It is even more difficult to perform legal QA for low-resource languages like Vietnamese where labeled data are rare and pre-trained language models are still limited. In this paper, we try to overcome these limitations by implementing a Vietnamese article-level retrieval-based legal QA system and introduce a novel method to improve the performance of language models by improving data quality through weak labeling. Our hypothesis is that in contexts where labeled data are limited, efficient data enrichment can help increase overall performance. Our experiments are designed to test multiple aspects, which demonstrate the effectiveness of the proposed technique.","['Thi-Hai-Yen Vuong', 'Ha-Thanh Nguyen', 'Quang-Huy Nguyen', 'Le-Minh Nguyen', 'Xuan-Hieu Phan']",2023-06-08T00:24:29Z,http://arxiv.org/abs/2306.04841v1,Law & Policy,Legal QA,question answering (QA) in law is a challenging problem . labeled data are rare and pre-trained language models are limited . a novel method is proposed to improve the performance of language models .
Fine-tuning Large Language Models for Improving Factuality in Legal   Question Answering,"Hallucination, or the generation of incorrect or fabricated information, remains a critical challenge in large language models (LLMs), particularly in high-stake domains such as legal question answering (QA). In order to mitigate the hallucination rate in legal QA, we first introduce a benchmark called LegalHalBench and three automatic metrics to evaluate the common hallucinations when LLMs answer legal questions. We then propose a hallucination mitigation method that integrates behavior cloning and a novel Hard Sample-aware Iterative Direct Preference Optimization (HIPO). We conduct extensive real-data experiments to validate the effectiveness of our approach. Our results demonstrate remarkable improvements in various metrics, including the newly proposed Non-Hallucinated Statute Rate, Statute Relevance Rate, Legal Claim Truthfulness, as well as traditional metrics such as METEOR, BERTScore, ROUGE-L, and win rates.","['Yinghao Hu', 'Leilei Gan', 'Wenyi Xiao', 'Kun Kuang', 'Fei Wu']",2025-01-11T12:08:15Z,http://arxiv.org/abs/2501.06521v1,Law & Policy,Legal QA,hallucination remains a critical challenge in large language models (LLMs) we introduce a benchmark called LegalHalBench and three automatic metrics . we then propose a method that integrates behavior cloning and a novel it
Answer Retrieval in Legal Community Question Answering,"The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses. Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites. To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs. CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain. Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers' effectiveness. While we study our proposed method in the legal domain, we believe that our method can be applied in similar applications in other domains.","['Arian Askari', 'Zihui Yang', 'Zhaochun Ren', 'Suzan Verberne']",2024-01-09T23:48:54Z,http://arxiv.org/abs/2401.04852v1,Law & Policy,Legal QA,the task of answer retrieval in the legal domain aims to help users to seek legal advice . two main challenges hinder applying existing answers retrieval approaches . we propose a novel cross-encoder (CE) re-ranker based on
LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question   Answering,"We present LEGAL-UQA, the first Urdu legal question-answering dataset derived from Pakistan's constitution. This parallel English-Urdu dataset includes 619 question-answer pairs, each with corresponding legal article contexts, addressing the need for domain-specific NLP resources in low-resource languages. We describe the dataset creation process, including OCR extraction, manual refinement, and GPT-4-assisted translation and generation of QA pairs. Our experiments evaluate the latest generalist language and embedding models on LEGAL-UQA, with Claude-3.5-Sonnet achieving 99.19% human-evaluated accuracy. We fine-tune mt5-large-UQA-1.0, highlighting the challenges of adapting multilingual models to specialized domains. Additionally, we assess retrieval performance, finding OpenAI's text-embedding-3-large outperforms Mistral's mistral-embed. LEGAL-UQA bridges the gap between global NLP advancements and localized applications, particularly in constitutional law, and lays the foundation for improved legal information access in Pakistan.","['Faizan Faisal', 'Umair Yousaf']",2024-10-16T20:14:45Z,http://arxiv.org/abs/2410.13013v1,Law & Policy,Legal QA,Claude-3.5-Sonnet achieves 99.19% human-evaluated accuracy on LEGAL-UQA . openAI's text-embedding-3-large outperforms mistral's .
NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai   Legal Question Answering,"The application of large language models (LLMs) in the legal domain holds significant potential for information retrieval and question answering, yet Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks and the complexity of Thai legal structures. This paper introduces NitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering general Thai financial law, and the NitiBench-Tax, which includes real-world tax law cases requiring advanced legal reasoning. We evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches to address three key research questions: the impact of domain-specific components like section-based chunking and cross-referencing, the comparative performance of different retrievers and LLMs, and the viability of long-context LLMs as an alternative to RAG. Our results show that section-based chunking significantly improves retrieval and end-to-end performance, current retrievers struggle with complex queries, and long-context LLMs still underperform RAG-based systems in Thai legal QA. To support fair evaluation, we propose tailored multi-label retrieval metrics and the use of an LLM-as-judge for coverage and contradiction detection method. These findings highlight the limitations of current Thai legal NLP solutions and provide a foundation for future research in the field. We also open-sourced our codes and dataset to available publicly.","['Pawitsapak Akarajaradwong', 'Pirat Pothavorn', 'Chompakorn Chaksangchaichot', 'Panuthep Tasawong', 'Thitiwat Nopparatbundit', 'Sarana Nutanong']",2025-02-15T17:52:14Z,http://arxiv.org/abs/2502.10868v3,Law & Policy,Legal QA,Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks . we evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches . section-based chunking significantly improves retrieval and end-to-end performance .
Improving Legal Information Retrieval by Distributional Composition with   Term Order Probabilities,"Legal professionals worldwide are currently trying to get up-to-pace with the explosive growth in legal document availability through digital means. This drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done with the use of disambiguation rules, applied over the rankings obtained through n-gram statistics. After the ranking is done, its results are evaluated for ambiguity, and disambiguation is done if a result is decided to be unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, an analysis of error and improvement cases is presented for a better understanding of the contributions.","['Danilo S. Carvalho', 'Duc-Vu Tran', 'Van-Khanh Tran', 'Le-Nguyen Minh']",2017-06-04T06:57:09Z,http://arxiv.org/abs/1706.01038v2,Law & Policy,Legal QA,legal professionals are trying to get up-to-pace with the explosive growth in legal document availability . this drives a need for high efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods .
Elevating Legal LLM Responses: Harnessing Trainable Logical Structures   and Semantic Knowledge with Legal Reasoning,"Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods.","['Rujing Yao', 'Yang Wu', 'Chenghao Wang', 'Jingwei Xiong', 'Fang Wang', 'Xiaozhong Liu']",2025-02-11T19:33:07Z,http://arxiv.org/abs/2502.07912v1,Law & Policy,Legal QA,"LLMs often generate generalized answers that lack the logical specificity required for legal advice . existing approaches typically focus only on semantic similarity, neglecting logical structure . our experiments on a real-world legal QA dataset demonstrate that LSIM significantly enhances accuracy ."
NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation   using Few-Shot Multi-Choice QA,"This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure. We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate. Firstly, we fine-tuned pre-trained BERT-based models and found that models trained on domain knowledge perform better. Secondly, we performed few-shot prompting on GPT models and found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model. Our best submission is a BERT-based model that achieved the 7th place out of 20.","['Anish Pahilajani', 'Samyak Rajesh Jain', 'Devasha Trivedi']",2024-04-04T01:50:20Z,http://arxiv.org/abs/2404.03150v1,Law & Policy,Legal QA,our best submission is a BERT-based model that achieved the 7th place out of 20 . we fine-tuned pre-trained models and found that models trained on domain knowledge perform better .
Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes   Domains,"Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users' confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain$o1$s, which enhances LLMs' reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models' explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading performance and explainability. Our code is available at https://github.com/Hyalinesky/Domaino1s.","['Xu Chu', 'Zhijie Tan', 'Hanlin Xue', 'Guanyu Wang', 'Tong Mo', 'Weiping Li']",2025-01-24T11:57:39Z,http://arxiv.org/abs/2501.14431v2,Law & Policy,Legal QA,Domain$o1$s enhances LLMs' reasoning capabilities on domain tasks . it uses supervised fine-tuning and tree search . PROOF-Score is a new metric to evaluate domain models' explainability .
Aspect Classification for Legal Depositions,"Attorneys and others have a strong interest in having a digital library with suitable services (e.g., summarizing, searching, and browsing) to help them work with large corpora of legal depositions. Their needs often involve understanding the semantics of such documents. That depends in part on the role of the deponent, e.g., plaintiff, defendant, law enforcement personnel, expert, etc. In the case of tort litigation associated with property and casualty insurance claims, such as relating to an injury, it is important to know not only about liability, but also about events, accidents, physical conditions, and treatments.   We hypothesize that a legal deposition consists of various aspects that are discussed as part of the deponent testimony. Accordingly, we developed an ontology of aspects in a legal deposition for accident and injury cases. Using that, we have developed a classifier that can identify portions of text for each of the aspects of interest. Doing so was complicated by the peculiarities of this genre, e.g., that deposition transcripts generally consist of data in the form of question-answer (QA) pairs. Accordingly, our automated system starts with pre-processing, and then transforms the QA pairs into a canonical form made up of declarative sentences. Classifying the declarative sentences that are generated, according to the aspect, can then help with downstream tasks such as summarization, segmentation, question-answering, and information retrieval.   Our methods have achieved a classification F1 score of 0.83. Having the aspects classified with a good accuracy will help in choosing QA pairs that can be used as candidate summary sentences, and to generate an informative summary for legal professionals or insurance claim agents. Our methodology could be extended to legal depositions of other kinds, and to aid services like searching.","['Saurabh Chakravarty', 'Satvik Chekuri', 'Maanav Mehrotra', 'Edward A. Fox']",2020-09-09T18:00:15Z,http://arxiv.org/abs/2009.04485v1,Law & Policy,Legal QA,deposition transcripts generally consist of data in the form of question-answer (QA) pairs . a classifier can identify portions of text for each of the aspects of interest . the method has achieved a classification F1 score of 0.83 .
Do Language Models Learn about Legal Entity Types during Pretraining?,"Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.","['Claire Barale', 'Michael Rovatsos', 'Nehal Bhuta']",2023-10-19T18:47:21Z,http://arxiv.org/abs/2310.13092v1,Law & Policy,Legal QA,"language models (LMs) have proven their ability to acquire diverse linguistic knowledge . there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. we compare diverse types and lengths of entities both general and domain specific entities ."
"Case law retrieval: problems, methods, challenges and evaluations in the   last 20 years","Case law retrieval is the retrieval of judicial decisions relevant to a legal question. Case law retrieval comprises a significant amount of a lawyer's time, and is important to ensure accurate advice and reduce workload. We survey methods for case law retrieval from the past 20 years and outline the problems and challenges facing evaluation of case law retrieval systems going forward. Limited published work has focused on improving ranking in ad-hoc case law retrieval. But there has been significant work in other areas of case law retrieval, and legal information retrieval generally. This is likely due to legal search providers being unwilling to give up the secrets of their success to competitors. Most evaluations of case law retrieval have been undertaken on small collections and focus on related tasks such as question-answer systems or recommender systems. Work has not focused on Cranfield style evaluations and baselines of methods for case law retrieval on publicly available test collections are not present. This presents a major challenge going forward. But there are reasons to question the extent of this problem, at least in a commercial setting. Without test collections to baseline approaches it cannot be known whether methods are promising. Works by commercial legal search providers show the effectiveness of natural language systems as well as query expansion for case law retrieval. Machine learning is being applied to more and more legal search tasks, and undoubtedly this represents the future of case law retrieval.","['Daniel Locke', 'Guido Zuccon']",2022-02-15T06:01:36Z,http://arxiv.org/abs/2202.07209v1,Law & Policy,Case Law Retrieval,case law retrieval is the retrieval of judicial decisions relevant to a legal question . there has been significant work in the past 20 years in the field . most evaluations have been undertaken on small collections and focus on related tasks .
DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based   re-ranking for case law retrieval,"In this paper, we present our approaches for the case law retrieval and the legal case entailment task in the Competition on Legal Information Extraction/Entailment (COLIEE) 2021. As first stage retrieval methods combined with neural re-ranking methods using contextualized language models like BERT achieved great performance improvements for information retrieval in the web and news domain, we evaluate these methods for the legal domain. A distinct characteristic of legal case retrieval is that the query case and case description in the corpus tend to be long documents and therefore exceed the input length of BERT. We address this challenge by combining lexical and dense retrieval methods on the paragraph-level of the cases for the first stage retrieval. Here we demonstrate that the retrieval on the paragraph-level outperforms the retrieval on the document-level. Furthermore the experiments suggest that dense retrieval methods outperform lexical retrieval. For re-ranking we address the problem of long documents by summarizing the cases and fine-tuning a BERT-based re-ranker with the summaries. Overall, our best results were obtained with a combination of BM25 and dense passage retrieval using domain-specific embeddings.","['Sophia Althammer', 'Arian Askari', 'Suzan Verberne', 'Allan Hanbury']",2021-08-09T11:07:11Z,http://arxiv.org/abs/2108.03937v1,Law & Policy,Case Law Retrieval,a distinct characteristic of legal case retrieval is that the query case and case description in the corpus tend to be long documents . the retrieval on the paragraph-level outperforms the retrieving on the document-level . our best results were obtained with a combination of BM25 and dense passage retrieval .
THUIR@COLIEE-2020: Leveraging Semantic Understanding and Exact Matching   for Legal Case Retrieval and Entailment,"In this paper, we present our methodologies for tackling the challenges of legal case retrieval and entailment in the Competition on Legal Information Extraction / Entailment 2020 (COLIEE-2020). We participated in the two case law tasks, i.e., the legal case retrieval task and the legal case entailment task. Task 1 (the retrieval task) aims to automatically identify supporting cases from the case law corpus given a new case, and Task 2 (the entailment task) to identify specific paragraphs that entail the decision of a new case in a relevant case. In both tasks, we employed the neural models for semantic understanding and the traditional retrieval models for exact matching. As a result, our team (TLIR) ranked 2nd among all of the teams in Task 1 and 3rd among teams in Task 2. Experimental results suggest that combing models of semantic understanding and exact matching benefits the legal case retrieval task while the legal case entailment task relies more on semantic understanding.","['Yunqiu Shao', 'Bulou Liu', 'Jiaxin Mao', 'Yiqun Liu', 'Min Zhang', 'Shaoping Ma']",2020-12-24T04:59:45Z,http://arxiv.org/abs/2012.13102v1,Law & Policy,Case Law Retrieval,methodologies for tackling the challenges of legal case retrieval and entailment . our team (TLIR) ranked 2nd among all of the teams in Task 1 and 3rd in Task 2 . experimental results suggest that combing models of semantic understanding and exact matching benefits .
Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case   Reformulation,"Legal case retrieval for sourcing similar cases is critical in upholding judicial fairness. Different from general web search, legal case retrieval involves processing lengthy, complex, and highly specialized legal documents. Existing methods in this domain often overlook the incorporation of legal expert knowledge, which is crucial for accurately understanding and modeling legal cases, leading to unsatisfactory retrieval performance. This paper introduces KELLER, a legal knowledge-guided case reformulation approach based on large language models (LLMs) for effective and interpretable legal case retrieval. By incorporating professional legal knowledge about crimes and law articles, we enable large language models to accurately reformulate the original legal case into concise sub-facts of crimes, which contain the essential information of the case. Extensive experiments on two legal case retrieval benchmarks demonstrate superior retrieval performance and robustness on complex legal case queries of KELLER over existing methods.","['Chenlong Deng', 'Kelong Mao', 'Zhicheng Dou']",2024-06-28T08:59:45Z,http://arxiv.org/abs/2406.19760v1,Law & Policy,Case Law Retrieval,this paper introduces a legal knowledge-guided case reformulation approach . it incorporates professional legal knowledge about crimes and law articles . this allows large language models to accurately reformulate the original legal case .
CaseLink: Inductive Graph Learning for Legal Case Retrieval,"In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input is not applicable for testing. Thus, in this paper, a CaseLink model based on inductive graph learning is proposed to utilise the intrinsic case connectivity for legal case retrieval, a novel Global Case Graph is incorporated to represent both the case semantic relationship and the case legal charge relationship. A novel contrastive objective with a regularisation on the degree of case nodes is proposed to leverage the information carried by the case reference relationship to optimise the model. Extensive experiments have been conducted on two benchmark datasets, which demonstrate the state-of-the-art performance of CaseLink. The code has been released on https://github.com/yanran-tang/CaseLink.","['Yanran Tang', 'Ruihong Qiu', 'Hongzhi Yin', 'Xue Li', 'Zi Huang']",2024-03-26T15:13:16Z,http://arxiv.org/abs/2403.17780v3,Law & Policy,Case Law Retrieval,"in case law, the precedents are the relevant cases that are used to support the decisions made by the judges and opinions of lawyers . to efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners . the intrinsic case connectivity relationships among cases have not been well exploited for case encoding ."
Explicitly Integrating Judgment Prediction with Legal Document   Retrieval: A Law-Guided Generative Approach,"Legal document retrieval and judgment prediction are crucial tasks in intelligent legal systems. In practice, determining whether two documents share the same judgments is essential for establishing their relevance in legal retrieval. However, existing legal retrieval studies either ignore the vital role of judgment prediction or rely on implicit training objectives, expecting a proper alignment of legal documents in vector space based on their judgments. Neither approach provides explicit evidence of judgment consistency for relevance modeling, leading to inaccuracies and a lack of transparency in retrieval. To address this issue, we propose a law-guided method, namely GEAR, within the generative retrieval framework. GEAR explicitly integrates judgment prediction with legal document retrieval in a sequence-to-sequence manner. Experiments on two Chinese legal case retrieval datasets show the superiority of GEAR over state-of-the-art methods while maintaining competitive judgment prediction performance. Moreover, we validate its robustness across languages and domains on a French statutory article retrieval dataset.","['Weicong Qin', 'Zelin Cao', 'Weijie Yu', 'Zihua Si', 'Sirui Chen', 'Jun Xu']",2023-12-15T08:01:55Z,http://arxiv.org/abs/2312.09591v2,Law & Policy,Case Law Retrieval,legal document retrieval and judgment prediction are crucial tasks in intelligent legal systems . existing retrieval studies either ignore the vital role of judgment prediction . neither approach provides explicit evidence of judgment consistency for relevance modeling .
PILOT: Legal Case Outcome Prediction with Case Law,"Machine learning shows promise in predicting the outcome of legal cases, but most research has concentrated on civil law cases rather than case law systems. We identified two unique challenges in making legal case outcome predictions with case law. First, it is crucial to identify relevant precedent cases that serve as fundamental evidence for judges during decision-making. Second, it is necessary to consider the evolution of legal principles over time, as early cases may adhere to different legal contexts. In this paper, we proposed a new framework named PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises two modules for relevant case retrieval and temporal pattern handling, respectively. To benchmark the performance of existing legal case outcome prediction models, we curated a dataset from a large-scale case law database. We demonstrate the importance of accurately identifying precedent cases and mitigating the temporal shift when making predictions for case law, as our method shows a significant improvement over the prior methods that focus on civil law case outcome predictions.","['Lang Cao', 'Zifeng Wang', 'Cao Xiao', 'Jimeng Sun']",2024-01-28T21:18:05Z,http://arxiv.org/abs/2401.15770v3,Law & Policy,Case Law Retrieval,most research has focused on civil law cases rather than case law systems . we propose a new framework called PILOT for case outcome prediction . it comprises two modules for relevant case retrieval and temporal pattern handling .
nigam@COLIEE-22: Legal Case Retrieval and Entailment using Cascading of   Lexical and Semantic-based models,"This paper describes our submission to the Competition on Legal Information Extraction/Entailment 2022 (COLIEE-2022) workshop on case law competition for tasks 1 and 2. Task 1 is a legal case retrieval task, which involves reading a new case and extracting supporting cases from the provided case law corpus to support the decision. Task 2 is the legal case entailment task, which involves the identification of a paragraph from existing cases that entails the decision in a relevant case. We employed the neural models Sentence-BERT and Sent2Vec for semantic understanding and the traditional retrieval model BM25 for exact matching in both tasks. As a result, our team (""nigam"") ranked 5th among all the teams in Tasks 1 and 2. Experimental results indicate that the traditional retrieval model BM25 still outperforms neural network-based models.","['Shubham Kumar Nigam', 'Navansh Goel']",2022-04-16T18:10:02Z,http://arxiv.org/abs/2204.07853v1,Law & Policy,Case Law Retrieval,"our team (""nigam"") ranked 5th among all the teams in Tasks 1 and 2 . the traditional retrieval model BM25 still outperforms neural network-based models ."
Logic Rules as Explanations for Legal Case Retrieval,"In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logical, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logically correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts reasoning on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable nature of the logic rules, NS-LCR is equipped with built-in faithful explainability. We also show that NS-LCR is a model-agnostic framework that can be plugged in for multiple legal retrieval models. To showcase NS-LCR's superiority, we enhance existing benchmarks by adding manually annotated logic rules and introducing a novel explainability metric using Large Language Models (LLMs). Our comprehensive experiments reveal NS-LCR's effectiveness for ranking, alongside its proficiency in delivering reliable explanations for legal case retrieval.","['Zhongxiang Sun', 'Kepu Zhang', 'Weijie Yu', 'Haoyu Wang', 'Jun Xu']",2024-03-03T09:22:21Z,http://arxiv.org/abs/2403.01457v1,Law & Policy,Case Law Retrieval,we address the issue of using logic rules to explain the results from legal case retrieval . this paper proposes a framework that explicitly conducts reasoning on the matching of legal cases . NS-LCR is equipped with built-in faithful explainability .
Nyay-Darpan: Enhancing Decision Making Through Summarization and Case   Retrieval for Consumer Law in India,"AI-based judicial assistance and case prediction have been extensively studied in criminal and civil domains, but remain largely unexplored in consumer law, especially in India. In this paper, we present Nyay-Darpan, a novel two-in-one framework that (i) summarizes consumer case files and (ii) retrieves similar case judgements to aid decision-making in consumer dispute resolution. Our methodology not only addresses the gap in consumer law AI tools but also introduces an innovative approach to evaluate the quality of the summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice', symbolizing the ability of our tool to reflect the core of consumer disputes through precise summarization and intelligent case retrieval. Our system achieves over 75 percent accuracy in similar case prediction and approximately 70 percent accuracy across material summary evaluation metrics, demonstrating its practical effectiveness. We will publicly release the Nyay-Darpan framework and dataset to promote reproducibility and facilitate further research in this underexplored yet impactful domain.","['Swapnil Bhattacharyya', 'Shrey Ganatra', 'Harshvivek Kashid', 'Spandan Anaokar', 'Shruti Nair', 'Reshma Sekhar', 'Siddharth Manohar', 'Rahul Hemrajani', 'Pushpak Bhattacharyya']",2025-07-08T15:30:49Z,http://arxiv.org/abs/2507.06090v1,Law & Policy,Case Law Retrieval,'Nyay-Darpan' translates into 'Mirror of Justice' system achieves over 75 percent accuracy in similar case prediction and approximately 70 percent accuracy across material summary evaluation metrics .
Attentive Deep Neural Networks for Legal Document Retrieval,"Legal text retrieval serves as a key component in a wide range of legal text processing tasks such as legal question answering, legal case entailment, and statute law retrieval. The performance of legal text retrieval depends, to a large extent, on the representation of text, both query and legal documents. Based on good representations, a legal text retrieval model can effectively match the query to its relevant documents. Because legal documents often contain long articles and only some parts are relevant to queries, it is quite a challenge for existing models to represent such documents. In this paper, we study the use of attentive neural network-based text representation for statute law document retrieval. We propose a general approach using deep neural networks with attention mechanisms. Based on it, we develop two hierarchical architectures with sparse attention to represent long sentences and articles, and we name them Attentive CNN and Paraformer. The methods are evaluated on datasets of different sizes and characteristics in English, Japanese, and Vietnamese. Experimental results show that: i) Attentive neural methods substantially outperform non-neural methods in terms of retrieval performance across datasets and languages; ii) Pretrained transformer-based models achieve better accuracy on small datasets at the cost of high computational complexity while lighter weight Attentive CNN achieves better accuracy on large datasets; and iii) Our proposed Paraformer outperforms state-of-the-art methods on COLIEE dataset, achieving the highest recall and F2 scores in the top-N retrieval task.","['Ha-Thanh Nguyen', 'Manh-Kien Phi', 'Xuan-Bach Ngo', 'Vu Tran', 'Le-Minh Nguyen', 'Minh-Phuong Tu']",2022-12-13T01:37:27Z,http://arxiv.org/abs/2212.13899v1,Law & Policy,Case Law Retrieval,"legal text retrieval is a key component in a wide range of legal text processing tasks . performance of retrieval depends, to a large extent, on representation of text, both query and legal documents . because legal documents often contain long articles, it is quite a challenge for existing models ."
LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset,"As an important component of intelligent legal systems, legal case retrieval plays a critical role in ensuring judicial justice and fairness. However, the development of legal case retrieval technologies in the Chinese legal system is restricted by three problems in existing datasets: limited data size, narrow definitions of legal relevance, and naive candidate pooling strategies used in data sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale Legal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents. To the best of our knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval datasets, providing extensive coverage of criminal charges. Additionally, we enrich the existing relevance criteria by considering three key aspects: characterization, penalty, procedure. This comprehensive criteria enriches the dataset and may provides a more holistic perspective. Furthermore, we propose a two-level candidate set pooling strategy that effectively identify potential candidates for each query case. It's important to note that all cases in the dataset have been annotated by multiple legal experts specializing in criminal law. Their expertise ensures the accuracy and reliability of the annotations. We evaluate several state-of-the-art retrieval models at LeCaRDv2, demonstrating that there is still significant room for improvement in legal case retrieval. The details of LeCaRDv2 can be found at the anonymous website https://github.com/anonymous1113243/LeCaRDv2.","['Haitao Li', 'Yunqiu Shao', 'Yueyue Wu', 'Qingyao Ai', 'Yixiao Ma', 'Yiqun Liu']",2023-10-26T17:32:55Z,http://arxiv.org/abs/2310.17609v1,Law & Policy,Case Law Retrieval,"leCaRDv2 is a large-scale legal case retrieval dataset . it consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents . the dataset has been annotated by multiple legal experts specializing in criminal law ."
Leverage Knowledge Graph and Large Language Model for Law Article   Recommendation: A Case Study of Chinese Criminal Law,"Court efficiency is vital for social stability. However, in most countries around the world, the grassroots courts face case backlogs, with decisions relying heavily on judicial personnel's cognitive labor, lacking intelligent tools to improve efficiency. To address this issue, we propose an efficient law article recommendation approach utilizing a Knowledge Graph (KG) and a Large Language Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge Graph (CLAKG) as a database to store current law statutes, historical case information, and correspondence between law articles and historical cases. Additionally, we introduce an automated CLAKG construction method based on LLM. On this basis, we propose a closed-loop law article recommendation method. Finally, through a series of experiments using judgment documents from the website ""China Judgements Online"", we have improved the accuracy of law article recommendation in cases from 0.549 to 0.694, demonstrating that our proposed method significantly outperforms baseline approaches.","['Yongming Chen', 'Miner Chen', 'Ye Zhu', 'Juan Pei', 'Siyu Chen', 'Yu Zhou', 'Yi Wang', 'Yifan Zhou', 'Hao Li', 'Songan Zhang']",2024-10-07T11:45:04Z,http://arxiv.org/abs/2410.04949v2,Law & Policy,Case Law Retrieval,"grassroots courts face case backlogs, lacking intelligent tools to improve efficiency . we propose an efficient law article recommendation approach utilizing a KG and a large language model ."
LeiBi@COLIEE 2022: Aggregating Tuned Lexical Models with a   Cluster-driven BERT-based Model for Case Law Retrieval,"This paper summarizes our approaches submitted to the case law retrieval task in the Competition on Legal Information Extraction/Entailment (COLIEE) 2022. Our methodology consists of four steps; in detail, given a legal case as a query, we reformulate it by extracting various meaningful sentences or n-grams. Then, we utilize the pre-processed query case to retrieve an initial set of possible relevant legal cases, which we further re-rank. Lastly, we aggregate the relevance scores obtained by the first stage and the re-ranking models to improve retrieval effectiveness. In each step of our methodology, we explore various well-known and novel methods. In particular, to reformulate the query cases aiming to make them shorter, we extract unigrams using three different statistical methods: KLI, PLM, IDF-r, as well as models that leverage embeddings (e.g., KeyBERT). Moreover, we investigate if automatic summarization using Longformer-Encoder-Decoder (LED) can produce an effective query representation for this retrieval task. Furthermore, we propose a novel re-ranking cluster-driven approach, which leverages Sentence-BERT models that are pre-tuned on large amounts of data for embedding sentences from query and candidate documents. Finally, we employ a linear aggregation method to combine the relevance scores obtained by traditional IR models and neural-based models, aiming to incorporate the semantic understanding of neural models and the statistically measured topical relevance. We show that aggregating these relevance scores can improve the overall retrieval effectiveness.","['Arian Askari', 'Georgios Peikos', 'Gabriella Pasi', 'Suzan Verberne']",2022-05-26T13:32:33Z,http://arxiv.org/abs/2205.13351v1,Law & Policy,Case Law Retrieval,"this paper summarizes our approaches submitted to the case law retrieval task . given a legal case as a query, we reformulate it by extracting various meaningful sentences . then, we utilize the pre-processed query case to retrieve an initial set of possible legal cases ."
How Vital is the Jurisprudential Relevance: Law Article Intervened Legal   Case Retrieval and Matching,"Legal case retrieval (LCR) aims to automatically scour for comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching (LCM) task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction (LAP) sub-task, without any additional assumptions in inference. Besides, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on law distribution, which is more effective than conventional semantic similarity. Weperform a series of exhaustive experiments including two different tasks involving four real-world datasets. Results demonstrate that LCM-LAI achieves state-of-the-art performance.","['Nuo Xu', 'Pinghui Wang', 'Zi Liang', 'Junzhou Zhao', 'Xiaohong Guan']",2025-02-25T15:29:07Z,http://arxiv.org/abs/2502.18292v1,Law & Policy,Case Law Retrieval,legal case retrieval (LCM) aims to automatically scour for comparable legal cases . a daunting challenge is assessing the legal-rational similarity within the judicial domain . past works either tagged domain-specific factors or incorporated reference laws .
Limb-darkening and exoplanets II: Choosing the Best Law for Optimal   Retrieval of Transit Parameters,"Very precise measurements of exoplanet transit lightcurves both from ground and space based observatories make it now possible to fit the limb-darkening coefficients in the transit-fitting procedure rather than fix them to theoretical values. This strategy has been shown to give better results, as fixing the coefficients to theoretical values can give rise to important systematic errors which directly impact the physical properties of the system derived from such lightcurves such as the planetary radius. However, studies of the effect of limb darkening assumptions on the retrieved parameters have mostly focused on the widely used quadratic limb-darkening law, leaving out other proposed laws that are either simpler or better descriptions of model intensity profiles. In this work, we show that laws such as the logarithmic, square-root and three-parameter law do a better job than the quadratic and linear laws when deriving parameters from transit lightcurves, both in terms of bias and precision, for a wide range of situations. We therefore recommend to study which law to use on a case-by-case basis. We provide code to guide the decision of when to use each of these laws and select the optimal one in a mean-square error sense, which we note has a dependence on both stellar and transit parameters. Finally, we demonstrate that the so-called exponential law is non-physical as it typically produces negative intensities close to the limb and should therefore not be used.","['Néstor Espinoza', 'Andrés Jordán']",2016-01-21T01:12:13Z,http://arxiv.org/abs/1601.05485v2,Law & Policy,Case Law Retrieval,limb-darkening coefficients can be fitted to exoplanet transit lightcurves . this strategy has been shown to give better results . but studies of the effect of limb darkening assumptions have mostly focused on quadratic law . the so-called exponential law is non-physical as it typically produces negative intensities .
Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented   Generation,"While Retrieval-Augmented Generation (RAG) plays a crucial role in the application of Large Language Models (LLMs), existing retrieval methods in knowledge-dense domains like law and medicine still suffer from a lack of multi-perspective views, which are essential for improving interpretability and reliability. Previous research on multi-view retrieval often focused solely on different semantic forms of queries, neglecting the expression of specific domain knowledge perspectives. This paper introduces a novel multi-view RAG framework, MVRAG, tailored for knowledge-dense domains that utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision, thereby improving the effectiveness of the final inference. Experiments conducted on legal and medical case retrieval demonstrate significant improvements in recall and precision rates with our framework. Our multi-perspective retrieval approach unleashes the potential of multi-view information enhancing RAG tasks, accelerating the further application of LLMs in knowledge-intensive fields.","['Guanhua Chen', 'Wenhan Yu', 'Lei Sha']",2024-04-19T13:27:38Z,http://arxiv.org/abs/2404.12879v1,Law & Policy,Case Law Retrieval,"existing retrieval methods in knowledge-dense domains like law and medicine lack multi-perspective views . a novel multi-view RAG framework, MVRAG, is tailored for knowledge-dense domains . it utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision ."
Law Article-Enhanced Legal Case Matching: a Causal Learning Approach,"Legal case matching, which automatically constructs a model to estimate the similarities between the source and target cases, has played an essential role in intelligent legal systems. Semantic text matching models have been applied to the task where the source and target legal cases are considered as long-form text documents. These general-purpose matching models make the predictions solely based on the texts in the legal cases, overlooking the essential role of the law articles in legal case matching. In the real world, the matching results (e.g., relevance labels) are dramatically affected by the law articles because the contents and the judgments of a legal case are radically formed on the basis of law. From the causal sense, a matching decision is affected by the mediation effect from the cited law articles by the legal cases, and the direct effect of the key circumstances (e.g., detailed fact descriptions) in the legal cases. In light of the observation, this paper proposes a model-agnostic causal learning framework called Law-Match, under which the legal case matching models are learned by respecting the corresponding law articles. Given a pair of legal cases and the related law articles, Law-Match considers the embeddings of the law articles as instrumental variables (IVs), and the embeddings of legal cases as treatments. Using IV regression, the treatments can be decomposed into law-related and law-unrelated parts, respectively reflecting the mediation and direct effects. These two parts are then combined with different weights to collectively support the final matching prediction. We show that the framework is model-agnostic, and a number of legal case matching models can be applied as the underlying models. Comprehensive experiments show that Law-Match can outperform state-of-the-art baselines on three public datasets.","['Zhongxiang Sun', 'Jun Xu', 'Xiao Zhang', 'Zhenhua Dong', 'Ji-Rong Wen']",2022-10-20T04:37:12Z,http://arxiv.org/abs/2210.11012v2,Law & Policy,Case Law Retrieval,"legal case matching has played an essential role in intelligent legal systems . general-purpose matching models make predictions solely based on the texts in the legal cases . in the real world, the matching results are dramatically affected by the law articles ."
Reliable Transmission Spectrum Extraction with a Three-Parameter Limb   Darkening Law,"Stellar limb darkening must be properly accounted for to accurately determine the radii of exoplanets at various wavelengths. The standard approach to address limb darkening involves either using laws with coefficients from modelled stellar spectra or determining the coefficients empirically during light curve fitting of the data. Here, we test how accurately three common laws -- quadratic, power, and a three-parameter law -- can reproduce stellar limb darkening at different wavelengths and across a broad range of stars. We show that using a quadratic limb darkening law, which is most frequently employed by the community, leads to wavelength-dependent offsets in retrieved transmission spectra. For planets with high impact parameters ($b$ larger than about 0.5) the amplitude of these offsets can reach 1\% of the transit depth which is some cases is comparable to and can even exceed the expected signals from the planetary atmosphere. Furthermore, the quadratic law causes an offset in the value of the impact parameter when it is determined by fitting the broadband transit light curves. In contrast, using the Kipping--Sing three-parameter law leads to robust retrievals. We advocate the use of this law in retrievals, especially for transits with large impact parameters.","['Rosa E. Keers', 'Alexander I. Shapiro', 'Nadiia M. Kostogryz', 'Ana Glidden', 'Prajwal Niraula', 'Benjamin V. Rackham', 'Sara Seager Sami K. Solanki', 'Yvonne C. Unruh', 'Valeriy Vasilyev', 'Julien de Wit']",2024-10-24T10:17:43Z,http://arxiv.org/abs/2410.18617v1,Law & Policy,Case Law Retrieval,"limb darkening must be properly accounted for to accurately determine the radii of exoplanets . using quadratic, power, and a three-parameter law leads to robust retrievals . we advocate the use of this law in retrievals, especially for planets with large impact parameters ."
CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case   Encoding,"Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to enhance the model's ability to recognize case relevance in fine grains. Experimental results on multiple benchmarks demonstrate that CaseEncoder significantly outperforms both existing general pre-training models and legal-specific pre-training models in zero-shot legal case retrieval.","['Yixiao Ma', 'Yueyue Wu', 'Weihang Su', 'Qingyao Ai', 'Yiqun Liu']",2023-05-09T12:40:19Z,http://arxiv.org/abs/2305.05393v1,Law & Policy,Case Law Retrieval,legal case retrieval is a critical process for modern legal information systems . caseEncoder leverages fine-grained legal knowledge in both the data sampling and pre-training phases .
Separated by an Un-common Language: Towards Judgment Language Informed   Vector Space Modeling,"A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores. In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments.","['Ira Leviant', 'Roi Reichart']",2015-08-01T10:24:27Z,http://arxiv.org/abs/1508.00106v5,Law & Policy,Judgment Prediction,"a common evaluation practice in the vector space models literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs . most existing evaluation sets consist of scores collected for english word pairs only, ignoring the potential impact of the judgment language ."
Explicitly Integrating Judgment Prediction with Legal Document   Retrieval: A Law-Guided Generative Approach,"Legal document retrieval and judgment prediction are crucial tasks in intelligent legal systems. In practice, determining whether two documents share the same judgments is essential for establishing their relevance in legal retrieval. However, existing legal retrieval studies either ignore the vital role of judgment prediction or rely on implicit training objectives, expecting a proper alignment of legal documents in vector space based on their judgments. Neither approach provides explicit evidence of judgment consistency for relevance modeling, leading to inaccuracies and a lack of transparency in retrieval. To address this issue, we propose a law-guided method, namely GEAR, within the generative retrieval framework. GEAR explicitly integrates judgment prediction with legal document retrieval in a sequence-to-sequence manner. Experiments on two Chinese legal case retrieval datasets show the superiority of GEAR over state-of-the-art methods while maintaining competitive judgment prediction performance. Moreover, we validate its robustness across languages and domains on a French statutory article retrieval dataset.","['Weicong Qin', 'Zelin Cao', 'Weijie Yu', 'Zihua Si', 'Sirui Chen', 'Jun Xu']",2023-12-15T08:01:55Z,http://arxiv.org/abs/2312.09591v2,Law & Policy,Judgment Prediction,legal document retrieval and judgment prediction are crucial tasks in intelligent legal systems . existing retrieval studies either ignore the vital role of judgment prediction . neither approach provides explicit evidence of judgment consistency for relevance modeling .
Sequential Multi-task Learning with Task Dependency for Appeal Judgment   Prediction,"Legal Judgment Prediction (LJP) aims to automatically predict judgment results, such as charges, relevant law articles, and the term of penalty. It plays a vital role in legal assistant systems and has become a popular research topic in recent years. This paper concerns a worthwhile but not well-studied LJP task, Appeal judgment Prediction (AJP), which predicts the judgment of an appellate court on an appeal case based on the textual description of case facts and grounds of appeal. There are two significant challenges in practice to solve the AJP task. One is how to model the appeal judgment procedure appropriately. The other is how to improve the interpretability of the prediction results. We propose a Sequential Multi-task Learning Framework with Task Dependency for Appeal Judgement Prediction (SMAJudge) to address these challenges. SMAJudge utilizes two sequential components to model the complete proceeding from the lower court to the appellate court and employs an attention mechanism to make the prediction more explainable, which handles the challenges of AJP effectively. Experimental results obtained with a dataset consisting of more than 30K appeal judgment documents have revealed the effectiveness and superiority of SMAJudge.","['Lianxin Song', 'Xiaohui Han', 'Guangqi Liu', 'Wentong Wang', 'Chaoran Cui', 'Yilong Yin']",2022-03-09T08:51:13Z,http://arxiv.org/abs/2204.07046v1,Law & Policy,Judgment Prediction,Legal Judgment Prediction (LJP) aims to automatically predict judgment results . it predicts the judgment of an appellate court on an appeal case . there are two significant challenges in practice to solve the AJP task .
CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction,"In this paper, we introduce the \textbf{C}hinese \textbf{AI} and \textbf{L}aw challenge dataset (CAIL2018), the first large-scale Chinese legal dataset for judgment prediction. \dataset contains more than $2.6$ million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both \dataset and baselines will be released after the CAIL competition\footnote{http://cail.cipsc.org.cn/}.","['Chaojun Xiao', 'Haoxi Zhong', 'Zhipeng Guo', 'Cunchao Tu', 'Zhiyuan Liu', 'Maosong Sun', 'Yansong Feng', 'Xianpei Han', 'Zhen Hu', 'Heng Wang', 'Jianfeng Xu']",2018-07-04T02:09:06Z,http://arxiv.org/abs/1807.02478v1,Law & Policy,Judgment Prediction,dataset contains more than $2.6$ million criminal cases published by the Supreme People's Court of China . the annotations of judgment results are more detailed and rich .
Legal Syllogism Prompting: Teaching Large Language Models for Legal   Judgment Prediction,"Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.","['Cong Jiang', 'Xiaolei Yang']",2023-07-17T08:38:46Z,http://arxiv.org/abs/2307.08321v1,Law & Policy,Judgment Prediction,legal syllogism prompting (LoT) is a simple prompting method to teach large language models . the method enables models to predict judgment along with law articles and justification . results show that LLMs with LoT achieve better performance on diverse reasoning tasks .
RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with   Large Language Models,"Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.","['Yue Zhang', 'Zhiliang Tian', 'Shicheng Zhou', 'Haiyang Wang', 'Wenqing Hou', 'Yuying Liu', 'Xuechen Zhao', 'Minlie Huang', 'Ye Wang', 'Bin Zhou']",2025-05-27T14:50:21Z,http://arxiv.org/abs/2505.21281v1,Law & Policy,Judgment Prediction,"legal Judgment Prediction (LJP) is a pivotal task in legal AI . existing semantic-enhanced models integrate judicial precedents and legal knowledge . but they neglect legal reasoning logic, a critical component of legal judgments . experimental results on two public datasets show superior performance ."
Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction,"Legal judgment prediction is essential for enhancing judicial efficiency. In this work, we identify that existing large language models (LLMs) underperform in this domain due to challenges in understanding case complexities and distinguishing between similar charges. To adapt LLMs for effective legal judgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT) reasoning framework inspired by human judicial reasoning. ADAPT involves decomposing case facts, discriminating among potential charges, and predicting the final judgment. We further enhance LLMs through fine-tuning with multi-task synthetic trajectories to improve legal judgment prediction accuracy and efficiency under our ADAPT framework. Extensive experiments conducted on two widely-used datasets demonstrate the superior performance of our framework in legal judgment prediction, particularly when dealing with complex and confusing charges.","['Chenlong Deng', 'Kelong Mao', 'Yuyao Zhang', 'Zhicheng Dou']",2024-07-02T05:43:15Z,http://arxiv.org/abs/2407.01964v4,Law & Policy,Judgment Prediction,"large language models (LLMs) underperform due to challenges in understanding case complexities . ask-discriminate-predict (ADAPT) framework is inspired by human judicial reasoning . ADAPT involves decomposing case facts, discriminating among potential charges ."
Unveiling factors influencing judgment variation in Sentiment Analysis   with Natural Language Processing and Statistics,"TripAdvisor reviews and comparable data sources play an important role in many tasks in Natural Language Processing (NLP), providing a data basis for the identification and classification of subjective judgments, such as hotel or restaurant reviews, into positive or negative polarities. This study explores three important factors influencing variation in crowdsourced polarity judgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are tested: the role of Part Of Speech (POS), the impact of sentiment words such as ""tasty"", and the influence of neutral words like ""ok"" on judgment variation. The study's methodology employs one-word titles, demonstrating their efficacy in studying polarity variation of words. Statistical tests on mean equality are performed on word groups of our interest. The results of this study reveal that adjectives in one-word titles tend to result in lower judgment variation compared to other word types or POS. Sentiment words contribute to lower judgment variation as well, emphasizing the significance of sentiment words in research on polarity judgments, and neutral words are associated with higher judgment variation as expected. However, these effects cannot be always reproduced in longer titles, which suggests that longer titles do not represent the best data source for testing the ambiguity of single words due to the influence on word polarity by other words like negation in longer titles. This empirical investigation contributes valuable insights into the factors influencing polarity variation of words, providing a foundation for NLP practitioners that aim to capture and predict polarity judgments in Spanish and for researchers that aim to understand factors influencing judgment variation.","['Olga Kellert', 'Carlos Gómez-Rodríguez', 'Mahmud Uz Zaman']",2024-05-20T14:24:18Z,http://arxiv.org/abs/2405.12055v1,Law & Policy,Judgment Prediction,this study explores factors influencing variation in crowdsourced polarity judgments . it focuses on TripAdvisor reviews in Spanish . adjectives in one-word titles tend to result in lower judgment variation .
FAIR: A Causal Framework for Accurately Inferring Judgments Reversals,"Artificial intelligence researchers have made significant advances in legal intelligence in recent years. However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence. In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments. We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge. And then, our framework is validated on a challenging dataset as a legal judgment prediction task. The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance. In addition, we discuss the generalization ability of large language models for legal intelligence tasks using ChatGPT as an example. Our experiment has found that the generalization ability of large language models still has defects, and mining causal relationships can effectively improve the accuracy and explain ability of model predictions.","['Minghua He', 'Nanfei Gu', 'Yuntao Shi', 'Qionghui Zhang', 'Yaying Chen']",2023-06-20T15:02:25Z,http://arxiv.org/abs/2306.11585v2,Law & Policy,Judgment Prediction,a causal framework for accurately inferring case reversals is proposed . the framework is validated on a challenging dataset as a legal judgment prediction task . our experimental results show that our framework can tap the most critical factors .
"Judgment in macroeconomic output growth predictions: Efficiency,   accuracy and persistence","The present study applies observations of individual predictions of the first three releases of the US output growth rate to evaluate how the applied judgment affects prediction efficiency and accuracy as well as if judgment is persistent. While the first two issues have been assessed in other studies, there is little evidence on the formation of judgment in macroeconomic projections. Most of the forecasters produce unbiased predictions, but employing the median Bloomberg projection as baseline, it turns out that judgment generally does not improve accuracy. There seems to be persistence in the judgment applied by forecasters in the sense that the sign of the adjustment in the first release prediction carries over to the projections of the two following revisions. One possible explanation is that forecasters use some kind of anchor-and-adjustment heuristic.",['Michael Pedersen'],2024-04-05T14:00:37Z,http://arxiv.org/abs/2404.04105v1,Law & Policy,Judgment Prediction,study evaluates how applied judgment affects prediction efficiency and accuracy . there is little evidence on the formation of judgment in macroeconomic projections . one possible explanation is that forecasters use some kind of anchor-and-adjustment heuristic.
Incoherent Probability Judgments in Large Language Models,"Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.","['Jian-Qiao Zhu', 'Thomas L. Griffiths']",2024-01-30T00:40:49Z,http://arxiv.org/abs/2401.16646v2,Law & Policy,Judgment Prediction,autoregressive large language models (LLMs) trained for next-word prediction produce coherent text . but are they equally adept at forming coherent probability judgments? we use probabilistic identities and repeated judgments to assess the coherence .
The Effect of Context on Metaphor Paraphrase Aptness Judgments,"We conduct two experiments to study the effect of context on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN's predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out of context ratings and decreasing high out of context scores. We offer a provisional explanation for this compression effect.","['Yuri Bizzoni', 'Shalom Lappin']",2018-09-04T16:03:06Z,http://arxiv.org/abs/1809.01060v1,Law & Policy,Judgment Prediction,two experiments to study effect of context on metaphor paraphrase aptness judgments . adding document context compresses the scores towards the center of the scale . we offer a provisional explanation for this effect .
Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning,"Multiple defendants in a criminal fact description generally exhibit complex interactions, and cannot be well handled by existing Legal Judgment Prediction (LJP) methods which focus on predicting judgment results (e.g., law articles, charges, and terms of penalty) for single-defendant cases. To address this problem, we propose the task of multi-defendant LJP, which aims to automatically predict the judgment results for each defendant of multi-defendant cases. Two challenges arise with the task of multi-defendant LJP: (1) indistinguishable judgment results among various defendants; and (2) the lack of a real-world dataset for training and evaluation. To tackle the first challenge, we formalize the multi-defendant judgment process as hierarchical reasoning chains and introduce a multi-defendant LJP method, named Hierarchical Reasoning Network (HRN), which follows the hierarchical reasoning chains to determine criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty for each defendant. To tackle the second challenge, we collect a real-world multi-defendant LJP dataset, namely MultiLJP, to accelerate the relevant research in the future. Extensive experiments on MultiLJP verify the effectiveness of our proposed HRN.","['Yougang Lyu', 'Jitai Hao', 'Zihan Wang', 'Kai Zhao', 'Shen Gao', 'Pengjie Ren', 'Zhumin Chen', 'Fang Wang', 'Zhaochun Ren']",2023-12-10T04:46:30Z,http://arxiv.org/abs/2312.05762v1,Law & Policy,Judgment Prediction,the task of multi-defendant LJP aims to automatically predict the judgment results for each defendant . two challenges arise: (1) indistinguishable judgment results among various defendants; (2) lack of a real-world dataset for training and evaluation .
Enriching ImageNet with Human Similarity Judgments and Psychological   Embeddings,"Advances in object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks---such as ILSVRC---are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of human similarity judgments that supplement the ILSVRC validation set. The new dataset supports a range of task and performance metrics, including the evaluation of unsupervised learning algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. Scaling to the full 50,000 image set was made possible through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. This methodological innovation not only enables scaling, but should also improve the quality of solutions by focusing sampling where it is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. Collectively, ImageNet-HSJ assets support the appraisal of internal representations and the development of more human-like models.","['Brett D. Roads', 'Bradley C. Love']",2020-11-22T13:41:54Z,http://arxiv.org/abs/2011.11015v1,Law & Policy,Judgment Prediction,imageNet-HSJ is composed of human similarity judgments that supplement ILSVRC . embedding space contains an order of magnitude more points than previous efforts based on human judgments . more complex models that perform better on task-specific benchmarks do not better conform to humans .
Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning   in the Real Court Setting,"Legal judgment prediction(LJP) is an essential task for legal AI. While prior methods studied on this topic in a pseudo setting by employing the judge-summarized case narrative as the input to predict the judgment, neglecting critical case life-cycle information in real court setting could threaten the case logic representation quality and prediction correctness. In this paper, we introduce a novel challenging dataset from real courtrooms to predict the legal judgment in a reasonably encyclopedic manner by leveraging the genuine input of the case -- plaintiff's claims and court debate data, from which the case's facts are automatically recognized by comprehensively understanding the multi-role dialogues of the court debate, and then learnt to discriminate the claims so as to reach the final judgment through multi-task learning. An extensive set of experiments with a large civil trial data set shows that the proposed model can more accurately characterize the interactions among claims, fact and debate for legal judgment prediction, achieving significant improvements over strong state-of-the-art baselines. Moreover, the user study conducted with real judges and law school students shows the neural predictions can also be interpretable and easily observed, and thus enhancing the trial efficiency and judgment quality.","['Luyao Ma', 'Yating Zhang', 'Tianyi Wang', 'Xiaozhong Liu', 'Wei Ye', 'Changlong Sun', 'Shikun Zhang']",2021-07-12T04:27:14Z,http://arxiv.org/abs/2107.05192v1,Law & Policy,Judgment Prediction,"legal judgment prediction(LJP) is an essential task for legal AI . previous studies used judge-summarized case narrative as input to predict judgment . proposed model can more accurately characterize interactions between claims, fact and debate ."
Understanding Human Judgments of Causality,"Discriminating between causality and correlation is a major problem in machine learning, and theoretical tools for determining causality are still being developed. However, people commonly make causality judgments and are often correct, even in unfamiliar domains. What are humans doing to make these judgments? This paper examines differences in human experts' and non-experts' ability to attribute causality by comparing their performances to those of machine-learning algorithms. We collected human judgments by using Amazon Mechanical Turk (MTurk) and then divided the human subjects into two groups: experts and non-experts. We also prepared expert and non-expert machine algorithms based on different training of convolutional neural network (CNN) models. The results showed that human experts' judgments were similar to those made by an ""expert"" CNN model trained on a large number of examples from the target domain. The human non-experts' judgments resembled the prediction outputs of the CNN model that was trained on only the small number of examples used during the MTurk instruction. We also analyzed the differences between the expert and non-expert machine algorithms based on their neural representations to evaluate the performances, providing insight into the human experts' and non-experts' cognitive abilities.","['Masahiro Kazama', 'Yoshihiko Suhara', 'Andrey Bogomolov', ""Alex `Sandy' Pentland""]",2019-12-19T03:08:11Z,http://arxiv.org/abs/1912.08998v1,Law & Policy,Judgment Prediction,"paper examines differences in human experts' and non-experts' ability to attribute causality . experts' judgments were similar to those made by an ""expert"" CNN model . human nonexpert judgments resembled those of the CNN model trained on only a small number of examples ."
Predicting Human Similarity Judgments Using Large Language Models,"Similarity judgments provide a well-established method for accessing mental representations, with applications in psychology, neuroscience and machine learning. However, collecting similarity judgments can be prohibitively expensive for naturalistic datasets as the number of comparisons grows quadratically in the number of stimuli. One way to tackle this problem is to construct approximation procedures that rely on more accessible proxies for predicting similarity. Here we leverage recent advances in language models and online recruitment, proposing an efficient domain-general procedure for predicting human similarity judgments based on text descriptions. Intuitively, similar stimuli are likely to evoke similar descriptions, allowing us to use description similarity to predict pairwise similarity judgments. Crucially, the number of descriptions required grows only linearly with the number of stimuli, drastically reducing the amount of data required. We test this procedure on six datasets of naturalistic images and show that our models outperform previous approaches based on visual information.","['Raja Marjieh', 'Ilia Sucholutsky', 'Theodore R. Sumers', 'Nori Jacoby', 'Thomas L. Griffiths']",2022-02-09T21:09:25Z,http://arxiv.org/abs/2202.04728v1,Law & Policy,Judgment Prediction,"similarity judgments are a well-established method for accessing mental representations . however, collecting similarity judgements can be prohibitively expensive . a domain-general procedure is proposed to predict human similarity based on text descriptions ."
Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era   of Large Language Models,"This study investigates judgment prediction in a realistic scenario within the context of Indian judgments, utilizing a range of transformer-based models, including InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and GPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are predicted at the point when a case is presented for a decision in court, using only the information available at that time, such as the facts of the case, statutes, precedents, and arguments. This approach mimics real-world conditions, where decisions must be made without the benefit of hindsight, unlike retrospective analyses often found in previous studies. For transformer models, we experiment with hierarchical transformers and the summarization of judgment facts to optimize input for these models. Our experiments with LLMs reveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust performance in judgment prediction. Furthermore, incorporating additional legal information, such as statutes and precedents, significantly improves the outcome of the prediction task. The LLMs also provide explanations for their predictions. To evaluate the quality of these predictions and explanations, we introduce two human evaluation metrics: Clarity and Linking. Our findings from both automatic and human evaluations indicate that, despite advancements in LLMs, they are yet to achieve expert-level performance in judgment prediction and explanation tasks.","['Shubham Kumar Nigam', 'Aniket Deroy', 'Subhankar Maity', 'Arnab Bhattacharya']",2024-10-14T14:22:12Z,http://arxiv.org/abs/2410.10542v1,Law & Policy,Judgment Prediction,study investigates judgment prediction in a realistic scenario within the context of Indian judgments . we simulate how judgments are predicted at the point when a case is presented for a decision . our experiments with LLMs reveal that GPT-3.5 Turbo excels in realistic scenarios .
Aggregating Quantitative Relative Judgments: From Social Choice to   Ranking Prediction,"Quantitative Relative Judgment Aggregation (QRJA) is a new research topic in (computational) social choice. In the QRJA model, agents provide judgments on the relative quality of different candidates, and the goal is to aggregate these judgments across all agents. In this work, our main conceptual contribution is to explore the interplay between QRJA in a social choice context and its application to ranking prediction. We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a ""judgment"" on the contestants' relative abilities. This allows us to aggregate results from multiple races to evaluate the contestants' true qualities. At a technical level, we introduce new aggregation rules for QRJA and study their structural and computational properties. We evaluate the proposed methods on data from various real races and show that QRJA-based methods offer effective and interpretable ranking predictions.","['Yixuan Even Xu', 'Hanrui Zhang', 'Yu Cheng', 'Vincent Conitzer']",2024-10-07T23:20:32Z,http://arxiv.org/abs/2410.05550v1,Law & Policy,Judgment Prediction,QRJA is a new research topic in (computational) social choice . agents provide judgments on the relative quality of different candidates . goal is to aggregate these judgments across all agents .
Legal Judgment Prediction (LJP) Amid the Advent of Autonomous AI Legal   Reasoning,"Legal Judgment Prediction (LJP) is a longstanding and open topic in the theory and practice-of-law. Predicting the nature and outcomes of judicial matters is abundantly warranted, keenly sought, and vigorously pursued by those within the legal industry and also by society as a whole. The tenuous act of generating judicially laden predictions has been limited in utility and exactitude, requiring further advancement. Various methods and techniques to predict legal cases and judicial actions have emerged over time, especially arising via the advent of computer-based modeling. There has been a wide range of approaches attempted, including simple calculative methods to highly sophisticated and complex statistical models. Artificial Intelligence (AI) based approaches have also been increasingly utilized. In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy. Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu.",['Lance Eliot'],2020-09-29T00:12:42Z,http://arxiv.org/abs/2009.14620v1,Law & Policy,Judgment Prediction,legal judgment prediction (LJP) is a longstanding and open topic in the theory and practice-of-law . the tenuous act of generating judicially laden predictions has been limited in utility and exactitude . a review of the literature encompassing Legal Judgment Prediction is undertaken .
LawPal : A Retrieval Augmented Generation Based System for Enhanced   Legal Accessibility in India,"Access to legal knowledge in India is often hindered by a lack of awareness, misinformation and limited accessibility to judicial resources. Many individuals struggle to navigate complex legal frameworks, leading to the frequent misuse of laws and inadequate legal protection. To address these issues, we propose a Retrieval-Augmented Generation (RAG)-based legal chatbot powered by vectorstore oriented FAISS for efficient and accurate legal information retrieval. Unlike traditional chatbots, our model is trained using an extensive dataset comprising legal books, official documentation and the Indian Constitution, ensuring accurate responses to even the most complex or misleading legal queries. The chatbot leverages FAISS for rapid vector-based search, significantly improving retrieval speed and accuracy. It is also prompt-engineered to handle twisted or ambiguous legal questions, reducing the chances of incorrect interpretations. Apart from its core functionality of answering legal queries, the platform includes additional features such as real-time legal news updates, legal blogs, and access to law-related books, making it a comprehensive resource for users. By integrating advanced AI techniques with an optimized retrieval system, our chatbot aims to democratize legal knowledge, enhance legal literacy, and prevent the spread of misinformation. The study demonstrates that our approach effectively improves legal accessibility while maintaining high accuracy and efficiency, thereby contributing to a more informed and empowered society.","['Dnyanesh Panchal', 'Aaryan Gole', 'Vaibhav Narute', 'Raunak Joshi']",2025-02-23T13:45:47Z,http://arxiv.org/abs/2502.16573v1,Law & Policy,Legal Chatbots,the chatbot leverages vectorstore oriented FAISS for rapid vector-based search . it is prompt-engineered to handle twisted or ambiguous legal questions . the platform includes additional features such as real-time legal news updates .
Towards the Exploitation of LLM-based Chatbot for Providing Legal   Support to Palestinian Cooperatives,"With the ever-increasing utilization of natural language processing (NLP), we started to witness over the past few years a significant transformation in our interaction with legal texts. This technology has advanced the analysis and enhanced the understanding of complex legal terminology and contexts. The development of recent large language models (LLMs), particularly ChatGPT, has also introduced a revolutionary contribution to the way that legal texts can be processed and comprehended. In this paper, we present our work on a cooperative-legal question-answering LLM-based chatbot, where we developed a set of legal questions about Palestinian cooperatives, associated with their regulations and compared the auto-generated answers by the chatbot to their correspondences that are designed by a legal expert. To evaluate the proposed chatbot, we have used 50 queries generated by the legal expert and compared the answers produced by the chart to their relevance judgments. Finding demonstrated that an overall accuracy rate of 82% has been achieved when answering the queries, while exhibiting an F1 score equivalent to 79%.","['Rabee Qasem', 'Banan Tantour', 'Mohammed Maree']",2023-06-09T11:57:57Z,http://arxiv.org/abs/2306.05827v1,Law & Policy,Legal Chatbots,chatGPT has revolutionized the way legal texts can be processed and comprehended . the chatbot answers legal questions about Palestinian cooperatives . an overall accuracy rate of 82% was achieved when answering the queries .
Development of a Legal Document AI-Chatbot,"With the exponential growth of digital data and the increasing complexity of legal documentation, there is a pressing need for efficient and intelligent tools to streamline the handling of legal documents.With the recent developments in the AI field, especially in chatbots, it cannot be ignored as a very compelling solution to this problem.An insight into the process of creating a Legal Documentation AI Chatbot with as many relevant features as possible within the given time frame is presented.The development of each component of the chatbot is presented in detail.Each component's workings and functionality has been discussed.Starting from the build of the Android app and the Langchain query processing code till the integration of both through a Flask backend and REST API methods.","['Pranav Nataraj Devaraj', 'Rakesh Teja P V', 'Aaryav Gangrade', 'Manoj Kumar R']",2023-11-21T16:48:10Z,http://arxiv.org/abs/2311.12719v1,Law & Policy,Legal Chatbots,a legal document AI chatbot with as many relevant features as possible is presented . the android app and the Langchain query processing code are discussed .
Intention and Context Elicitation with Large Language Models in the   Legal Aid Intake Process,"Large Language Models (LLMs) and chatbots show significant promise in streamlining the legal intake process. This advancement can greatly reduce the workload and costs for legal aid organizations, improving availability while making legal assistance more accessible to a broader audience. However, a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation. As a result, clients may not realize the importance of providing essential additional context or expressing their underlying intentions, which are crucial for their legal cases. Traditionally, logic based decision trees have been used to automate intake for specific access to justice issues, such as immigration and eviction. But those solutions lack scalability. We demonstrate a proof-of-concept using LLMs to elicit and infer clients' underlying intentions and specific legal circumstances through free-form, language-based interactions. We also propose future research directions to use supervised fine-tuning or offline reinforcement learning to automatically incorporate intention and context elicitation in chatbots without explicit prompting.","['Nick Goodson', 'Rongfei Lu']",2023-11-22T10:04:29Z,http://arxiv.org/abs/2311.13281v1,Law & Policy,Legal Chatbots,large language models (LLMs) and chatbots show significant promise in streamlining the legal intake process . but a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data . clients may not realize the importance of providing essential additional context .
$\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large   Language Models,"Access to consumer grievance redressal in India is often hindered by procedural complexity, legal jargon, and jurisdictional challenges. To address this, we present $\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that streamlines the process using open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities through a concise and up-to-date knowledge base. We introduce three novel datasets: $\textit{GeneralQA}$ (general consumer law), $\textit{SectoralQA}$ (sector-specific knowledge) and $\textit{SyntheticQA}$ (for RAG evaluation), along with $\textit{NyayChat}$, a dataset of 300 annotated chatbot conversations. We also introduce $\textit{Judgments}$ data sourced from Indian Consumer Courts to aid the chatbot in decision making and to enhance user trust. We also propose $\textbf{HAB}$ metrics ($\textbf{Helpfulness, Accuracy, Brevity}$) to evaluate chatbot performance. Legal domain experts validated Grahak-Nyay's effectiveness. Code and datasets will be released.","['Shrey Ganatra', 'Swapnil Bhattacharyya', 'Harshvivek Kashid', 'Spandan Anaokar', 'Shruti Nair', 'Reshma Sekhar', 'Siddharth Manohar', 'Rahul Hemrajani', 'Pushpak Bhattacharyya']",2025-07-07T10:26:42Z,http://arxiv.org/abs/2507.04854v1,Law & Policy,Legal Chatbots,$textbfGrahak-Nyay$ is a chatbot that simplifies legal complexities . it uses open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) code and datasets will be released .
Mitigating Manipulation and Enhancing Persuasion: A Reflective   Multi-Agent Approach for Legal Argument Generation,"Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents--a Factor Analyst and an Argument Polisher--in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate Reflective Multi-Agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: ""arguable"", ""mismatched"", and ""non-arguable"". Results demonstrate Reflective Multi-Agent's significant superiority in successful abstention (preventing generation when arguments cannot be grounded), marked improvements in hallucination accuracy (reducing fabricated and misattributed factors), particularly in ""non-arguable"" scenarios, and enhanced factor utilization recall (improving the use of provided case facts). These findings suggest that structured reflection within a multi-agent framework offers a robust computable method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems, a critical step towards trustworthy AI in law. Project page: https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/","['Li Zhang', 'Kevin D. Ashley']",2025-06-03T15:28:30Z,http://arxiv.org/abs/2506.02992v1,Law & Policy,Legal Chatbots,"a reflective multi-agent method has been developed for legal argument generation . it employs a Factor Analyst and an Argument Polisher in an iterative refinement process . the method generates 3-ply legal arguments (plaintiff, defendant, rebuttal)"
FedBot: Enhancing Privacy in Chatbots with Federated Learning,"Chatbots are mainly data-driven and usually based on utterances that might be sensitive. However, training deep learning models on shared data can violate user privacy. Such issues have commonly existed in chatbots since their inception. In the literature, there have been many approaches to deal with privacy, such as differential privacy and secure multi-party computation, but most of them need to have access to users' data. In this context, Federated Learning (FL) aims to protect data privacy through distributed learning methods that keep the data in its location. This paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale customer support data. The POC combines Deep Bidirectional Transformer models and federated learning algorithms to protect customer data privacy during collaborative model training. The results of the proof-of-concept showcase the potential for privacy-preserving chatbots to transform the customer support industry by delivering personalized and efficient customer service that meets data privacy regulations and legal requirements. Furthermore, the system is specifically designed to improve its performance and accuracy over time by leveraging its ability to learn from previous interactions.","['Addi Ait-Mlouk', 'Sadi Alawadi', 'Salman Toor', 'Andreas Hellander']",2023-04-04T23:13:52Z,http://arxiv.org/abs/2304.03228v1,Law & Policy,Legal Chatbots,"training deep learning models on shared data can violate user privacy . such issues have commonly existed in chatbots since their inception . this paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot ."
Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights,"The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use. Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems.","['Maksym Taranukhin', 'Sahithya Ravi', 'Gabor Lukacs', 'Evangelos Milios', 'Vered Shwartz']",2024-03-19T12:24:20Z,http://arxiv.org/abs/2403.12678v2,Law & Policy,Legal Chatbots,"canada's air travel sector has seen a significant increase in flight delays, cancellations . a chatbot is used to retrieve information from a collection of documents detailing air travel regulations . the system delivers accurate answers, free of hallucinations, that passengers can rely on ."
Hallucination-Free? Assessing the Reliability of Leading AI Legal   Research Tools,"Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to ""hallucinate,"" or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as ""eliminating"" (Casetext, 2023) or ""avoid[ing]"" hallucinations (Thomson Reuters, 2023), or guaranteeing ""hallucination-free"" legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.","['Varun Magesh', 'Faiz Surani', 'Matthew Dahl', 'Mirac Suzgun', 'Christopher D. Manning', 'Daniel E. Ho']",2024-05-30T17:56:05Z,http://arxiv.org/abs/2405.20362v1,Law & Policy,Legal Chatbots,"legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI) tools are designed to assist with a wide range of core legal tasks . large language models used in these tools are prone to ""hallucinate,"" or make up false information ."
Several categories of Large Language Models (LLMs): A Short Survey,"Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future directions.","['Saurabh Pahune', 'Manoj Chandrasekharan']",2023-07-05T18:18:23Z,http://arxiv.org/abs/2307.10188v1,Law & Policy,Legal Chatbots,this essay offers a succinct summary of various LLM subcategories . the survey emphasizes recent developments and efforts made for different LLM types . it highlights unresolved problems in the field of developing chatbots and virtual assistants .
Let's have a chat with the EU AI Act,"As artificial intelligence (AI) regulations evolve and the regulatory landscape develops and becomes more complex, ensuring compliance with ethical guidelines and legal frameworks remains a challenge for AI developers. This paper introduces an AI-driven self-assessment chatbot designed to assist users in navigating the European Union AI Act and related standards. Leveraging a Retrieval-Augmented Generation (RAG) framework, the chatbot enables real-time, context-aware compliance verification by retrieving relevant regulatory texts and providing tailored guidance. By integrating both public and proprietary standards, it streamlines regulatory adherence, reduces complexity, and fosters responsible AI development. The paper explores the chatbot's architecture, comparing naive and graph-based RAG models, and discusses its potential impact on AI governance.","['Adam Kovari', 'Yasin Ghafourian', 'Csaba Hegedus', 'Belal Abu Naim', 'Kitti Mezei', 'Pal Varga', 'Markus Tauber']",2025-05-17T10:24:08Z,http://arxiv.org/abs/2505.11946v1,Law & Policy,Legal Chatbots,"this paper introduces an AI-driven self-assessment chatbot . it enables real-time, context-aware compliance verification . by integrating both public and proprietary standards, it streamlines regulatory adherence ."
An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of   Service-oriented Systems,"Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more efficient explanations. Chat4XAI leverages modern AI chatbot technology and dedicated prompt engineering. Compared to earlier work on natural-language explanations using classical software-based dialogue systems, using an AI chatbot eliminates the need for eliciting and defining potential questions and answers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API and evaluate the fidelity and stability of its explanations using an adaptive service exemplar.","['Andreas Metzger', 'Jone Bartel', 'Jan Laufer']",2023-09-25T09:05:36Z,http://arxiv.org/abs/2309.14391v1,Law & Policy,Legal Chatbots,deep RL is increasingly used to cope with the open-world assumption in service-oriented systems . we introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations .
Legal and ethical considerations regarding the use of ChatGPT in   education,"Artificial intelligence has evolved enormously over the last two decades, becoming mainstream in different scientific domains including education, where so far, it is mainly utilized to enhance administrative and intelligent tutoring systems services and academic support. ChatGPT, an artificial intelligence-based chatbot, developed by OpenAI and released in November 2022, has rapidly gained attention from the entire international community for its impressive performance in generating comprehensive, systematic, and informative human-like responses to user input through natural language processing. Inevitably, it has also rapidly posed several challenges, opportunities, and potential issues and concerns raised regarding its use across various scientific disciplines. This paper aims to discuss the legal and ethical implications arising from this new technology, identify potential use cases, and enrich our understanding of Generative AI, such as ChatGPT, and its capabilities in education.","['Fereniki Panagopoulou', 'Christina Parpoula', 'Kostas Karpouzis']",2023-06-09T14:54:09Z,http://arxiv.org/abs/2306.10037v1,Law & Policy,Legal Chatbots,chatGPT is an artificial intelligence-based chatbot released in 2022 . it generates human-like responses to user input through natural language processing . this paper aims to discuss the legal and ethical implications arising from this new technology .
Experts-in-the-Loop: Establishing an Effective Workflow in Crafting   Privacy Q&A,"Privacy policies play a vital role in safeguarding user privacy as legal jurisdictions worldwide emphasize the need for transparent data processing. While the suitability of privacy policies to enhance transparency has been critically discussed, employing conversational AI systems presents unique challenges in informing users effectively. In this position paper, we propose a dynamic workflow for transforming privacy policies into privacy question-and-answer (Q&A) pairs to make privacy policies easily accessible through conversational AI. Thereby, we facilitate interdisciplinary collaboration among legal experts and conversation designers, while also considering the utilization of large language models' generative capabilities and addressing associated challenges. Our proposed workflow underscores continuous improvement and monitoring throughout the construction of privacy Q&As, advocating for comprehensive review and refinement through an experts-in-the-loop approach.","['Zahra Kolagar', 'Anna Katharina Leschanowsky', 'Birgit Popp']",2023-11-18T20:32:59Z,http://arxiv.org/abs/2311.11161v1,Law & Policy,Legal Chatbots,"in this position paper, we propose a workflow for transforming privacy policies into privacy question-and-answer (Q&A) pairs . we facilitate interdisciplinary collaboration among legal experts and conversation designers . our proposed workflow underscores continuous improvement throughout the construction of privacy Q&As ."
"Towards VEsNA, a Framework for Managing Virtual Environments via Natural   Language Agents","Automating a factory where robots are involved is neither trivial nor cheap. Engineering the factory automation process in such a way that return of interest is maximized and risk for workers and equipment is minimized, is hence of paramount importance. Simulation can be a game changer in this scenario but requires advanced programming skills that domain experts and industrial designers might not have. In this paper we present the preliminary design and implementation of a general-purpose framework for creating and exploiting Virtual Environments via Natural language Agents (VEsNA). VEsNA takes advantage of agent-based technologies and natural language processing to enhance the design of virtual environments. The natural language input provided to VEsNA is understood by a chatbot and passed to a cognitive intelligent agent that implements the logic behind displacing objects in the virtual environment. In the VEsNA vision, the intelligent agent will be able to reason on this displacement and on its compliance to legal and normative constraints. It will also be able to implement what-if analysis and case-based reasoning. Objects populating the virtual environment will include active objects and will populate a dynamic simulation whose outcomes will be interpreted by the cognitive agent; explanations and suggestions will be passed back to the user by the chatbot.","['Andrea Gatti', 'Viviana Mascardi']",2022-07-20T07:26:59Z,http://arxiv.org/abs/2207.09711v1,Law & Policy,Legal Chatbots,the paper presents a framework for creating and exploiting virtual environments . natural language input is understood by a chatbot and passed to a cognitive intelligent agent . VEsNA takes advantage of agent-based technologies and natural language processing . the agent will be able to reason on the displacement of objects in the virtual environment .
Are LLM-based methods good enough for detecting unfair terms of service?,"Countless terms of service (ToS) are being signed everyday by users all over the world while interacting with all kinds of apps and websites. More often than not, these online contracts spanning double-digit pages are signed blindly by users who simply want immediate access to the desired service. What would normally require a consultation with a legal team, has now become a mundane activity consisting of a few clicks where users potentially sign away their rights, for instance in terms of their data privacy, to countless online entities/companies. Large language models (LLMs) are good at parsing long text-based documents, and could potentially be adopted to help users when dealing with dubious clauses in ToS and their underlying privacy policies. To investigate the utility of existing models for this task, we first build a dataset consisting of 12 questions applied individually to a set of privacy policies crawled from popular websites. Thereafter, a series of open-source as well as commercial chatbots such as ChatGPT, are queried over each question, with the answers being compared to a given ground truth. Our results show that some open-source models are able to provide a higher accuracy compared to some commercial models. However, the best performance is recorded from a commercial chatbot (ChatGPT4). Overall, all models perform only slightly better than random at this task. Consequently, their performance needs to be significantly improved before they can be adopted at large for this purpose.","['Mirgita Frasheri', 'Arian Bakhtiarnia', 'Lukas Esterle', 'Alexandros Iosifidis']",2024-08-24T09:26:59Z,http://arxiv.org/abs/2409.00077v2,Law & Policy,Legal Chatbots,large language models (LLMs) are good at parsing long text-based documents . they could potentially be adopted to help users dealing with dubious clauses . the best performance is recorded from a commercial chatbot (ChatGPT4)
Regulating Chatbot Output via Inter-Informational Competition,"The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empirical evidence has demonstrated that market competition among information outlets can effectively mitigate most risks and that overreliance on regulation is not only unnecessary but detrimental, as well. This Article argues that sufficient competition among chatbots and other information outlets in the information marketplace can sufficiently mitigate and even resolve most content risks posed by generative AI technologies. This renders certain loudly advocated regulatory strategies, like mandatory prohibitions, licensure, curation of datasets, and notice-and-response regimes, truly unnecessary and even toxic to desirable competition and innovation throughout the AI industry. Ultimately, the ideas that I advance in this Article should pour some much-needed cold water on the regulatory frenzy over generative AI and steer the issue back to a rational track.",['Jiawei Zhang'],2024-03-17T00:11:15Z,http://arxiv.org/abs/2403.11046v2,Law & Policy,Legal Chatbots,the advent of chatGPT has sparked over a year of regulatory frenzy . but few studies have rigorously questioned the assumption of harm to human affairs . authors argue sufficient competition among chatbots can mitigate most risks .
Friend or Foe? Exploring the Implications of Large Language Models on   the Science System,"The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.","['Benedikt Fecher', 'Marcel Hebing', 'Melissa Laufer', 'Jörg Pohle', 'Fabian Sofsky']",2023-06-16T15:50:17Z,http://arxiv.org/abs/2306.09928v1,Law & Policy,Legal Chatbots,chatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education . there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice .
Align on the Fly: Adapting Chatbot Behavior to Established Norms,"In this paper, we aim to align large language models with the ever-changing, complex, and diverse human values (e.g., social norms) across time and locations. This presents a challenge to existing alignment techniques, such as supervised fine-tuning, which internalize values within model parameters. To overcome this, we propose an On-the-fly Preference Optimization (OPO) method, which is a real-time alignment that works in a streaming way. It employs an external memory to store established rules for alignment, which can constrain LLMs' behaviors without further training, allowing for convenient updates and customization of human values. We also introduce a scalable evaluation to assess the proposed method more effectively. Experimental results on both human-annotated and auto-generated questions from legal and moral domains indicate the effectiveness of the proposed OPO method. Our code and data are released at https://github.com/GAIR-NLP/OPO.","['Chunpu Xu', 'Steffi Chern', 'Ethan Chern', 'Ge Zhang', 'Zekun Wang', 'Ruibo Liu', 'Jing Li', 'Jie Fu', 'Pengfei Liu']",2023-12-26T06:51:09Z,http://arxiv.org/abs/2312.15907v1,Law & Policy,Legal Chatbots,paper aims to align large language models with ever-changing human values . it proposes an on-the-fly method that works in a streaming way . experimental results indicate the effectiveness of the proposed method .
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,"Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.","['Philip Feldman', 'James R. Foulds', 'Shimei Pan']",2024-03-02T12:19:04Z,http://arxiv.org/abs/2403.01193v3,Law & Policy,Legal Chatbots,"large language models (LLMs) like ChatGPT generate plausible but false information . their tendency to hallucinate poses a significant challenge, as seen in recent court cases . this paper explores how RAG can counter hallucinations by integrating external knowledge ."
PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for   Privacy Policy Compliance Verification,"Data protection and privacy is becoming increasingly crucial in the digital era. Numerous companies depend on third-party vendors and service providers to carry out critical functions within their operations, encompassing tasks such as data handling and storage. However, this reliance introduces potential vulnerabilities, as these vendors' security measures and practices may not always align with the standards expected by regulatory bodies. Businesses are required, often under the penalty of law, to ensure compliance with the evolving regulatory rules. Interpreting and implementing these regulations pose challenges due to their complexity. Regulatory documents are extensive, demanding significant effort for interpretation, while vendor-drafted privacy policies often lack the detail required for full legal compliance, leading to ambiguity. To ensure a concise interpretation of the regulatory requirements and compliance of organizational privacy policy with said regulations, we propose a Large Language Model (LLM) and Semantic Web based approach for privacy compliance. In this paper, we develop the novel Privacy Policy Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to efficiently store and retrieve comprehensive information concerning privacy policies, regulatory frameworks, and domain-specific knowledge pertaining to the legal landscape of privacy. Using Retrieval Augmented Generation, we identify the relevant sections in a privacy policy with corresponding regulatory rules. This information about individual privacy policies is populated into the PrivComp-KG. Combining this with the domain context and rules, the PrivComp-KG can be queried to check for compliance with privacy policies by each vendor against relevant policy regulations. We demonstrate the relevance of the PrivComp-KG, by verifying compliance of privacy policy documents for various organizations.","['Leon Garza', 'Lavanya Elluri', 'Anantaa Kotal', 'Aritran Piplai', 'Deepti Gupta', 'Anupam Joshi']",2024-04-30T17:44:44Z,http://arxiv.org/abs/2404.19744v1,Law & Policy,Privacy Law Compliance,data protection and privacy is becoming increasingly crucial in the digital era . businesses are required to ensure compliance with the evolving regulatory rules . a large language model (LLM) and semantic web based approach is proposed .
Brain Surgery: Ensuring GDPR Compliance in Large Language Models via   Concept Erasure,"As large-scale AI systems proliferate, ensuring compliance with data privacy laws such as the General Data Protection Regulation (GDPR) has become critical. This paper introduces Brain Surgery, a transformative methodology for making every local AI model GDPR-ready by enabling real-time privacy management and targeted unlearning. Building on advanced techniques such as Embedding-Corrupted Prompts (ECO Prompts), blockchain-based privacy management, and privacy-aware continual learning, Brain Surgery provides a modular solution that can be deployed across various AI architectures. This tool not only ensures compliance with privacy regulations but also empowers users to define their own privacy limits, creating a new paradigm in AI ethics and governance.",['Michele Laurelli'],2024-09-22T21:42:20Z,http://arxiv.org/abs/2409.14603v1,Law & Policy,Privacy Law Compliance,"this paper introduces Brain Surgery, a methodology for making AI GDPR-ready . Brain Surgery provides a modular solution that can be deployed across various AI architectures . it empowers users to define their own privacy limits, creating a new paradigm in AI ethics ."
Compliance Generation for Privacy Documents under GDPR: A Roadmap for   Implementing Automation and Machine Learning,"Most prominent research today addresses compliance with data protection laws through consumer-centric and public-regulatory approaches. We shift this perspective with the Privatech project to focus on corporations and law firms as agents of compliance. To comply with data protection laws, data processors must implement accountability measures to assess and document compliance in relation to both privacy documents and privacy practices. In this paper, we survey, on the one hand, current research on GDPR automation, and on the other hand, the operational challenges corporations face to comply with GDPR, and that may benefit from new forms of automation. We attempt to bridge the gap. We provide a roadmap for compliance assessment and generation by identifying compliance issues, breaking them down into tasks that can be addressed through machine learning and automation, and providing notes about related developments in the Privatech project.","['David Restrepo Amariles', 'Aurore Clément Troussel', 'Rajaa El Hamdani']",2020-12-23T14:46:51Z,http://arxiv.org/abs/2012.12718v1,Law & Policy,Privacy Law Compliance,"the Privatech project focuses on corporations and law firms as agents of compliance . to comply with data protection laws, data processors must implement accountability measures . the paper provides a roadmap for compliance assessment and generation ."
"Synthetic Data, Similarity-based Privacy Metrics, and Regulatory   (Non-)Compliance","In this paper, we argue that similarity-based privacy metrics cannot ensure regulatory compliance of synthetic data. Our analysis and counter-examples show that they do not protect against singling out and linkability and, among other fundamental issues, completely ignore the motivated intruder test.",['Georgi Ganev'],2024-07-24T01:45:41Z,http://arxiv.org/abs/2407.16929v2,Law & Policy,Privacy Law Compliance,"in this paper, we argue that similarity-based privacy metrics cannot ensure regulatory compliance of synthetic data . our analysis and counter-examples show they do not protect against singling out and linkability ."
LegiLM: A Fine-Tuned Legal Language Model for Data Compliance,"Ensuring compliance with international data protection standards for privacy and data security is a crucial but complex task, often requiring substantial legal expertise. This paper introduces LegiLM, a novel legal language model specifically tailored for consulting on data or information compliance. LegiLM leverages a pre-trained GDPR Fines dataset and has been fine-tuned to automatically assess whether particular actions or events breach data security and privacy regulations. By incorporating a specialized dataset that includes global data protection laws, meticulously annotated policy documents, and relevant privacy policies, LegiLM is optimized for addressing data compliance challenges. The model integrates advanced legal reasoning methods and information retrieval enhancements to enhance accuracy and reliability in practical legal consulting scenarios. Our evaluation using a custom benchmark dataset demonstrates that LegiLM excels in detecting data regulation breaches, offering sound legal justifications, and recommending necessary compliance modifications, setting a new benchmark for AI-driven legal compliance solutions. Our resources are publicly available at https://github.com/DAOLegalAI/LegiLM","['Linkai Zhu', 'Lu Yang', 'Chaofan Li', 'Shanwen Hu', 'Lu Liu', 'Bin Yin']",2024-09-09T02:06:52Z,http://arxiv.org/abs/2409.13721v1,Law & Policy,Privacy Law Compliance,"LegiLM leverages a pre-trained GDPR Fines dataset and has been fine-tuned . the model is optimized for addressing data compliance challenges . LegiLM excels in detecting data regulation breaches, offering sound legal justifications ."
Privacy Law Enforcement Under Centralized Governance: A Qualitative   Analysis of Four Years' Special Privacy Rectification Campaigns,"In recent years, major privacy laws like the GDPR have brought about positive changes. However, challenges remain in enforcing the laws, particularly due to under-resourced regulators facing a large number of potential privacy-violating software applications (apps) and the high costs of investigating them. Since 2019, China has launched a series of privacy enforcement campaigns known as Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy violations in its mobile application (app) ecosystem. Unlike the enforcement of the GDPR, SPRCs are characterized by large-scale privacy reviews and strict sanctions, under the strong control of central authorities. In SPRCs, central government authorities issue administrative orders to mobilize various resources for market-wide privacy reviews of mobile apps. They enforce strict sanctions by requiring privacy-violating apps to rectify issues within a short timeframe or face removal from app stores. While there are a few reports on SPRCs, the effectiveness and potential problems of this campaign-style privacy enforcement approach remain unclear to the community. In this study, we conducted 18 semi-structured interviews with app-related engineers involved in SPRCs to better understand the campaign-style privacy enforcement. Based on the interviews, we reported our findings on a variety of aspects of SPRCs, such as the processes that app engineers regularly follow to achieve privacy compliance in SPRCs, the challenges they encounter, the solutions they adopt to address these challenges, and the impacts of SPRCs, etc. We found that app engineers face a series of challenges in achieving privacy compliance in their apps...","['Tao Jing', 'Yao Li', 'Jingzhou Ye', 'Jie Wang', 'Xueqiang Wang']",2025-03-11T15:56:09Z,http://arxiv.org/abs/2503.08568v1,Law & Policy,Privacy Law Compliance,"china has launched a series of privacy enforcement campaigns since 2019 . called special privacy rectification campaigns (SPRCs), they address widespread privacy violations . the effectiveness and potential problems of this campaign-style privacy enforcement approach remain unclear ."
From Legal Text to Tech Specs: Generative AI's Interpretation of Consent   in Privacy Law,"Privacy law and regulation have turned to ""consent"" as the legitimate basis for collecting and processing individuals' data. As governments have rushed to enshrine consent requirements in their privacy laws, such as the California Consumer Privacy Act (CCPA), significant challenges remain in understanding how these legal mandates are operationalized in software. The opaque nature of software development processes further complicates this translation. To address this, we explore the use of Large Language Models (LLMs) in requirements engineering to bridge the gap between legal requirements and technical implementation. This study employs a three-step pipeline that involves using an LLM to classify software use cases for compliance, generating LLM modifications for non-compliant cases, and manually validating these changes against legal standards. Our preliminary findings highlight the potential of LLMs in automating compliance tasks, while also revealing limitations in their reasoning capabilities. By benchmarking LLMs against real-world use cases, this research provides insights into leveraging AI-driven solutions to enhance legal compliance of software.","['Aniket Kesari', 'Travis Breaux', 'Tom Norton', 'Sarah Santos', 'Anmol Singhal']",2025-07-05T23:36:05Z,http://arxiv.org/abs/2507.04185v1,Law & Policy,Privacy Law Compliance,"privacy law and regulation have turned to ""consent"" as the legitimate basis for processing data . a new study explores the use of large language models (LLMs) in requirements engineering . it uses an LLM to classify software use cases for compliance, generating modifications for non-compliant cases ."
Evolution of repositories and privacy laws: commit activities in the   GDPR and CCPA era,"Free and open source software has gained a lot of momentum in the industry and the research community. The latest advances in privacy legislation, including the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), have forced the community to pay special attention to users' data privacy. The main aim of this work is to examine software repositories that are acting on privacy laws. We have collected commit data from GitHub repositories in order to understand indications on main data privacy laws (GDPR, CCPA, CPRA, UK DPA) in the last years. Via an automated process, we analyzed 37,213 commits from 12,391 repositories since 2016, whereas 594 commits from the 70 most popular repositories of the dataset were manually analyzed. We observe that most commits were performed on the year the law came into effect and privacy relevant terms appear in the commit messages, whereas reference to specific data privacy user rights is scarce. The study showed that more educational activities on data privacy user rights are needed, as well as tools for privacy recommendations, whereas verifying actual compliance via source code execution is a useful direction for software engineering researchers.","['Georgia M. Kapitsaki', 'Maria Papoutsoglou']",2025-05-28T11:10:58Z,http://arxiv.org/abs/2505.22234v1,Law & Policy,Privacy Law Compliance,free and open source software has gained a lot of momentum in the industry and the research community . recent advances in privacy legislation have forced the community to pay special attention to users' data privacy . the main aim of this work is to examine software repositories that are acting on privacy laws .
Consumer Beware! Exploring Data Brokers' CCPA Compliance,"Data brokers collect and sell the personal information of millions of individuals, often without their knowledge or consent. The California Consumer Privacy Act (CCPA) grants consumers the legal right to request access to, or deletion of, their data. To facilitate these requests, California maintains an official registry of data brokers. However, the extent to which these entities comply with the law is unclear.   This paper presents the first large-scale, systematic study of CCPA compliance of all 543 officially registered data brokers. Data access requests were manually submitted to each broker, followed by in-depth analyses of their responses (or lack thereof). Above 40% failed to respond at all, in an apparent violation of the CCPA. Data brokers that responded requested personal information as part of their identity verification process, including details they had not previously collected. Paradoxically, this means that exercising one's privacy rights under CCPA introduces new privacy risks.   Our findings reveal rampant non-compliance and lack of standardization of the data access request process. These issues highlight an urgent need for stronger enforcement, clearer guidelines, and standardized, periodic compliance checks to enhance consumers' privacy protections and improve data broker accountability.","['Elina van Kempen', 'Isita Bagayatkar', 'Pavel Frolikov', 'Chloe Georgiou', 'Gene Tsudik']",2025-06-27T04:57:32Z,http://arxiv.org/abs/2506.21914v1,Law & Policy,Privacy Law Compliance,"data brokers collect and sell the personal information of millions of individuals . the extent to which these entities comply with the law is unclear . above 40% failed to respond at all, in an apparent violation of the CCPA ."
"""I Always Felt that SomethingWasWrong."": Understanding Compliance Risks   and Mitigation Strategies when Highly-Skilled Compliance KnowledgeWorkers Use   Large Language Models","The rapid advancement of Large Language Models (LLMs) has transformed knowledge-intensive has led to its widespread usage by knowledge workers to enhance their productivity. As these professionals handle sensitive information, and the training of text-based GenAI models involves the use of extensive data, there are thus concerns about privacy, security, and broader compliance with regulations and laws. While existing research has addressed privacy and security concerns, the specific compliance risks faced by highly-skilled knowledge workers when using the LLMs, and their mitigation strategies, remain underexplored. As understanding these risks and strategies is crucial for the development of industry-specific compliant LLM mechanisms, this research conducted semi-structured interviews with 24 knowledge workers from knowledge-intensive industries to understand their practices and experiences when integrating LLMs into their workflows. Our research explored how these workers ensure compliance and the resources and challenges they encounter when minimizing risks. Our preliminary findings showed that knowledge workers were concerned about the leakage of sensitive information and took proactive measures such as distorting input data and limiting prompt details to mitigate such risks. Their ability to identify and mitigate risks, however, was significantly hampered by a lack of LLM-specific compliance guidance and training. Our findings highlight the importance of improving knowledge workers' compliance awareness and establishing support systems and compliance cultures within organizations.","['Siying Hu', 'Piaohong Wang', 'Yaxing Yao', 'Zhicong Lu']",2024-11-07T09:58:20Z,http://arxiv.org/abs/2411.04576v2,Law & Policy,Privacy Law Compliance,"the rapid advancement of large language models (LLMs) has transformed knowledge-intensive . there are concerns about privacy, security, and broader compliance with regulations and laws . this research conducted semi-structured interviews with 24 knowledge workers . the findings highlight the importance of improving knowledge workers' compliance awareness ."
The Saudi Privacy Policy Dataset,"This paper introduces the Saudi Privacy Policy Dataset, a diverse compilation of Arabic privacy policies from various sectors in Saudi Arabia, annotated according to the 10 principles of the Personal Data Protection Law (PDPL); the PDPL was established to be compatible with General Data Protection Regulation (GDPR); one of the most comprehensive data regulations worldwide. Data were collected from multiple sources, including the Saudi Central Bank, the Saudi Arabia National United Platform, the Council of Health Insurance, and general websites using Google and Wikipedia. The final dataset includes 1,000 websites belonging to 7 sectors, 4,638 lines of text, 775,370 tokens, and a corpus size of 8,353 KB. The annotated dataset offers significant reuse potential for assessing privacy policy compliance, benchmarking privacy practices across industries, and developing automated tools for monitoring adherence to data protection regulations. By providing a comprehensive and annotated dataset of privacy policies, this paper aims to facilitate further research and development in the areas of privacy policy analysis, natural language processing, and machine learning applications related to privacy and data protection, while also serving as an essential resource for researchers, policymakers, and industry professionals interested in understanding and promoting compliance with privacy regulations in Saudi Arabia.","['Hend Al-Khalifa', 'Malak Mashaabi', 'Ghadi Al-Yahya', 'Raghad Alnashwan']",2023-04-05T21:40:37Z,http://arxiv.org/abs/2304.02757v1,Law & Policy,Privacy Law Compliance,"this paper introduces the Saudi Privacy Policy Dataset, a diverse compilation of privacy policies in Saudi Arabia . data were collected from multiple sources including the Saudi Central Bank and the council of health insurance . the annotated dataset offers significant reuse potential for assessing privacy policy compliance ."
A Framework for Extracting and Modeling HIPAA Privacy Rules for   Healthcare Applications,"Some organizations use software applications to manage their customers' personal, medical, or financial information. In the United States, those software applications are obligated to preserve users' privacy and to comply with the United States federal privacy laws and regulations. To formally guarantee compliance with those regulations, it is essential to extract and model the privacy rules from the text of the law using a formal framework. In this work we propose a goal-oriented framework for modeling and extracting the privacy requirements from regulatory text using natural language processing techniques.","['Tariq Alshugran', 'Julius Dichter']",2016-03-09T16:57:18Z,http://arxiv.org/abs/1603.02964v1,Law & Policy,Privacy Law Compliance,"in the united states, software applications are obligated to preserve users' privacy . to formally guarantee compliance with those regulations, it is essential to extract rules from text . in this work, we propose a goal-oriented framework for extracting privacy requirements ."
Detecting Compliance of Privacy Policies with Data Protection Laws,"Privacy Policies are the legal documents that describe the practices that an organization or company has adopted in the handling of the personal data of its users. But as policies are a legal document, they are often written in extensive legal jargon that is difficult to understand. Though work has been done on privacy policies but none that caters to the problem of verifying if a given privacy policy adheres to the data protection laws of a given country or state. We aim to bridge that gap by providing a framework that analyzes privacy policies in light of various data protection laws, such as the General Data Protection Regulation (GDPR). To achieve that, firstly we labeled both the privacy policies and laws. Then a correlation scheme is developed to map the contents of a privacy policy to the appropriate segments of law that a policy must conform to. Then we check the compliance of privacy policy's text with the corresponding text of the law using NLP techniques. By using such a tool, users would be better equipped to understand how their personal data is managed. For now, we have provided a mapping for the GDPR and PDPA, but other laws can easily be incorporated in the already built pipeline.","['Ayesha Qamar', 'Tehreem Javed', 'Mirza Omer Beg']",2021-02-21T09:15:15Z,http://arxiv.org/abs/2102.12362v1,Law & Policy,Privacy Law Compliance,"privacy policies are legal documents that describe the practices that an organization or company has adopted . but as they are a legal document, they are often written in extensive legal jargon that is difficult to understand . we aim to bridge that gap by providing a framework that analyzes privacy policies ."
An Exploratory Mixed-Methods Study on General Data Protection Regulation   (GDPR) Compliance in Open-Source Software,"Background: Governments worldwide are considering data privacy regulations. These laws, e.g. the European Union's General Data Protection Regulation (GDPR), require software developers to meet privacy-related requirements when interacting with users' data. Prior research describes the impact of such laws on software development, but only for commercial software. Open-source software is commonly integrated into regulated software, and thus must be engineered or adapted for compliance. We do not know how such laws impact open-source software development.   Aims: To understand how data privacy laws affect open-source software development. We studied the European Union's GDPR, the most prominent such law. We investigated how GDPR compliance activities influence OSS developer activity (RQ1), how OSS developers perceive fulfilling GDPR requirements (RQ2), the most challenging GDPR requirements to implement (RQ3), and how OSS developers assess GDPR compliance (RQ4).   Method: We distributed an online survey to explore perceptions of GDPR implementations from open-source developers (N=56). We further conducted a repository mining study to analyze development metrics on pull requests (N=31462) submitted to open-source GitHub repositories.   Results: GDPR policies complicate open-source development processes and introduce challenges for developers, primarily regarding the management of users' data, implementation costs and time, and assessments of compliance. Moreover, we observed negative perceptions of GDPR from open-source developers and significant increases in development activity, in particular metrics related to coding and reviewing activity, on GitHub pull requests related to GDPR compliance.   Conclusions: Our findings motivate policy-related resources and automated tools to support data privacy regulation implementation and compliance efforts in open-source software.","['Lucas Franke', 'Huayu Liang', 'Sahar Farzanehpour', 'Aaron Brantly', 'James C. Davis', 'Chris Brown']",2024-06-20T20:38:33Z,http://arxiv.org/abs/2406.14724v1,Law & Policy,Privacy Law Compliance,"the European Union's GDPR requires software developers to meet privacy-related requirements when interacting with users' data . open-source software is commonly integrated into regulated software, and thus must be engineered for compliance ."
A Fine-grained Chinese Software Privacy Policy Dataset for Sequence   Labeling and Regulation Compliant Identification,"Privacy protection raises great attention on both legal levels and user awareness. To protect user privacy, countries enact laws and regulations requiring software privacy policies to regulate their behavior. However, privacy policies are written in natural languages with many legal terms and software jargon that prevent users from understanding and even reading them. It is desirable to use NLP techniques to analyze privacy policies for helping users understand them. Furthermore, existing datasets ignore law requirements and are limited to English. In this paper, we construct the first Chinese privacy policy dataset, namely CA4P-483, to facilitate the sequence labeling tasks and regulation compliance identification between privacy policies and software. Our dataset includes 483 Chinese Android application privacy policies, over 11K sentences, and 52K fine-grained annotations. We evaluate families of robust and representative baseline models on our dataset. Based on baseline performance, we provide findings and potential research directions on our dataset. Finally, we investigate the potential applications of CA4P-483 combing regulation requirements and program analysis.","['Kaifa Zhao', 'Le Yu', 'Shiyao Zhou', 'Jing Li', 'Xiapu Luo', 'Yat Fei Aemon Chiu', 'Yutong Liu']",2022-12-04T05:59:59Z,http://arxiv.org/abs/2212.04357v1,Law & Policy,Privacy Law Compliance,privacy policies are written in natural languages with many legal terms and software jargon . existing datasets ignore law requirements and are limited to english . this paper analyzes the first Chinese privacy policy dataset .
Automated Detection of GDPR Disclosure Requirements in Privacy Policies   using Deep Active Learning,"Since GDPR came into force in May 2018, companies have worked on their data practices to comply with this privacy law. In particular, since the privacy policy is the essential communication channel for users to understand and control their privacy, many companies updated their privacy policies after GDPR was enforced. However, most privacy policies are verbose, full of jargon, and vaguely describe companies' data practices and users' rights. Therefore, it is unclear if they comply with GDPR. In this paper, we create a privacy policy dataset of 1,080 websites labeled with the 18 GDPR requirements and develop a Convolutional Neural Network (CNN) based model which can classify the privacy policies with an accuracy of 89.2%. We apply our model to perform a measurement on the compliance in the privacy policies. Our results show that even after GDPR went into effect, 97% of websites still fail to comply with at least one requirement of GDPR.","['Tamjid Al Rahat', 'Tu Le', 'Yuan Tian']",2021-11-08T01:28:27Z,http://arxiv.org/abs/2111.04224v1,Law & Policy,Privacy Law Compliance,"most privacy policies are verbose, full of jargon, and vaguely describe companies' data practices . 97% of websites still fail to comply with at least one requirement of GDPR ."
Privacy Perspectives and Practices of Chinese Smart Home Product Teams,"Previous research has explored the privacy needs and concerns of device owners, primary users, and different bystander groups with regard to smart home devices like security cameras, smart speakers, and hubs, but little is known about the privacy views and practices of smart home product teams, particularly those in non-Western contexts. This paper presents findings from 27 semi-structured interviews with Chinese smart home product team members, including product/project managers, software/hardware engineers, user experience (UX) designers, legal/privacy experts, and marketers/operation specialists. We examine their privacy perspectives, practices, and risk mitigation strategies. Our results show that participants emphasized compliance with Chinese data privacy laws, which typically prioritized national security over individual privacy rights. China-specific cultural, social, and legal factors also influenced participants' ethical considerations and attitudes toward balancing user privacy and security with convenience. Drawing on our findings, we propose a set of recommendations for smart home product teams, along with socio-technical and legal interventions to address smart home privacy issues-especially those belonging to at-risk groups-in Chinese multi-user smart homes.","['Shijing He', 'Yaxiong Lei', 'Xiao Zhan', 'Chi Zhang', 'Juan Ye', 'Ruba Abu-Salma', 'Jose Such']",2025-06-06T23:49:48Z,http://arxiv.org/abs/2506.06591v1,Law & Policy,Privacy Law Compliance,"this paper presents findings from 27 semi-structured interviews with smart home product team members . results show participants emphasized compliance with Chinese data privacy laws . china-specific cultural, social, and legal factors also influenced participants' ethical considerations ."
SynthGuard: Redefining Synthetic Data Generation with a Scalable and   Privacy-Preserving Workflow Framework,"The growing reliance on data-driven applications in sectors such as healthcare, finance, and law enforcement underscores the need for secure, privacy-preserving, and scalable mechanisms for data generation and sharing. Synthetic data generation (SDG) has emerged as a promising approach but often relies on centralized or external processing, raising concerns about data sovereignty, domain ownership, and compliance with evolving regulatory standards. To overcome these issues, we introduce SynthGuard, a framework designed to ensure computational governance by enabling data owners to maintain control over SDG workflows. SynthGuard supports modular and privacy-preserving workflows, ensuring secure, auditable, and reproducible execution across diverse environments. In this paper, we demonstrate how SynthGuard addresses the complexities at the intersection of domain-specific needs and scalable SDG by aligning with requirements for data sovereignty and regulatory compliance. Developed iteratively with domain expert input, SynthGuard has been validated through real-world use cases, demonstrating its ability to balance security, privacy, and scalability while ensuring compliance. The evaluation confirms its effectiveness in implementing and executing SDG workflows and integrating privacy and utility assessments across various computational environments.","['Eduardo Brito', 'Mahmoud Shoush', 'Kristian Tamm', 'Paula Etti', 'Liina Kamm']",2025-07-14T17:11:20Z,http://arxiv.org/abs/2507.10489v1,Law & Policy,Privacy Law Compliance,"Synthetic data generation (SDG) relies on centralized or external processing . this raises concerns about data sovereignty, domain ownership, and compliance . we introduce SynthGuard, a framework designed to ensure computational governance ."
"A Comparative Audit of Privacy Policies from Healthcare Organizations in   USA, UK and India","Data privacy in healthcare is of paramount importance (and thus regulated using laws like HIPAA) due to the highly sensitive nature of patient data. To that end, healthcare organizations mention how they collect/process/store/share this data (i.e., data practices) via their privacy policies. Thus there is a need to audit these policies and check compliance with respective laws. This paper addresses this need and presents a large-scale data-driven study to audit privacy policies from healthcare organizations in three countries -- USA, UK, and India.   We developed a three-stage novel \textit{workflow} for our audit. First, we collected the privacy policies of thousands of healthcare organizations in these countries and cleaned this privacy policy data using a clustering-based mixed-method technique. We identified data practices regarding users' private medical data (medical history) and site privacy (cookie, logs) in these policies. Second, we adopted a summarization-based technique to uncover exact broad data practices across countries and notice important differences. Finally, we evaluated the cross-country data practices using the lens of legal compliance (with legal expert feedback) and grounded in the theory of Contextual Integrity (CI). Alarmingly, we identified six themes of non-alignment (observed in 21.8\% of data practices studied in India) pointed out by our legal experts. Furthermore, there are four \textit{potential violations} according to case verdicts from Indian Courts as pointed out by our legal experts. We conclude this paper by discussing the utility of our auditing workflow and the implication of our findings for different stakeholders.","['Gunjan Balde', 'Aryendra Singh', 'Niloy Ganguly', 'Mainack Mondal']",2023-06-20T14:21:37Z,http://arxiv.org/abs/2306.11557v1,Law & Policy,Privacy Law Compliance,data privacy in healthcare is of paramount importance due to the sensitive nature of patient data . there is a need to audit privacy policies from healthcare organizations in three countries . we developed a three-stage novel textitworkflow for our audit .
Adaptive PII Mitigation Framework for Large Language Models,"Artificial Intelligence (AI) faces growing challenges from evolving data protection laws and enforcement practices worldwide. Regulations like GDPR and CCPA impose strict compliance requirements on Machine Learning (ML) models, especially concerning personal data use. These laws grant individuals rights such as data correction and deletion, complicating the training and deployment of Large Language Models (LLMs) that rely on extensive datasets. Public data availability does not guarantee its lawful use for ML, amplifying these challenges.   This paper introduces an adaptive system for mitigating risk of Personally Identifiable Information (PII) and Sensitive Personal Information (SPI) in LLMs. It dynamically aligns with diverse regulatory frameworks and integrates seamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses advanced NLP techniques, context-aware analysis, and policy-driven masking to ensure regulatory compliance.   Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for Passport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon Comprehend (0.54). In human evaluations, the system achieved an average user trust score of 4.6/5, with participants acknowledging its accuracy and transparency. Observations demonstrate stricter anonymization under GDPR compared to CCPA, which permits pseudonymization and user opt-outs. These results validate the system as a scalable and robust solution for enterprise privacy compliance.","['Shubhi Asthana', 'Ruchi Mahindru', 'Bing Zhang', 'Jorge Sanz']",2025-01-21T19:22:45Z,http://arxiv.org/abs/2501.12465v1,Law & Policy,Privacy Law Compliance,"regulations like GDPR and CCPA impose strict compliance requirements . public data availability does not guarantee its lawful use for ML . system uses advanced NLP techniques, context-aware analysis, and policy-driven masking ."
Game and Reference: Policy Combination Synthesis for Epidemic Prevention   and Control,"In recent years, epidemic policy-making models are increasingly being used to provide reference for governors on prevention and control policies against catastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are currently constrained by two issues: First, previous methods develop policies based on effect evaluation, since few of factors in real-world decision-making can be modeled, the output policies will then easily become extreme. Second, the subjectivity and cognitive limitation of human make the historical policies not always optimal for the training of decision models. To these ends, we present a novel Policy Combination Synthesis (PCS) model for epidemic policy-making. Specially, to prevent extreme decisions, we introduce adversarial learning between the model-made policies and the real policies to force the output policies to be more human-liked. On the other hand, to minimize the impact of sub-optimal historical policies, we employ contrastive learning to let the model draw on experience from the best historical policies under similar scenarios. Both adversarial and contrastive learning are adaptive based on the comprehensive effects of real policies to ensure the model always learns useful information. Extensive experiments on real-world data prove the effectiveness of the proposed model.","['Zhiyi Tan', 'Bingkun Bao']",2024-03-16T00:26:59Z,http://arxiv.org/abs/2403.10744v1,Law & Policy,Policy Modeling,epidemic policy-making models are increasingly being used to provide reference for governors . subjectivity and cognitive limitation of human make historical policies not always optimal . authors present a novel policy combination synthesis (PCS) model .
Predicting the outcomes of policy diffusion from U.S. states to federal   law,"In the United States, national policies often begin as state laws, which then spread from state to state until they gain momentum to become enacted as a national policy. However, not every state policy reaches the national level. Previous work has suggested that state-level policies are more likely to become national policies depending on their geographic origin, their category of legislation, or some characteristic of their initiating states, such as wealth, urbanicity, or ideological liberalism. Here, we tested these hypotheses by divorcing the set of traits from the states' identities and building predictive forecasting models of state policies becoming national policies. Using a large, longitudinal data set of state level policies and their traits, we train models to predict (i) whether policies become national policy, and (ii) how many states must pass a given policy before it becomes national. Using these models as components, we then develop a logistic growth model to forecast when a currently spreading state-level policy is likely to pass at the national level. Our results indicate that traits of initiating states are not systematically correlated with becoming national policy and they predict neither how many states must enact a policy before it becomes national nor whether it ultimately becomes a national law. In contrast, the cumulative number of state-level adoptions of a policy is reasonably predictive of when a policy becomes national. For the policies of same sex marriage and methamphetamine precursor laws, we investigate how well the logistic growth model could forecast the probable time horizon for true national action. We close with a data-driven forecast of when marijuana legalization and ""stand your ground"" laws will become national policy.","['Nora Connor', 'Aaron Clauset']",2018-10-21T16:51:00Z,http://arxiv.org/abs/1810.08988v1,Law & Policy,Policy Modeling,"national policies often begin as state laws, then spread from state to state . but not every state policy reaches the national level . we test hypotheses by divorcing traits from states' identities ."
Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies,"Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermore, we leverage the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of the GMM policy via Riemannian optimization. We evaluate our approach on common robotic settings: Reaching motions, collision-avoidance behaviors, and multi-goal tasks. Our results show that our method outperforms common policy optimization baselines in terms of task success rate and low-variance solutions.","['Hanna Ziesche', 'Leonel Rozo']",2023-05-17T17:48:24Z,http://arxiv.org/abs/2305.10411v1,Law & Policy,Policy Modeling,robots often rely on previously-learned motion policies for performing tasks of diverse complexities . policy optimization is the emphde facto paradigm to adapt robot policies as a function of task-specific objectives . we leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem .
Doubly Robust Off-Policy Actor-Critic Algorithms for Reinforcement   Learning,"We study the problem of off-policy critic evaluation in several variants of value-based off-policy actor-critic algorithms. Off-policy actor-critic algorithms require an off-policy critic evaluation step, to estimate the value of the new policy after every policy gradient update. Despite enormous success of off-policy policy gradients on control tasks, existing general methods suffer from high variance and instability, partly because the policy improvement depends on gradient of the estimated value function. In this work, we present a new way of off-policy policy evaluation in actor-critic, based on the doubly robust estimators. We extend the doubly robust estimator from off-policy policy evaluation (OPE) to actor-critic algorithms that consist of a reward estimator performance model. We find that doubly robust estimation of the critic can significantly improve performance in continuous control tasks. Furthermore, in cases where the reward function is stochastic that can lead to high variance, doubly robust critic estimation can improve performance under corrupted, stochastic reward signals, indicating its usefulness for robust and safe reinforcement learning.","['Riashat Islam', 'Raihan Seraj', 'Samin Yeasar Arnob', 'Doina Precup']",2019-12-11T04:21:47Z,http://arxiv.org/abs/1912.05109v1,Law & Policy,Policy Modeling,the problem of off-policy critic evaluation is investigated in several variants . existing general methods suffer from high variance and instability . we extend the doubly robust estimator to actor-critic algorithms .
Supervised Off-Policy Ranking,"Off-policy evaluation (OPE) is to evaluate a target policy with data generated by other policies. Most previous OPE methods focus on precisely estimating the true performance of a policy. We observe that in many applications, (1) the end goal of OPE is to compare two or multiple candidate policies and choose a good one, which is a much simpler task than precisely evaluating their true performance; and (2) there are usually multiple policies that have been deployed to serve users in real-world systems and thus the true performance of these policies can be known. Inspired by the two observations, in this work, we study a new problem, supervised off-policy ranking (SOPR), which aims to rank a set of target policies based on supervised learning by leveraging off-policy data and policies with known performance. We propose a method to solve SOPR, which learns a policy scoring model by minimizing a ranking loss of the training policies rather than estimating the precise policy performance. The scoring model in our method, a hierarchical Transformer based model, maps a set of state-action pairs to a score, where the state of each pair comes from the off-policy data and the action is taken by a target policy on the state in an offline manner. Extensive experiments on public datasets show that our method outperforms baseline methods in terms of rank correlation, regret value, and stability. Our code is publicly available at GitHub.","['Yue Jin', 'Yue Zhang', 'Tao Qin', 'Xudong Zhang', 'Jian Yuan', 'Houqiang Li', 'Tie-Yan Liu']",2021-07-03T07:01:23Z,http://arxiv.org/abs/2107.01360v2,Law & Policy,Policy Modeling,supervised off-policy ranking (SOPR) aims to rank target policies based on supervised learning . method learns a policy scoring model by minimizing a ranking loss of the training policies . the method outperforms baseline methods in terms of rank correlation and regret value .
Paging and Registration in Cellular Networks: Jointly Optimal Policies   and an Iterative Algorithm,"This paper explores optimization of paging and registration policies in cellular networks. Motion is modeled as a discrete-time Markov process, and minimization of the discounted, infinite-horizon average cost is addressed. The structure of jointly optimal paging and registration policies is investigated through the use of dynamic programming for partially observed Markov processes. It is shown that there exist policies with a certain simple form that are jointly optimal, though the dynamic programming approach does not directly provide an efficient method to find the policies.   An iterative algorithm for policies with the simple form is proposed and investigated. The algorithm alternates between paging policy optimization and registration policy optimization. It finds a pair of individually optimal policies, but an example is given showing that the policies need not be jointly optimal. Majorization theory and Riesz's rearrangement inequality are used to show that jointly optimal paging and registration policies are given for symmetric or Gaussian random walk models by the nearest-location-first paging policy and distance threshold registration policies.","['Bruce Hajek', 'Kevin Mitzel', 'Sichao Yang']",2007-02-18T20:51:55Z,http://arxiv.org/abs/cs/0702102v1,Law & Policy,Policy Modeling,motion is modeled as a discrete-time Markov process . it is shown that there exist policies with a certain simple form that are jointly optimal . an iterative algorithm for policies with the simple form is proposed and investigated .
Tolerance-Guided Policy Learning for Adaptable and Transferrable   Delicate Industrial Insertion,"Policy learning for delicate industrial insertion tasks (e.g., PC board assembly) is challenging. This paper considers two major problems: how to learn a diversified policy (instead of just one average policy) that can efficiently handle different workpieces with minimum amount of training data, and how to handle defects of workpieces during insertion. To address the problems, we propose tolerance-guided policy learning. To encourage transferability of the learned policy to different workpieces, we add a task embedding to the policy's input space using the insertion tolerance. Then we train the policy using generative adversarial imitation learning with reward shaping (RS-GAIL) on a variety of representative situations. To encourage adaptability of the learned policy to handle defects, we build a probabilistic inference model that can output the best inserting pose based on failed insertions using the tolerance model. The best inserting pose is then used as a reference to the learned policy. This proposed method is validated on a sequence of IC socket insertion tasks in simulation. The results show that 1) RS-GAIL can efficiently learn optimal policies under sparse rewards; 2) the tolerance embedding can enhance the transferability of the learned policy; 3) the probabilistic inference makes the policy robust to defects on the workpieces.","['Boshen Niu', 'Chenxi Wang', 'Changliu Liu']",2021-08-04T22:23:41Z,http://arxiv.org/abs/2108.02303v1,Law & Policy,Policy Modeling,"policy learning for delicate industrial insertion tasks is challenging . to address the problems, we propose tolerance-guided policy learning . we train the policy using generative adversarial imitation learning with reward shaping ."
Minimax Model Learning,"We present a novel off-policy loss function for learning a transition model in model-based reinforcement learning. Notably, our loss is derived from the off-policy policy evaluation objective with an emphasis on correcting distribution shift. Compared to previous model-based techniques, our approach allows for greater robustness under model misspecification or distribution shift induced by learning/evaluating policies that are distinct from the data-generating policy. We provide a theoretical analysis and show empirical improvements over existing model-based off-policy evaluation methods. We provide further analysis showing our loss can be used for off-policy optimization (OPO) and demonstrate its integration with more recent improvements in OPO.","['Cameron Voloshin', 'Nan Jiang', 'Yisong Yue']",2021-03-02T23:16:36Z,http://arxiv.org/abs/2103.02084v1,Law & Policy,Policy Modeling,we present a novel off-policy loss function for learning a transition model . our approach allows for greater robustness under model misspecification or distribution shift . we provide a theoretical analysis and show empirical improvements .
Dr Jekyll and Mr Hyde: the Strange Case of Off-Policy Policy Updates,"The policy gradient theorem states that the policy should only be updated in states that are visited by the current policy, which leads to insufficient planning in the off-policy states, and thus to convergence to suboptimal policies. We tackle this planning issue by extending the policy gradient theory to policy updates with respect to any state density. Under these generalized policy updates, we show convergence to optimality under a necessary and sufficient condition on the updates' state densities, and thereby solve the aforementioned planning issue. We also prove asymptotic convergence rates that significantly improve those in the policy gradient literature.   To implement the principles prescribed by our theory, we propose an agent, Dr Jekyll & Mr Hyde (JH), with a double personality: Dr Jekyll purely exploits while Mr Hyde purely explores. JH's independent policies allow to record two separate replay buffers: one on-policy (Dr Jekyll's) and one off-policy (Mr Hyde's), and therefore to update JH's models with a mixture of on-policy and off-policy updates. More than an algorithm, JH defines principles for actor-critic algorithms to satisfy the requirements we identify in our analysis. We extensively test on finite MDPs where JH demonstrates a superior ability to recover from converging to a suboptimal policy without impairing its speed of convergence. We also implement a deep version of the algorithm and test it on a simple problem where it shows promising results.","['Romain Laroche', 'Remi Tachet']",2021-09-29T21:15:29Z,http://arxiv.org/abs/2109.14727v1,Law & Policy,Policy Modeling,policy should only be updated in states that are visited by the current policy . this leads to insufficient planning in the off-policy states . we prove asymptotic convergence rates that significantly improve those in the policy gradient literature .
Policy Constraint by Only Support Constraint for Offline Reinforcement   Learning,"Offline reinforcement learning (RL) aims to optimize a policy by using pre-collected datasets, to maximize cumulative rewards. However, offline reinforcement learning suffers challenges due to the distributional shift between the learned and behavior policies, leading to errors when computing Q-values for out-of-distribution (OOD) actions. To mitigate this issue, policy constraint methods aim to constrain the learned policy's distribution with the distribution of the behavior policy or confine action selection within the support of the behavior policy. However, current policy constraint methods tend to exhibit excessive conservatism, hindering the policy from further surpassing the behavior policy's performance. In this work, we present Only Support Constraint (OSC) which is derived from maximizing the total probability of learned policy in the support of behavior policy, to address the conservatism of policy constraint. OSC presents a regularization term that only restricts policies to the support without imposing extra constraints on actions within the support. Additionally, to fully harness the performance of the new policy constraints, OSC utilizes a diffusion model to effectively characterize the support of behavior policies. Experimental evaluations across a variety of offline RL benchmarks demonstrate that OSC significantly enhances performance, alleviating the challenges associated with distributional shifts and mitigating conservatism of policy constraints. Code is available at https://github.com/MoreanP/OSC.","['Yunkai Gao', 'Jiaming Guo', 'Fan Wu', 'Rui Zhang']",2025-03-07T07:55:51Z,http://arxiv.org/abs/2503.05207v1,Law & Policy,Policy Modeling,"offline reinforcement learning suffers challenges due to the distributional shift between the learned and behavior policies . current policy constraint methods tend to exhibit excessive conservatism, hindering the policy from further surpassing the behavior policy's performance ."
SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in   Language Model Preference Learning,"Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.   In this work, we show that on-policy and off-policy data offer complementary strengths in preference optimization: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on open-ended tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it outperforms prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.","['Tianjian Li', 'Daniel Khashabi']",2025-05-05T04:54:44Z,http://arxiv.org/abs/2505.02363v1,Law & Policy,Policy Modeling,"on-policy data is particularly effective for reasoning tasks like math and coding . meanwhile, off-poliicy data performs better on open-ended tasks like creative writing . we introduce SIMPLEMIX, an approach to combine complementary strengths . it improves upon on- and off-Policy DPO by an average of 6.03% on alpaca ."
Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient   Estimation for Deep Reinforcement Learning,"Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.","['Shixiang Gu', 'Timothy Lillicrap', 'Zoubin Ghahramani', 'Richard E. Turner', 'Bernhard Schölkopf', 'Sergey Levine']",2017-06-01T17:00:52Z,http://arxiv.org/abs/1706.00387v1,Law & Policy,Policy Modeling,this paper examines approaches to merging on- and off-policy updates for deep reinforcement learning . the results show that off-policies updates can be interpolated with on-police updates whilst still satisfying performance bounds .
A fine-grained policy model for Provenance-based Access Control and   Policy Algebras.pdf,"A fine-grained provenance-based access control policy model is proposed in this paper, in order to improve the express performance of existing model. This method employs provenance as conditions to determine whether a piece of data can be accessed because historical operations performed on data could reveal clues about its sensitivity and vulnerability. Particularly, our proposed work provides a four-valued decision set which allows showing status to match a restriction particularly. This framework consists of target policy, access control policy, and policy algebras. With the complete definition and algebra system construction, a practical fine-grained access control policy model is developed.","['Xinyu Fan', 'Faen Zhang', 'Jianfei Song', 'Jingming Guo', 'Fujie Gao']",2020-01-07T09:39:38Z,http://arxiv.org/abs/2001.01945v1,Law & Policy,Policy Modeling,a provenance-based access control policy model is proposed in this paper . this method employs provenance as conditions to determine whether data can be accessed . historical operations performed on data could reveal clues about its sensitivity and vulnerability .
Case-based off-policy policy evaluation using prototype learning,"Importance sampling (IS) is often used to perform off-policy policy evaluation but is prone to several issues, especially when the behavior policy is unknown and must be estimated from data. Significant differences between the target and behavior policies can result in uncertain value estimates due to, for example, high variance and non-evaluated actions. If the behavior policy is estimated using black-box models, it can be hard to diagnose potential problems and to determine for which inputs the policies differ in their suggested actions and resulting values. To address this, we propose estimating the behavior policy for IS using prototype learning. We apply this approach in the evaluation of policies for sepsis treatment, demonstrating how the prototypes give a condensed summary of differences between the target and behavior policies while retaining an accuracy comparable to baseline estimators. We also describe estimated values in terms of the prototypes to better understand which parts of the target policies have the most impact on the estimates. Using a simulator, we study the bias resulting from restricting models to use prototypes.","['Anton Matsson', 'Fredrik D. Johansson']",2021-11-22T11:03:45Z,http://arxiv.org/abs/2111.11113v1,Law & Policy,Policy Modeling,"importance sampling (IS) is often used to perform off-policy policy evaluation . if the behavior policy is estimated using black-box models, it can be hard to diagnose problems . we propose estimating behavior policy for IS using prototype learning ."
Relative Policy-Transition Optimization for Fast Policy Transfer,"We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two environments, policy and transition updates are completed in one closed loop to form a principled learning framework for policy transfer. We demonstrate the effectiveness of RPTO on a set of MuJoCo continuous control tasks by creating policy transfer problems via variant dynamics.","['Jiawei Xu', 'Cheng Zhou', 'Yizheng Zhang', 'Baoxiang Wang', 'Lei Han']",2022-06-13T09:55:04Z,http://arxiv.org/abs/2206.06009v3,Law & Policy,Policy Modeling,a lemma measures the relativity gap between two arbitrary Markov Decision Processes . we propose two algorithms that offer fast policy transfer and dynamics modelling . the algorithms are based on existing theoretical results in reinforcement learning .
Model-Based Offline Meta-Reinforcement Learning with Regularization,"Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, offline Meta-RL could be outperformed by offline single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between ""exploring"" the out-of-distribution state-actions by following the meta-policy and ""exploiting"" the offline dataset by staying close to the behavior policy. Motivated by such empirical analysis, we explore model-based offline Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efficient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via offline Meta-RL. Experiments corroborate the superior performance of MerPO over existing offline Meta-RL methods.","['Sen Lin', 'Jialin Wan', 'Tengyu Xu', 'Yingbin Liang', 'Junshan Zhang']",2022-02-07T04:15:20Z,http://arxiv.org/abs/2202.02929v2,Law & Policy,Policy Modeling,offline meta-RL is emerging as a promising approach to address distributional shift . it aims to learn an informative meta-policy from a collection of tasks . but it could be outperformed by offline single-task RL methods on good quality datasets .
Quantifying Policy Administration Cost in an Active Learning Framework,"This paper proposes a computational model for policy administration. As an organization evolves, new users and resources are gradually placed under the mediation of the access control model. Each time such new entities are added, the policy administrator must deliberate on how the access control policy shall be revised to reflect the new reality. A well-designed access control model must anticipate such changes so that the administration cost does not become prohibitive when the organization scales up. Unfortunately, past Access Control research does not offer a formal way to quantify the cost of policy administration. In this work, we propose to model ongoing policy administration in an active learning framework. Administration cost can be quantified in terms of query complexity. We demonstrate the utility of this approach by applying it to the evolution of protection domains. We also modelled different policy administration strategies in our framework. This allowed us to formally demonstrate that domain-based policies have a cost advantage over access control matrices because of the use of heuristic reasoning when the policy evolves. To the best of our knowledge, this is the first work to employ an active learning framework to study the cost of policy deliberation and demonstrate the cost advantage of heuristic policy administration.","['Si Zhang', 'Philip W. L. Fong']",2023-12-29T22:12:53Z,http://arxiv.org/abs/2401.00086v1,Law & Policy,Policy Modeling,"paper proposes a computational model for policy administration . as an organization evolves, new users and resources are gradually placed under the mediation of the access control model ."
Machine Learning-Based Security Policy Analysis,"Security-Enhanced Linux (SELinux) is a robust security mechanism that enforces mandatory access controls (MAC), but its policy language's complexity creates challenges for policy analysis and management. This research investigates the automation of SELinux policy analysis using graph-based techniques combined with machine learning approaches to detect policy anomalies. The study addresses two key questions: Can SELinux policy analysis be automated through graph analysis, and how do different anomaly detection models compare in analyzing SELinux policies? We will be comparing different machine learning models by evaluating their effectiveness in detecting policy violations and anomalies. Our approach utilizes Neo4j for graph representation of policies, with Node2vec transforming these graph structures into meaningful vector embeddings that can be processed by our machine learning models. In our results, the MLP Neural Network consistently demonstrated superior performance across different dataset sizes, achieving 95% accuracy with balanced precision and recall metrics, while both Random Forest and SVM models showed competitive but slightly lower performance in detecting policy violations. This combination of graph-based modeling and machine learning provides a more sophisticated and automated approach to understanding and analyzing complex SELinux policies compared to traditional manual analysis methods.","['Krish Jain', 'Joann Sum', 'Pranav Kapoor', 'Amir Eaman']",2024-12-30T18:24:27Z,http://arxiv.org/abs/2501.00085v2,Law & Policy,Policy Modeling,security-Enhanced Linux (SELinux) enforces mandatory access controls . but its complexity creates challenges for policy analysis and management . this research compares different machine learning models to detect anomalies .
Acceptable Use Policies for Foundation Models,"As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies: legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers' acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Developers also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Nevertheless, acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the overall AI ecosystem.",['Kevin Klyman'],2024-08-29T06:04:16Z,http://arxiv.org/abs/2409.09041v1,Law & Policy,Policy Modeling,foundation models have accumulated hundreds of millions of users . developers have begun to take steps to prevent harmful types of uses . acceptable use policies prohibit users from using a model for specific purposes .
Verifiable Reinforcement Learning via Policy Extraction,"While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.","['Osbert Bastani', 'Yewen Pu', 'Armando Solar-Lezama']",2018-05-22T00:14:32Z,http://arxiv.org/abs/1805.08328v2,Law & Policy,Policy Modeling,VIPER is an algorithm that learns decision tree policies guided by a DNN policy . it can represent complex policies (since they are nonparametric) but can be efficiently verified using existing techniques . the algorithm significantly outperforms two baselines .
An Ontological AI-and-Law Framework for the Autonomous Levels of AI   Legal Reasoning,"A framework is proposed that seeks to identify and establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR). Doing so provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law, and can be utilized by scholars in academic pursuits of AI legal reasoning, along with being used by law practitioners and legal professionals in gauging how advances in AI are aiding the practice of law and the realization of aspirational versus achieved results. A set of seven levels of autonomy for AI and Legal Reasoning are meticulously proffered and mindfully discussed.",['Lance Eliot'],2020-08-04T16:12:30Z,http://arxiv.org/abs/2008.07328v1,Law & Policy,AI in Legal Reasoning,a framework is proposed that seeks to establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR) the framework provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law .
Robustness and Overcoming Brittleness of AI-Enabled Legal   Micro-Directives: The Role of Autonomous Levels of AI Legal Reasoning,"Recent research by legal scholars suggests that the law might inevitably be transformed into legal micro-directives consisting of legal rules that are derived from legal standards or that are otherwise produced automatically or via the consequent derivations of legal goals and then propagated via automation for everyday use as readily accessible lawful directives throughout society. This paper examines and extends the legal micro-directives theories in three crucial respects: (1) By indicating that legal micro-directives are likely to be AI-enabled and evolve over time in scope and velocity across the autonomous levels of AI Legal Reasoning, (2) By exploring the trade-offs between legal standards and legal rules as the imprinters of the micro-directives, and (3) By illuminating a set of brittleness exposures that can undermine legal micro-directives and proffering potential mitigating remedies to seek greater robustness in the instantiation and promulgation of such AI-powered lawful directives.",['Lance Eliot'],2020-08-31T05:09:03Z,http://arxiv.org/abs/2009.02243v1,Law & Policy,AI in Legal Reasoning,recent research suggests that the law might inevitably be transformed into legal micro-directives . these are legal rules that are derived from legal standards or otherwise produced automatically . they are then propagated via automation for everyday use as readily accessible lawful directives .
AI and Legal Argumentation: Aligning the Autonomous Levels of AI Legal   Reasoning,"Legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law, and extensive research has attempted to augment or undertake legal argumentation via the use of computer-based automation including Artificial Intelligence (AI). AI advances in Natural Language Processing (NLP) and Machine Learning (ML) have especially furthered the capabilities of leveraging AI for aiding legal professionals, doing so in ways that are modeled here as CARE, namely Crafting, Assessing, Refining, and Engaging in legal argumentation. In addition to AI-enabled legal argumentation serving to augment human-based lawyering, an aspirational goal of this multi-disciplinary field consists of ultimately achieving autonomously effected human-equivalent legal argumentation. As such, an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to the maturation of AI and Legal Argumentation (AILA), proffering a new means of gauging progress in this ever-evolving and rigorously sought domain.",['Lance Eliot'],2020-09-11T22:05:40Z,http://arxiv.org/abs/2009.11180v1,Law & Policy,AI in Legal Reasoning,"legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law . research has attempted to augment or undertake legal arguments via the use of AI . an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning to the maturation of AI and Legal Argumentation (AILA)"
"Tasks and Roles in Legal AI: Data Curation, Annotation, and Verification","The application of AI tools to the legal field feels natural: large legal document collections could be used with specialized AI to improve workflow efficiency for lawyers and ameliorate the ""justice gap"" for underserved clients. However, legal documents differ from the web-based text that underlies most AI systems. The challenges of legal AI are both specific to the legal domain, and confounded with the expectation of AI's high performance in high-stakes settings. We identify three areas of special relevance to practitioners: data curation, data annotation, and output verification. First, it is difficult to obtain usable legal texts. Legal collections are inconsistent, analog, and scattered for reasons technical, economic, and jurisdictional. AI tools can assist document curation efforts, but the lack of existing data also limits AI performance. Second, legal data annotation typically requires significant expertise to identify complex phenomena such as modes of judicial reasoning or controlling precedents. We describe case studies of AI systems that have been developed to improve the efficiency of human annotation in legal contexts and identify areas of underperformance. Finally, AI-supported work in the law is valuable only if results are verifiable and trustworthy. We describe both the abilities of AI systems to support evaluation of their outputs, as well as new approaches to systematic evaluation of computational systems in complex domains. We call on both legal and AI practitioners to collaborate across disciplines and to release open access materials to support the development of novel, high-performing, and reliable AI tools for legal applications.","['Allison Koenecke', 'Jed Stiglitz', 'David Mimno', 'Matthew Wilkens']",2025-04-02T04:34:58Z,http://arxiv.org/abs/2504.01349v1,Law & Policy,AI in Legal Reasoning,legal documents differ from web-based text that underlies most AI systems . challenges of legal AI are both specific to the legal domain . lack of existing data limits AI performance .
Legal Judgment Prediction (LJP) Amid the Advent of Autonomous AI Legal   Reasoning,"Legal Judgment Prediction (LJP) is a longstanding and open topic in the theory and practice-of-law. Predicting the nature and outcomes of judicial matters is abundantly warranted, keenly sought, and vigorously pursued by those within the legal industry and also by society as a whole. The tenuous act of generating judicially laden predictions has been limited in utility and exactitude, requiring further advancement. Various methods and techniques to predict legal cases and judicial actions have emerged over time, especially arising via the advent of computer-based modeling. There has been a wide range of approaches attempted, including simple calculative methods to highly sophisticated and complex statistical models. Artificial Intelligence (AI) based approaches have also been increasingly utilized. In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy. Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu.",['Lance Eliot'],2020-09-29T00:12:42Z,http://arxiv.org/abs/2009.14620v1,Law & Policy,AI in Legal Reasoning,legal judgment prediction (LJP) is a longstanding and open topic in the theory and practice-of-law . the tenuous act of generating judicially laden predictions has been limited in utility and exactitude . a review of the literature encompassing Legal Judgment Prediction is undertaken .
Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal   Assistance,"Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well.","['Jatin Gupta', 'Akhil Sharma', 'Saransh Singhania', 'Ali Imam Abidi']",2025-05-28T06:06:53Z,http://arxiv.org/abs/2505.22003v1,Law & Policy,AI in Legal Reasoning,"Legal Assist AI is a transformer-based model designed to bridge this gap . the system retrieves relevant legal information from a curated database . it generates accurate responses, enabling effective assistance for diverse users ."
Multidimensionality of Legal Singularity: Parametric Analysis and the   Autonomous Levels of AI Legal Reasoning,"Legal scholars have in the last several years embarked upon an ongoing discussion and debate over a potential Legal Singularity that might someday occur, involving a variant or law-domain offshoot leveraged from the Artificial Intelligence (AI) realm amid its many decades of deliberations about an overarching and generalized technological singularity (referred to classically as The Singularity). This paper examines the postulated Legal Singularity and proffers that such AI and Law cogitations can be enriched by these three facets addressed herein: (1) dovetail additionally salient considerations of The Singularity into the Legal Singularity, (2) make use of an in-depth and innovative multidimensional parametric analysis of the Legal Singularity as posited in this paper, and (3) align and unify the Legal Singularity with the Levels of Autonomy (LoA) associated with AI Legal Reasoning (AILR) as propounded in this paper.",['Lance Eliot'],2020-08-24T17:28:35Z,http://arxiv.org/abs/2008.10575v1,Law & Policy,AI in Legal Reasoning,this paper examines the postulated Legal Singularity . it posits a variant or law-domain offshoot leveraged from the Artificial Intelligence realm . this paper proposes that such AI and Law cogitations can be enriched .
LeKUBE: A Legal Knowledge Update BEnchmark,"Recent advances in Large Language Models (LLMs) have significantly shaped the applications of AI in multiple fields, including the studies of legal intelligence. Trained on extensive legal texts, including statutes and legal documents, the legal LLMs can capture important legal knowledge/concepts effectively and provide important support for downstream legal applications such as legal consultancy. Yet, the dynamic nature of legal statutes and interpretations also poses new challenges to the use of LLMs in legal applications. Particularly, how to update the legal knowledge of LLMs effectively and efficiently has become an important research problem in practice. Existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain and cannot address the specific challenges of the legal domain, such as the nuanced application of new legal knowledge, the complexity and lengthiness of legal regulations, and the intricate nature of legal reasoning. To address this gap, we introduce the Legal Knowledge Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for legal LLMs across five dimensions. Specifically, we categorize the needs of knowledge updates in the legal domain with the help of legal professionals, and then hire annotators from law schools to create synthetic updates to the Chinese Criminal and Civil Code as well as sets of questions of which the answers would change after the updates. Through a comprehensive evaluation of state-of-the-art knowledge update methods, we reveal a notable gap between existing knowledge update methods and the unique needs of the legal domain, emphasizing the need for further research and development of knowledge update mechanisms tailored for legal LLMs.","['Changyue Wang', 'Weihang Su', 'Hu Yiran', 'Qingyao Ai', 'Yueyue Wu', 'Cheng Luo', 'Yiqun Liu', 'Min Zhang', 'Shaoping Ma']",2024-07-19T10:40:10Z,http://arxiv.org/abs/2407.14192v2,Law & Policy,AI in Legal Reasoning,how to update the legal knowledge of LLMs effectively and efficiently is an important research problem . existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain .
Legal Sentiment Analysis and Opinion Mining (LSAOM): Assimilating   Advances in Autonomous AI Legal Reasoning,"An expanding field of substantive interest for the theory of the law and the practice-of-law entails Legal Sentiment Analysis and Opinion Mining (LSAOM), consisting of two often intertwined phenomena and actions underlying legal discussions and narratives: (1) Sentiment Analysis (SA) for the detection of expressed or implied sentiment about a legal matter within the context of a legal milieu, and (2) Opinion Mining (OM) for the identification and illumination of explicit or implicit opinion accompaniments immersed within legal discourse. Efforts to undertake LSAOM have historically been performed by human hand and cognition, and only thinly aided in more recent times by the use of computer-based approaches. Advances in Artificial Intelligence (AI) involving especially Natural Language Processing (NLP) and Machine Learning (ML) are increasingly bolstering how automation can systematically perform either or both of Sentiment Analysis and Opinion Mining, all of which is being inexorably carried over into engagement within a legal context for improving LSAOM capabilities. This research paper examines the evolving infusion of AI into Legal Sentiment Analysis and Opinion Mining and proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR), plus provides additional insights regarding AI LSAOM in its mechanizations and potential impact to the study of law and the practicing of law.",['Lance Eliot'],2020-10-02T04:15:21Z,http://arxiv.org/abs/2010.02726v1,Law & Policy,AI in Legal Reasoning,a growing field of substantive interest entails legal Sentiment Analysis and Opinion Mining (LSAOM) advances in Artificial Intelligence (AI) are bolstering how automation can perform either or both of LSAOM . this research paper proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning .
Continual Pre-Training is (not) What You Need in Domain Adaption,"The recent advances in Legal Large Language Models (LLMs) have transformed the landscape of legal research and practice by automating tasks, enhancing research precision, and supporting complex decision-making processes. However, effectively adapting LLMs to the legal domain remains challenging due to the complexity of legal reasoning, the need for precise interpretation of specialized language, and the potential for hallucinations. This paper examines the efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the legal reasoning capabilities of LLMs. Through a series of experiments on legal reasoning tasks within the Taiwanese legal framework, we demonstrate that while DACP enhances domain-specific knowledge, it does not uniformly improve performance across all legal tasks. We discuss the trade-offs involved in DACP, particularly its impact on model generalization and performance in prompt-based tasks, and propose directions for future research to optimize domain adaptation strategies in legal AI.","['Pin-Er Chen', 'Da-Chen Lian', 'Shu-Kai Hsieh', 'Sieh-Chuen Huang', 'Hsuan-Lei Shao', 'Jun-Wei Chiu', 'Yang-Hsien Lin', 'Zih-Ching Chen', 'Cheng-Kuang', 'Eddie TC Huang', 'Simon See']",2025-04-18T10:14:51Z,http://arxiv.org/abs/2504.13603v1,Law & Policy,AI in Legal Reasoning,this paper examines the efficacy of Domain-Adaptive Continual Pre-Training . DACP does not uniformly improve performance across all legal tasks .
Legal Evalutions and Challenges of Large Language Models,"In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.","['Jiaqi Wang', 'Huan Zhao', 'Zhenyuan Yang', 'Peng Shu', 'Junhao Chen', 'Haobo Sun', 'Ruixi Liang', 'Shixin Li', 'Pengcheng Shi', 'Longjun Ma', 'Zongjia Liu', 'Zhengliang Liu', 'Tianyang Zhong', 'Yutong Zhang', 'Chong Ma', 'Xin Zhang', 'Tuo Zhang', 'Tianli Ding', 'Yudan Ren', 'Tianming Liu', 'Xi Jiang', 'Shu Zhang']",2024-11-15T12:23:12Z,http://arxiv.org/abs/2411.10137v1,Law & Policy,AI in Legal Reasoning,"in this paper, we review legal testing methods based on large language models . systematic tests are conducted on English and Chinese legal cases . the experimental results highlight both the potential and limitations of LLMs ."
Using LLMs to Discover Legal Factors,"Factors are a foundational component of legal analysis and computational models of legal reasoning. These factor-based representations enable lawyers, judges, and AI and Law researchers to reason about legal cases. In this paper, we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain. Our method takes as input raw court opinions and produces a set of factors and associated definitions. We demonstrate that a semi-automated approach, incorporating minimal human involvement, produces factor representations that can predict case outcomes with moderate success, if not yet as well as expert-defined factors can.","['Morgan Gray', 'Jaromir Savelka', 'Wesley Oliver', 'Kevin Ashley']",2024-10-10T00:42:10Z,http://arxiv.org/abs/2410.07504v1,Law & Policy,AI in Legal Reasoning,we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain . our method takes as input raw court opinions and produces a set of factors .
An Impact Model of AI on the Principles of Justice: Encompassing the   Autonomous Levels of AI Legal Reasoning,"Efforts furthering the advancement of Artificial Intelligence (AI) will increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the practice of law. It is argued in this research paper that the infusion of AI into existing and future legal activities and the judicial structure needs to be undertaken by mindfully observing an alignment with the core principles of justice. As such, the adoption of AI has a profound twofold possibility of either usurping the principles of justice, doing so in a Dystopian manner, and yet also capable to bolster the principles of justice, doing so in a Utopian way. By examining the principles of justice across the Levels of Autonomy (LoA) of AI Legal Reasoning, the case is made that there is an ongoing tension underlying the efforts to develop and deploy AI that can demonstrably determine the impacts and sway upon each core principle of justice and the collective set.",['Lance Eliot'],2020-08-26T22:56:41Z,http://arxiv.org/abs/2008.12615v1,Law & Policy,AI in Legal Reasoning,"eric liu: infusion of AI needs to be mindfully observing core principles of justice . he says adoption of AI has twofold possibility of usurping the justice principles . there is an ongoing tension underlying efforts to develop and deploy AI, he writes ."
Promises and pitfalls of artificial intelligence for legal applications,"Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.","['Sayash Kapoor', 'Peter Henderson', 'Arvind Narayanan']",2024-01-10T19:50:37Z,http://arxiv.org/abs/2402.01656v1,Law & Policy,AI in Legal Reasoning,we dive into AI's increasingly prevalent roles in three types of legal tasks . tasks that would lead to most significant changes to the legal profession are harder to evaluate . we recommend better evaluation and deployment of AI in legal contexts .
CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation,"Legal case documents play a critical role in judicial proceedings. As the number of cases continues to rise, the reliance on manual drafting of legal case documents is facing increasing pressure and challenges. The development of large language models (LLMs) offers a promising solution for automating document generation. However, existing benchmarks fail to fully capture the complexities involved in drafting legal case documents in real-world scenarios. To address this gap, we introduce CaseGen, the benchmark for multi-stage legal case documents generation in the Chinese legal domain. CaseGen is based on 500 real case samples annotated by legal experts and covers seven essential case sections. It supports four key tasks: drafting defense statements, writing trial facts, composing legal reasoning, and generating judgment results. To the best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs in the context of legal case document generation. To ensure an accurate and comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and validate its effectiveness through human annotations. We evaluate several widely used general-domain LLMs and legal-specific LLMs, highlighting their limitations in case document generation and pinpointing areas for potential improvement. This work marks a step toward a more effective framework for automating legal case documents drafting, paving the way for the reliable application of AI in the legal field. The dataset and code are publicly available at https://github.com/CSHaitao/CaseGen.","['Haitao Li', 'Jiaying Ye', 'Yiran Hu', 'Jia Chen', 'Qingyao Ai', 'Yueyue Wu', 'Junjie Chen', 'Yifan Chen', 'Cheng Luo', 'Quan Zhou', 'Yiqun Liu']",2025-02-25T08:03:32Z,http://arxiv.org/abs/2502.17943v1,Law & Policy,AI in Legal Reasoning,"caseGen is the benchmark for multi-stage legal case documents generation . it supports drafting defense statements, writing trial facts, composing legal reasoning . the benchmark is based on 500 real case samples annotated by legal experts ."
An Argumentation-Based Legal Reasoning Approach for DL-Ontology,"Ontology is a popular method for knowledge representation in different domains, including the legal domain, and description logics (DL) is commonly used as its description language. To handle reasoning based on inconsistent DL-based legal ontologies, the current paper presents a structured argumentation framework particularly for reasoning in legal contexts on the basis of ASPIC+, and translates the legal ontology into formulas and rules of an argumentation theory. With a particular focus on the design of autonomous vehicles from the perspective of legal AI, we show that using this combined theory of formal argumentation and DL-based legal ontology, acceptable assertions can be obtained based on inconsistent ontologies, and the traditional reasoning tasks of DL ontologies can also be accomplished. In addition, a formal definition of explanations for the result of reasoning is presented.","['Zhe Yu', 'Yiwei Lu']",2022-09-07T11:08:08Z,http://arxiv.org/abs/2209.03070v2,Law & Policy,AI in Legal Reasoning,"ontology is a popular method for knowledge representation in different domains . description logics (DL) is commonly used as its description language . acceptable assertions can be obtained based on inconsistent ontologies, paper shows ."
Artificial Intelligence and Legal Analysis: Implications for Legal   Education and the Profession,"This article reports the results of a study examining the ability of legal and non-legal Large Language Models to perform legal analysis using the Issue-Rule-Application-Conclusion framework. LLMs were tested on legal reasoning tasks involving rule analysis and analogical reasoning. The results show that LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail, an inability to commit to answers, false confidence, and hallucinations. The study compares legal and nonlegal LLMs, identifies shortcomings, and explores traits that may hinder their ability to think like a lawyer. It also discusses the implications for legal education and practice, highlighting the need for critical thinking skills in future lawyers and the potential pitfalls of overreliance on artificial intelligence AI resulting in a loss of logic, reasoning, and critical thinking skills.",['Lee Peoples'],2025-02-04T19:50:48Z,http://arxiv.org/abs/2502.03487v1,Law & Policy,AI in Legal Reasoning,"study examines ability of legal and non-legal Large Language Models to perform legal analysis . results show LLMs can conduct basic IRAC analysis, but are limited by brief responses lacking detail . study highlights the need for critical thinking skills in future lawyers ."
Turing Test and the Practice of Law: The Role of Autonomous Levels of AI   Legal Reasoning,"Artificial Intelligence (AI) is increasingly being applied to law and a myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR) autonomous capabilities. A major question that has generally been unaddressed involves how we will know when AILR has achieved autonomous capacities. The field of AI has grappled with similar quandaries over how to assess the attainment of Artificial General Intelligence (AGI), a persistently discussed issue among scholars since the inception of AI, with the Turing Test communally being considered as the bellwether for ascertaining such matters. This paper proposes a variant of the Turing Test that is customized for specific use in the AILR realm, including depicting how this famous gold standard of AI fulfillment can be robustly applied across the autonomous levels of AI Legal Reasoning.",['Lance Eliot'],2020-08-18T04:50:23Z,http://arxiv.org/abs/2008.07743v1,Law & Policy,AI in Legal Reasoning,artificial intelligence (AI) is increasingly being applied to law and a myriad of legal tasks . a major question that has generally been unaddressed is how we will know when AILR has achieved autonomous capacities . the field of AI has grappled with similar quandaries over how to assess the attainment of Artificial General Intelligence (AGI)
JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for   Legal Reasoning,"The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX","['Huanghai Liu', 'Quzhe Huang', 'Qingjing Chen', 'Yiran Hu', 'Jiayu Ma', 'Yun Liu', 'Weixing Shen', 'Yansong Feng']",2025-02-24T14:02:00Z,http://arxiv.org/abs/2502.17166v1,Law & Policy,AI in Legal Reasoning,"the Four-Element Theory is a fundamental framework in criminal law . many large language models (LLMs) attempt to incorporate this theory when handling legal tasks . to address this limitation, we introduce an expert-annotated knowledge base covering 155 criminal charges ."
RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with   Large Language Models,"Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.","['Yue Zhang', 'Zhiliang Tian', 'Shicheng Zhou', 'Haiyang Wang', 'Wenqing Hou', 'Yuying Liu', 'Xuechen Zhao', 'Minlie Huang', 'Ye Wang', 'Bin Zhou']",2025-05-27T14:50:21Z,http://arxiv.org/abs/2505.21281v1,Law & Policy,AI in Legal Reasoning,"legal Judgment Prediction (LJP) is a pivotal task in legal AI . existing semantic-enhanced models integrate judicial precedents and legal knowledge . but they neglect legal reasoning logic, a critical component of legal judgments . experimental results on two public datasets show superior performance ."
Intelligent Tutors Beyond K-12: An Observational Study of Adult Learner   Engagement and Academic Impact,"Intelligent tutors have proven to be effective in K-12 education, though their impact on adult learners -- especially as a supplementary resource -- remains underexplored. Understanding how adults voluntarily engage with educational technologies can inform the design of tools that support skill re-learning and enhancement. More critically, it helps determine whether tutoring systems, which are typically built for K-12 learners, can also support adult populations. This study examines the adoption, usage patterns, and effectiveness of a novel tutoring system, Apprentice Tutors, among adult learners at a state technical college. We analyze three types of data including, user demographics, grades, and tutor interactions, to assess whether voluntary tutor usage translates into measurable learning gains. Our findings reveal key temporal patterns in tutor engagement and provide evidence of learning within tutors, as determined through skill improvement in knowledge components across tutors. We also found evidence that this learning transferred outside the tutor, as observed through higher course assessment scores following tutor usage. These results suggest that intelligent tutors are a viable tool for adult learners, warranting further research into their long-term impact on this population.","['Adit Gupta', 'Christopher MacLellan']",2025-02-23T15:36:22Z,http://arxiv.org/abs/2502.16613v1,Education & Social Science,Intelligent Tutoring Systems,"a study examines the adoption, usage patterns, and effectiveness of a new tutoring system . the system, Apprentice Tutors, was used by adult learners at a state technical college . findings suggest that intelligent tutors are a viable tool for adult learners ."
Efficacy of a Computer Tutor that Models Expert Human Tutors,"Tutoring is highly effective for promoting learning. However, the contribution of expertise to tutoring effectiveness is unclear and continues to be debated. We conducted a 9-week learning efficacy study of an intelligent tutoring system (ITS) for biology modeled on expert human tutors with two control conditions: human tutors who were experts in the domain but not in tutoring and a no-tutoring condition. All conditions were supplemental to classroom instruction, and students took learning tests immediately before and after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis using logistic mixed-effects modeling indicates significant positive effects on the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which are in the 99th percentile of meta-analytic effects, as well as significant positive effects on the delayed post-test for the ITS (d =.36) and human tutors (d =.39). We discuss implications for the role of expertise in tutoring and the design of future studies.","['Andrew M. Olney', ""Sidney K. D'Mello"", 'Natalie Person', 'Whitney Cade', 'Patrick Hays', 'Claire W. Dempsey', 'Blair Lehman', 'Betsy Williams', 'Art Graesser']",2025-04-21T17:41:28Z,http://arxiv.org/abs/2504.16132v1,Education & Social Science,Intelligent Tutoring Systems,the contribution of expertise to tutoring effectiveness is unclear . human tutors were experts in the domain but not in tutoring . a 9-week learning efficacy study was conducted .
Educators' Perceptions of Large Language Models as Tutors: Comparing   Human and AI Tutors in a Blind Text-only Setting,"The rapid development of Large Language Models (LLMs) opens up the possibility of using them as personal tutors. This has led to the development of several intelligent tutoring systems and learning assistants that use LLMs as back-ends with various degrees of engineering. In this study, we seek to compare human tutors with LLM tutors in terms of engagement, empathy, scaffolding, and conciseness. We ask human tutors to annotate and compare the performance of an LLM tutor with that of a human tutor in teaching grade-school math word problems on these qualities. We find that annotators with teaching experience perceive LLMs as showing higher performance than human tutors in all 4 metrics. The biggest advantage is in empathy, where 80% of our annotators prefer the LLM tutor more often than the human tutors. Our study paints a positive picture of LLMs as tutors and indicates that these models can be used to reduce the load on human teachers in the future.","['Sankalan Pal Chowdhury', 'Terry Jingchen Zhang', 'Donya Rooein', 'Dirk Hovy', 'Tanja Käser', 'Mrinmaya Sachan']",2025-06-10T11:29:34Z,http://arxiv.org/abs/2506.08702v1,Education & Social Science,Intelligent Tutoring Systems,"the rapid development of Large Language Models (LLMs) opens up the possibility of using them as personal tutors . this has led to the development of several intelligent tutoring systems and learning assistants that use LLMs as back-ends with various degrees of engineering . in this study, we ask human tutors to annotate and compare the performance of an LLM tutor with that of a human tutor ."
Visualizing Intelligent Tutor Interactions for Responsive Pedagogy,"Intelligent tutoring systems leverage AI models of expert learning and student knowledge to deliver personalized tutoring to students. While these intelligent tutors have demonstrated improved student learning outcomes, it is still unclear how teachers might integrate them into curriculum and course planning to support responsive pedagogy. In this paper, we conducted a design study with five teachers who have deployed Apprentice Tutors, an intelligent tutoring platform, in their classes. We characterized their challenges around analyzing student interaction data from intelligent tutoring systems and built VisTA (Visualizations for Tutor Analytics), a visual analytics system that shows detailed provenance data across multiple coordinated views. We evaluated VisTA with the same five teachers, and found that the visualizations helped them better interpret intelligent tutor data, gain insights into student problem-solving provenance, and decide on necessary follow-up actions - such as providing students with further support or reviewing skills in the classroom. Finally, we discuss potential extensions of VisTA into sequence query and detection, as well as the potential for the visualizations to be useful for encouraging self-directed learning in students.","['Grace Guo', 'Aishwarya Mudgal Sunil Kumar', 'Adit Gupta', 'Adam Coscia', 'Chris MacLellan', 'Alex Endert']",2024-04-19T15:21:26Z,http://arxiv.org/abs/2404.12944v1,Education & Social Science,Intelligent Tutoring Systems,apprentice Tutors is an intelligent tutoring platform . it leverages AI models of expert learning and student knowledge to deliver personalized tutoring to students . but it is unclear how teachers might integrate them into curriculum to support responsive pedagogy .
Apprentice Tutor Builder: A Platform For Users to Create and Personalize   Intelligent Tutors,"Intelligent tutoring systems (ITS) are effective for improving students' learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization. Instructors can utilize ATB's drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB's design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design recommendations for our platform and others that utilize interactive AI agents for tutor creation and customization.","['Glen Smith', 'Adit Gupta', 'Christopher MacLellan']",2024-04-11T16:14:23Z,http://arxiv.org/abs/2404.07883v1,Education & Social Science,Intelligent Tutoring Systems,instructors can utilize the apprentice Tutor Builder (ATB) to build tutor interfaces . instructors can then interactively train the tutors' underlying AI agent to produce expert models that can solve problems .
Beyond Final Answers: Evaluating Large Language Models for Math Tutoring,"Researchers have made notable progress in applying Large Language Models (LLMs) to solve math problems, as demonstrated through efforts like GSM8k, ProofNet, AlphaGeometry, and MathOdyssey. This progress has sparked interest in their potential use for tutoring students in mathematics. However, the reliability of LLMs in tutoring contexts -- where correctness and instructional quality are crucial -- remains underexplored. Moreover, LLM problem-solving capabilities may not necessarily translate into effective tutoring support for students. In this work, we present two novel approaches to evaluate the correctness and quality of LLMs in math tutoring contexts. The first approach uses an intelligent tutoring system for college algebra as a testbed to assess LLM problem-solving capabilities. We generate benchmark problems using the tutor, prompt a diverse set of LLMs to solve them, and compare the solutions to those generated by the tutor. The second approach evaluates LLM as tutors rather than problem solvers. We employ human evaluators, who act as students seeking tutoring support from each LLM. We then assess the quality and correctness of the support provided by the LLMs via a qualitative coding process. We applied these methods to evaluate several ChatGPT models, including 3.5 Turbo, 4, 4o, o1-mini, and o1-preview. Our findings show that when used as problem solvers, LLMs generate correct final answers for 85.5% of the college algebra problems tested. When employed interactively as tutors, 90% of LLM dialogues show high-quality instructional support; however, many contain errors -- only 56.6% are entirely correct. We conclude that, despite their potential, LLMs are not yet suitable as intelligent tutors for math without human oversight or additional mechanisms to ensure correctness and quality.","['Adit Gupta', 'Jennifer Reddig', 'Tommaso Calo', 'Daniel Weitekamp', 'Christopher J. MacLellan']",2025-02-23T15:43:45Z,http://arxiv.org/abs/2503.16460v1,Education & Social Science,Intelligent Tutoring Systems,"researchers have made notable progress in applying large language models to solve math problems . the reliability of LLMs in tutoring contexts remains underexplored . despite their potential, they are not yet suitable as intelligent tutors for math ."
Aligning Tutor Discourse Supporting Rigorous Thinking with Tutee Content   Mastery for Predicting Math Achievement,"This work investigates how tutoring discourse interacts with students' proximal knowledge to explain and predict students' learning outcomes. Our work is conducted in the context of high-dosage human tutoring where 9th-grade students (N= 1080) attended small group tutorials and individually practiced problems on an Intelligent Tutoring System (ITS). We analyzed whether tutors' talk moves and students' performance on the ITS predicted scores on math learning assessments. We trained Random Forest Classifiers (RFCs) to distinguish high and low assessment scores based on tutor talk moves, student's ITS performance metrics, and their combination. A decision tree was extracted from each RFC to yield an interpretable model. We found AUCs of 0.63 for talk moves, 0.66 for ITS, and 0.77 for their combination, suggesting interactivity among the two feature sources. Specifically, the best decision tree emerged from combining the tutor talk moves that encouraged rigorous thinking and students' ITS mastery. In essence, tutor talk that encouraged mathematical reasoning predicted achievement for students who demonstrated high mastery on the ITS, whereas tutors' revoicing of students' mathematical ideas and contributions was predictive for students with low ITS mastery. Implications for practice are discussed.","['Mark Abdelshiheed', 'Jennifer K. Jacobs', ""Sidney K. D'Mello""]",2024-05-10T03:04:59Z,http://arxiv.org/abs/2405.06218v1,Education & Social Science,Intelligent Tutoring Systems,this work investigates how tutoring discourse interacts with students' proximal knowledge to explain and predict students' learning outcomes . 9th-grade students attended small group tutorials and individually practiced problems on an intelligent Tutoring System (ITS) we analyzed whether tutors' talk moves and students' performance predicted scores on math learning assessments .
Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI,"Small-group tutoring in Computer Science (CS) is effective, but presents the challenge of providing a dedicated tutor for each group and encouraging collaboration among group members at scale. We present Pensieve Discuss, a software platform that integrates synchronous editing for scaffolded programming problems with online human and AI tutors, designed to improve student collaboration and experience during group tutoring sessions. Our semester-long deployment to 800 students in a CS1 course demonstrated consistently high collaboration rates, positive feedback about the AI tutor's helpfulness and correctness, increased satisfaction with the group tutoring experience, and a substantial increase in question volume. The use of our system was preferred over an interface lacking AI tutors and synchronous editing capabilities. Our experiences suggest that small-group tutoring sessions are an important avenue for future research in educational AI.","['Yoonseok Yang', 'Jack Liu', 'J. D. Zamfirescu-Pereira', 'John DeNero']",2024-07-24T05:07:53Z,http://arxiv.org/abs/2407.17007v1,Education & Social Science,Intelligent Tutoring Systems,pensieve Discuss integrates synchronous editing for scaffolded programming problems with online human and AI tutors . our semester-long deployment to 800 students in a CS1 course demonstrated high collaboration rates .
Integrating AI Tutors in a Programming Course,"RAGMan is an LLM-powered tutoring system that can support a variety of course-specific and homework-specific AI tutors. RAGMan leverages Retrieval Augmented Generation (RAG), as well as strict instructions, to ensure the alignment of the AI tutors' responses. By using RAGMan's AI tutors, students receive assistance with their specific homework assignments without directly obtaining solutions, while also having the ability to ask general programming-related questions.   RAGMan was deployed as an optional resource in an introductory programming course with an enrollment of 455 students. It was configured as a set of five homework-specific AI tutors. This paper describes the interactions the students had with the AI tutors, the students' feedback, and a comparative grade analysis. Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions. When students posed questions within the intended scope, the AI tutors delivered accurate responses 98% of the time. Within the students used AI tutors, 78% reported that the tutors helped their learning. Beyond AI tutors' ability to provide valuable suggestions, students reported appreciating them for fostering a safe learning environment free from judgment.","['Iris Ma', 'Alberto Krone Martins', 'Cristina Videira Lopes']",2024-07-14T00:42:39Z,http://arxiv.org/abs/2407.15718v1,Education & Social Science,Intelligent Tutoring Systems,RAGMan is an LLM-powered tutoring system . it can support a variety of course-specific and homework-specific AI tutors . 78% of students reported that the tutors helped their learning .
Review of intelligent tutoring systems using bayesian approach,"With advancement in computer science research on artificial intelligence and in cognitive psychology research on human learning and performance, the next generation of computer-based tutoring systems moved beyond the simple presentation of pages of text or graphics. These new intelligent tutoring systems (ITSs) called cognitive tutors; incorporated model-tracing technology which is a cognitive model of student problem solving that captures students multiple strategies and common misconceptions. Such Intelligent tutoring systems or Knowledge Based Tutoring Systems can guide learners to progress in the learning process at their best. This paper deals with the review of various Intelligent tutoring systems using Bayesian Networks and how Bayesian Networks can be used for efficient decision making.","['R. Santhi', 'B. Priya', 'J. M. Nandhini']",2013-02-28T04:46:59Z,http://arxiv.org/abs/1302.7081v1,Education & Social Science,Intelligent Tutoring Systems,next generation of computer-based tutoring systems called cognitive tutors . incorporated model-tracing technology that captures students multiple strategies . intelligent tutors can guide learners to progress in the learning process at their best .
LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing   Online Learners,"Intelligent tutoring systems (ITS) using artificial intelligence (AI) technology have shown promise in supporting learners with diverse abilities; however, they often fail to meet the specific communication needs and cultural nuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language models (LLMs) provide new opportunities to incorporate personas to AI-based tutors and support dynamic interactive dialogue, this paper explores how DHH learners perceive LLM-powered ITS with different personas and identified design suggestions for improving the interaction. We developed an interface that allows DHH learners to interact with ChatGPT and three LLM-powered AI tutors with different experiences in DHH education while the learners watch an educational video. A user study with 16 DHH participants showed that they perceived conversations with the AI tutors who had DHH education experiences to be more human-like and trustworthy due to the tutors' cultural knowledge of DHH communities. Participants also suggested providing more transparency regarding the tutors' background information to clarify each AI tutor's position within the DHH community. We discuss design implications for more inclusive LLM-based systems, such as supports for the multimodality of sign language.","['Haocong Cheng', 'Si Chen', 'Christopher Perdriau', 'Yun Huang']",2024-11-15T01:48:08Z,http://arxiv.org/abs/2411.09873v1,Education & Social Science,Intelligent Tutoring Systems,a user study with 16 d/deaf and hard-of-hearing (DHH) participants showed that they perceived conversations with the AI tutors to be more human-like . participants suggested providing more transparency regarding the tutors' background information to clarify each AI tutor's position within the DHH community .
TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students,"Recent improvements in large language model (LLM) performance on academic benchmarks, such as MATH and GSM8K, have emboldened their use as standalone tutors and as simulations of human learning. However, these new applications require more than evaluations of final solution generation. We introduce TutorGym to evaluate these applications more directly. TutorGym is a standard interface for testing artificial intelligence (AI) agents within existing intelligent tutoring systems (ITS) that have been tested and refined in classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and OATutors. TutorGym is more than a simple problem-solution benchmark, it situates AI agents within the interactive interfaces of existing ITSs. At each step of problem-solving, AI agents are asked what they would do as a tutor or as a learner. As tutors, AI agents are prompted to provide tutoring support -- such as generating examples, hints, and step-level correctness feedback -- which can be evaluated directly against the adaptive step-by-step support provided by existing ITSs. As students, agents directly learn from ITS instruction, and their mistakes and learning trajectories can be compared to student data. TutorGym establishes a common framework for training and evaluating diverse AI agents, including LLMs, computational models of learning, and reinforcement learning agents, within a growing suite of learning environments. Currently, TutorGym includes 223 different tutor domains. In an initial evaluation, we find that current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions, and next-step actions were correct only ~52-70% of the time -- but they could produce remarkably human-like learning curves when trained as students with in-context learning.","['Daniel Weitekamp', 'Momin N. Siddiqui', 'Christopher J. MacLellan']",2025-05-02T20:03:21Z,http://arxiv.org/abs/2505.01563v1,Education & Social Science,Intelligent Tutoring Systems,"TutorGym is a standard interface for testing AI agents within existing ITSs . as tutors, AI agents are prompted to provide tutoring support . current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions . but they could produce remarkably human-like learning curves when trained as students ."
Examining the Influence of Varied Levels of Domain Knowledge Base   Inclusion in GPT-based Intelligent Tutors,"Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Results suggest that, although these intelligent tutors still demonstrate a lower accuracy compared to domain experts, the accuracy of the intelligent tutors increases when access to a KB is granted. We also observe that the intelligent tutors with KB access exhibit better pedagogical abilities to speak like a teacher and understand students than those of domain experts, while their ability to help students remains lagging behind domain experts.","['Blake Castleman', 'Mehmet Kerem Turkcan']",2023-09-16T17:12:05Z,http://arxiv.org/abs/2309.12367v2,Education & Social Science,Intelligent Tutoring Systems,a knowledge base (KB) is a scaleable KB that integrates lesson curricula . the KB is then automatically processed by the intelligent tutoring system . results suggest that the tutors with KB access exhibit better pedagogical abilities .
BIPED: Pedagogically Informed Tutoring System for ESL Education,"Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.","['Soonwoo Kwon', 'Sojung Kim', 'Minju Park', 'Seunghyun Lee', 'Kyuseok Kim']",2024-06-05T17:49:24Z,http://arxiv.org/abs/2406.03486v1,Education & Social Science,Intelligent Tutoring Systems,"existing LLMs are designed to teach only simple concepts or lack depth . we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human tutoring interactions . using post-hoc analysis, we come up with 34 tutor acts and 9 student acts ."
Intelligent Tutoring Systems for Generation Z's Addiction,"As generation Z's big data is flooding the Internet through social nets, neural network based data processing is turning an important cornerstone, showing significant potential for fast extraction of data patterns. Online course delivery and associated tutoring are transforming into customizable, on-demand services driven by the learner. Besides automated grading, strong potential exists for the development and deployment of next generation intelligent tutoring software agents. Self-adaptive, online tutoring agents exhibiting ""intelligent-like"" behavior, being capable ""to learn"" from the learner, will become the next educational superstars. Over the past decade, computer-based tutoring agents were deployed in a variety of extended reality environments, from patient rehabilitation to psychological trauma healing. Most of these agents are driven by a set of conditional control statements and a large answers/questions pairs dataset. This article provides a brief introduction on Generation Z's addiction to digital information, highlights important efforts for the development of intelligent dialogue systems, and explains the main components and important design decisions for Intelligent Tutoring System.","['Ioana R. Goldbach', 'Felix G. Hamza-Lup']",2020-04-15T19:10:12Z,http://arxiv.org/abs/2005.05024v1,Education & Social Science,Intelligent Tutoring Systems,generation Z's big data is flooding the Internet through social nets . neural network based data processing is turning an important cornerstone . strong potential exists for the development and deployment of next generation intelligent tutoring agents .
Intelligent Tutors for Adult Learners: An Analysis of Needs and   Challenges,"This work examines the sociotechnical factors that influence the adoption and usage of intelligent tutoring systems in self-directed learning contexts, focusing specifically on adult learners. The study is divided into two parts. First, we present Apprentice Tutors, a novel intelligent tutoring system designed to address the unique needs of adult learners. The platform includes adaptive problem selection, real-time feedback, and visual dashboards to support learning in college algebra topics. Second, we investigate the specific needs and experiences of adult users through a deployment study and a series of focus groups. Using thematic analysis, we identify key challenges and opportunities to improve tutor design and adoption. Based on these findings, we offer actionable design recommendations to help developers create intelligent tutoring systems that better align with the motivations and learning preferences of adult learners. This work contributes to a wider understanding of how to improve educational technologies to support lifelong learning and professional development.","['Adit Gupta', 'Momin Siddiqui', 'Glen Smith', 'Jenn Reddig', 'Christopher MacLellan']",2024-11-19T19:05:04Z,http://arxiv.org/abs/2412.04477v3,Education & Social Science,Intelligent Tutoring Systems,study examines sociotechnical factors that influence the adoption and usage of intelligent tutoring systems in self-directed learning contexts . Apprentice Tutors is a novel intelligent tutor system designed to address the unique needs of adult learners .
Towards Educator-Driven Tutor Authoring: Generative AI Approaches for   Creating Intelligent Tutor Interfaces,"Intelligent Tutoring Systems (ITSs) have shown great potential in delivering personalized and adaptive education, but their widespread adoption has been hindered by the need for specialized programming and design skills. Existing approaches overcome the programming limitations with no-code authoring through drag and drop, however they assume that educators possess the necessary skills to design effective and engaging tutor interfaces. To address this assumption we introduce generative AI capabilities to assist educators in creating tutor interfaces that meet their needs while adhering to design principles. Our approach leverages Large Language Models (LLMs) and prompt engineering to generate tutor layout and contents based on high-level requirements provided by educators as inputs. However, to allow them to actively participate in the design process, rather than relying entirely on AI-generated solutions, we allow generation both at the entire interface level and at the individual component level. The former provides educators with a complete interface that can be refined using direct manipulation, while the latter offers the ability to create specific elements to be added to the tutor interface. A small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design. Moving forward, we raise critical questions for assisting educators with generative AI capabilities to create personalized, effective, and engaging tutors, ultimately enhancing their adoption.","['Tommaso Calo', 'Christopher J. MacLellan']",2024-05-23T15:46:10Z,http://arxiv.org/abs/2405.14713v1,Education & Social Science,Intelligent Tutoring Systems,a small-scale comparison shows the potential of our approach to enhance the efficiency of tutor interface design . our approach leverages large language models (LLMs) and prompt engineering .
A Knowledge Discovery Framework for Learning Task Models from User   Interactions in Intelligent Tutoring Systems,"Domain experts should provide relevant domain knowledge to an Intelligent Tutoring System (ITS) so that it can guide a learner during problemsolving learning activities. However, for many ill-defined domains, the domain knowledge is hard to define explicitly. In previous works, we showed how sequential pattern mining can be used to extract a partial problem space from logged user interactions, and how it can support tutoring services during problem-solving exercises. This article describes an extension of this approach to extract a problem space that is richer and more adapted for supporting tutoring services. We combined sequential pattern mining with (1) dimensional pattern mining (2) time intervals, (3) the automatic clustering of valued actions and (4) closed sequences mining. Some tutoring services have been implemented and an experiment has been conducted in a tutoring system.","['P. Fournier-Viger', 'R. Nkambou', 'E. Mephu Nguifo']",2009-01-29T19:58:09Z,http://arxiv.org/abs/0901.4761v1,Education & Social Science,Intelligent Tutoring Systems,"domain experts should provide relevant domain knowledge to an ITS . for many ill-defined domains, the domain knowledge is hard to define explicitly . this article describes an extension of this approach to extract a problem space ."
"Interactive, Intelligent Tutoring for Auxiliary Constructions in   Geometry Proofs","Geometry theorem proving forms a major and challenging component in the K-12 mathematics curriculum. A particular difficult task is to add auxiliary constructions (i.e, additional lines or points) to aid proof discovery. Although there exist many intelligent tutoring systems proposed for geometry proofs, few teach students how to find auxiliary constructions. And the few exceptions are all limited by their underlying reasoning processes for supporting auxiliary constructions. This paper tackles these weaknesses of prior systems by introducing an interactive geometry tutor, the Advanced Geometry Proof Tutor (AGPT). It leverages a recent automated geometry prover to provide combined benefits that any geometry theorem prover or intelligent tutoring system alone cannot accomplish. In particular, AGPT not only can automatically process images of geometry problems directly, but also can interactively train and guide students toward discovering auxiliary constructions on their own. We have evaluated AGPT via a pilot study with 78 high school students. The study results show that, on training students how to find auxiliary constructions, there is no significant perceived difference between AGPT and human tutors, and AGPT is significantly more effective than the state-of-the-art geometry solver that produces human-readable proofs.","['Ke Wang', 'Zhendong Su']",2017-11-20T05:39:58Z,http://arxiv.org/abs/1711.07154v1,Education & Social Science,Intelligent Tutoring Systems,the Advanced Geometry Proof Tutor (AGPT) is an interactive geometry tutor . it can automatically process images of geometry problems directly . the tutor can also interactively train and guide students toward discovering auxiliary constructions .
Advancing Education through Tutoring Systems: A Systematic Literature   Review,"This study systematically reviews the transformative role of Tutoring Systems, encompassing Intelligent Tutoring Systems (ITS) and Robot Tutoring Systems (RTS), in addressing global educational challenges through advanced technologies. As many students struggle with proficiency in core academic areas, Tutoring Systems emerge as promising solutions to bridge learning gaps by delivering personalized and adaptive instruction. ITS leverages artificial intelligence (AI) models, such as Bayesian Knowledge Tracing and Large Language Models, to provide precise cognitive support, while RTS enhances social and emotional engagement through human-like interactions. This systematic review, adhering to the PRISMA framework, analyzed 86 representative studies. We evaluated the pedagogical and technological advancements, engagement strategies, and ethical considerations surrounding these systems. Based on these parameters, Latent Class Analysis was conducted and identified three distinct categories: computer-based ITS, robot-based RTS, and multimodal systems integrating various interaction modes. The findings reveal significant advancements in AI techniques that enhance adaptability, engagement, and learning outcomes. However, challenges such as ethical concerns, scalability issues, and gaps in cognitive adaptability persist. The study highlights the complementary strengths of ITS and RTS, proposing integrated hybrid solutions to maximize educational benefits. Future research should focus on bridging gaps in scalability, addressing ethical considerations comprehensively, and advancing AI models to support diverse educational needs.","['Vincent Liu', 'Ehsan Latif', 'Xiaoming Zhai']",2025-03-12T18:47:07Z,http://arxiv.org/abs/2503.09748v1,Education & Social Science,Intelligent Tutoring Systems,Tutoring Systems emerge as promising solutions to bridge learning gaps . ITS leverages artificial intelligence (AI) models to provide precise cognitive support . RTS enhances social and emotional engagement through human-like interactions .
Adapting to Educate: Conversational AI's Role in Mathematics Education   Across Different Educational Contexts,"As educational settings increasingly integrate artificial intelligence (AI), understanding how AI tools identify -- and adapt their responses to -- varied educational contexts becomes paramount. This study examines conversational AI's effectiveness in supporting K-12 mathematics education across various educational contexts. Through qualitative content analysis, we identify educational contexts and key instructional needs present in educator prompts and assess AI's responsiveness. Our findings indicate that educators focus their AI conversations on assessment methods, how to set the cognitive demand level of their instruction, and strategies for making meaningful real-world connections. However, educators' conversations with AI about instructional practices do vary across revealed educational contexts; they shift their emphasis to tailored, rigorous content that addresses their students' unique needs. Educators often seek actionable guidance from AI and reject responses that do not align with their inquiries. While AI can provide accurate, relevant, and useful information when educational contexts or instructional practices are specified in conversation queries, its ability to consistently adapt responses along these evaluation dimensions varies across different educational settings. Significant work remains to realize the response-differentiating potential of conversational AI tools in complex educational use cases. This research contributes insights into developing AI tools that are responsive, proactive, and anticipatory, adapting to evolving educational needs before they are explicitly stated, and provides actionable recommendations for both developers and educators to enhance AI integration in educational practices.","['Alex Liu', 'Lief Esbenshade', 'Min Sun', 'Shawon Sarkar', 'Jian He', 'Victor Tian', 'Zachary Zhang']",2025-03-04T20:47:15Z,http://arxiv.org/abs/2503.02999v1,Education & Social Science,AI in Education,this study examines conversational AI's effectiveness in supporting K-12 mathematics education . educators' conversations with AI about instructional practices do vary across revealed contexts . Educators often seek actionable guidance from AI and reject responses that do not align .
"Navigating the Future of Education: Educators' Insights on AI   Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia","Understanding teachers' perspectives on AI in Education (AIEd) is crucial for its effective integration into the educational framework. This paper aims to explore how teachers currently use AI and how it can enhance the educational process. We conducted a cross-national study spanning Greece, Hungary, Latvia, Ireland, and Armenia, surveying 1754 educators through an online questionnaire, addressing three research questions. Our first research question examines educators' understanding of AIEd, their skepticism, and its integration within schools. Most educators report a solid understanding of AI and acknowledge its potential risks. AIEd is primarily used for educator support and engaging students. However, concerns exist about AI's impact on fostering critical thinking and exposing students to biased data. The second research question investigates student engagement with AI tools from educators' perspectives. Teachers indicate that students use AI mainly to manage their academic workload, while outside school, AI tools are primarily used for entertainment. The third research question addresses future implications of AI in education. Educators are optimistic about AI's potential to enhance educational processes, particularly through personalized learning experiences. Nonetheless, they express significant concerns about AI's impact on cultivating critical thinking and ethical issues related to potential misuse. There is a strong emphasis on the need for professional development through training seminars, workshops, and online courses to integrate AI effectively into teaching practices. Overall, the findings highlight a cautious optimism among educators regarding AI in education, alongside a clear demand for targeted professional development to address concerns and enhance skills in using AI tools.","['Evangelia Daskalaki', 'Katerina Psaroudaki', 'Paraskevi Fragopoulou']",2024-08-28T10:22:05Z,http://arxiv.org/abs/2408.15686v1,Education & Social Science,AI in Education,"this paper explores how teachers currently use AI and how it can enhance the educational process . a cross-national study spanning Greece, Hungary, Latvia, Ireland, and Armenia addressed three research questions . most educators report a solid understanding of AI and acknowledge its potential risks ."
A Transparency Index Framework for AI in Education,"Numerous AI ethics checklists and frameworks have been proposed focusing on different dimensions of ethical AI such as fairness, explainability, and safety. Yet, no such work has been done on developing transparent AI systems for real-world educational scenarios. This paper presents a Transparency Index framework that has been iteratively co-designed with different stakeholders of AI in education, including educators, ed-tech experts, and AI practitioners. We map the requirements of transparency for different categories of stakeholders of AI in education and demonstrate that transparency considerations are embedded in the entire AI development process from the data collection stage until the AI system is deployed in the real world and iteratively improved. We also demonstrate how transparency enables the implementation of other ethical AI dimensions in Education like interpretability, accountability, and safety. In conclusion, we discuss the directions for future research in this newly emerging field. The main contribution of this study is that it highlights the importance of transparency in developing AI-powered educational technologies and proposes an index framework for its conceptualization for AI in education.","['Muhammad Ali Chaudhry', 'Mutlu Cukurova', 'Rose Luckin']",2022-05-09T10:10:47Z,http://arxiv.org/abs/2206.03220v1,Education & Social Science,AI in Education,this paper presents a Transparency Index framework . it has been iteratively co-designed with different stakeholders of AI in education . transparency considerations are embedded in the entire AI development process .
Need of AI in Modern Education: in the Eyes of Explainable AI (xAI),"Modern Education is not \textit{Modern} without AI. However, AI's complex nature makes understanding and fixing problems challenging. Research worldwide shows that a parent's income greatly influences a child's education. This led us to explore how AI, especially complex models, makes important decisions using Explainable AI tools. Our research uncovered many complexities linked to parental income and offered reasonable explanations for these decisions. However, we also found biases in AI that go against what we want from AI in education: clear transparency and equal access for everyone. These biases can impact families and children's schooling, highlighting the need for better AI solutions that offer fair opportunities to all. This chapter tries to shed light on the complex ways AI operates, especially concerning biases. These are the foundational steps towards better educational policies, which include using AI in ways that are more reliable, accountable, and beneficial for everyone involved.","['Supriya Manna', 'Niladri Sett']",2024-07-31T08:11:33Z,http://arxiv.org/abs/2408.00025v3,Education & Social Science,AI in Education,"this chapter tries to shed light on the complex ways AI operates, especially biases . a parent's income influences a child's education, so we explored how AI makes important decisions . however, we also found biased AI that go against what we want from AI in education ."
Towards responsible AI for education: Hybrid human-AI to confront the   Elephant in the room,"Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education.","['Danial Hooshyar', 'Gustav Šír', 'Yeongwook Yang', 'Eve Kikas', 'Raija Hämäläinen', 'Tommi Kärkkäinen', 'Dragan Gašević', 'Roger Azevedo']",2025-04-22T13:20:43Z,http://arxiv.org/abs/2504.16148v1,Education & Social Science,AI in Education,this critical analysis examines nine persistent challenges in AI in education . they include: lack of clarity around what AI for education truly means . widespread neglect of essential learning processes in AI-driven learner modelling .
AfricAIED 2024: 2nd Workshop on Artificial Intelligence in Education in   Africa,"Recent AI advancements offer transformative potential for global education, yet their application often overlooks Africa's unique educational landscape. AfricAIED 2024 will address this gap, spotlighting efforts to develop AI in Education (AIED) systems tailored to Africa's needs. Building on the success of the inaugural workshop, AfricAIED 2024 will feature an online AI Hackathon focused on democratizing preparation for Ghana's National Science & Maths Quiz (NSMQ). Participants will create open-source AI tools leveraging resources from the Brilla AI project to level the academic playing field and enhance science and math education across Africa. The workshop will showcase top competitors' solutions, invite discussions on AIED opportunities and challenges in Africa, and highlight the latest advancements in AI education integration. AfricAIED 2024 aims to foster collaboration and innovation, amplifying African voices in the AIED community and driving positive change in African education through AI.","['George Boateng', 'Victor Kumbol']",2024-04-30T18:36:45Z,http://arxiv.org/abs/2405.00139v1,Education & Social Science,AI in Education,AfricAIED 2024 will feature an online AI Hackathon focused on democratizing preparation for Ghana's national science & maths quiz . participants will create open-source AI tools leveraging resources from the Brilla AI project .
White Paper: The Generative Education (GenEd) Framework,"The Generative Education (GenEd) Framework explores the transition from Large Language Models (LLMs) to Large Multimodal Models (LMMs) in education, envisioning a harmonious relationship between AI and educators to enhance learning experiences. This paper delves into the potential of LMMs to create personalized, interactive, and emotionally-aware learning environments. Through addressing the Two-Sigma problem and the introduction of a conceptual product named Harmony, the narrative emphasizes educator development, adapting policy frameworks, and fostering cross-sector collaboration to realize the envisioned AI-enhanced education landscape. The discussion underscores the urgency for proactive adaptation amidst AI's evolution, offering a pragmatic roadmap to navigate the technical, ethical, and policy intricacies of integrating AI in education.",['Daniel Leiker'],2023-10-16T23:30:42Z,http://arxiv.org/abs/2311.10732v2,Education & Social Science,AI in Education,"the Generative Education (GenEd) Framework explores the transition from Large Language Models to Large Multimodal Models (LMMs) in education . the narrative emphasizes educator development, adapting policy frameworks, and fostering cross-sector collaboration ."
Human-Centric eXplainable AI in Education,"As artificial intelligence (AI) becomes more integrated into educational environments, how can we ensure that these systems are both understandable and trustworthy? The growing demand for explainability in AI systems is a critical area of focus. This paper explores Human-Centric eXplainable AI (HCXAI) in the educational landscape, emphasizing its role in enhancing learning outcomes, fostering trust among users, and ensuring transparency in AI-driven tools, particularly through the innovative use of large language models (LLMs). What challenges arise in the implementation of explainable AI in educational contexts? This paper analyzes these challenges, addressing the complexities of AI models and the diverse needs of users. It outlines comprehensive frameworks for developing HCXAI systems that prioritize user understanding and engagement, ensuring that educators and students can effectively interact with these technologies. Furthermore, what steps can educators, developers, and policymakers take to create more effective, inclusive, and ethically responsible AI solutions in education? The paper provides targeted recommendations to address this question, highlighting the necessity of prioritizing explainability. By doing so, how can we leverage AI's transformative potential to foster equitable and engaging educational experiences that support diverse learners?","['Subhankar Maity', 'Aniket Deroy']",2024-10-18T14:02:47Z,http://arxiv.org/abs/2410.19822v1,Education & Social Science,AI in Education,"this paper explores human-centric eXplainable AI (HCXAI) in the educational landscape . it emphasizes its role in enhancing learning outcomes, fostering trust among users . the paper provides targeted recommendations to address this question ."
A Manifesto for a Pro-Actively Responsible AI in Education,"This paper examines the historical foundations, current practices, and emerging challenges for Artificial Intelligence in Education (AIED) within broader AI practices. It highlights AIED's unique and rich potential for contributing to the current AI policy and practices, especially in the context of responsible AI. It also discusses the key gaps in the AIED field, which need to be addressed by the community to elevate the field from a cottage industry to the level where it will deservedly be seen as key to advancin AI research and practical applications. The paper offers a five-point manifesto aimed to revitalise AIED' contributions to education and broader AI community, suggesting enhanced interdisciplinary collaboration, a broadened understanding of AI's impact on human functioning, and commitment to setting agendas for human-centred educational innovations.This approach positions AIED to significantly influence educational technologies to achieve genuine positive impact across diverse societal segments.",['Kaska Porayska-Pomsta'],2024-05-03T14:23:41Z,http://arxiv.org/abs/2407.05423v1,Education & Social Science,AI in Education,"paper highlights AIED's unique and rich potential for contributing to the current AI policy and practices . it also discusses the key gaps in the AIED field, which need to be addressed by the community . paper suggests enhanced interdisciplinary collaboration, broadened understanding of AI's impact on human functioning ."
AI in Design Education at College Level-Educators' Perspectives and   Challenges,"Artificial intelligence has deeply permeated numerous fields, especially the design area which relies on technology as a tool for innovation. This change naturally extends to the field of design education, which is closest to design practice. This has led to further exploration of the impact of AI on college-level education in the design discipline. This study aims to examine how current design educators perceive the role of AI in college-level design education, their perspectives on integrating AI into teaching and research, and their concerns regarding its potential challenges in design education and research. Through qualitative, semi-structured, in-depth interviews with seven faculties in U.S. design colleges, the findings reveal that AI, as a tool and source of information, has become an integral part of design education. AI- derived functionalities are increasingly utilized in design software, and educators are actively incorporating AI as a theoretical framework in their teaching. Educators can guide students in using AI tools, but only if they first acquire a strong foundation in basic design principles and skills. This study also indicates the importance of promoting a cooperative relationship between design educators and AI. At the same time, educators express anticipation for advancements in ethical standards, authenticity, and the resolution of copyright issues related to AI.","['Lizhu Zhang', 'Cecilia X. Wang']",2025-07-23T13:02:37Z,http://arxiv.org/abs/2507.17481v1,Education & Social Science,AI in Education,"this study examines how current design educators perceive the role of AI in college-level design education . it aims to examine their perspectives on integrating AI into teaching and research . findings reveal that AI has become an integral part of design education, the study finds ."
"Enhancing AI-Driven Education: Integrating Cognitive Frameworks,   Linguistic Feedback Analysis, and Ethical Considerations for Improved Content   Generation","Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.","['Antoun Yaacoub', 'Sansiri Tarnpradab', 'Phattara Khumprom', 'Zainab Assaghir', 'Lionel Prevost', 'Jérôme Da-Rugna']",2025-05-01T06:36:21Z,http://arxiv.org/abs/2505.00339v1,Education & Social Science,AI in Education,"this paper proposes a framework for enhancing AI-driven educational tools . it integrates cognitive assessment frameworks, linguistic analysis of AI-generated feedback . the practical application of this framework is demonstrated through its integration into OneClickQuiz ."
"Generative AI in Education: A Study of Educators' Awareness, Sentiments,   and Influencing Factors","The rapid advancement of artificial intelligence (AI) and the expanding integration of large language models (LLMs) have ignited a debate about their application in education. This study delves into university instructors' experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators' perspectives on AI's role in the classroom and its potential impacts on teaching and learning. The objective of this research is to investigate the level of awareness, overall sentiment towardsadoption, and the factors influencing these attitudes for LLMs and generative AI-based tools in higher education. Data was collected through a survey using a Likert scale, which was complemented by follow-up interviews to gain a more nuanced understanding of the instructors' viewpoints. The collected data was processed using statistical and thematic analysis techniques. Our findings reveal that educators are increasingly aware of and generally positive towards these tools. We find no correlation between teaching style and attitude toward generative AI. Finally, while CS educators show far more confidence in their technical understanding of generative AI tools and more positivity towards them than educators in other fields, they show no more confidence in their ability to detect AI-generated work.","['Aashish Ghimire', 'James Prather', 'John Edwards']",2024-03-22T19:21:29Z,http://arxiv.org/abs/2403.15586v1,Education & Social Science,AI in Education,"this study delves into university instructors' experiences and attitudes toward AI language models . data was collected through a survey using a likert scale, complemented by follow-up interviews . findings reveal that educators are increasingly aware of and generally positive towards these tools ."
Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017   New and Future AI Educator Program,"The 7th Symposium on Educational Advances in Artificial Intelligence (EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following ""blue sky"" questions:   * How could/should Artificial Intelligence (AI) courses incorporate ethics into the curriculum?   * How could we teach AI topics at an early undergraduate or a secondary school level?   * AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields?   This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.","['Eric Eaton', 'Sven Koenig', 'Claudia Schulz', 'Francesco Maurelli', 'John Lee', 'Joshua Eckroth', 'Mark Crowley', 'Richard G. Freedman', 'Rogelio E. Cardona-Rivera', 'Tiago Machado', 'Tom Williams']",2017-02-01T05:16:55Z,http://arxiv.org/abs/1702.00137v1,Education & Social Science,AI in Education,"the 7th Symposium on Educational Advances in Artificial Intelligence (EAAI'17) launched the EAAI New and Future AI Educator Program . awardees were asked to address one of the following ""blue sky"" questions: * How could/should AI courses incorporate ethics into the curriculum? * AI has the potential for broad impact to numerous disciplines ."
Citizenship Challenges in Artificial Intelligence Education,"This chapter addresses the citizenship challenges related to AI in education, particularly concerning students, teachers, and other educational stakeholders in the context of AI integration. We first explore how to foster AI awareness and education, along with various strategies to promote a socio-critical approach to AI training, aiming to identify relevant and ethical uses to prioritise. In the second part, we discuss critical thinking and computational thinking skills that can be mobilised within certain AI-supported educational activities, depending on the degree of creative and transformative engagement those activities require.",['Margarida Romero'],2025-06-23T13:34:09Z,http://arxiv.org/abs/2506.18955v1,Education & Social Science,AI in Education,"this chapter addresses the citizenship challenges related to AI in education . we explore how to foster AI awareness and education, and strategies to promote a socio-critical approach ."
Readying Medical Students for Medical AI: The Need to Embed AI Ethics   Education,"Medical students will almost inevitably encounter powerful medical AI systems early in their careers. Yet, contemporary medical education does not adequately equip students with the basic clinical proficiency in medical AI needed to use these tools safely and effectively. Education reform is urgently needed, but not easily implemented, largely due to an already jam-packed medical curricula. In this article, we propose an education reform framework as an effective and efficient solution, which we call the Embedded AI Ethics Education Framework. Unlike other calls for education reform to accommodate AI teaching that are more radical in scope, our framework is modest and incremental. It leverages existing bioethics or medical ethics curricula to develop and deliver content on the ethical issues associated with medical AI, especially the harms of technology misuse, disuse, and abuse that affect the risk-benefit analyses at the heart of healthcare. In doing so, the framework provides a simple tool for going beyond the ""What?"" and the ""Why?"" of medical AI ethics education, to answer the ""How?"", giving universities, course directors, and/or professors a broad road-map for equipping their students with the necessary clinical proficiency in medical AI.","['Thomas P Quinn', 'Simon Coghlan']",2021-09-07T04:57:29Z,http://arxiv.org/abs/2109.02866v1,Education & Social Science,AI in Education,medical education does not adequately equip students with clinical proficiency in medical AI . the Embedded AI Ethics Education Framework is modest and incremental . it leverages existing bioethics or medical ethics curricula to develop and deliver content .
From Algorithm Worship to the Art of Human Learning: Insights from   50-year journey of AI in Education,"Current discourse surrounding Artificial Intelligence (AI) oscillates between hope and apprehension, painting a future where AI reshapes every facet of human life, including Education. This paper delves into the complexities of AI's role in Education, addressing the mixed messages that have both enthused and alarmed educators, policymakers, and the public. It explores the promises that AI holds for enhancing learning through personalisation at scale, against the backdrop of concerns about ethical implications, the devaluation of non-STEM subjects, and the potential transformative impact on our neurocognitive and socio-emotional functioning. Drawing on recent research and global discourse, the paper seeks to unpack the reasons behind the vagueness of current discussions on AI in Education (AIED) and the implications of this ambiguity for future educational practices and policies. By highlighting insights from educational research and synthesising evidence-based best practices in AIED, the aim is to provide a clearer understanding of how AI technologies can be aligned with the fundamental principles of learning and teaching, and explore what concrete actions may need to be prioritised now to truly enhance learning experiences and outcomes for all in the future.",['Kaska Porayska-Pomsta'],2024-02-05T16:12:14Z,http://arxiv.org/abs/2403.05544v1,Education & Social Science,AI in Education,this paper delves into the complexities of AI's role in Education . it explores the promises that AI holds for enhancing learning through personalisation . the paper seeks to unpack the reasons behind the vagueness of current discussions .
AI for Accessible Education: Personalized Audio-Based Learning for Blind   Students,"Blind and visually impaired (BVI) students face significant challenges in traditional educational settings. While screen readers and braille materials offer some accessibility, they often lack interactivity and real-time adaptability to individual learning needs. This paper presents Audemy, an AI-powered audio-based learning platform designed to provide personalized, accessible, and engaging educational experiences for BVI students. Audemy uses adaptive learning techniques to customize content based on student accuracy, pacing preferences, and engagement patterns. The platform has been iteratively developed with input from over 20 educators specializing in accessibility and currently serves over 2,000 BVI students. Educator insights show key considerations for accessible AI, including the importance of engagement, intuitive design, compatibility with existing assistive technologies, and the role of positive reinforcement in maintaining student motivation. Beyond accessibility, this paper explores the ethical implications of AI in education, emphasizing data privacy, security, and transparency. Audemy demonstrates how AI can empower BVI students with personalized and equitable learning opportunities, advancing the broader goal of inclusive education.","['Crystal Yang', 'Paul Taele']",2025-04-23T21:57:53Z,http://arxiv.org/abs/2504.17117v1,Education & Social Science,AI in Education,"screen readers and braille materials offer some accessibility, but lack real-time adaptability . audemy uses adaptive learning techniques to customize content based on student accuracy . the platform has been iteratively developed with input from over 20 educators ."
Brave new world: Artificial Intelligence in teaching and learning,"We exemplify how Large Language Models are used in both teaching and learning. We also discuss the AI incidents that have already occurred in the education domain, and we argue for the urgent need to introduce AI policies in universities and for the ongoing strategies to regulate AI. Regarding policy for AI, our view is that each institution should have a policy for AI in teaching and learning. This is important from at least twofolds: (i) to raise awareness on the numerous educational tools that can both positively and negatively affect education; (ii) to minimise the risk of AI incidents in education.","['Adrian Groza', 'Anca Marginean']",2023-09-27T15:22:05Z,http://arxiv.org/abs/2310.06856v1,Education & Social Science,AI in Education,we exemplify how Large Language Models are used in both teaching and learning . we argue for the urgent need to introduce AI policies in universities .
"Leveraging AI to Advance Science and Computing Education across Africa:   Challenges, Progress and Opportunities","Across the African continent, students grapple with various educational challenges, including limited access to essential resources such as computers, internet connectivity, reliable electricity, and a shortage of qualified teachers. Despite these challenges, recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education. Yet, these AI tools tend to be deployed and evaluated predominantly within the context of Western educational settings, with limited attention directed towards the unique needs and challenges faced by students in Africa. In this chapter, we discuss challenges with using AI to advance education across Africa. Then, we describe our work developing and deploying AI in Education tools in Africa for science and computing education: (1) SuaCode, an AI-powered app that enables Africans to learn to code using their smartphones, (2) AutoGrad, an automated grading, and feedback tool for graphical and interactive coding assignments, (3) a tool for code plagiarism detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses, (5) Kwame for Science, a web-based AI teaching assistant that provides instant answers to students' science questions and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz competition. Finally, we discuss potential opportunities to leverage AI to advance education across Africa.",['George Boateng'],2024-02-12T04:10:09Z,http://arxiv.org/abs/2402.07397v2,Education & Social Science,AI in Education,"recent advances in AI such as BERT, and GPT-4 have demonstrated their potential for advancing education . but, these tools tend to be deployed and evaluated predominantly within the context of Western educational settings . this chapter discusses challenges with using AI to advance education across africa ."
"Unpacking the ""Black Box"" of AI in Education","Recent advances in Artificial Intelligence (AI) have sparked renewed interest in its potential to improve education. However, AI is a loose umbrella term that refers to a collection of methods, capabilities, and limitations-many of which are often not explicitly articulated by researchers, education technology companies, or other AI developers. In this paper, we seek to clarify what ""AI"" is and the potential it holds to both advance and hamper educational opportunities that may improve the human condition. We offer a basic introduction to different methods and philosophies underpinning AI, discuss recent advances, explore applications to education, and highlight key limitations and risks. We conclude with a set of questions that educationalists may ask as they encounter AI in their research and practice. Our hope is to make often jargon-laden terms and concepts accessible, so that all are equipped to understand, interrogate, and ultimately shape the development of human centered AI in education.","['Nabeel Gillani', 'Rebecca Eynon', 'Catherine Chiabaut', 'Kelsey Finkel']",2022-12-31T18:27:21Z,http://arxiv.org/abs/2301.01602v1,Education & Social Science,AI in Education,"recent advances in Artificial Intelligence (AI) have sparked renewed interest in its potential to improve education . but, AI is a loose umbrella term that refers to a collection of methods, capabilities, and limitations . this paper seeks to clarify what ""AI"" is and the potential it holds to advance and hamper educational opportunities ."
Explainable Student Performance Prediction With Personalized Attention   for Explaining Why A Student Fails,"As student failure rates continue to increase in higher education, predicting student performance in the following semester has become a significant demand. Personalized student performance prediction helps educators gain a comprehensive view of student status and effectively intervene in advance. However, existing works scarcely consider the explainability of student performance prediction, which educators are most concerned about. In this paper, we propose a novel Explainable Student performance prediction method with Personalized Attention (ESPA) by utilizing relationships in student profiles and prior knowledge of related courses. The designed Bidirectional Long Short-Term Memory (BiLSTM) architecture extracts the semantic information in the paths with specific patterns. As for leveraging similar paths' internal relations, a local and global-level attention mechanism is proposed to distinguish the influence of different students or courses for making predictions. Hence, valid reasoning on paths can be applied to predict the performance of students. The ESPA consistently outperforms the other state-of-the-art models for student performance prediction, and the results are intuitively explainable. This work can help educators better understand the different impacts of behavior on students' studies.","['Kun Niu', 'Xipeng Cao', 'Yicong Yu']",2021-10-15T08:45:43Z,http://arxiv.org/abs/2110.08268v1,Education & Social Science,Student Performance Prediction,the paper proposes a novel explainable student performance prediction method . it uses relationships in student profiles and prior knowledge of related courses . the method consistently outperforms the other state-of-the-art models .
Predicting Student's Performance Through Data Mining,Predicting the performance of students early and as accurately as possible is one of the biggest challenges of educational institutions. Analyzing the performance of students early can help in finding the strengths and weakness of students and help the perform better in examinations. Using machine learning the student's performance can be predicted with the help of students' data collected from Learning Management Systems (LMS). The data collected from LMSs can provide insights about student's behavior that will result in good or bad performance in examinations which then can be studied and used in helping students performing poorly in examinations to perform better.,['Aaditya Bhusal'],2021-11-20T10:47:39Z,http://arxiv.org/abs/2112.01247v1,Education & Social Science,Student Performance Prediction,analyzing the performance of students early can help in finding the strengths and weaknesses of students . the data collected from LMSs can provide insights about student's behavior that will result in good or bad performance .
Peer-inspired Student Performance Prediction in Interactive Online   Question Pools with Graph Neural Network,"Student performance prediction is critical to online education. It can benefit many downstream tasks on online learning platforms, such as estimating dropout rates, facilitating strategic intervention, and enabling adaptive online learning. Interactive online question pools provide students with interesting interactive questions to practice their knowledge in online education. However, little research has been done on student performance prediction in interactive online question pools. Existing work on student performance prediction targets at online learning platforms with predefined course curriculum and accurate knowledge labels like MOOC platforms, but they are not able to fully model knowledge evolution of students in interactive online question pools. In this paper, we propose a novel approach using Graph Neural Networks (GNNs) to achieve better student performance prediction in interactive online question pools. Specifically, we model the relationship between students and questions using student interactions to construct the student-interaction-question network and further present a new GNN model, called R^2GCN, which intrinsically works for the heterogeneous networks, to achieve generalizable student performance prediction in interactive online question pools. We evaluate the effectiveness of our approach on a real-world dataset consisting of 104,113 mouse trajectories generated in the problem-solving process of over 4000 students on 1631 questions. The experiment results show that our approach can achieve a much higher accuracy of student performance prediction than both traditional machine learning approaches and GNN models.","['Haotian Li', 'Huan Wei', 'Yong Wang', 'Yangqiu Song', 'Huamin Qu']",2020-08-04T14:55:32Z,http://arxiv.org/abs/2008.01613v2,Education & Social Science,Student Performance Prediction,"little research has been done on student performance prediction in interactive online question pools . a novel approach using Graph Neural Networks (GNNs) is proposed . the model is called R2GCN, which intrinsically works for heterogeneous networks ."
Academic Performance Estimation with Attention-based Graph Convolutional   Networks,"Student's academic performance prediction empowers educational technologies including academic trajectory and degree planning, course recommender systems, early warning and advising systems. Given a student's past data (such as grades in prior courses), the task of student's performance prediction is to predict a student's grades in future courses. Academic programs are structured in a way that prior courses lay the foundation for future courses. The knowledge required by courses is obtained by taking multiple prior courses, which exhibits complex relationships modeled by graph structures. Traditional methods for student's performance prediction usually neglect the underlying relationships between multiple courses; and how students acquire knowledge across them. In addition, traditional methods do not provide interpretation for predictions needed for decision making. In this work, we propose a novel attention-based graph convolutional networks model for student's performance prediction. We conduct extensive experiments on a real-world dataset obtained from a large public university. The experimental results show that our proposed model outperforms state-of-the-art approaches in terms of grade prediction. The proposed model also shows strong accuracy in identifying students who are at-risk of failing or dropping out so that timely intervention and feedback can be provided to the student.","['Qian Hu', 'Huzefa Rangwala']",2019-12-26T23:11:27Z,http://arxiv.org/abs/2001.00632v1,Education & Social Science,Student Performance Prediction,"student's academic performance prediction empowers educational technologies . prior courses lay the foundation for future courses, authors say . traditional methods do not provide interpretation for predictions needed for decision making ."
A Comparative Analysis of Student Performance Predictions in Online   Courses using Heterogeneous Knowledge Graphs,"As online courses become the norm in the higher-education landscape, investigations into student performance between students who take online vs on-campus versions of classes become necessary. While attention has been given to looking at differences in learning outcomes through comparisons of students' end performance, less attention has been given in comparing students' engagement patterns between different modalities. In this study, we analyze a heterogeneous knowledge graph consisting of students, course videos, formative assessments and their interactions to predict student performance via a Graph Convolutional Network (GCN). Using students' performance on the assessments, we attempt to determine a useful model for identifying at-risk students. We then compare the models generated between 5 on-campus and 2 fully-online MOOC-style instances of the same course. The model developed achieved a 70-90\% accuracy of predicting whether a student would pass a particular problem set based on content consumed, course instance, and modality.","['Thomas Trask', 'Nicholas Lytle', 'Michael Boyle', 'David Joyner', 'Ahmed Mubarak']",2024-05-19T03:33:59Z,http://arxiv.org/abs/2407.12153v1,Education & Social Science,Student Performance Prediction,a Graph Convolutional Network (GCN) predicts student performance in online vs. on-campus classes . the model predicts whether a student would pass a particular problem set based on content consumed .
Behavior Pattern and Compiled Information Based Performance Prediction   in MOOCs,"With the development of MOOCs massive open online courses, increasingly more subjects can be studied online. Researchers currently show growing interest in the field of MOOCs, including dropout prediction, cheating detection and achievement prediction. Previous studies on achievement prediction mainly focused on students' video and forum behaviors, and few researchers have considered how well students perform their assignments. In this paper, we choose a C programming course as the experimental subject, which involved 1528 students. This paper mainly focuses on the students' accomplishment behaviors in programming assignments and compiled information from programming assignments. In this paper, feature sequences are extracted from the logs according to submission times, submission order and plagiarism. The experimental results show that the students who did not pass the exam had obvious sequence patterns but that the students who passed the test did not have an obvious sequence pattern. Then, we extract 23 features from the compiled information of students' programming assignments and select the most distinguishing features to predict the students' performances. The experimental results show that we can obtain an accuracy rate of 0.7049 for predicting students' performances.","['Shaojie Qu', 'Kan Li', 'Zheyi Fan', 'Sisi Wu', 'Xinyi Liu', 'Zhiguo Huang']",2019-08-04T09:40:06Z,http://arxiv.org/abs/1908.01304v1,Education & Social Science,Student Performance Prediction,"researchers show growing interest in the field of MOOCs, including dropout prediction . previous studies on achievement prediction mainly focused on video and forum behaviors . in this paper, we choose a C programming course as the experimental subject ."
Interactions between teaching assistants and students boost engagement   in physics labs,"Through in-class observations of teaching assistants (TAs) and students in the lab sections of a large introductory physics course, we study which TA behaviors can be used to predict student engagement and, in turn, how this engagement relates to learning. For the TAs, we record data to determine how they adhere to and deliver the lesson plan and how they interact with students during the lab. For the students, we use observations to record the level of student engagement and pre- and post-tests of lab skills to measure learning. We find that the frequency of TA-student interactions, especially those initiated by the TAs, is a positive and significant predictor of student engagement. Interestingly, the length of interactions is not significantly correlated with student engagement. In addition, we find that student engagement was a better predictor of post-test performance than pre-test scores. These results shed light on the manner in which students learn how to conduct inquiry and suggest that, by proactively engaging students, TAs may have a positive effect on student engagement, and therefore learning, in the lab.","['Jared B. Stang', 'Ido Roll']",2013-06-27T18:54:48Z,http://arxiv.org/abs/1306.6606v2,Education & Social Science,Student Performance Prediction,"the frequency of TA-student interactions is a positive predictor of student engagement . the length of interactions is not significantly correlated with student engagement, study finds . student engagement was a better predictor than pre-test scores, study concludes ."
Knowledge Tracing for Complex Problem Solving: Granular Rank-Based   Tensor Factorization,"Knowledge Tracing (KT), which aims to model student knowledge level and predict their performance, is one of the most important applications of user modeling. Modern KT approaches model and maintain an up-to-date state of student knowledge over a set of course concepts according to students' historical performance in attempting the problems. However, KT approaches were designed to model knowledge by observing relatively small problem-solving steps in Intelligent Tutoring Systems. While these approaches were applied successfully to model student knowledge by observing student solutions for simple problems, they do not perform well for modeling complex problem solving in students.M ost importantly, current models assume that all problem attempts are equally valuable in quantifying current student knowledge.However, for complex problems that involve many concepts at the same time, this assumption is deficient. In this paper, we argue that not all attempts are equivalently important in discovering students' knowledge state, and some attempts can be summarized together to better represent student performance. We propose a novel student knowledge tracing approach, Granular RAnk based TEnsor factorization (GRATE), that dynamically selects student attempts that can be aggregated while predicting students' performance in problems and discovering the concepts presented in them. Our experiments on three real-world datasets demonstrate the improved performance of GRATE, compared to the state-of-the-art baselines, in the task of student performance prediction. Our further analysis shows that attempt aggregation eliminates the unnecessary fluctuations from students' discovered knowledge states and helps in discovering complex latent concepts in the problems.","['Chunpai Wang', 'Shaghayegh Sahebi', 'Siqian Zhao', 'Peter Brusilovsky', 'Laura O. Moraes']",2022-10-06T06:22:46Z,http://arxiv.org/abs/2210.09013v1,Education & Social Science,Student Performance Prediction,knowledge tracing (KT) aims to model student knowledge level and predict their performance . modern KT approaches model and maintain an up-to-date state of student knowledge . but they do not perform well for modeling complex problem solving in students .
Early Performance Prediction using Interpretable Patterns in Programming   Process Data,"Instructors have limited time and resources to help struggling students, and these resources should be directed to the students who most need them. To address this, researchers have constructed models that can predict students' final course performance early in a semester. However, many predictive models are limited to static and generic student features (e.g. demographics, GPA), rather than computing-specific evidence that assesses a student's progress in class. Many programming environments now capture complete time-stamped records of students' actions during programming. In this work, we leverage this rich, fine-grained log data to build a model to predict student course outcomes. From the log data, we extract patterns of behaviors that are predictive of students' success using an approach called differential sequence mining. We evaluate our approach on a dataset from 106 students in a block-based, introductory programming course. The patterns extracted from our approach can predict final programming performance with 79% accuracy using only the first programming assignment, outperforming two baseline methods. In addition, we show that the patterns are interpretable and correspond to concrete, effective -- and ineffective -- novice programming behaviors. We also discuss these patterns and their implications for classroom instruction.","['Ge Gao', 'Samiha Marwan', 'Thomas W. Price']",2021-02-10T22:46:45Z,http://arxiv.org/abs/2102.05765v1,Education & Social Science,Student Performance Prediction,"many predictive models are limited to static and generic student features . many programming environments now capture complete time-stamped records of students' actions . we leverage this rich, fine-grained log data to build a model to predict course outcomes ."
A Probabilistic Generative Model for Tracking Multi-Knowledge Concept   Mastery Probability,"Knowledge tracing aims to track students' knowledge status over time to predict students' future performance accurately. Markov chain-based knowledge tracking (MCKT) models can track knowledge concept mastery probability over time. However, as the number of tracked knowledge concepts increases, the time complexity of MCKT predicting student performance increases exponentially (also called explaining away problem. In addition, the existing MCKT models only consider the relationship between students' knowledge status and problems when modeling students' responses but ignore the relationship between knowledge concepts in the same problem. To address these challenges, we propose an inTerpretable pRobAbilistiC gEnerative moDel (TRACED), which can track students' numerous knowledge concepts mastery probabilities over time. To solve \emph{explain away problem}, we design Long and Short-Term Memory (LSTM)-based networks to approximate the posterior distribution, predict students' future performance, and propose a heuristic algorithm to train LSTMs and probabilistic graphical model jointly. To better model students' exercise responses, we proposed a logarithmic linear model with three interactive strategies, which models students' exercise responses by considering the relationship among students' knowledge status, knowledge concept, and problems. We conduct experiments with four real-world datasets in three knowledge-driven tasks. The experimental results show that TRACED outperforms existing knowledge tracing methods in predicting students' future performance and can learn the relationship among students, knowledge concepts, and problems from students' exercise sequences. We also conduct several case studies. The case studies show that TRACED exhibits excellent interpretability and thus has the potential for personalized automatic feedback in the real-world educational environment.","['Hengyu Liu', 'Tiancheng Zhang', 'Fan Li', 'Minghe Yu', 'Ge Yu']",2023-02-17T03:50:49Z,http://arxiv.org/abs/2302.08673v1,Education & Social Science,Student Performance Prediction,"knowledge tracing aims to track students' knowledge status over time to predict students' future performance accurately . existing MCKT models only consider the relationship between knowledge status and problems when modeling students' responses . but as the number of tracked knowledge concepts increases, the time complexity of predicting student performance increases exponentially ."
Predicting Student Performance in Interactive Online Question Pools   Using Mouse Interaction Features,"Modeling student learning and further predicting the performance is a well-established task in online learning and is crucial to personalized education by recommending different learning resources to different students based on their needs. Interactive online question pools (e.g., educational game platforms), an important component of online education, have become increasingly popular in recent years. However, most existing work on student performance prediction targets at online learning platforms with a well-structured curriculum, predefined question order and accurate knowledge tags provided by domain experts. It remains unclear how to conduct student performance prediction in interactive online question pools without such well-organized question orders or knowledge tags by experts. In this paper, we propose a novel approach to boost student performance prediction in interactive online question pools by further considering student interaction features and the similarity between questions. Specifically, we introduce new features (e.g., think time, first attempt, and first drag-and-drop) based on student mouse movement trajectories to delineate students' problem-solving details. In addition, heterogeneous information network is applied to integrating students' historical problem-solving information on similar questions, enhancing student performance predictions on a new question. We evaluate the proposed approach on the dataset from a real-world interactive question pool using four typical machine learning models.","['Huan Wei', 'Haotian Li', 'Meng Xia', 'Yong Wang', 'Huamin Qu']",2020-01-09T14:29:12Z,http://arxiv.org/abs/2001.03012v1,Education & Social Science,Student Performance Prediction,modeling student learning and further predicting the performance is a well-established task in online learning . interactive online question pools have become increasingly popular in recent years . it remains unclear how to conduct student performance prediction in such a pool .
Aligning Tutor Discourse Supporting Rigorous Thinking with Tutee Content   Mastery for Predicting Math Achievement,"This work investigates how tutoring discourse interacts with students' proximal knowledge to explain and predict students' learning outcomes. Our work is conducted in the context of high-dosage human tutoring where 9th-grade students (N= 1080) attended small group tutorials and individually practiced problems on an Intelligent Tutoring System (ITS). We analyzed whether tutors' talk moves and students' performance on the ITS predicted scores on math learning assessments. We trained Random Forest Classifiers (RFCs) to distinguish high and low assessment scores based on tutor talk moves, student's ITS performance metrics, and their combination. A decision tree was extracted from each RFC to yield an interpretable model. We found AUCs of 0.63 for talk moves, 0.66 for ITS, and 0.77 for their combination, suggesting interactivity among the two feature sources. Specifically, the best decision tree emerged from combining the tutor talk moves that encouraged rigorous thinking and students' ITS mastery. In essence, tutor talk that encouraged mathematical reasoning predicted achievement for students who demonstrated high mastery on the ITS, whereas tutors' revoicing of students' mathematical ideas and contributions was predictive for students with low ITS mastery. Implications for practice are discussed.","['Mark Abdelshiheed', 'Jennifer K. Jacobs', ""Sidney K. D'Mello""]",2024-05-10T03:04:59Z,http://arxiv.org/abs/2405.06218v1,Education & Social Science,Student Performance Prediction,this work investigates how tutoring discourse interacts with students' proximal knowledge to explain and predict students' learning outcomes . 9th-grade students attended small group tutorials and individually practiced problems on an intelligent Tutoring System (ITS) we analyzed whether tutors' talk moves and students' performance predicted scores on math learning assessments .
A Predictive Model for Student Performance in Classrooms Using Student   Interactions With an eTextbook,"With the rise of online eTextbooks and Massive Open Online Courses (MOOCs), a huge amount of data has been collected related to students' learning. With the careful analysis of this data, educators can gain useful insights into the performance of their students and their behavior in learning a particular topic. This paper proposes a new model for predicting student performance based on an analysis of how students interact with an interactive online eTextbook. By being able to predict students' performance early in the course, educators can easily identify students at risk and provide a suitable intervention. We considered two main issues the prediction of good/bad performance and the prediction of the final exam grade. To build the proposed model, we evaluated the most popular classification and regression algorithms on data from a data structures and algorithms course (CS2) offered in a large public research university. Random Forest Regression and Multiple Linear Regression have been applied in Regression. While Logistic Regression, decision tree, Random Forest Classifier, K Nearest Neighbors, and Support Vector Machine have been applied in classification.","['Ahmed Abd Elrahman', 'Taysir Hassan A Soliman', 'Ahmed I. Taloba', 'Mohammed F. Farghally']",2022-02-16T11:59:53Z,http://arxiv.org/abs/2203.03713v1,Education & Social Science,Student Performance Prediction,the paper proposes a new model for predicting student performance based on an analysis of how students interact with an online eTextbook . it considers two main issues: the prediction of good/bad performance and the predicted final exam grade . the proposed model was built on data from a data structures and algorithms course .
BKT-LSTM: Efficient Student Modeling for knowledge tracing and student   performance prediction,"Recently, we have seen a rapid rise in usage of online educational platforms. The personalized education became crucially important in future learning environments. Knowledge tracing (KT) refers to the detection of students' knowledge states and predict future performance given their past outcomes for providing adaptive solution to Intelligent Tutoring Systems (ITS). Bayesian Knowledge Tracing (BKT) is a model to capture mastery level of each skill with psychologically meaningful parameters and widely used in successful tutoring systems. However, it is unable to detect learning transfer across skills because each skill model is learned independently and shows lower efficiency in student performance prediction. While recent KT models based on deep neural networks shows impressive predictive power but it came with a price. Ten of thousands of parameters in neural networks are unable to provide psychologically meaningful interpretation that reflect to cognitive theory. In this paper, we proposed an efficient student model called BKT-LSTM. It contains three meaningful components: individual \textit{skill mastery} assessed by BKT, \textit{ability profile} (learning transfer across skills) detected by k-means clustering and \textit{problem difficulty}. All these components are taken into account in student's future performance prediction by leveraging predictive power of LSTM. BKT-LSTM outperforms state-of-the-art student models in student's performance prediction by considering these meaningful features instead of using binary values of student's past interaction in DKT. We also conduct ablation studies on each of BKT-LSTM model components to examine their value and each component shows significant contribution in student's performance prediction. Thus, it has potential for providing adaptive and personalized instruction in real-world educational systems.",['Sein Minn'],2020-12-22T18:05:36Z,http://arxiv.org/abs/2012.12218v3,Education & Social Science,Student Performance Prediction,the personalized education became crucially important in future learning environments . knowledge tracing (KT) refers to the detection of students' knowledge states . it is unable to detect learning transfer across skills because each skill model is learned independently and shows lower efficiency in student performance prediction .
Using Machine Learning to Predict Engineering Technology Students'   Success with Computer Aided Design,"Computer-aided design (CAD) programs are essential to engineering as they allow for better designs through low-cost iterations. While CAD programs are typically taught to undergraduate students as a job skill, such software can also help students learn engineering concepts. A current limitation of CAD programs (even those that are specifically designed for educational purposes) is that they are not capable of providing automated real-time help to students. To encourage CAD programs to build in assistance to students, we used data generated from students using a free, open source CAD software called Aladdin to demonstrate how student data combined with machine learning techniques can predict how well a particular student will perform in a design task. We challenged students to design a house that consumed zero net energy as part of an introductory engineering technology undergraduate course. Using data from 128 students, along with the scikit-learn Python machine learning library, we tested our models using both total counts of design actions and sequences of design actions as inputs. We found that our models using early design sequence actions are particularly valuable for prediction. Our logistic regression model achieved a >60% chance of predicting if a student would succeed in designing a zero net energy house. Our results suggest that it would be feasible for Aladdin to provide useful feedback to students when they are approximately halfway through their design. Further improvements to these models could lead to earlier predictions and thus provide students feedback sooner to enhance their learning.","['Jasmine Singh', 'Viranga Perera', 'Alejandra J. Magana', 'Brittany Newell', 'Jin Wei-Kocsis', 'Ying Ying Seah', 'Greg J. Strimel', 'Charles Xie']",2021-08-12T20:24:54Z,http://arxiv.org/abs/2108.05955v1,Education & Social Science,Student Performance Prediction,computer-aided design (CAD) programs are essential to engineering . they allow for better designs through low-cost iterations . a current limitation of CAD programs is that they are not capable of providing real-time help to students .
ClickTree: A Tree-based Method for Predicting Math Students' Performance   Based on Clickstream Data,"The prediction of student performance and the analysis of students' learning behavior play an important role in enhancing online courses. By analysing a massive amount of clickstream data that captures student behavior, educators can gain valuable insights into the factors that influence academic outcomes and identify areas of improvement in courses. In this study, we developed ClickTree, a tree-based methodology, to predict student performance in mathematical assignments based on students' clickstream data. We extracted a set of features, including problem-level, assignment-level and student-level features, from the extensive clickstream data and trained a CatBoost tree to predict whether a student successfully answers a problem in an assignment. The developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup 2023 and ranked second in the competition. Furthermore, our results indicate that students encounter more difficulties in the problem types that they must select a subset of answers from a given set as well as problem subjects of Algebra II. Additionally, students who performed well in answering end-unit assignment problems engaged more with in-unit assignments and answered more problems correctly, while those who struggled had higher tutoring request rate. The proposed method can be utilized to improve students' learning experiences, and the above insights can be integrated into mathematical courses to enhance students' learning outcomes.","['Narjes Rohani', 'Behnam Rohani', 'Areti Manataki']",2024-03-01T23:39:03Z,http://arxiv.org/abs/2403.14664v1,Education & Social Science,Student Performance Prediction,clickTree predicts student performance in math assignments based on clickstream data . method achieved an AUC of 0.78844 in the Educational Data Mining Cup 2023 .
Predicting Academic Performance for College Students: A Campus Behavior   Perspective,"Detecting abnormal behaviors of students in time and providing personalized intervention and guidance at the early stage is important in educational management. Academic performance prediction is an important building block to enabling this pre-intervention and guidance. Most of the previous studies are based on questionnaire surveys and self-reports, which suffer from small sample size and social desirability bias. In this paper, we collect longitudinal behavioral data from 6,597 students' smart cards and propose three major types of discriminative behavioral factors, diligence, orderliness, and sleep patterns. Empirical analysis demonstrates these behavioral factors are strongly correlated with academic performance. Furthermore, motivated by social influence theory, we analyze the correlation between each student's academic performance with his/her behaviorally similar students'. Statistical tests indicate this correlation is significant. Based on these factors, we further build a multi-task predictive framework based on a learning-to-rank algorithm for academic performance prediction. This framework captures inter-semester correlation, inter-major correlation and integrates student similarity to predict students' academic performance. The experiments on a large-scale real-world dataset show the effectiveness of our methods for predicting academic performance and the effectiveness of proposed behavioral factors.","['Huaxiu Yao', 'Defu Lian', 'Yi Cao', 'Yifan Wu', 'Tao Zhou']",2019-03-15T18:10:00Z,http://arxiv.org/abs/1903.06726v1,Education & Social Science,Student Performance Prediction,"academic performance prediction is an important building block to enabling pre-intervention . diligence, orderliness, and sleep patterns are proposed in this paper . a large-scale real-world dataset shows the effectiveness of our methods ."
Interpretable Knowledge Tracing: Simple and Efficient Student Modeling   with Causal Relations,"Intelligent Tutoring Systems have become critically important in future learning environments. Knowledge Tracing (KT) is a crucial part of that system. It is about inferring the skill mastery of students and predicting their performance to adjust the curriculum accordingly. Deep Learning-based KT models have shown significant predictive performance compared with traditional models. However, it is difficult to extract psychologically meaningful explanations from the tens of thousands of parameters in neural networks, that would relate to cognitive theory. There are several ways to achieve high accuracy in student performance prediction but diagnostic and prognostic reasoning is more critical in learning sciences. Since KT problem has few observable features (problem ID and student's correctness at each practice), we extract meaningful latent features from students' response data by using machine learning and data mining techniques. In this work, we present Interpretable Knowledge Tracing (IKT), a simple model that relies on three meaningful latent features: individual skill mastery, ability profile (learning transfer across skills), and problem difficulty. IKT's prediction of future student performance is made using a Tree-Augmented Naive Bayes Classifier (TAN), therefore its predictions are easier to explain than deep learning-based student models. IKT also shows better student performance prediction than deep learning-based student models without requiring a huge amount of parameters. We conduct ablation studies on each feature to examine their contribution to student performance prediction. Thus, IKT has great potential for providing adaptive and personalized instructions with causal reasoning in real-world educational systems.","['Sein Minn', 'Jill-Jenn Vie', 'Koh Takeuchi', 'Hisashi Kashima', 'Feida Zhu']",2021-12-15T19:05:48Z,http://arxiv.org/abs/2112.11209v1,Education & Social Science,Student Performance Prediction,deep learning-based KT models have shown significant predictive performance . it is difficult to extract psychologically meaningful explanations from the tens of thousands of parameters . diagnostic and prognostic reasoning is more critical in learning sciences .
Predicting Student Performance in an Educational Game Using a Hidden   Markov Model,"Contributions: Prior studies on education have mostly followed the model of the cross sectional study, namely, examining the pretest and the posttest scores. This paper shows that students' knowledge throughout the intervention can be estimated by time series analysis using a hidden Markov model. Background: Analyzing time series and the interaction between the students and the game data can result in valuable information that cannot be gained by only cross sectional studies of the exams. Research Questions: Can a hidden Markov model be used to analyze the educational games? Can a hidden Markov model be used to make a prediction of the students' performance? Methodology: The study was conducted on (N=854) students who played the Save Patch game. Students were divided into class 1 and class 2. Class 1 students are those who scored lower in the test than class 2 students. The analysis is done by choosing various features of the game as the observations. Findings: The state trajectories can predict the students' performance accurately for both class 1 and class 2.","['Manie Tadayon', 'Greg Pottie']",2019-04-24T06:55:50Z,http://arxiv.org/abs/1904.11857v1,Education & Social Science,Student Performance Prediction,this paper shows that students' knowledge throughout the intervention can be estimated by time series analysis using a hidden Markov model . study was conducted on (N=854) students who played the Save Patch game .
Analyzing the Capabilities of Nature-inspired Feature Selection   Algorithms in Predicting Student Performance,"Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. As educational data grows larger, more effective means of analyzing student data in a timely manner are needed in order to provide useful predictions and interventions. In this paper, an analysis was conducted to determine the relative performance of a suite of nature-inspired algorithms in the feature-selection portion of ensemble algorithms used to predict student performance. A Swarm Intelligence ML engine (SIMLe) was developed to run this suite in tandem with a series of traditional ML classification algorithms to analyze three student datasets: instance-based clickstream data, hybrid single-course performance, and student meta-performance when taking multiple courses simultaneously. These results were then compared to previous predictive algorithms and, for all datasets analyzed, it was found that leveraging an ensemble approach using nature-inspired algorithms for feature selection and traditional ML algorithms for classification significantly increased predictive accuracy while also reducing feature set size by up to 65 percent.",['Thomas Trask'],2023-08-15T21:18:52Z,http://arxiv.org/abs/2308.08574v2,Education & Social Science,Student Performance Prediction,a suite of nature-inspired algorithms was used to predict student performance . a Swarm Intelligence ML engine (SIMLe) was developed to run the suite . the results were compared to previous predictive algorithms .
Taxonomy of academic plagiarism methods,"The article gives an overview of the plagiarism domain, with focus on academic plagiarism. The article defines plagiarism, explains the origin of the term, as well as plagiarism related terms. It identifies the extent of the plagiarism domain and then focuses on the plagiarism subdomain of text documents, for which it gives an overview of current classifications and taxonomies and then proposes a more comprehensive classification according to several criteria: their origin and purpose, technical implementation, consequence, complexity of detection and according to the number of linguistic sources. The article suggests the new classification of academic plagiarism, describes sorts and methods of plagiarism, types and categories, approaches and phases of plagiarism detection, the classification of methods and algorithms for plagiarism detection. The title of the article explicitly targets the academic community, but it is sufficiently general and interdisciplinary, so it can be useful for many other professionals like software developers, linguists and librarians.","['Tedo Vrbanec', 'Ana Mestrovic']",2021-05-25T16:49:08Z,http://arxiv.org/abs/2105.12068v1,Education & Social Science,Academic Plagiarism Detection,"the article defines plagiarism, explains the origin of the term, as well as related terms . it focuses on the plagiarism subdomain of text documents . the article suggests the new classification of academic plagiarism ."
Academic Source Code Plagiarism Detection by Measuring Program   Behavioural Similarity,"Source code plagiarism is a long-standing issue in tertiary computer science education. Many source code plagiarism detection tools have been proposed to aid in the detection of source code plagiarism. However, existing detection tools are not robust to pervasive plagiarism-hiding transformations, and as a result can be inaccurate in the detection of plagiarised source code. This article presents BPlag, a behavioural approach to source code plagiarism detection. BPlag is designed to be both robust to pervasive plagiarism-hiding transformations, and accurate in the detection of plagiarised source code. Greater robustness and accuracy is afforded by analysing the behaviour of a program, as behaviour is perceived to be the least susceptible aspect of a program impacted upon by plagiarism-hiding transformations. BPlag applies symbolic execution to analyse execution behaviour and represent a program in a novel graph-based format. Plagiarism is then detected by comparing these graphs and evaluating similarity scores. BPlag is evaluated for robustness, accuracy and efficiency against 5 commonly used source code plagiarism detection tools. It is then shown that BPlag is more robust to plagiarism-hiding transformations and more accurate in the detection of plagiarised source code, but is less efficient than compared tools.","['Hayden Cheers', 'Yuqing Lin', 'Shamus P. Smith']",2021-02-08T04:27:55Z,http://arxiv.org/abs/2102.03995v1,Education & Social Science,Academic Plagiarism Detection,BPlag is a behavioural approach to source code plagiarism detection . it is designed to be robust to pervasive plagiarism-hiding transformations . the tool is more accurate in the detection of plagiarised source code .
The Struggle with Academic Plagiarism: Approaches based on Semantic   Similarity,"Academic plagiarism is a serious problem nowadays. Due to the existence of inexhaustible sources of digital information, today it is easier to plagiarize more than ever before. The good thing is that plagiarism detection techniques have improved and are powerful enough to detect attempts of plagiarism in education. We are now witnessing efficient plagiarism detection software in action, such as Turnitin, iThenticate or SafeAssign. In the introduction we explore software that is used within the Croatian academic community for plagiarism detection in universities and/or in scientific journals. The question is: is this enough? Current software has proven to be successful, however the problem of identifying paraphrasing or obfuscation plagiarism remains unresolved. In this paper we present a report of how semantic similarity measures can be used in the plagiarism detection task.","['Tedo Vrbanec', 'Ana Mestrovic']",2021-06-02T20:00:33Z,http://arxiv.org/abs/2106.04404v1,Education & Social Science,Academic Plagiarism Detection,"academic plagiarism is a serious problem nowadays . the good thing is that plagiarism detection techniques have improved . current software has proven to be successful, but the problem remains unresolved ."
`CodeAliker' - Plagiarism Detection on the Cloud,"Plagiarism is a burning problem that academics have been facing in all of the varied levels of the educational system. With the advent of digital content, the challenge to ensure the integrity of academic work has been amplified. This paper discusses on defining a precise definition of plagiarized computer code, various solutions available for detecting plagiarism and building a cloud platform for plagiarism disclosure. 'CodeAliker', our application thus developed automates the submission of assignments and the review process associated for essay text as well as computer code. It has been made available under the GNU's General Public License as a Free and Open Source Software.",['Nitish Upreti'],2012-08-13T02:52:26Z,http://arxiv.org/abs/1208.2486v1,Education & Social Science,Academic Plagiarism Detection,'CodeAliker' automates the submission of assignments and the review process associated with essay text and computer code . it has been made available under the GNU's general public license as a Free and Open Source Software .
Analyzing Similarity in Mathematical Content To Enhance the Detection of   Academic Plagiarism,"Despite the effort put into the detection of academic plagiarism, it continues to be a ubiquitous problem spanning all disciplines. Various tools have been developed to assist human inspectors by automatically identifying suspicious documents. However, to our knowledge currently none of these tools use mathematical content for their analysis. This is problematic, because mathematical content potentially represents a significant amount of the scientific contribution in academic documents. Hence, ignoring mathematical content limits the detection of plagiarism considerably, especially in disciplines with frequent use of mathematics.   This paper aims to help close this gap by providing an overview of existing approaches in mathematical information retrieval and an analysis of their applicability for different possible cases of mathematical plagiarism. I find that whereas syntax-based approaches perform particularly well in detecting undisguised plagiarism, structure-based and hybrid approaches promise to also detect forms of disguised mathematical plagiarism, such as plagiarism with renamed identifiers. However, more research in this area is needed to enable the detection of more complex mathematical plagiarism: the scope of current approaches is restricted to the formula-level, an extension to the section-level is needed. Additionally, the general detection of equivalence transformations is currently not feasible. Despite these remaining problems, I conclude that the presented approaches could already be used for a basic automated detection system targeting mathematical plagiarism and therefore enhance current plagiarism detection systems.",['Maurice-Roman Isele'],2018-01-25T15:02:16Z,http://arxiv.org/abs/1801.08439v1,Education & Social Science,Academic Plagiarism Detection,a number of tools have been developed to detect academic plagiarism . but none of these tools use mathematical content for their analysis . this paper aims to help close this gap by providing an overview of existing approaches .
Shape-Based Plagiarism Detection for Flowchart Figures in Texts,"Plagiarism detection is well known phenomenon in the academic arena. Copying other people is considered as serious offence that needs to be checked. There are many plagiarism detection systems such as turn-it-in that has been developed to provide this checks. Most, if not all, discard the figures and charts before checking for plagiarism. Discarding the figures and charts results in look holes that people can take advantage. That means people can plagiarized figures and charts easily without the current plagiarism systems detecting it. There are very few papers which talks about flowcharts plagiarism detection. Therefore, there is a need to develop a system that will detect plagiarism in figures and charts. This paper presents a method for detecting flow chart figure plagiarism based on shape-based image processing and multimedia retrieval. The method managed to retrieve flowcharts with ranked similarity according to different matching sets.","['Senosy Arrish', 'Fadhil Noer Afif', 'Ahmadu Maidorawa', 'Naomie Salim']",2014-03-12T10:21:25Z,http://arxiv.org/abs/1403.2871v1,Education & Social Science,Academic Plagiarism Detection,flowcharts plagiarism detection is a well-known phenomenon in the academic arena . people can plagiarize figures and charts easily without current systems detecting it . this paper presents a method for detecting flow chart figure plagiarism .
Analyzing Non-Textual Content Elements to Detect Academic Plagiarism,"Identifying academic plagiarism is a pressing problem, among others, for research institutions, publishers, and funding organizations. Detection approaches proposed so far analyze lexical, syntactical, and semantic text similarity. These approaches find copied, moderately reworded, and literally translated text. However, reliably detecting disguised plagiarism, such as strong paraphrases, sense-for-sense translations, and the reuse of non-textual content and ideas, is an open research problem.   The thesis addresses this problem by proposing plagiarism detection approaches that implement a different concept: analyzing non-textual content in academic documents, specifically citations, images, and mathematical content.   To validate the effectiveness of the proposed detection approaches, the thesis presents five evaluations that use real cases of academic plagiarism and exploratory searches for unknown cases.   The evaluation results show that non-textual content elements contain a high degree of semantic information, are language-independent, and largely immutable to the alterations that authors typically perform to conceal plagiarism. Analyzing non-textual content complements text-based detection approaches and increases the detection effectiveness, particularly for disguised forms of academic plagiarism.   To demonstrate the benefit of combining non-textual and text-based detection methods, the thesis describes the first plagiarism detection system that integrates the analysis of citation-based, image-based, math-based, and text-based document similarity. The system's user interface employs visualizations that significantly reduce the effort and time users must invest in examining content similarity.",['Norman Meuschke'],2021-06-10T14:11:52Z,http://arxiv.org/abs/2106.05764v1,Education & Social Science,Academic Plagiarism Detection,thesis presents five evaluations that use real cases of academic plagiarism . results show that non-textual content elements contain a high degree of semantic information . system's user interface employs visualizations that significantly reduce effort and time .
Improving Academic Plagiarism Detection for STEM Documents by Analyzing   Mathematical Content and Citations,"Identifying academic plagiarism is a pressing task for educational and research institutions, publishers, and funding agencies. Current plagiarism detection systems reliably find instances of copied and moderately reworded text. However, reliably detecting concealed plagiarism, such as strong paraphrases, translations, and the reuse of nontextual content and ideas is an open research problem. In this paper, we extend our prior research on analyzing mathematical content and academic citations. Both are promising approaches for improving the detection of concealed academic plagiarism primarily in Science, Technology, Engineering and Mathematics (STEM). We make the following contributions: i) We present a two-stage detection process that combines similarity assessments of mathematical content, academic citations, and text. ii) We introduce new similarity measures that consider the order of mathematical features and outperform the measures in our prior research. iii) We compare the effectiveness of the math-based, citation-based, and text-based detection approaches using confirmed cases of academic plagiarism. iv) We demonstrate that the combined analysis of math-based and citation-based content features allows identifying potentially suspicious cases in a collection of 102K STEM documents. Overall, we show that analyzing the similarity of mathematical content and academic citations is a striking supplement for conventional text-based detection approaches for academic literature in the STEM disciplines.","['Norman Meuschke', 'Vincent Stange', 'Moritz Schubotz', 'Michael Karmer', 'Bela Gipp']",2019-06-27T16:07:47Z,http://arxiv.org/abs/1906.11761v1,Education & Social Science,Academic Plagiarism Detection,current systems reliably find instances of copied and moderately reworded text . but reliably detecting concealed plagiarism is an open research problem . authors present a two-stage detection process that combines similarity assessments .
"A Survey of Plagiarism Detection Systems: Case of Use with English,   French and Arabic Languages","In academia, plagiarism is certainly not an emerging concern, but it became of a greater magnitude with the popularisation of the Internet and the ease of access to a worldwide source of content, rendering human-only intervention insufficient. Despite that, plagiarism is far from being an unaddressed problem, as computer-assisted plagiarism detection is currently an active area of research that falls within the field of Information Retrieval (IR) and Natural Language Processing (NLP). Many software solutions emerged to help fulfil this task, and this paper presents an overview of plagiarism detection systems for use in Arabic, French, and English academic and educational settings. The comparison was held between eight systems and was performed with respect to their features, usability, technical aspects, as well as their performance in detecting three levels of obfuscation from different sources: verbatim, paraphrase, and cross-language plagiarism. An indepth examination of technical forms of plagiarism was also performed in the context of this study. In addition, a survey of plagiarism typologies and classifications proposed by different authors is provided.","['Mehdi Abdelhamid', 'Faical Azouaou', 'Sofiane Batata']",2022-01-10T16:11:54Z,http://arxiv.org/abs/2201.03423v1,Education & Social Science,Academic Plagiarism Detection,plagiarism detection is an area of research that falls within the field of IR and NLP . many software solutions emerged to help fulfil this task . this paper presents an overview of plagiarism detection systems for use in academic settings .
Hamtajoo: A Persian Plagiarism Checker for Academic Manuscripts,"In recent years, due to the high availability of electronic documents through the Web, the plagiarism has become a serious challenge, especially among scholars. Various plagiarism detection systems have been developed to prevent text re-use and to confront plagiarism. Although it is almost easy to detect duplicate text in academic manuscripts, finding patterns of text re-use that has been semantically changed is of great importance. Another important issue is to deal with less resourced languages, which there are low volume of text for training purposes and also low performance in tools for NLP applications. In this paper, we introduce Hamtajoo, a Persian plagiarism detection system for academic manuscripts. Moreover, we describe the overall structure of the system along with the algorithms used in each stage. In order to evaluate the performance of the proposed system, we used a plagiarism detection corpus comply with the PAN standards.","['Vahid Zarrabi', 'Salar Mohtaj', 'Habibollah Asghari']",2021-12-27T15:45:35Z,http://arxiv.org/abs/2112.13742v1,Education & Social Science,Academic Plagiarism Detection,"in recent years, the plagiarism has become a serious challenge, especially among scholars . various plagiarism detection systems have been developed to prevent text re-use . a new system for academic manuscripts has been introduced ."
Plagiarism Detection - State-of-the-art systems (2016) and evaluation   methods,"Plagiarism detection systems comprise various approaches that aim to create a fair environment for academic publications and appropriately acknowledge the authors' works. While the need for a reliable and performant plagiarism detection system increases with an increasing amount of publications, current systems still have shortcomings. Particularly intelligent research plagiarism detection still leaves room for improvement. An important factor for progress in research is a suitable evaluation framework. In this paper, we give an overview on the evaluation of plagiarism detection. We then use a taxonomy provided in former research, to classify recent approaches of plagiarism detection. Based on this, we asses the current research situation in the field of plagiarism detection and derive further research questions and approaches to be tackled in the future.",['Christina Kraus'],2016-03-08T18:31:13Z,http://arxiv.org/abs/1603.03014v1,Education & Social Science,Academic Plagiarism Detection,the need for a reliable and performant plagiarism detection system increases with increasing amount of publications . current systems still have shortcomings . particularly intelligent research plagiarism detection still leaves room for improvement .
AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based   Assignments,"Plagiarism is one of the growing issues in academia and is always a concern in Universities and other academic institutions. The situation is becoming even worse with the availability of ample resources on the web. This paper focuses on creating an effective and fast tool for plagiarism detection for text based electronic assignments. Our plagiarism detection tool named AntiPlag is developed using the tri-gram sequence matching technique. Three sets of text based assignments were tested by AntiPlag and the results were compared against an existing commercial plagiarism detection tool. AntiPlag showed better results in terms of false positives compared to the commercial tool due to the pre-processing steps performed in AntiPlag. In addition, to improve the detection latency, AntiPlag applies a data clustering technique making it four times faster than the commercial tool considered. AntiPlag could be used to isolate plagiarized text based assignments from non-plagiarised assignments easily. Therefore, we present AntiPlag, a fast and effective tool for plagiarism detection on text based electronic assignments.","['M. A. C. Jiffriya', 'M. A. C. Akmal Jahan', 'R. G. Ragel', 'S. Deegalla']",2014-03-06T01:16:01Z,http://arxiv.org/abs/1403.1310v1,Education & Social Science,Academic Plagiarism Detection,this paper focuses on creating an effective and fast tool for plagiarism detection . antiPlag is developed using the tri-gram sequence matching technique . results were compared against an existing commercial plagiarism detection tool .
PasteTrace: A Single Source Plagiarism Detection Tool For Introductory   Programming Courses,"Introductory Computer Science classes are important for laying the foundation for advanced programming courses. However, students without prior programming experience may find these courses challenging, leading to difficulties in understanding concepts and engaging in academic dishonesty such as plagiarism. While there exists plagiarism detection techniques and tools, not all of them are suitable for academic settings, especially in introductory programming courses. This paper introduces PasteTrace, a novel open-source plagiarism detection tool designed specifically for introductory programming courses. Unlike traditional methods, PasteTrace operates within an Integrated Development Environment that tracks the student's coding activities in real-time for evidence of plagiarism. Our evaluation of PasteTrace in two introductory programming courses demonstrates the tool's ability to provide insights into student behavior and detect various forms of plagiarism, outperforming an existing well-established tool.   A video demonstration of PasteTrace and its source code, and case study data are made available at https://doi.org/10.6084/m9.figshare.27115852","['Jesse McDonald', 'Scott Robertson', 'Anthony Peruma']",2025-06-20T04:21:07Z,http://arxiv.org/abs/2506.17355v1,Education & Social Science,Academic Plagiarism Detection,"this paper introduces PasteTrace, a novel open-source plagiarism detection tool . the tool is designed specifically for introductory programming courses . it provides insights into student behavior and detects various forms of plagiarism ."
Methods for Detecting Paraphrase Plagiarism,"Paraphrase plagiarism is one of the difficult challenges facing plagiarism detection systems. Paraphrasing occur when texts are lexically or syntactically altered to look different, but retain their original meaning. Most plagiarism detection systems (many of which are commercial based) are designed to detect word co-occurrences and light modifications, but are unable to detect severe semantic and structural alterations such as what is seen in many academic documents. Hence many paraphrase plagiarism cases go undetected. In this paper, we approached the problem of paraphrase plagiarism by proposing methods for detecting the most common techniques (phenomena) used in paraphrasing texts (namely; lexical substitution, insertion/deletion and word and phrase reordering), and combined the methods into a paraphrase detection model. We evaluated our proposed methods and model on collections containing paraphrase texts. Experimental results show significant improvement in performance when the methods were combined (the proposed model) as opposed to running them individually. The results also show that the proposed paraphrase detection model outperformed a standard baseline (based on greedy string tilling), and previous studies.",['Victor Thompson'],2017-12-29T18:53:12Z,http://arxiv.org/abs/1712.10309v1,Education & Social Science,Academic Plagiarism Detection,paraphrase plagiarism occurs when texts are lexically or syntactically altered . most plagiarism detection systems are designed to detect word co-occurrences . but they are unable to detect severe semantic and structural alterations .
A Study of Obstacles in Plagiarism Software Subscribing by Colleges in   Tamil Nadu,"This article attempts to comprehend the current issues and hurdles that Indian colleges affiliated with Tamil Nadu State Universities encounter when trying to subscribe to a software that detects plagiarism. The study goals are to determine whether colleges employ anti-plagiarism software, whether they ensure that their student-given assignments are free of copyright infringement, whether tutors teach about academic misconduct, and what people seem to think of anti-plagiarism software. We surveyed for this study and distributed the questionnaires among college administrators, principals, and librarians.","['Subaveerapandiyan A', 'Sakthivel N']",2022-10-19T08:29:21Z,http://arxiv.org/abs/2210.10372v1,Education & Social Science,Academic Plagiarism Detection,"study aims to determine if colleges employ anti-plagiarism software . tnu surveyed college administrators, principals, and librarians ."
Web Based Cross Language Plagiarism Detection,"As the Internet help us cross language and cultural border by providing different types of translation tools, cross language plagiarism, also known as translation plagiarism are bound to arise. Especially among the academic works, such issue will definitely affect the student's works including the quality of their assignments and paper works. In this paper, we propose a new approach in detecting cross language plagiarism. Our web based cross language plagiarism detection system is specially tuned to detect translation plagiarism by implementing different techniques and tools to assist the detection process. Google Translate API is used as our translation tool and Google Search API, which is used in our information retrieval process. Our system is also integrated with the fingerprint matching technique, which is a widely used plagiarism detection technique. In general, our proposed system is started by translating the input documents from Malay to English, followed by removal of stop words and stemming words, identification of similar documents in corpus, comparison of similar pattern and finally summary of the result. Three least-frequent 4-grams fingerprint matching is used to implement the core comparison phase during the plagiarism detection process. In K-gram fingerprint matching technique, although any value of K can be considered, yet K = 4 was stated as an ideal choice. This is because smaller values of K (i.e., K = 1, 2, or 3), do not provide good discrimination between sentences. On the other hand, the larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination of words in one sentence from words in another.","['Chow Kok Kent', 'Naomie Salim']",2009-12-20T02:19:05Z,http://arxiv.org/abs/0912.3959v1,Education & Social Science,Academic Plagiarism Detection,"web-based plagiarism detection system is specially tuned to detect cross language plagiarism . system is integrated with fingerprint matching technique, widely used plagiarism detection technique . three least-frequent 4-grams fingerprint matching is used during the plagiarism detection process ."
Evaluating Software Plagiarism Detection in the Age of AI: Automated   Obfuscation and Lessons for Academic Integrity,"Plagiarism in programming assignments is a persistent issue in computer science education, increasingly complicated by the emergence of automated obfuscation attacks. While software plagiarism detectors are widely used to identify suspicious similarities at scale and are resilient to simple obfuscation techniques, they are vulnerable to advanced obfuscation based on structural modification of program code that preserves the original program behavior. While different defense mechanisms have been proposed to increase resilience against these attacks, their current evaluation is limited to the scope of attacks used and lacks a comprehensive investigation regarding AI-based obfuscation. In this paper, we investigate the resilience of these defense mechanisms against a broad range of automated obfuscation attacks, including both algorithmic and AI-generated methods, and for a wide variety of real-world datasets. We evaluate the improvements of two defense mechanisms over the plagiarism detector JPlag across over four million pairwise program comparisons. Our results show significant improvements in detecting obfuscated plagiarism instances, and we observe an improved detection of AI-generated programs, even though the defense mechanisms are not designed for this use case. Based on our findings, we provide an in-depth discussion of their broader implications for academic integrity and the role of AI in education.","['Timur Sağlam', 'Larissa Schmid']",2025-05-26T15:59:01Z,http://arxiv.org/abs/2505.20158v1,Education & Social Science,Academic Plagiarism Detection,automated obfuscation attacks are a persistent issue in computer science education . software plagiarism detectors are widely used to identify suspicious similarities at scale . but they are vulnerable to advanced attacks based on modification of program code .
Survey on Plagiarism Detection in Large Language Models: The Impact of   ChatGPT and Gemini on Academic Integrity,"The rise of Large Language Models (LLMs) such as ChatGPT and Gemini has posed new challenges for the academic community. With the help of these models, students can easily complete their assignments and exams, while educators struggle to detect AI-generated content. This has led to a surge in academic misconduct, as students present work generated by LLMs as their own, without putting in the effort required for learning. As AI tools become more advanced and produce increasingly human-like text, detecting such content becomes more challenging. This development has significantly impacted the academic world, where many educators are finding it difficult to adapt their assessment methods to this challenge.   This research first demonstrates how LLMs have increased academic dishonesty, and then reviews state-of-the-art solutions for academic plagiarism in detail. A survey of datasets, algorithms, tools, and evasion strategies for plagiarism detection has been conducted, focusing on how LLMs and AI-generated content (AIGC) detection have affected this area. The survey aims to identify the gaps in existing solutions. Lastly, potential long-term solutions are presented to address the issue of academic plagiarism using LLMs based on AI tools and educational approaches in an ever-changing world.","['Shushanta Pudasaini', 'Luis Miralles-Pechuán', 'David Lillis', 'Marisa Llorens Salvador']",2024-06-04T09:38:03Z,http://arxiv.org/abs/2407.13105v1,Education & Social Science,Academic Plagiarism Detection,large language models (LLMs) such as ChatGPT and Gemini have posed new challenges for the academic community . this has led to a surge in academic misconduct . many educators are finding it difficult to adapt their assessment methods to this challenge .
Discovering and exploring cases of educational source code plagiarism   with Dolos,"Source code plagiarism is a significant issue in educational practice, and educators need user-friendly tools to cope with such academic dishonesty. This article introduces the latest version of Dolos, a state-of-the-art ecosystem of tools for detecting and preventing plagiarism in educational source code. In this new version, the primary focus has been on enhancing the user experience. Educators can now run the entire plagiarism detection pipeline from a new web app in their browser, eliminating the need for any installation or configuration. Completely redesigned analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism and how widespread plagiarism is within the collection. The dashboards support hierarchically structured navigation to facilitate zooming in and out of suspect cases. Clusters are an essential new component of the dashboard design, reflecting the observation that plagiarism can occur among larger groups of students. To meet various user needs, the Dolos software stack for source code plagiarism detections now includes a web interface, a JSON application programming interface (API), a command line interface (CLI), a JavaScript library and a preconfigured Docker container. Clear documentation and a free-to-use instance of the web app can be found at https://dolos.ugent.be. The source code is also available on GitHub.","['Rien Maertens', 'Maarten Van Neyghem', 'Maxiem Geldhof', 'Charlotte Van Petegem', 'Niko Strijbol', 'Peter Dawyndt', 'Bart Mesuere']",2024-02-16T17:47:11Z,http://arxiv.org/abs/2402.10853v2,Education & Social Science,Academic Plagiarism Detection,"a new version of Dolos detects source code plagiarism in educational source code . a web app is now available, eliminating the need for installation or configuration . analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism ."
The Failure of Plagiarism Detection in Competitive Programming,"Plagiarism in programming courses remains a persistent challenge, especially in competitive programming contexts where assignments often have unique, known solutions. This paper examines why traditional code plagiarism detection methods frequently fail in these environments and explores the implications of emerging factors such as generative AI (genAI). Drawing on the author's experience teaching a Competitive Programming 1 (CP1) course over seven semesters at Purdue University (with $\approx 100$ students each term) and completely redesigning the CP1/2/3 course sequence, we provide an academically grounded analysis. We review literature on code plagiarism in computer science education, survey current detection tools (Moss, Kattis, etc.) and methods (manual review, code-authorship interviews), and analyze their strengths and limitations. Experience-based observations are presented to illustrate real-world detection failures and successes. We find that widely-used automated similarity checkers can be thwarted by simple code transformations or novel AI-generated code, while human-centric approaches like oral interviews, though effective, are labor-intensive. The paper concludes with opinions and preliminary recommendations for improving academic integrity in programming courses, advocating for a multi-faceted approach that combines improved detection algorithms, mastery-based learning techniques, and authentic assessment practices to better ensure code originality.",['Ethan Dickey'],2025-05-13T05:43:49Z,http://arxiv.org/abs/2505.08244v1,Education & Social Science,Academic Plagiarism Detection,this paper examines why traditional code plagiarism detection methods fail . it explores the implications of emerging factors such as genAI . the paper concludes with opinions and preliminary recommendations for improving academic integrity .
Reinforcement Learning Guided Multi-Objective Exam Paper Generation,"To reduce the repetitive and complex work of instructors, exam paper generation (EPG) technique has become a salient topic in the intelligent education field, which targets at generating high-quality exam paper automatically according to instructor-specified assessment criteria. The current advances utilize the ability of heuristic algorithms to optimize several well-known objective constraints, such as difficulty degree, number of questions, etc., for producing optimal solutions. However, in real scenarios, considering other equally relevant objectives (e.g., distribution of exam scores, skill coverage) is extremely important. Besides, how to develop an automatic multi-objective solution that finds an optimal subset of questions from a huge search space of large-sized question datasets and thus composes a high-quality exam paper is urgent but non-trivial. To this end, we skillfully design a reinforcement learning guided Multi-Objective Exam Paper Generation framework, termed MOEPG, to simultaneously optimize three exam domain-specific objectives including difficulty degree, distribution of exam scores, and skill coverage. Specifically, to accurately measure the skill proficiency of the examinee group, we first employ deep knowledge tracing to model the interaction information between examinees and response logs. We then design the flexible Exam Q-Network, a function approximator, which automatically selects the appropriate question to update the exam paper composition process. Later, MOEPG divides the decision space into multiple subspaces to better guide the updated direction of the exam paper. Through extensive experiments on two real-world datasets, we demonstrate that MOEPG is feasible in addressing the multiple dilemmas of exam paper generation scenario.","['Yuhu Shang', 'Xuexiong Luo', 'Lihong Wang', 'Hao Peng', 'Xiankun Zhang', 'Yimeng Ren', 'Kun Liang']",2023-03-02T07:55:52Z,http://arxiv.org/abs/2303.01042v1,Education & Social Science,Exam Question Generation,"exam paper generation (EPG) technique has become a salient topic in the intelligent education field . the technique optimizes several well-known objective constraints for producing optimal solutions . but in real scenarios, considering other equally relevant objectives is extremely important ."
An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments,"Current IR evaluation is based on relevance judgments, created either manually or automatically, with decisions outsourced to Large Language Models (LLMs). We offer an alternative paradigm, that never relies on relevance judgments in any form. Instead, a text is defined as relevant if it contains information that enables the answering of key questions. We use this idea to design the EXAM Answerability Metric to evaluate information retrieval/generation systems for their ability to provide topically relevant information.   We envision the role of a human judge to edit and define an exam question bank that will test for the presence of relevant information in text. We support this step by generating an initial set of exam questions. In the next phase, an LLM-based question answering system will automatically grade system responses by tracking which exam questions are answerable with which system responses. We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the precision-oriented EXAM Qrels metric, the latter which can be implemented with trec_eval. This paradigm not only allows for the expansion of the exam question set post-hoc but also facilitates the ongoing evaluation of future information systems, whether they focus on retrieval, generation, or both.","['Naghmeh Farzi', 'Laura Dietz']",2024-02-01T03:46:11Z,http://arxiv.org/abs/2402.00309v1,Education & Social Science,Exam Question Generation,"current IR evaluation is based on relevance judgments, created manually or automatically . instead, a text is defined as relevant if it contains information that enables the answering of key questions . we envision the role of a human judge to edit and define an exam question bank ."
Optimal Weighting for Exam Composition,"A problem faced by many instructors is that of designing exams that accurately assess the abilities of the students. Typically these exams are prepared several days in advance, and generic question scores are used based on rough approximation of the question difficulty and length. For example, for a recent class taught by the author, there were 30 multiple choice questions worth 3 points, 15 true/false with explanation questions worth 4 points, and 5 analytical exercises worth 10 points. We describe a novel framework where algorithms from machine learning are used to modify the exam question weights in order to optimize the exam scores, using the overall class grade as a proxy for a student's true ability. We show that significant error reduction can be obtained by our approach over standard weighting schemes, and we make several new observations regarding the properties of the ""good"" and ""bad"" exam questions that can have impact on the design of improved future evaluation methods.","['Sam Ganzfried', 'Farzana Yusuf']",2017-12-24T05:35:47Z,http://arxiv.org/abs/1801.06043v1,Education & Social Science,Exam Question Generation,machine learning algorithms are used to modify the exam question weights . overall class grade is used as a proxy for a student's true ability . we show significant error reduction can be obtained .
ExamGAN and Twin-ExamGAN for Exam Script Generation,"Nowadays, the learning management system (LMS) has been widely used in different educational stages from primary to tertiary education for student administration, documentation, tracking, reporting, and delivery of educational courses, training programs, or learning and development programs. Towards effective learning outcome assessment, the exam script generation problem has attracted many attentions and been investigated recently. But the research in this field is still in its early stage. There are opportunities to further improve the quality of generated exam scripts in various aspects. In particular, two essential issues have been ignored largely by existing solutions. First, given a course, it is unknown yet how to generate an exam script which can result in a desirable distribution of student scores in a class (or across different classes). Second, while it is frequently encountered in practice, it is unknown so far how to generate a pair of high quality exam scripts which are equivalent in assessment (i.e., the student scores are comparable by taking either of them) but have significantly different sets of questions. To fill the gap, this paper proposes ExamGAN (Exam Script Generative Adversarial Network) to generate high quality exam scripts, and then extends ExamGAN to T-ExamGAN (Twin-ExamGAN) to generate a pair of high quality exam scripts. Based on extensive experiments on three benchmark datasets, it has verified the superiority of proposed solutions in various aspects against the state-of-the-art. Moreover, we have conducted a case study which demonstrated the effectiveness of proposed solution in a real teaching scenario.","['Zhengyang Wu', 'Ke Deng', 'Judy Qiu', 'Yong Tang']",2021-08-22T07:34:15Z,http://arxiv.org/abs/2108.09656v1,Education & Social Science,Exam Question Generation,"Towards effective learning outcome assessment, the exam script generation problem has attracted many attentions . the research in this field is still in its early stage . there are opportunities to further improve the quality of generated exam scripts in various aspects."
Automated Evaluation of Retrieval-Augmented Language Models with   Task-Specific Exam Generation,"We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.","['Gauthier Guinet', 'Behrooz Omidvar-Tehrani', 'Anoop Deoras', 'Laurent Callot']",2024-05-22T13:14:11Z,http://arxiv.org/abs/2405.13622v1,Education & Social Science,Exam Question Generation,we propose a new method to measure the task-specific accuracy of large language models . we leverage Item Response Theory (IRT) to estimate the quality of an exam . choosing the right retrieval algorithms often leads to bigger performance gains .
Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission   Exams,"The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem.","['Desnes Nunes', 'Ricardo Primi', 'Ramon Pires', 'Roberto Lotufo', 'Rodrigo Nogueira']",2023-03-29T20:10:13Z,http://arxiv.org/abs/2303.17003v1,Education & Social Science,Exam Question Generation,"the exame Nacional do Ensino M'edio (ENEM) is widely adopted by universities . the exam poses challenging tasks for LMs, since its questions may span into multiple fields . different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts ."
From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the   Aristo Project,"AI has achieved remarkable mastery over games such as Chess, Go, and Poker, and even Jeopardy, but the rich variety of standardized exams has remained a landmark challenge. Even in 2016, the best AI system achieved merely 59.3% on an 8th Grade science exam challenge. This paper reports unprecedented success on the Grade 8 New York Regents Science Exam, where for the first time a system scores more than 90% on the exam's non-diagram, multiple choice (NDMC) questions. In addition, our Aristo system, building upon the success of recent language models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC questions. The results, on unseen test questions, are robust across different test years and different variations of this kind of test. They demonstrate that modern NLP methods can result in mastery on this task. While not a full solution to general question-answering (the questions are multiple choice, and the domain is restricted to 8th Grade science), it represents a significant milestone for the field.","['Peter Clark', 'Oren Etzioni', 'Daniel Khashabi', 'Tushar Khot', 'Bhavana Dalvi Mishra', 'Kyle Richardson', 'Ashish Sabharwal', 'Carissa Schoenick', 'Oyvind Tafjord', 'Niket Tandon', 'Sumithra Bhakthavatsalam', 'Dirk Groeneveld', 'Michal Guerquin', 'Michael Schmitz']",2019-09-04T17:33:42Z,http://arxiv.org/abs/1909.01958v3,Education & Social Science,Exam Question Generation,"this paper reports unprecedented success on the Grade 8 New York Regents Science Exam . for the first time a system scores more than 90% on the exam's non-diagram, multiple choice questions . the results, on unseen test questions, are robust across different test years ."
Application of Large Language Models in Automated Question Generation: A   Case Study on ChatGLM's Structured Questions for National Teacher   Certification Exams,"This study delves into the application potential of the large language models (LLMs) ChatGLM in the automatic generation of structured questions for National Teacher Certification Exams (NTCE). Through meticulously designed prompt engineering, we guided ChatGLM to generate a series of simulated questions and conducted a comprehensive comparison with questions recollected from past examinees. To ensure the objectivity and professionalism of the evaluation, we invited experts in the field of education to assess these questions and their scoring criteria. The research results indicate that the questions generated by ChatGLM exhibit a high level of rationality, scientificity, and practicality similar to those of the real exam questions across most evaluation criteria, demonstrating the model's accuracy and reliability in question generation. Nevertheless, the study also reveals limitations in the model's consideration of various rating criteria when generating questions, suggesting the need for further optimization and adjustment. This research not only validates the application potential of ChatGLM in the field of educational assessment but also provides crucial empirical support for the development of more efficient and intelligent educational automated generation systems in the future.","['Ling He', 'Yanxin Chen', 'Xiaoqiang Hu']",2024-08-19T13:32:14Z,http://arxiv.org/abs/2408.09982v2,Education & Social Science,Exam Question Generation,this study delves into the application potential of the large language models (LLMs) ChatGLM in the automatic generation of structured questions for National Teacher Certification Exams (NTCE) the research results show that the questions generated by chatGLM exhibit a high level of rationality .
A Study on the Vulnerability of Test Questions against ChatGPT-based   Cheating,"ChatGPT is a chatbot that can answer text prompts fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT. In this paper, we try to provide an answer to an important question: how well ChatGPT can answer test questions and how we can detect whether the questions of a test can be answered correctly by ChatGPT. We generated ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions ChatGPT answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to ChatGPT in a collection of questions or a sample exam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test questions.","['Shanker Ram', 'Chen Qian']",2024-02-21T23:51:06Z,http://arxiv.org/abs/2402.14881v1,Education & Social Science,Exam Question Generation,"chatGPT can answer text prompts fairly accurately, even performing very well on postgraduate questions . many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating . in this paper, we ask how we can detect whether the questions of a test can be answered correctly ."
From Human Days to Machine Seconds: Automatically Answering and   Generating Machine Learning Final Exams,"A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level, on finals available online after the models were trained, and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.","['Iddo Drori', 'Sarah J. Zhang', 'Reece Shuttleworth', 'Sarah Zhang', 'Keith Tyser', 'Zad Chin', 'Pedro Lantigua', 'Saisamrit Surbehera', 'Gregory Hunter', 'Derek Austin', 'Leonard Tang', 'Yann Hicke', 'Sage Simhon', 'Sathwik Karnik', 'Darnell Granberry', 'Madeleine Udell']",2022-06-11T06:38:06Z,http://arxiv.org/abs/2206.05442v7,Education & Social Science,Exam Question Generation,"a final exam in machine learning typically takes faculty days to write, and students hours to solve . this work shows that large language models pass machine learning finals at a human level . we highlight the transformative potential of language models to streamline the writing of large-scale assessments ."
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using   Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn credits for renewing agronomist certifications, answering 93% of the questions correctly and outperforming earlier general-purpose models, which achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest performance when compared to human subjects. This performance suggests that GPT-4 could potentially pass on major graduate education admission tests or even earn credits for renewing agronomy certificates. We also explore the models' capacity to address general agriculture-related questions and generate crop management guidelines for Brazilian and Indian farmers, utilizing robust datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate program exams from India. The results suggest that GPT-4, ER, and RAG can contribute meaningfully to agricultural education, assessment, and crop management practice, offering valuable insights to farmers and agricultural professionals.","['Bruno Silva', 'Leonardo Nunes', 'Roberto Estevão', 'Vijay Aski', 'Ranveer Chandra']",2023-10-10T00:39:04Z,http://arxiv.org/abs/2310.06225v2,Education & Social Science,Exam Question Generation,"large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding . we present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT . our analysis highlights GPT-4's ability to achieve a passing score on exams ."
"Generative AI Takes a Statistics Exam: A Comparison of Performance   between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini","Many believe that use of generative AI as a private tutor has the potential to shrink access and achievement gaps between students and schools with abundant resources versus those with fewer resources. Shrinking the gap is possible only if paid and free versions of the platforms perform with the same accuracy. In this experiment, we investigate the performance of GPT versions 3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class of first-year graduate students. While we do not advocate using any generative AI platform to complete an exam, the use of exam questions allows us to explore aspects of ChatGPT's responses to typical questions that students might encounter in a statistics course. Results on accuracy indicate that GPT 3.5 would fail the exam, GPT4 would perform well, and GPT4o-mini would perform somewhere in between. While we acknowledge the existence of other Generative AI/LLMs, our discussion concerns only ChatGPT because it is the most widely used platform on college campuses at this time. We further investigate differences among the AI platforms in the answers for each problem using methods developed for text analytics, such as reading level evaluation and topic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics that are more similar than either of them have with GPT4.","['Monnie McGee', 'Bivin Sadler']",2025-01-15T21:46:01Z,http://arxiv.org/abs/2501.09171v1,Education & Social Science,Exam Question Generation,"generative AI has the potential to shrink achievement gaps between students and schools . this experiment investigates the performance of GPT versions 3.5, 4.0, and 4o-mini . results indicate that GPT 3.5 would fail the exam, GPT4 would perform well ."
Can Chat GPT solve a Linguistics Exam?,"The present study asks if ChatGPT4, the version of ChatGPT which uses the language model GPT4, can successfully solve introductory linguistic exams. Previous exam questions of an Introduction to Linguistics course at a German university are used to test this. The exam questions were fed into ChatGPT4 with only minimal preprocessing. The results show that the language model is very successful in the interpretation even of complex and nested tasks. It proved surprisingly successful in the task of broad phonetic transcription, but performed less well in the analysis of morphemes and phrases. In simple cases it performs sufficiently well, but rarer cases, particularly with missing one-to-one correspondence, are currently treated with mixed results. The model is not yet able to deal with visualisations, such as the analysis or generation of syntax trees. More extensive preprocessing, which translates these tasks into text data, allow the model to also solve these tasks successfully.","['Patricia Ronan', 'Gerold Schneider']",2023-11-04T20:02:57Z,http://arxiv.org/abs/2311.02499v1,Education & Social Science,Exam Question Generation,the language model is very successful in the interpretation even of complex tasks . it performed less well in the analysis of morphemes and phrases . the model is not yet able to deal with visualisations .
Polish Medical Exams: A new dataset for cross-lingual medical knowledge   transfer assessment,"Large Language Models (LLMs) have demonstrated significant potential in handling specialized tasks, including medical problem-solving. However, most studies predominantly focus on English-language contexts. This study introduces a novel benchmark dataset based on Polish medical licensing and specialization exams (LEK, LDEK, PES) taken by medical doctor candidates and practicing doctors pursuing specialization. The dataset was web-scraped from publicly available resources provided by the Medical Examination Center and the Chief Medical Chamber. It comprises over 24,000 exam questions, including a subset of parallel Polish-English corpora, where the English portion was professionally translated by the examination center for foreign candidates. By creating a structured benchmark from these existing exam questions, we systematically evaluate state-of-the-art LLMs, including general-purpose, domain-specific, and Polish-specific models, and compare their performance against human medical students. Our analysis reveals that while models like GPT-4o achieve near-human performance, significant challenges persist in cross-lingual translation and domain-specific understanding. These findings underscore disparities in model performance across languages and medical specialties, highlighting the limitations and ethical considerations of deploying LLMs in clinical practice.","['Łukasz Grzybowski', 'Jakub Pokrywka', 'Michał Ciesiółka', 'Jeremi I. Kaczmarek', 'Marek Kubis']",2024-11-30T19:02:34Z,http://arxiv.org/abs/2412.00559v1,Education & Social Science,Exam Question Generation,"the dataset was web-scraped from publicly available resources . it comprises over 24,000 exam questions, including a subset of parallel Polish-English corpora . models like GPT-4o achieve near-human performance, but challenges persist ."
"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining   Large Language Models","Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.","['Wenxuan Zhang', 'Sharifah Mahani Aljunied', 'Chang Gao', 'Yew Ken Chia', 'Lidong Bing']",2023-06-08T13:21:29Z,http://arxiv.org/abs/2306.05179v2,Education & Social Science,Exam Question Generation,"human exams are a more suitable means of evaluating general intelligence for LLMs . the benchmark contains 12,317 questions in 9 diverse languages with three educational levels . current models, including GPT-4, still struggle with multilingual text ."
Question-Score Identity Detection (Q-SID): A Statistical Algorithm to   Detect Collusion Groups with Error Quantification from Exam Question Scores,"Collusion between students in online exams is a major problem that undermines the integrity of the exam results. Although there exist methods that use exam data to identify pairs of students who have likely copied each other's answers, these methods are restricted to specific formats of multiple-choice exams. Here we present a statistical algorithm, Q-SID, that efficiently detects groups of students who likely have colluded, i.e., collusion groups, with error quantification. Q-SID uses graded numeric question scores only, so it works for many formats of multiple-choice and non-multiple-choice exams. Q-SID reports two false-positive rates (FPRs) for each collusion group: (1) empirical FPR, whose null data are from 36 strictly proctored exam datasets independent of the user-input exam data and (2) synthetic FPR, whose null data are simulated from a copula-based probabilistic model, which is first fitted to the user-input exam data and then modified to have no collusion. On 34 unproctored exam datasets, including two benchmark datasets with true positives and negatives verified by textural analysis, we demonstrate that Q-SID is a collusion detection algorithm with powerful and robust performance across exam formats, numbers of questions and students, and exam complexity.","['Guanao Yan', 'Jingyi Jessica Li', 'Mark D. Biggin']",2024-07-10T07:21:44Z,http://arxiv.org/abs/2407.07420v2,Education & Social Science,Exam Question Generation,collusion between students in online exams is a major problem that undermines the integrity of the exam results . there are methods that use exam data to identify pairs of students who have likely copied each other's answers . but these methods are restricted to specific formats of multiple-choice exams .
Crowdsourcing Multiple Choice Science Questions,"We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.","['Johannes Welbl', 'Nelson F. Liu', 'Matt Gardner']",2017-07-19T17:28:46Z,http://arxiv.org/abs/1707.06209v1,Education & Social Science,Exam Question Generation,"we present a novel method for obtaining high-quality, domain-targeted multiple choice questions . method produces model suggestions for document selection and answer distractor choice . we demonstrate that the method produces in-domain questions by providing an analysis of this new dataset ."
O.D.E.S. : An Online Dynamic Examination System based on a CMS Wordpress   plugin,"This paper describes the online dynamic examination application plugin named O.D.E.S., developed according to the open source software philosophy, where the CMS Wordpress is used as programmers/coders are given the potential to develop applications from scratch with safety and ease. In ODES application there exists two types of users: admin/teacher and student. The admin/teacher can create/edit/delete/view questions, categories of questions and examination papers. The questions are divided in two types, multiple choice questions (including true/false) and long answer questions (essays). The teacher can create exams choosing the number of questions and the types of questions that exist in the pool of questions that are previously created. The selection is done randomly by the application and the teacher just determines the total number of both multiple choice or long answer questions as well as the importance (weight) of each one of them (including negative grades also). The student takes the random generated exam and receives his/hers grades. The grades of the multiple choice questions are done automatically, whereas for the long answer questions the teacher is responsible to put grades on. After the completion of the exam the teacher can view the student's final score via the control panel or a report.","['George F. Fragulis', 'Lazaros Lazaridis', 'Maria Papatsimouli', 'Ioannis A. Skordas']",2018-05-07T17:01:51Z,http://arxiv.org/abs/1805.05426v1,Education & Social Science,Exam Question Generation,this paper describes the online dynamic examination application plugin named O.D.E.S. the plugin is developed according to the open source software philosophy . it allows programmers/coders to develop applications from scratch .
Unique Exams: Designing assessments for integrity and fairness,"Educators have faced new challenges in effective course assessment during the recent, unprecedented shift to remote online learning during the COVID-19 pandemic. In place of typical proctored, timed exams, instructors must now rethink their methodology for assessing course-level learning goals. Are exams appropriate---or even feasible---in this new online, open-internet learning environment? In this experience paper, we discuss the unique exams framework: our framework for upholding exam integrity and student privacy. In our Probability for Computer Scientists Course at an R1 University, we developed autogenerated, unique exams where each student had the same four problem skeletons with unique numeric variations per problem. Without changing the process of the traditional exam, unique exams provide a layer of security for both students and instructors about exam reliability for any classroom environment---in-person or online. In addition to sharing our experience designing unique exams, we also present a simple end-to-end tool and example question templates for different CS subjects that other instructors can adapt to their own courses.","['Gili Rusak', 'Lisa Yan']",2020-09-03T14:44:00Z,http://arxiv.org/abs/2009.01713v1,Education & Social Science,Exam Question Generation,"instructors have faced new challenges in effective course assessment during the COVID-19 pandemic . in this experience paper, we discuss the unique exams framework for upholding exam integrity . unique exams provide a layer of security for both students and instructors about exam reliability ."
ChatGPT Participates in a Computer Science Exam,"We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. The program was evaluated on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of ChatGPT's training data. Therefore, it would be inadequate to conclude from this experiment that ChatGPT has any understanding of computer science. We also assess the improvements brought by GPT-4. We find that GPT-4 would have obtained about 17\% more exam points than GPT-3.5, reaching the performance of the average student. The transcripts of our conversations with ChatGPT are available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper.","['Sebastian Bordt', 'Ulrike von Luxburg']",2023-03-08T15:46:14Z,http://arxiv.org/abs/2303.09461v2,Education & Social Science,Exam Question Generation,"chatGPT was asked to participate in an undergraduate computer science exam on ''Algorithms and Data Structures'' we hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students . we find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points ."
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive   Systems using Lifelong Self-Adaptation,"Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift corresponds to novel class appearance, a type of concept drift in target data that common ML techniques have problems dealing with. To tackle this problem, we present a novel approach to self-adaptation that enhances learning-based self-adaptive systems with a lifelong ML layer. We refer to this approach as lifelong self-adaptation. The lifelong ML layer tracks the system and its environment, associates this knowledge with the current tasks, identifies new tasks based on differences, and updates the learning models of the self-adaptive system accordingly. A human stakeholder may be involved to support the learning process and adjust the learning and goal models. We present a general architecture for lifelong self-adaptation and apply it to the case of drift of adaptation spaces that affects the decision-making in self-adaptation. We validate the approach for a series of scenarios using the DeltaIoT exemplar.","['Omid Gheibi', 'Danny Weyns']",2022-11-04T07:45:48Z,http://arxiv.org/abs/2211.02658v4,Education & Social Science,Adaptive Learning,"machine learning (ML) has become a popular approach to support self-adaptation . but, exploiting ML comes with inherent challenges . we present a novel approach that enhances learning-based systems with a lifelong ML layer ."
Deep Learning for Effective and Efficient Reduction of Large Adaptation   Spaces in Self-Adaptive Systems,"Many software systems today face uncertain operating conditions, such as sudden changes in the availability of resources or unexpected user behavior. Without proper mitigation these uncertainties can jeopardize the system goals. Self-adaptation is a common approach to tackle such uncertainties. When the system goals may be compromised, the self-adaptive system has to select the best adaptation option to reconfigure by analyzing the possible adaptation options, i.e., the adaptation space. Yet, analyzing large adaptation spaces using rigorous methods can be resource- and time-consuming, or even be infeasible. One approach to tackle this problem is by using online machine learning to reduce adaptation spaces. However, existing approaches require domain expertise to perform feature engineering to define the learner, and support online adaptation space reduction only for specific goals. To tackle these limitations, we present 'Deep Learning for Adaptation Space Reduction Plus' -- DLASeR+ in short. DLASeR+ offers an extendable learning framework for online adaptation space reduction that does not require feature engineering, while supporting three common types of adaptation goals: threshold, optimization, and set-point goals. We evaluate DLASeR+ on two instances of an Internet-of-Things application with increasing sizes of adaptation spaces for different combinations of adaptation goals. We compare DLASeR+ with a baseline that applies exhaustive analysis and two state-of-the-art approaches for adaptation space reduction that rely on learning. Results show that DLASeR+ is effective with a negligible effect on the realization of the adaptation goals compared to an exhaustive analysis approach, and supports three common types of adaptation goals beyond the state-of-the-art approaches.","['Danny Weyns', 'Omid Gheibi', 'Federico Quin', 'Jeroen Van Der Donckt']",2022-04-13T08:51:06Z,http://arxiv.org/abs/2204.06254v1,Education & Social Science,Adaptive Learning,without proper mitigation these uncertainties can jeopardize the system goals . analyzing large adaptation spaces using rigorous methods can be resource- and time-consuming . existing approaches require domain expertise to define the learner .
Non-Adaptive Learning a Hidden Hipergraph,We give a new deterministic algorithm that non-adaptively learns a hidden hypergraph from edge-detecting queries. All previous non-adaptive algorithms either run in exponential time or have non-optimal query complexity. We give the first polynomial time non-adaptive learning algorithm for learning hypergraph that asks almost optimal number of queries.,"['Hasan Abasi', 'Nader H. Bshouty', 'Hanna Mazzawi']",2015-02-13T21:32:12Z,http://arxiv.org/abs/1502.04137v1,Education & Social Science,Adaptive Learning,we give a new deterministic algorithm that non-adaptively learns a hidden hypergraph . previous algorithms either run in exponential time or have non-optimal query complexity .
Feature-Model-Guided Online Learning for Self-Adaptive Systems,"A self-adaptive system can modify its own structure and behavior at runtime based on its perception of the environment, of itself and of its requirements. To develop a self-adaptive system, software developers codify knowledge about the system and its environment, as well as how adaptation actions impact on the system. However, the codified knowledge may be insufficient due to design time uncertainty, and thus a self-adaptive system may execute adaptation actions that do not have the desired effect. Online learning is an emerging approach to address design time uncertainty by employing machine learning at runtime. Online learning accumulates knowledge at runtime by, for instance, exploring not-yet executed adaptation actions. We address two specific problems with respect to online learning for self-adaptive systems. First, the number of possible adaptation actions can be very large. Existing online learning techniques randomly explore the possible adaptation actions, but this can lead to slow convergence of the learning process. Second, the possible adaptation actions can change as a result of system evolution. Existing online learning techniques are unaware of these changes and thus do not explore new adaptation actions, but explore adaptation actions that are no longer valid. We propose using feature models to give structure to the set of adaptation actions and thereby guide the exploration process during online learning. Experimental results involving four real-world systems suggest that considering the hierarchical structure of feature models may speed up convergence by 7.2% on average. Considering the differences between feature models before and after an evolution step may speed up convergence by 64.6% on average. [...]","['Andreas Metzger', 'Clément Quinton', 'Zoltán Ádám Mann', 'Luciano Baresi', 'Klaus Pohl']",2019-07-22T07:04:11Z,http://arxiv.org/abs/1907.09158v1,Education & Social Science,Adaptive Learning,a self-adaptive system can modify its own structure and behavior at runtime . online learning accumulates knowledge by exploring not-yet executed adaptation actions . the number of possible adaptation actions can be very large .
ADAPT : Awesome Domain Adaptation Python Toolbox,"In this paper, we introduce the ADAPT library, an open source Python API providing the implementation of the main transfer learning and domain adaptation methods. The library is designed with a user friendly approach to facilitate the access to domain adaptation for a wide public. ADAPT is compatible with scikit-learn and TensorFlow and a full documentation is proposed online https://adapt-python.github.io/adapt/ with a substantial gallery of examples.","['Antoine de Mathelin', 'Mounir Atiq', 'Guillaume Richard', 'Alejandro de la Concha', 'Mouad Yachouti', 'François Deheeger', 'Mathilde Mougeot', 'Nicolas Vayatis']",2021-07-07T07:20:21Z,http://arxiv.org/abs/2107.03049v2,Education & Social Science,Adaptive Learning,the ADAPT library is an open source Python API . it provides the implementation of transfer learning and domain adaptation methods . the library is compatible with scikit-learn and TensorFlow .
Adaptable Adapters,"State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture -- i.e., the same adapter layer on top of each layer of the pretrained model -- for every dataset, regardless of the properties of the dataset or the amount of available training data. In this work, we introduce adaptable adapters that contain (1) learning different activation functions for different layers and different input data, and (2) a learnable switch to select and only use the beneficial adapter layers. We show that adaptable adapters achieve on-par performances with the standard adapter architecture while using a considerably smaller number of adapter layers. In addition, we show that the selected adapter architecture by adaptable adapters transfers well across different data settings and similar tasks. We propose to use adaptable adapters for designing efficient and effective adapter architectures. The resulting adapters (a) contain about 50% of the learning parameters of the standard adapter and are therefore more efficient at training and inference, and require less storage space, and (b) achieve considerably higher performances in low-data settings.","['Nafise Sadat Moosavi', 'Quentin Delfosse', 'Kristian Kersting', 'Iryna Gurevych']",2022-05-03T14:59:27Z,http://arxiv.org/abs/2205.01549v1,Education & Social Science,Adaptive Learning,state-of-the-art pretrained NLP models contain a hundred million to trillion parameters . adapters provide a parameter-efficient alternative for the full finetuning . we show adaptable adapters achieve on-par performances with standard adapter architecture .
Adaptive Hierarchical Hyper-gradient Descent,"In this study, we investigate learning rate adaption at different levels based on the hyper-gradient descent framework and propose a method that adaptively learns the optimizer parameters by combining multiple levels of learning rates with hierarchical structures. Meanwhile, we show the relationship between regularizing over-parameterized learning rates and building combinations of adaptive learning rates at different levels. The experiments on several network architectures, including feed-forward networks, LeNet-5 and ResNet-18/34, show that the proposed multi-level adaptive approach can outperform baseline adaptive methods in a variety of circumstances.","['Renlong Jie', 'Junbin Gao', 'Andrey Vasnev', 'Minh-Ngoc Tran']",2020-08-17T13:01:36Z,http://arxiv.org/abs/2008.07277v3,Education & Social Science,Adaptive Learning,"in this study, we investigate learning rate adaption at different levels . we propose a method that adaptively learns the optimizer parameters . the proposed multi-level adaptive approach can outperform baseline adaptive methods ."
Learning to adapt: a meta-learning approach for speaker adaptation,"The performance of automatic speech recognition systems can be improved by adapting an acoustic model to compensate for the mismatch between training and testing conditions, for example by adapting to unseen speakers. The success of speaker adaptation methods relies on selecting weights that are suitable for adaptation and using good adaptation schedules to update these weights in order not to overfit to the adaptation data. In this paper we investigate a principled way of adapting all the weights of the acoustic model using a meta-learning. We show that the meta-learner can learn to perform supervised and unsupervised speaker adaptation and that it outperforms a strong baseline adapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also report initial experiments on adapting TDNN AMs, where the meta-learner achieves comparable performance with LHUC.","['Ondřej Klejch', 'Joachim Fainberg', 'Peter Bell']",2018-08-30T11:47:07Z,http://arxiv.org/abs/1808.10239v1,Education & Social Science,Adaptive Learning,the performance of automatic speech recognition systems can be improved by adapting an acoustic model to compensate for the mismatch between training and testing conditions . the success of speaker adaptation methods relies on selecting weights that are suitable for adaptation and using good adaptation schedules .
On the Impact of Applying Machine Learning in the Decision-Making of   Self-Adaptive Systems,"Recently, we have been witnessing an increasing use of machine learning methods in self-adaptive systems. Machine learning methods offer a variety of use cases for supporting self-adaptation, e.g., to keep runtime models up to date, reduce large adaptation spaces, or update adaptation rules. Yet, since machine learning methods apply in essence statistical methods, they may have an impact on the decisions made by a self-adaptive system. Given the wide use of formal approaches to provide guarantees for the decisions made by self-adaptive systems, it is important to investigate the impact of applying machine learning methods when such approaches are used. In this paper, we study one particular instance that combines linear regression to reduce the adaptation space of a self-adaptive system with statistical model checking to analyze the resulting adaptation options. We use computational learning theory to determine a theoretical bound on the impact of the machine learning method on the predictions made by the verifier. We illustrate and evaluate the theoretical result using a scenario of the DeltaIoT artifact. To conclude, we look at opportunities for future research in this area.","['Omid Gheibi', 'Danny Weyns', 'Federico Quin']",2021-03-18T11:59:50Z,http://arxiv.org/abs/2103.10194v1,Education & Social Science,Adaptive Learning,a growing use of machine learning methods in self-adaptive systems . the methods may have an impact on the decisions made by the system . this paper examines the impact of applying them when they are used .
An Adaptive and Momental Bound Method for Stochastic Learning,"Training deep neural networks requires intricate initialization and careful selection of learning rates. The emergence of stochastic gradient optimization methods that use adaptive learning rates based on squared past gradients, e.g., AdaGrad, AdaDelta, and Adam, eases the job slightly. However, such methods have also been proven problematic in recent studies with their own pitfalls including non-convergence issues and so on. Alternative variants have been proposed for enhancement, such as AMSGrad, AdaShift and AdaBound. In this work, we identify a new problem of adaptive learning rate methods that exhibits at the beginning of learning where Adam produces extremely large learning rates that inhibit the start of learning. We propose the Adaptive and Momental Bound (AdaMod) method to restrict the adaptive learning rates with adaptive and momental upper bounds. The dynamic learning rate bounds are based on the exponential moving averages of the adaptive learning rates themselves, which smooth out unexpected large learning rates and stabilize the training of deep neural networks. Our experiments verify that AdaMod eliminates the extremely large learning rates throughout the training and brings significant improvements especially on complex networks such as DenseNet and Transformer, compared to Adam. Our implementation is available at: https://github.com/lancopku/AdaMod","['Jianbang Ding', 'Xuancheng Ren', 'Ruixuan Luo', 'Xu Sun']",2019-10-27T12:22:08Z,http://arxiv.org/abs/1910.12249v1,Education & Social Science,Adaptive Learning,"training deep neural networks requires intricate initialization and careful selection of learning rates . emergence of stochastic gradient optimization methods eases the job slightly . but such methods have also been proven problematic in recent studies . alternative variants have been proposed for enhancement, such as AMSGrad, AdaShift and AdaBound ."
Efficient Continual Adaptation of Pretrained Robotic Policy with Online   Meta-Learned Adapters,"Continual adaptation is essential for general autonomous agents. For example, a household robot pretrained with a repertoire of skills must still adapt to unseen tasks specific to each household. Motivated by this, building upon parameter-efficient fine-tuning in language models, prior works have explored lightweight adapters to adapt pretrained policies, which can preserve learned features from the pretraining phase and demonstrate good adaptation performances. However, these approaches treat task learning separately, limiting knowledge transfer between tasks. In this paper, we propose Online Meta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can facilitate knowledge transfer from previously learned tasks to current learning tasks through a novel meta-learning objective. Extensive experiments in both simulated and real-world environments demonstrate that OMLA can lead to better adaptation performances compared to the baseline methods. The project link: https://ricky-zhu.github.io/OMLA/.","['Ruiqi Zhu', 'Endong Sun', 'Guanhe Huang', 'Oya Celiktutan']",2025-03-24T13:55:47Z,http://arxiv.org/abs/2503.18684v2,Education & Social Science,Adaptive Learning,"in this paper, we propose Online Meta-Learned adapters (OMLA) the project link: https://ricky-zhu.github.io/OMLA/."
Performance comparison of an AI-based Adaptive Learning System in China,"Adaptive learning systems stand apart from traditional learning systems by offering a personalized learning experience to students according to their different knowledge states. Adaptive systems collect and analyse students' behavior data, update learner profiles, then accordingly provide timely individualized feedback to each student. Such interactions between the learning system and students can improve the engagement of students and the efficiency of learning. This paper evaluates the effectiveness of an adaptive learning system, ""Yixue Squirrel AI"" (or Yixue), on English and math learning in middle school. The effectiveness of the Yixue's math and English learning systems is respectively compared against (1) traditional classroom math instruction conducted by expert human teachers and (2) BOXFiSH, another adaptive learning platform for English language learning. Results suggest that students achieved better performance using Yixue adaptive learning system than both traditional classroom instruction by expert teachers and another adaptive learning platform.","['Wei Cui', 'Zhen Xue', 'Khanh-Phuong Thai']",2019-01-29T13:21:03Z,http://arxiv.org/abs/1901.10268v1,Education & Social Science,Adaptive Learning,this paper evaluates the effectiveness of an adaptive learning system . Yixue's math and english learning systems are compared against traditional classroom instruction . results suggest that students achieved better performance using Yiixue .
An Adaptive Strategy for Active Learning with Smooth Decision Boundary,"We present the first adaptive strategy for active learning in the setting of classification with smooth decision boundary. The problem of adaptivity (to unknown distributional parameters) has remained opened since the seminal work of Castro and Nowak (2007), which first established (active learning) rates for this setting. While some recent advances on this problem establish adaptive rates in the case of univariate data, adaptivity in the more practical setting of multivariate data has so far remained elusive. Combining insights from various recent works, we show that, for the multivariate case, a careful reduction to univariate-adaptive strategies yield near-optimal rates without prior knowledge of distributional parameters.","['Andrea Locatelli', 'Alexandra Carpentier', 'Samory Kpotufe']",2017-11-25T21:23:46Z,http://arxiv.org/abs/1711.09294v1,Education & Social Science,Adaptive Learning,the problem of adaptivity (to unknown distributional parameters) has remained open since the seminal work of Castro and Nowak (2007) . adaptivity in the more practical setting of multivariate data has so far remained elusive .
Automatic Domain Adaptation by Transformers in In-Context Learning,"Selecting or designing an appropriate domain adaptation algorithm for a given problem remains challenging. This paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset in the in-context learning framework, where a foundation model performs new tasks without updating its parameters at test time. Specifically, we prove that Transformers can approximate instance-based and feature-based unsupervised domain adaptation algorithms and automatically select an algorithm suited for a given dataset. Numerical results indicate that in-context learning demonstrates an adaptive domain adaptation surpassing existing methods.","['Ryuichiro Hataya', 'Kota Matsui', 'Masaaki Imaizumi']",2024-05-27T04:33:53Z,http://arxiv.org/abs/2405.16819v1,Education & Social Science,Adaptive Learning,this paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset . a foundation model performs new tasks without updating its parameters at test time . in-context learning demonstrates an adaptive domain adaptation exceeding existing methods .
Adaptation to Unknown Situations as the Holy Grail of Learning-Based   Self-Adaptive Systems: Research Directions,"Self-adaptive systems continuously adapt to changes in their execution environment. Capturing all possible changes to define suitable behaviour beforehand is unfeasible, or even impossible in the case of unknown changes, hence human intervention may be required. We argue that adapting to unknown situations is the ultimate challenge for self-adaptive systems. Learning-based approaches are used to learn the suitable behaviour to exhibit in the case of unknown situations, to minimize or fully remove human intervention. While such approaches can, to a certain extent, generalize existing adaptations to new situations, there is a number of breakthroughs that need to be achieved before systems can adapt to general unknown and unforeseen situations. We posit the research directions that need to be explored to achieve unanticipated adaptation from the perspective of learning-based self-adaptive systems. At minimum, systems need to define internal representations of previously unseen situations on-the-fly, extrapolate the relationship to the previously encountered situations to evolve existing adaptations, and reason about the feasibility of achieving their intrinsic goals in the new set of conditions. We close discussing whether, even when we can, we should indeed build systems that define their own behaviour and adapt their goals, without involving a human supervisor.","['Ivana Dusparic', 'Nicolas Cardozo']",2021-03-11T19:07:02Z,http://arxiv.org/abs/2103.06908v1,Education & Social Science,Adaptive Learning,self-adaptive systems continuously adapt to changes in their execution environment . human intervention may be required in the case of unknown changes . learning-based approaches are used to learn the suitable behaviour to exhibit .
Transfer Learning and Meta Learning Based Fast Downlink Beamforming   Adaptation,"This paper studies fast adaptive beamforming optimization for the signal-to-interference-plus-noise ratio balancing problem in a multiuser multiple-input single-output downlink system. Existing deep learning based approaches to predict beamforming rely on the assumption that the training and testing channels follow the same distribution which may not hold in practice. As a result, a trained model may lead to performance deterioration when the testing network environment changes. To deal with this task mismatch issue, we propose two offline adaptive algorithms based on deep transfer learning and meta-learning, which are able to achieve fast adaptation with the limited new labelled data when the testing wireless environment changes. Furthermore, we propose an online algorithm to enhance the adaptation capability of the offline meta algorithm in realistic non-stationary environments. Simulation results demonstrate that the proposed adaptive algorithms achieve much better performance than the direct deep learning algorithm without adaptation in new environments. The meta-learning algorithm outperforms the deep transfer learning algorithm and achieves near optimal performance. In addition, compared to the offline meta-learning algorithm, the proposed online meta-learning algorithm shows superior adaption performance in changing environments.","['Yi Yuan', 'Gan Zheng', 'Kai-Kit Wong', 'Björn Ottersten', 'Zhi-Quan Luo']",2020-11-02T11:30:04Z,http://arxiv.org/abs/2011.00903v1,Education & Social Science,Adaptive Learning,this paper studies fast adaptive beamforming optimization for the signal-to-interference-plus-noise ratio balancing problem . existing deep learning based approaches to predict beamforming rely on the assumption that training and testing channels follow the same distribution .
Adaptive Interaction Using the Adaptive Agent Oriented Software   Architecture (AAOSA),"User interfaces that adapt their characteristics to those of the user are referred to as adaptive interfaces. We propose Adaptive Agent Oriented Software Architecture (AAOSA) as a new way of designing adaptive interfaces. AAOSA is a new approach to software design based on an agent-oriented architecture. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is responsible for the independent specialized processes of the agent. A distributed learning policy that makes use of this architecture is used for purposes of system adaptability.","['Babak Hodjat', 'Makoto Amamiya']",1998-12-11T23:05:51Z,http://arxiv.org/abs/cs/9812015v1,Education & Social Science,Adaptive Learning,a new approach to software design based on an agent-oriented architecture is proposed . agents are considered adaptively communicating concurrent modules . a distributed learning policy that makes use of this architecture is used for purposes of system adaptability .
Self-Supervised Meta-Learning for All-Layer DNN-Based Adaptive Control   with Stability Guarantees,"A critical goal of adaptive control is enabling robots to rapidly adapt in dynamic environments. Recent studies have developed a meta-learning-based adaptive control scheme, which uses meta-learning to extract nonlinear features (represented by Deep Neural Networks (DNNs)) from offline data, and uses adaptive control to update linear coefficients online. However, such a scheme is fundamentally limited by the linear parameterization of uncertainties and does not fully unleash the capability of DNNs. This paper introduces a novel learning-based adaptive control framework that pretrains a DNN via self-supervised meta-learning (SSML) from offline trajectories and online adapts the full DNN via composite adaptation. In particular, the offline SSML stage leverages the time consistency in trajectory data to train the DNN to predict future disturbances from history, in a self-supervised manner without environment condition labels. The online stage carefully designs a control law and an adaptation law to update the full DNN with stability guarantees. Empirically, the proposed framework significantly outperforms (19-39%) various classic and learning-based adaptive control baselines, in challenging real-world quadrotor tracking problems under large dynamic wind disturbance.","['Guanqi He', 'Yogita Choudhary', 'Guanya Shi']",2024-10-10T03:23:44Z,http://arxiv.org/abs/2410.07575v2,Education & Social Science,Adaptive Learning,a meta-learning-based adaptive control scheme has been developed . it extracts nonlinear features (represented by Deep Neural Networks)) from offline data . the scheme does not fully unleash the capability of DNNs .
Knowledge Adaptation as Posterior Correction,"Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.",['Mohammad Emtiyaz Khan'],2025-06-17T07:22:32Z,http://arxiv.org/abs/2506.14262v1,Education & Social Science,Adaptive Learning,"despite progress on model adaptation, little is known about how machines can adapt . we show that all such adaptation methods can be seen as different ways of correcting' posteriors . more accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation ."
Reducing Large Adaptation Spaces in Self-Adaptive Systems Using Machine   Learning,"Modern software systems often have to cope with uncertain operation conditions, such as changing workloads or fluctuating interference in a wireless network. To ensure that these systems meet their goals these uncertainties have to be mitigated. One approach to realize this is self-adaptation that equips a system with a feedback loop. The feedback loop implements four core functions -- monitor, analyze, plan, and execute -- that share knowledge in the form of runtime models. For systems with a large number of adaptation options, i.e., large adaptation spaces, deciding which option to select for adaptation may be time consuming or even infeasible within the available time window to make an adaptation decision. This is particularly the case when rigorous analysis techniques are used to select adaptation options, such as formal verification at runtime, which is widely adopted. One technique to deal with the analysis of a large number of adaptation options is reducing the adaptation space using machine learning. State of the art has showed the effectiveness of this technique, yet, a systematic solution that is able to handle different types of goals is lacking. In this paper, we present ML2ASR+, short for Machine Learning to Adaptation Space Reduction Plus. Central to ML2ASR+ is a configurable machine learning pipeline that supports effective analysis of large adaptation spaces for threshold, optimization, and setpoint goals. We evaluate ML2ASR+ for two applications with different sizes of adaptation spaces: an Internet-of-Things application and a service-based system. The results demonstrate that ML2ASR+ can be applied to deal with different types of goals and is able to reduce the adaptation space and hence the time to make adaptation decisions with over 90%, with negligible effect on the realization of the adaptation goals.","['Federico Quin', 'Danny Weyns', 'Omid Gheibi']",2023-06-02T09:49:33Z,http://arxiv.org/abs/2306.01404v1,Education & Social Science,Adaptive Learning,machine learning can reduce the time to make adaptation decisions with over 90% . the results show that ML2ASR+ can be applied to deal with different types of goals . a machine learning pipeline can be used to analyze large adaptation spaces .
Building a Classification Model for Enrollment In Higher Educational   Courses using Data Mining Techniques,"Data Mining is the process of extracting useful patterns from the huge amount of database and many data mining techniques are used for mining these patterns. Recently, one of the remarkable facts in higher educational institute is the rapid growth data and this educational data is expanding quickly without any advantage to the educational management. The main aim of the management is to refine the education standard; therefore by applying the various data mining techniques on this data one can get valuable information. This research study proposed the ""classification model for the student's enrollment process in higher educational courses using data mining techniques"". Additionally, this study contributes to finding some patterns that are meaningful to management.",['Priyanka Saini'],2014-05-15T02:53:44Z,http://arxiv.org/abs/1405.3729v1,Education & Social Science,Education Data Mining,data mining is the process of extracting useful patterns from the huge amount of database . the main aim of the educational management is to refine the education standard . by applying the various data mining techniques one can get valuable information .
Data-Mining Research in Education,"As an interdisciplinary discipline, data mining (DM) is popular in education area especially when examining students' learning performances. It focuses on analyzing educational related data to develop models for improving learners' learning experiences and enhancing institutional effectiveness. Therefore, DM does help education institutions provide high-quality education for its learners. Applying data mining in education also known as educational data mining (EDM), which enables to better understand how students learn and identify how improve educational outcomes. Present paper is designed to justify the capabilities of data mining approaches in the filed of education. The latest trends on EDM research are introduced in this review. Several specific algorithms, methods, applications and gaps in the current literature and future insights are discussed here.",['Jiechao Cheng'],2017-03-28T06:14:11Z,http://arxiv.org/abs/1703.10117v2,Education & Social Science,Education Data Mining,data mining (DM) is popular in education area especially when examining students' learning performances . it focuses on analyzing educational related data to develop models for improving learners' learning experiences .
Discovering Knowledge from Multi-modal Lecture Recordings,"Educational media mining is the process of converting raw media data from educational systems to useful information that can be used to design learning systems, answer research questions and allow personalized learning experiences. Knowledge discovery encompasses a wide range of techniques ranging from database queries to more recent developments in machine learning and language technology. Educational media mining techniques are now being used in IT Services research worldwide. Multi-modal Lecture Recordings is one of the important types of educational media and this paper explores the research challenges for mining lecture recordings for the efficient personalized learning experiences. Keywords: Educational Media Mining; Lecture Recordings, Multimodal Information System, Personalized Learning; Online Course Ware; Skills and Competences;","['Rajkumar Kannan', 'Christian Guetl']",2010-01-04T05:44:57Z,http://arxiv.org/abs/1001.0443v1,Education & Social Science,Education Data Mining,"educational media mining is the process of converting raw media data into useful information . it can be used to design learning systems, answer research questions and allow personalized learning experiences . this paper explores the research challenges for mining lecture recordings ."
Text mining in education,"The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.","['R. Ferreira-Mello', 'M. Andre', 'A. Pinheiro', 'E. Costa', 'C. Romero']",2024-02-11T11:41:25Z,http://arxiv.org/abs/2403.00769v1,Education & Social Science,Education Data Mining,the growth of online education environments is generating a massive volume of text data . it produces exciting challenges on how to mine text data to find useful knowledge for educational stakeholders . this work presents a systematic overview of the current status of the educational text mining field .
Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title data mining in education. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.","['C. Romero', 'S. Ventura']",2024-02-10T18:48:45Z,http://arxiv.org/abs/2402.07956v1,Education & Social Science,Education Data Mining,"this survey reviews how educational data mining and learning analytics have been applied over educational data . in the last decade, this research area has evolved enormously . a wide range of related terms are now used in the bibliography ."
Big Data and Education: using big data analytics in language learning,"Working with big data using data mining tools is rapidly becoming a trend in education industry. The combination of the current capacity to collect, store, manage and process data in a timely manner, and data from online educational platforms represents an unprecedented opportunity for educational institutes, learners, educators, and researchers. In this position paper, we consider some basic concepts as well as most popular tools, methods and techniques regarding Educational Data Mining and Learning Analytics, and discuss big data applications in language learning, in particular.",['Vahid Ashrafimoghari'],2022-07-19T19:17:10Z,http://arxiv.org/abs/2207.10572v1,Education & Social Science,Education Data Mining,data mining tools are rapidly becoming a trend in the education industry . data from online educational platforms represents an unprecedented opportunity for educational institutes . this position paper considers some basic concepts and most popular tools .
A Review of Data Mining in Personalized Education: Current Trends and   Future Prospects,"Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.","['Zhang Xiong', 'Haoxuan Li', 'Zhuang Liu', 'Zhuofan Chen', 'Hao Zhou', 'Wenge Rong', 'Yuanxin Ouyang']",2024-02-27T06:09:48Z,http://arxiv.org/abs/2402.17236v1,Education & Social Science,Education Data Mining,"this paper presents a structured taxonomy for each area . it compiles commonly used datasets, and identifies future research directions . the paper focuses on educational recommendation, cognitive diagnosis and knowledge tracing ."
An Effective Learning Management System for Revealing Student   Performance Attributes,"A learning management system streamlines the management of the teaching process in a centralized place, recording, tracking, and reporting the delivery of educational courses and student performance. Educational knowledge discovery from such an e-learning system plays a crucial role in rule regulation, policy establishment, and system development. However, existing LMSs do not have embedded mining modules to directly extract knowledge. As educational modes become more complex, educational data mining efficiency from those heterogeneous student learning behaviours is gradually degraded. Therefore, an LMS incorporated with an advanced educational mining module is proposed in this study, as a means to mine efficiently from student performance records to provide valuable insights for educators in helping plan effective learning pedagogies, improve curriculum design, and guarantee quality of teaching. Through two illustrative case studies, experimental results demonstrate increased mining efficiency of the proposed mining module without information loss compared to classic educational mining algorithms. The mined knowledge reveals a set of attributes that significantly impact student academic performance, and further classification evaluation validates the identified attributes. The design and application of such an effective LMS can enable educators to learn from past student performance experiences, empowering them to guide and intervene with students in time, and eventually improve their academic success.","['Xinyu Zhang', 'Vincent CS Lee', 'Duo Xu', 'Jun Chen', 'Mohammad S. Obaidat']",2024-03-05T03:56:49Z,http://arxiv.org/abs/2403.13822v1,Education & Social Science,Education Data Mining,a learning management system streamlines the management of the teaching process in a centralized place . existing LMSs do not have embedded mining modules to directly extract knowledge . an LMS incorporated with an advanced educational mining module is proposed .
Data Mining as a Torch Bearer in Education Sector,"Every data has a lot of hidden information. The processing method of data decides what type of information data produce. In India education sector has a lot of data that can produce valuable information. This information can be used to increase the quality of education. But educational institution does not use any knowledge discovery process approach on these data. Information and communication technology puts its leg into the education sector to capture and compile low cost information. Now a day a new research community, educational data mining (EDM), is growing which is intersection of data mining and pedagogy. In this paper we present roadmap of research done in EDM in various segment of education sector.","['Umesh Kumar Pandey', 'Brijesh Kumar Bhardwaj', 'Saurabh pal']",2012-01-25T04:50:58Z,http://arxiv.org/abs/1201.5182v1,Education & Social Science,Education Data Mining,"in india education sector has a lot of data that can produce valuable information . but educational institution does not use any knowledge discovery process approach on these data . now a day a new research community, educational data mining (EDM), is growing ."
Role of Data Mining in Nigerian Tertiary Education Sector,"Over a decade there has been a rapid growth in Nigerian educational system particularly higher education. Various institutions have come up both from public and private sector offering many of courses both under and post graduate students. Therefore, rates of students enroll for higher educational institutions in Nigeria have also increased. Hence it is very important to understand the roles play by data mining in analyzing the collected data of students and their academic progression. It is a concern for today's education system and this gap has to be identified and properly addressed to the learning community. Data Mining it helps in various ways to resolve issues face in predictions students and staff performances within Nigerian education system. This paperwork we discuss the roles of Data Mining tools and techniques which can be used effectively in resolving issues in some functional unit of Nigerian tertiary institutions.","['Dauda Abdu', 'Almustapha Abdullahi Wakili', 'Lawan Nasiru', 'Buhari Ubale']",2024-11-07T19:59:27Z,http://arxiv.org/abs/2411.15152v1,Education & Social Science,Education Data Mining,over a decade there has been a rapid growth in Nigerian educational system . rates of students enroll for higher educational institutions in Nigeria have also increased . it is very important to understand the roles play by data mining in analyzing the collected data of students .
Mining Educational Data to Analyze Students' Performance,"The main objective of higher education institutions is to provide quality education to its students. One way to achieve highest level of quality in higher education system is by discovering knowledge for prediction regarding enrolment of students in a particular course, alienation of traditional classroom teaching model, detection of unfair means used in online examination, detection of abnormal values in the result sheets of the students, prediction about students' performance and so on. The knowledge is hidden among the educational data set and it is extractable through data mining techniques. Present paper is designed to justify the capabilities of data mining techniques in context of higher education by offering a data mining model for higher education system in the university. In this research, the classification task is used to evaluate student's performance and as there are many approaches that are used for data classification, the decision tree method is used here. By this task we extract knowledge that describes students' performance in end semester examination. It helps earlier in identifying the dropouts and students who need special attention and allow the teacher to provide appropriate advising/counseling. Keywords-Educational Data Mining (EDM); Classification; Knowledge Discovery in Database (KDD); ID3 Algorithm.","['Brijesh Kumar Baradwaj', 'Saurabh Pal']",2012-01-17T03:34:50Z,http://arxiv.org/abs/1201.3417v1,Education & Social Science,Education Data Mining,present paper is designed to justify the capabilities of data mining techniques . classification task is used to evaluate student's performance . knowledge that describes students' performance in end semester examination helps earlier in identifying the dropouts .
A Systematic Literature Review of Undergraduate Data Science Education   Research,"The presence of data science has been profound in the scientific community in almost every discipline. An important part of the data science education expansion has been at the undergraduate level. We conducted a systematic literature review to (1) portray current evidence and knowledge gaps in self-proclaimed undergraduate data science education research and (2) inform policymakers and the data science education community about what educators may encounter when searching for literature using the general keyword 'data science education.' While open-access publications that target a broader audience of data science educators and include multiple examples of data science programs and courses are a strength, significant knowledge gaps remain. The undergraduate data science literature that we identified often lacks empirical data, research questions and reproducibility. Certain disciplines are less visible. We recommend that we should (1) cherish data science as an interdisciplinary field; (2) adopt a consistent set of keywords/terminology to ensure data science education literature is easily identifiable; (3) prioritize investments in empirical studies.","['Mine Dogucu', 'Sinem Demirci', 'Harry Bendekgey', 'Federica Zoe Ricci', 'Catalina M. Medina']",2024-03-06T00:49:08Z,http://arxiv.org/abs/2403.03387v2,Education & Social Science,Education Data Mining,"the undergraduate data science literature that we identified often lacks empirical data, research questions and reproducibility . certain disciplines are less visible ."
"Sequential pattern mining in educational data: The application context,   potential, strengths, and limitations","Increasingly, researchers have suggested the benefits of temporal analysis to improve our understanding of the learning process. Sequential pattern mining (SPM), as a pattern recognition technique, has the potential to reveal the temporal aspects of learning and can be a valuable tool in educational data science. However, its potential is not well understood and exploited. This chapter addresses this gap by reviewing work that utilizes sequential pattern mining in educational contexts. We identify that SPM is suitable for mining learning behaviors, analyzing and enriching educational theories, evaluating the efficacy of instructional interventions, generating features for prediction models, and building educational recommender systems. SPM can contribute to these purposes by discovering similarities and differences in learners' activities and revealing the temporal change in learning behaviors. As a sequential analysis method, SPM can reveal unique insights about learning processes and be powerful for self-regulated learning research. It is more flexible in capturing the relative arrangement of learning events than the other sequential analysis methods. Future research may improve its utility in educational data science by developing tools for counting pattern occurrences as well as identifying and removing unreliable patterns. Future work needs to establish a systematic guideline for data preprocessing, parameter setting, and interpreting sequential patterns.","['Yingbin Zhang', 'Luc Paquette']",2023-02-03T06:56:31Z,http://arxiv.org/abs/2302.01932v1,Education & Social Science,Education Data Mining,researchers have suggested the benefits of temporal analysis to improve learning . but its potential is not well understood and exploited . this chapter reviews work that utilizes sequential pattern mining in educational contexts .
A Systematic Review on Process Mining for Curricular Analysis,"Educational Process Mining (EPM) is a data analysis technique that is used to improve educational processes. It is based on Process Mining (PM), which involves gathering records (logs) of events to discover process models and analyze the data from a process-centric perspective. One specific application of EPM is curriculum mining, which focuses on understanding the learning program students follow to achieve educational goals. This is important for institutional curriculum decision-making and quality improvement. Therefore, academic institutions can benefit from organizing the existing techniques, capabilities, and limitations. We conducted a systematic literature review to identify works on applying PM to curricular analysis and provide insights for further research. We reviewed 27 primary studies published across seven major databases. Our analysis classified these studies into five main research objectives: discovery of educational trajectories, identification of deviations in student behavior, bottleneck analysis, dropout / stopout analysis, and generation of recommendations. Key findings highlight challenges such as standardization to facilitate cross-university analysis, better integration of process and data mining techniques, and improved tools for educational stakeholders. This review provides a comprehensive overview of the current landscape in curricular process mining and outlines specific research opportunities to support more robust and actionable curricular analyses in educational settings.","['Daniel Calegari', 'Andrea Delgado']",2024-09-13T21:35:11Z,http://arxiv.org/abs/2409.09204v2,Education & Social Science,Education Data Mining,educational process mining (EPM) is a data analysis technique used to improve educational processes . a systematic literature review identified works on applying PM to curricular analysis . key findings highlight challenges such as standardization to facilitate cross-university analyses .
Applicability of Educational Data Mining in Afghanistan: Opportunities   and Challenges,"The author's own experience as a student and later as a lecturer in Afghanistan has shown that the methods used in the educational system are not only flawed, but also do not provide the minimum guidance to students to select proper course of study before they enter the national university entrance (Kankor) exam. Thus, it often results in high attrition rates and poor performance in higher education.   Based on the studies done in other countries, and by the author of this paper through online questionnaires distributed to university students in Afghanistan - it was found that proper procedures and specialized studies in high schools can help students in selecting their major field of study more systematically.   Additionally, it has come to be known that there are large amounts of data available for mining purposes, but the methods that the Ministry of Education and Ministry of Higher Education use to store and produce their data, only enable them to achieve simple facts and figures. Furthermore, from the results it can be concluded that there are potential opportunities for educational data mining application in the domain of Afghanistan's education systems. Finally, this study will provide the readers with approaches for using Educational Data Mining to improve the educational business processes. For instance, predict proper field of study for high school graduates, or, identify first year university students who are at high risk of attrition.",['Abdul Rahman Sherzad'],2017-03-02T19:45:45Z,http://arxiv.org/abs/1703.04523v1,Education & Social Science,Education Data Mining,author's own experience as a student and lecturer in Afghanistan has shown that the methods used in the educational system are flawed . it often results in high attrition rates and poor performance in higher education . proper procedures and specialized studies in high schools can help students in selecting their major field of study more systematically .
Framework for Opinion Mining Approach to Augment Education System   Performance,"The extensive expansion growth of social networking sites allows the people to share their views and experiences freely with their peers on internet. Due to this, huge amount of data is generated on everyday basis which can be used for the opinion mining to extract the views of people in a particular field. Opinion mining finds its applications in many areas such as Tourism, Politics, education and entertainment, etc. It has not been extensively implemented in area of education system. This paper discusses the malpractices in the present examination system. In the present scenario, Opinion mining is vastly used for decision making. The authors of this paper have designed a framework by applying Na\""ive Bayes approach to the education dataset. The various phases of Na\""ive Bayes approach include three steps: conversion of data into frequency table, making classes of dataset and apply the Na\""ive Bayes algorithm equation to calculate the probabilities of classes. Finally the highest probability class is the outcome of this prediction. These predictions are used to make improvements in the education system and help to provide better education.","['Amritpal Kaur', 'Harkiran Kaur']",2018-06-25T04:17:44Z,http://arxiv.org/abs/1806.09279v1,Education & Social Science,Education Data Mining,"opinion mining finds its applications in many areas such as tourism, politics, education and entertainment . this paper discusses the malpractices in the present examination system ."
Educational data mining using jmp,"Educational Data Mining is a growing trend in case of higher education. The quality of the Educational Institute may be enhanced through discovering hidden knowledge from the student databases/ data warehouses. Present paper is designed to carry out a comparative study with the TDC (Three Year Degree) Course students of different colleges affiliated to Dibrugarh University. The study is conducted with major subject wise, gender wise and category/caste wise. The experimental results may be visualized with Scatterplot3D, Bubble Plot, Fit Y by X, Run Chart, Control Chart etc. of the SAS JMP Software.","['Sadiq Hussain', 'G. C. Hazarika']",2014-11-08T05:11:01Z,http://arxiv.org/abs/1411.2081v1,Education & Social Science,Education Data Mining,"the paper is designed to carry out a comparative study with the TDC (Three Year Degree) Course students . the experimental results may be visualized with Scatterplot3D, Bubble Plot, Fit Y by X, Run Chart, Control Chart etc."
Epistemological Issues in Educational Data Mining,"Educational Data Mining (EDM) shows interesting scientific results lately. However, little has been discussed about philosophical questions regarding the type of knowledge produced in this area. This paper aims to present two epistemological issues in EDM: (i) a question of ontological nature about the content of the knowledge obtained; and (ii) a question of deontological nature, about the guidelines and principles adopted by the researcher in education, to the detriment of the results of his own research. In the end, some considerations and guidelines are outlined as a result of the discussion of the issues raised.",['Esdras Lins Bispo Jr'],2019-09-04T13:56:31Z,http://arxiv.org/abs/1909.01806v1,Education & Social Science,Education Data Mining,"educational data mining (EDM) shows interesting scientific results lately . but, little has been discussed about philosophical questions regarding the type of knowledge produced . this paper aims to present two epistemological issues in EDM ."
Towards Mining Creative Thinking Patterns from Educational Data,"Creativity, i.e., the process of generating and developing fresh and original ideas or products that are useful or effective, is a valuable skill in a variety of domains. Creativity is called an essential 21st-century skill that should be taught in schools. The use of educational technology to promote creativity is an active study field, as evidenced by several studies linking creativity in the classroom to beneficial learning outcomes. Despite the burgeoning body of research on adaptive technology for education, mining creative thinking patterns from educational data remains a challenging task. In this paper, to address this challenge, we put the first step towards formalizing educational knowledge by constructing a domain-specific Knowledge Base to identify essential concepts, facts, and assumptions in identifying creative patterns. We then introduce a pipeline to contextualize the raw educational data, such as assessments and class activities. Finally, we present a rule-based approach to learning from the Knowledge Base, and facilitate mining creative thinking patterns from contextualized data and knowledge. We evaluate our approach with real-world datasets and highlight how the proposed pipeline can help instructors understand creative thinking patterns from students' activities and assessment tasks.",['Nasrin Shabani'],2022-10-12T12:24:49Z,http://arxiv.org/abs/2210.06118v1,Education & Social Science,Education Data Mining,creativity is called an essential 21st-century skill that should be taught in schools . the use of educational technology to promote creativity is an active study field . mining creative thinking patterns from educational data remains a challenging task .
Data Mining Applications: A comparative Study for Predicting Student's   performance,Knowledge Discovery and Data Mining (KDD) is a multidisciplinary area focusing upon methodologies for extracting useful knowledge from data and there are several useful KDD tools to extracting the knowledge. This knowledge can be used to increase the quality of education. But educational institution does not use any knowledge discovery process approach on these data. Data mining can be used for decision making in educational system. A decision tree classifier is one of the most widely used supervised learning methods used for data exploration based on divide & conquer technique. This paper discusses use of decision trees in educational data mining. Decision tree algorithms are applied on students' past performance data to generate the model and this model can be used to predict the students' performance. It helps earlier in identifying the dropouts and students who need special attention and allow the teacher to provide appropriate advising/counseling.,"['Surjeet Kumar Yadav', 'Brijesh Bharadwaj', 'Saurabh Pal']",2012-02-22T04:15:54Z,http://arxiv.org/abs/1202.4815v2,Education & Social Science,Education Data Mining,data mining can be used for decision making in educational system . a decision tree classifier is one of the most widely used supervised learning methods used for data exploration based on divide & conquer technique .
What Explains Teachers' Trust of AI in Education across Six Countries?,"With growing expectations to use AI-based educational technology (AI-EdTech) to improve students' learning outcomes and enrich teaching practice, teachers play a central role in the adoption of AI-EdTech in classrooms. Teachers' willingness to accept vulnerability by integrating technology into their everyday teaching practice, that is, their trust in AI-EdTech, will depend on how much they expect it to benefit them versus how many concerns it raises for them. In this study, we surveyed 508 K-12 teachers across six countries on four continents to understand which teacher characteristics shape teachers' trust in AI-EdTech, and its proposed antecedents, perceived benefits and concerns about AI-EdTech. We examined a comprehensive set of characteristics including demographic and professional characteristics (age, gender, subject, years of experience, etc.), cultural values (Hofstede's cultural dimensions), geographic locations (Brazil, Israel, Japan, Norway, Sweden, USA), and psychological factors (self-efficacy and understanding). Using multiple regression analysis, we found that teachers with higher AI-EdTech self-efficacy and AI understanding perceive more benefits, fewer concerns, and report more trust in AI-EdTech. We also found geographic and cultural differences in teachers' trust in AI-EdTech, but no demographic differences emerged based on their age, gender, or level of education. The findings provide a comprehensive, international account of factors associated with teachers' trust in AI-EdTech. Efforts to raise teachers' understanding of, and trust in AI-EdTech, while considering their cultural values are encouraged to support its adoption in K-12 education.","['Olga Viberg', 'Mutlu Cukurova', 'Yael Feldman-Maggor', 'Giora Alexandron', 'Shizuka Shirai', 'Susumu Kanemune', 'Barbara Wasson', 'Cathrine Tømte', 'Daniel Spikol', 'Marcelo Milrad', 'Raquel Coelho', 'René F. Kizilcec']",2023-12-04T05:07:23Z,http://arxiv.org/abs/2312.01627v2,Education & Social Science,Misinformation in EdTech,"teachers' trust in AI-edTech will depend on how much they expect it to benefit them . teachers with higher self-efficacy and AI understanding perceive more benefits . no demographic differences emerged based on age, gender, or level of education ."
"""Don't Forget the Teachers"": Towards an Educator-Centered Understanding   of Harms from Large Language Models in Education","Education technologies (edtech) are increasingly incorporating new features built on large language models (LLMs), with the goals of enriching the processes of teaching and learning and ultimately improving learning outcomes. However, the potential downstream impacts of LLM-based edtech remain understudied. Prior attempts to map the risks of LLMs have not been tailored to education specifically, even though it is a unique domain in many respects: from its population (students are often children, who can be especially impacted by technology) to its goals (providing the correct answer may be less important for learners than understanding how to arrive at an answer) to its implications for higher-order skills that generalize across contexts (e.g., critical thinking and collaboration). We conducted semi-structured interviews with six edtech providers representing leaders in the K-12 space, as well as a diverse group of 23 educators with varying levels of experience with LLM-based edtech. Through a thematic analysis, we explored how each group is anticipating, observing, and accounting for potential harms from LLMs in education. We find that, while edtech providers focus primarily on mitigating technical harms, i.e., those that can be measured based solely on LLM outputs themselves, educators are more concerned about harms that result from the broader impacts of LLMs, i.e., those that require observation of interactions between students, educators, school systems, and edtech to measure. Overall, we (1) develop an education-specific overview of potential harms from LLMs, (2) highlight gaps between conceptions of harm by edtech providers and those by educators, and (3) make recommendations to facilitate the centering of educators in the design and development of edtech tools.","['Emma Harvey', 'Allison Koenecke', 'Rene F. Kizilcec']",2025-02-20T14:27:24Z,http://arxiv.org/abs/2502.14592v1,Education & Social Science,Misinformation in EdTech,education technologies (edtech) are increasingly incorporating new features built on large language models (LLMs) the potential downstream impacts of LLMs remain understudied . we conducted semi-structured interviews with six ed tech providers representing leaders in the K-12 space.
"Trust, Because You Can't Verify:Privacy and Security Hurdles in   Education Technology Acquisition Practices","The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs). This growth brings enormous complexity. Protecting the extensive data collected by these tools is crucial for HEIs as data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools. This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.   To address this gap, we conducted a semi-structured interview study with 13 participants who are in EdTech leadership roles at seven HEIs. Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons. We discuss certain observations about the status quo and conclude with recommendations for HEIs, researchers, and regulatory bodies to improve the situation.","['Easton Kelso', 'Ananta Soneji', 'Sazzadur Rahaman', 'Yan Soshitaishvili', 'Rakibul Hasan']",2024-05-20T01:15:57Z,http://arxiv.org/abs/2405.11712v2,Education & Social Science,Misinformation in EdTech,the education technology (EdTech) landscape is expanding rapidly in higher education institutes . a semi-structured interview study conducted with 13 participants who are in EdTech leadership roles . the study uncovers the acquisition process in the HEI context .
Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated   Misinformation in the Medical Domain,"The pervasive influence of misinformation has far-reaching and detrimental effects on both individuals and society. The COVID-19 pandemic has witnessed an alarming surge in the dissemination of medical misinformation. However, existing datasets pertaining to misinformation predominantly focus on textual information, neglecting the inclusion of visual elements, and tend to center solely on COVID-19-related misinformation, overlooking misinformation surrounding other diseases. Furthermore, the potential of Large Language Models (LLMs), such as the ChatGPT developed in late 2022, in generating misinformation has been overlooked in previous works. To overcome these limitations, we present Med-MMHL, a novel multi-modal misinformation detection dataset in a general medical domain encompassing multiple diseases. Med-MMHL not only incorporates human-generated misinformation but also includes misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate comprehensive research and development of methodologies for detecting misinformation across diverse diseases and various scenarios, including human and LLM-generated misinformation detection at the sentence, document, and multi-modal levels. To access our dataset and code, visit our GitHub repository: \url{https://github.com/styxsys0927/Med-MMHL}.","['Yanshen Sun', 'Jianfeng He', 'Shuo Lei', 'Limeng Cui', 'Chang-Tien Lu']",2023-06-15T05:59:11Z,http://arxiv.org/abs/2306.08871v1,Education & Social Science,Misinformation in EdTech,"the COVID-19 pandemic has witnessed an alarming surge in the dissemination of medical misinformation . existing datasets pertaining to misinformation focus on textual information, neglecting the inclusion of visual elements . to overcome these limitations, we present a novel multi-modal misinformation detection dataset in a general medical domain"
The Dynamics of (Not) Unfollowing Misinformation Spreaders,"Many studies explore how people 'come into' misinformation exposure. But much less is known about how people 'come out of' misinformation exposure. Do people organically sever ties to misinformation spreaders? And what predicts doing so? Over six months, we tracked the frequency and predictors of ~900K followers unfollowing ~5K health misinformation spreaders on Twitter. We found that misinformation ties are persistent. Monthly unfollowing rates are just 0.52%. In other words, 99.5% of misinformation ties persist each month. Users are also 31% more likely to unfollow non-misinformation spreaders than they are to unfollow misinformation spreaders. Although generally infrequent, the factors most associated with unfollowing misinformation spreaders are (1) redundancy and (2) ideology. First, users initially following many spreaders, or who follow spreaders that tweet often, are most likely to unfollow later. Second, liberals are more likely to unfollow than conservatives. Overall, we observe a strong persistence of misinformation ties. The fact that users rarely unfollow misinformation spreaders suggests a need for external nudges and the importance of preventing exposure from arising in the first place.","['Joshua Ashkinaze', 'Eric Gilbert', 'Ceren Budak']",2024-01-24T14:28:55Z,http://arxiv.org/abs/2401.13480v2,Education & Social Science,Misinformation in EdTech,900K followers unfollowed 5K health misinformation spreaders on twitter . monthly unfollowing rates are just 0.52% - 99.5% of misinformation ties persist each month . users are 31% more likely to unfollow non-misinformation spreader than they are .
Improve Academic Query Resolution through BERT-based Question Extraction   from Images,"Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.","['Nidhi Kamal', 'Saurabh Yadav', 'Jorawar Singh', 'Aditi Avasthi']",2024-04-28T19:11:08Z,http://arxiv.org/abs/2405.01587v1,Education & Social Science,Misinformation in EdTech,a method for extracting questions from text or images is proposed . it aims to improve the accuracy and efficiency of student query resolution in Edtech organizations .
When fairness is an abstraction: Equity and AI in Swedish compulsory   education,"Artificial intelligence experts often question whether AI is fair. They view fairness as a property of AI systems rather than of sociopolitical and economic systems. This paper emphasizes the need to be fair in the social, political, and economic contexts within which an educational system operates and uses AI. Taking Swedish decentralized compulsory education as the context, this paper examines whether and how the use of AI envisaged by national authorities and edtech companies exacerbates unfairness. A qualitative content analysis of selected Swedish policy documents and edtech reports was conducted using the concept of relevant social groups to understand how different groups view the risks and benefits of AI for fairness. Three groups that view efficiency as a key value of AI are identified, and interpreted as economical, pedagogical and accessibility-related. By separating fairness from social justice, this paper challenges the notion of fairness as the formal equality of opportunities.","['Marie Utterberg Modén', 'Marisa Ponti', 'Johan Lundin', 'Martin Tallvid']",2023-11-03T10:52:16Z,http://arxiv.org/abs/2311.01838v1,Education & Social Science,Misinformation in EdTech,paper examines whether and how use of AI exacerbates unfairness . three groups that view efficiency as a key value of AI are identified . paper challenges notion of fairness as formal equality of opportunities .
Identifying the Adoption or Rejection of Misinformation Targeting   COVID-19 Vaccines in Twitter Discourse,"Although billions of COVID-19 vaccines have been administered, too many people remain hesitant. Misinformation about the COVID-19 vaccines, propagating on social media, is believed to drive hesitancy towards vaccination. However, exposure to misinformation does not necessarily indicate misinformation adoption. In this paper we describe a novel framework for identifying the stance towards misinformation, relying on attitude consistency and its properties. The interactions between attitude consistency, adoption or rejection of misinformation and the content of microblogs are exploited in a novel neural architecture, where the stance towards misinformation is organized in a knowledge graph. This new neural framework is enabling the identification of stance towards misinformation about COVID-19 vaccines with state-of-the-art results. The experiments are performed on a new dataset of misinformation towards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter discourse. Because CoVaxLies provides a taxonomy of the misinformation about COVID-19 vaccines, we are able to show which type of misinformation is mostly adopted and which is mostly rejected.","['Maxwell Weinzierl', 'Sanda Harabagiu']",2022-02-18T22:01:49Z,http://arxiv.org/abs/2202.09445v1,Education & Social Science,Misinformation in EdTech,misinformation about COVID-19 vaccines is believed to drive hesitancy towards vaccination . but exposure to misinformation does not necessarily indicate misinformation adoption . a new neural framework is enabling the identification of stance towards misinformation .
Combating Misinformation in the Age of LLMs: Opportunities and   Challenges,"Misinformation such as fake news and rumors is a serious threat on information ecosystems and public trust. The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of combating misinformation. Generally, LLMs can be a double-edged sword in the fight. On the one hand, LLMs bring promising opportunities for combating misinformation due to their profound world knowledge and strong reasoning abilities. Thus, one emergent question is: how to utilize LLMs to combat misinformation? On the other hand, the critical challenge is that LLMs can be easily leveraged to generate deceptive misinformation at scale. Then, another important question is: how to combat LLM-generated misinformation? In this paper, we first systematically review the history of combating misinformation before the advent of LLMs. Then we illustrate the current efforts and present an outlook for these two fundamental questions respectively. The goal of this survey paper is to facilitate the progress of utilizing LLMs for fighting misinformation and call for interdisciplinary efforts from different stakeholders for combating LLM-generated misinformation.","['Canyu Chen', 'Kai Shu']",2023-11-09T00:05:27Z,http://arxiv.org/abs/2311.05656v1,Education & Social Science,Misinformation in EdTech,the emergence of large language models (LLMs) has great potential to reshape the fight . the goal of this paper is to facilitate the progress of utilizing LLMs for fighting misinformation .
Misinformation as Information Pollution,"Social media feed algorithms are designed to optimize online social engagements for the purpose of maximizing advertising profits, and therefore have an incentive to promote controversial posts including misinformation. By thinking about misinformation as information pollution, we can draw parallels with environmental policy for countering pollution such as carbon taxes. Similar to pollution, a Pigouvian tax on misinformation provides economic incentives for social media companies to control the spread of misinformation more effectively to avoid or reduce their misinformation tax, while preserving some degree of freedom in platforms' response. In this paper, we highlight a bird's eye view of a Pigouvian misinformation tax and discuss the key questions and next steps for implementing such a taxing scheme.","['Ashkan Kazemi', 'Rada Mihalcea']",2023-06-21T17:30:02Z,http://arxiv.org/abs/2306.12466v1,Education & Social Science,Misinformation in EdTech,social media feed algorithms are designed to optimize online social engagements . they have an incentive to promote controversial posts including misinformation . a Pigouvian misinformation tax provides economic incentives for social media companies .
"A Survey on the Role of Crowds in Combating Online Misinformation:   Annotators, Evaluators, and Creators","Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking misinformation, and develops automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. ""crowd"", who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation.   We first identify 88 papers related to crowd-based efforts, following a meticulous annotation process adhering to the PRISMA framework. We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds: (i)annotators who actively identify misinformation; (ii)evaluators who assess counter-misinformation effectiveness; (iii)creators who create counter-misinformation. This taxonomy explores the crowd's capabilities in misinformation detection, identifies prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. Then, we delve into (i)distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii)analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii)characterizing creation patterns and creator profiles for creators. Finally, we outline potential future research in this field.","['Bing He', 'Yibo Hu', 'Yeon-Chang Lee', 'Soyoung Oh', 'Gaurav Verma', 'Srijan Kumar']",2023-10-03T14:39:13Z,http://arxiv.org/abs/2310.02095v2,Education & Social Science,Misinformation in EdTech,social media users act as eyes-on-the-ground proactively questioning and countering misinformation . recent studies show that 96% of counter-misinformation responses originate from crowds . a new taxonomy explores the crowd's capabilities in misinformation detection .
How does Misinformation Affect Large Language Model Behaviors and   Preferences?,"Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at https://github.com/GKNL/MisBench.","['Miao Peng', 'Nuo Chen', 'Jianheng Tang', 'Jia Li']",2025-05-27T17:57:44Z,http://arxiv.org/abs/2505.21608v1,Education & Social Science,Misinformation in EdTech,large language models (LLMs) have remarkable capabilities in knowledge-intensive tasks . but they remain vulnerable when encountering misinformation . we propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation.
Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study   of COVID-19 Infodemic,"The spreading COVID-19 misinformation over social media already draws the attention of many researchers. According to Google Scholar, about 26000 COVID-19 related misinformation studies have been published to date. Most of these studies focusing on 1) detect and/or 2) analysing the characteristics of COVID-19 related misinformation. However, the study of the social behaviours related to misinformation is often neglected. In this paper, we introduce a fine-grained annotated misinformation tweets dataset including social behaviours annotation (e.g. comment or question to the misinformation). The dataset not only allows social behaviours analysis but also suitable for both evidence-based or non-evidence-based misinformation classification task. In addition, we introduce leave claim out validation in our experiments and demonstrate the misinformation classification performance could be significantly different when applying to real-world unseen misinformation.","['Ye Jiang', 'Xingyi Song', 'Carolina Scarton', 'Ahmet Aker', 'Kalina Bontcheva']",2021-06-22T12:17:53Z,http://arxiv.org/abs/2106.11702v4,Education & Social Science,Misinformation in EdTech,the spreading of COVID-19 misinformation over social media already draws the attention of many researchers . the study of the social behaviours related to misinformation is often neglected . this paper introduces a fine-grained annotated misinformation tweets dataset .
VaccineLies: A Natural Language Resource for Learning to Recognize   Misinformation about the COVID-19 and HPV Vaccines,"Billions of COVID-19 vaccines have been administered, but many remain hesitant. Misinformation about the COVID-19 vaccines and other vaccines, propagating on social media, is believed to drive hesitancy towards vaccination. The ability to automatically recognize misinformation targeting vaccines on Twitter depends on the availability of data resources. In this paper we present VaccineLies, a large collection of tweets propagating misinformation about two vaccines: the COVID-19 vaccines and the Human Papillomavirus (HPV) vaccines. Misinformation targets are organized in vaccine-specific taxonomies, which reveal the misinformation themes and concerns. The ontological commitments of the Misinformation taxonomies provide an understanding of which misinformation themes and concerns dominate the discourse about the two vaccines covered in VaccineLies. The organization into training, testing and development sets of VaccineLies invites the development of novel supervised methods for detecting misinformation on Twitter and identifying the stance towards it. Furthermore, VaccineLies can be a stepping stone for the development of datasets focusing on misinformation targeting additional vaccines.","['Maxwell Weinzierl', 'Sanda Harabagiu']",2022-02-18T22:09:38Z,http://arxiv.org/abs/2202.09449v1,Education & Social Science,Misinformation in EdTech,"VaccineLies is a large collection of tweets propagating misinformation about vaccines . misinformation is organized in vaccine-specific taxonomies, which reveal the misinformation themes and concerns . the ability to detect misinformation on twitter depends on the availability of data resources ."
Folk Models of Misinformation on Social Media,"In this paper we investigate what folk models of misinformation exist through semi-structured interviews with a sample of 235 social media users. Work on social media misinformation does not investigate how ordinary users - the target of misinformation - deal with it; rather, the focus is mostly on the anxiety, tensions, or divisions misinformation creates. Studying the aspects of creation, diffusion and amplification also overlooks how misinformation is internalized by users on social media and thus is quick to prescribe ""inoculation"" strategies for the presumed lack of immunity to misinformation. How users grapple with social media content to develop ""natural immunity"" as a precursor to misinformation resilience remains an open question. We have identified at least five folk models that conceptualize misinformation as either: political (counter)argumentation, out-of-context narratives, inherently fallacious information, external propaganda, or simply entertainment. We use the rich conceptualizations embodied in these folk models to uncover how social media users minimize adverse reactions to misinformation encounters in their everyday lives.","['Filipo Sharevski', 'Amy Devine', 'Emma Pieroni', 'Peter Jachim']",2022-07-26T00:40:26Z,http://arxiv.org/abs/2207.12589v1,Education & Social Science,Misinformation in EdTech,"in this paper we investigate what folk models of misinformation exist through semi-structured interviews with a sample of 235 social media users . the focus is mostly on the anxiety, tensions, or divisions misinformation creates . how users grapple with social media content to develop ""natural immunity"" remains an open question ."
Can Predominant Credible Information Suppress Misinformation in Crises?   Empirical Studies of Tweets Related to Prevention Measures during COVID-19,"During COVID-19, misinformation on social media affects the adoption of appropriate prevention behaviors. It is urgent to suppress the misinformation to prevent negative public health consequences. Although an array of studies has proposed misinformation suppression strategies, few have investigated the role of predominant credible information during crises. None has examined its effect quantitatively using longitudinal social media data. Therefore, this research investigates the temporal correlations between credible information and misinformation, and whether predominant credible information can suppress misinformation for two prevention measures (i.e. topics), i.e. wearing masks and social distancing using tweets collected from February 15 to June 30, 2020. We trained Support Vector Machine classifiers to retrieve relevant tweets and classify tweets containing credible information and misinformation for each topic. Based on cross-correlation analyses of credible and misinformation time series for both topics, we find that the previously predominant credible information can lead to the decrease of misinformation (i.e. suppression) with a time lag. The research findings provide empirical evidence for suppressing misinformation with credible information in complex online environments and suggest practical strategies for future information management during crises and emergencies.","['Yan Wang', 'Shangde Gao', 'Wenyu Gao']",2021-02-01T16:59:31Z,http://arxiv.org/abs/2102.00976v1,Education & Social Science,Misinformation in EdTech,research investigates temporal correlations between credible information and misinformation . topics include wearing masks and social distancing using tweets . results suggest practical strategies for future information management during crises and emergencies .
Misinformation detection in Luganda-English code-mixed social media text,"The increasing occurrence, forms, and negative effects of misinformation on social media platforms has necessitated more misinformation detection tools. Currently, work is being done addressing COVID-19 misinformation however, there are no misinformation detection tools for any of the 40 distinct indigenous Ugandan languages. This paper addresses this gap by presenting basic language resources and a misinformation detection data set based on code-mixed Luganda-English messages sourced from the Facebook and Twitter social media platforms. Several machine learning methods are applied on the misinformation detection data set to develop classification models for detecting whether a code-mixed Luganda-English message contains misinformation or not. A 10-fold cross validation evaluation of the classification methods in an experimental misinformation detection task shows that a Discriminative Multinomial Naive Bayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and 77.90% respectively. Also, Support Vector Machine and Bagging ensemble classification models achieve comparable results. These results are promising since the machine learning models are based on n-gram features from only the misinformation detection dataset.","['Peter Nabende', 'David Kabiito', 'Claire Babirye', 'Hewitt Tusiime', 'Joyce Nakatumba-Nabende']",2021-03-31T21:12:29Z,http://arxiv.org/abs/2104.00124v2,Education & Social Science,Misinformation in EdTech,the paper presents basic language resources and a misinformation detection data set . it is based on code-mixed Luganda-English messages sourced from the social media platforms . a Discriminative Multinomial Naive Bayes method achieves the highest accuracy .
Disagreement as a way to study misinformation and its effects,"Misinformation -- false or misleading information -- is considered a significant societal concern due to its associated ""misinformation effects,"" such as political polarization, erosion of trust in institutions, problematic behavior, and public health challenges. However, the prevailing concept is misaligned with what is studied. While misinformation focuses on instances of information about factual matters, the broad spectrum of effects often manifests at a societal level and is shaped by a wide range of interdependent factors such as identity, values, opinions, epistemologies, and disagreements. Unsurprisingly, misinformation effects can occur without the prevalence of misinformation, and misinformation does not necessarily increase the effects studied. Here, we propose using disagreement - conflicting attitudes and beliefs between individuals and communities - as a way to study misinformation effects because it addresses the identified conceptual limitations of misinformation. Furthermore, unlike misinformation, disagreement does not require researchers to determine whether a given information is false or misleading. Thus, it can be studied and, more importantly, measured without the need to make a normative judgment about a given information, even when the specific topic is entirely removed, as we show in a longitudinal disagreement measurement. We demonstrate that disagreement, as a holistic concept, provides better explanations for the occurrence of misinformation effects, enhances precision in developing appropriate interventions, and offers a promising approach for evaluating them through quantification. Finally, we show how disagreement addresses current misinformation research questions and conclude with recommendations for research practice.","['Damian Hodel', 'Jevin West']",2024-08-15T08:48:54Z,http://arxiv.org/abs/2408.08025v2,Education & Social Science,Misinformation in EdTech,"misinformation is considered a significant societal concern due to its associated ""misinformation effects"" but the prevailing concept is misaligned with what is studied . disagreement - conflicting attitudes and beliefs - addresses conceptual limitations of misinformation ."
Synthetic Misinformers: Generating and Combating Multimodal   Misinformation,"With the expansion of social media and the increasing dissemination of multimedia content, the spread of misinformation has become a major concern. This necessitates effective strategies for multimodal misinformation detection (MMD) that detect whether the combination of an image and its accompanying text could mislead or misinform. Due to the data-intensive nature of deep neural networks and the labor-intensive process of manual annotation, researchers have been exploring various methods for automatically generating synthetic multimodal misinformation - which we refer to as Synthetic Misinformers - in order to train MMD models. However, limited evaluation on real-world misinformation and a lack of comparisons with other Synthetic Misinformers makes difficult to assess progress in the field. To address this, we perform a comparative study on existing and new Synthetic Misinformers that involves (1) out-of-context (OOC) image-caption pairs, (2) cross-modal named entity inconsistency (NEI) as well as (3) hybrid approaches and we evaluate them against real-world misinformation; using the COSMOS benchmark. The comparative study showed that our proposed CLIP-based Named Entity Swapping can lead to MMD models that surpass other OOC and NEI Misinformers in terms of multimodal accuracy and that hybrid approaches can lead to even higher detection accuracy. Nevertheless, after alleviating information leakage from the COSMOS evaluation protocol, low Sensitivity scores indicate that the task is significantly more challenging than previous studies suggested. Finally, our findings showed that NEI-based Synthetic Misinformers tend to suffer from a unimodal bias, where text-only MMDs can outperform multimodal ones.","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Symeon Papadopoulos', 'Panagiotis C. Petrantonakis']",2023-03-02T12:59:01Z,http://arxiv.org/abs/2303.01217v1,Education & Social Science,Misinformation in EdTech,the spread of misinformation has become a major concern with the expansion of social media and the increasing dissemination of multimedia content . this necessitates effective strategies for multimodal misinformation detection (MMD) that detect whether the combination of an image and its accompanying text could mislead or misinform . limited evaluation on real-world misinformation makes difficult to assess progress in the field .
"Crisis, Country, and Party Lines: Politicians' Misinformation Behavior   and Public Engagement","Politicians with large media visibility and social media audiences have a significant influence on public discourse. Consequently, their dissemination of misinformation can have profound implications for society. This study investigated the misinformation-sharing behavior of 3,277 politicians and associated public engagement by using data from X (formerly Twitter) during 2020-2021. The analysis was grounded in a novel and comprehensive dataset including over 400,000 tweets covering multiple levels of governance-national executive, national legislative, and regional executive-in Germany, Italy, the UK, and the USA, representing distinct clusters of misinformation resilience. Striking cross-country differences in misinformation-sharing behavior and public engagement were observed. Politicians in Italy (4.9%) and the USA (2.2%) exhibited the highest rates of misinformation sharing, primarily among far-right and conservative legislators. Public engagement with misinformation also varied significantly. In the USA, misinformation attracted over 2.5 times the engagement of reliable information. In Italy, engagement levels were similar across content types. Italy is unique in crisis-related misinformation, particularly regarding COVID-19, which surpassed general misinformation in both prevalence and audience engagement. These insights underscore the critical roles of political affiliation, governance level, and crisis contexts in shaping the dynamics of misinformation. The study expands the literature by providing a cross-national, multi-level perspective, shedding light on how political actors influence the proliferation of misinformation during crisis.","['Jingyuan Yu', 'Emese Domahidi', ""Duccio Gamannossi degl'Innocenti"", 'Fabiana Zollo']",2025-02-21T09:17:38Z,http://arxiv.org/abs/2502.15321v1,Education & Social Science,Misinformation in EdTech,"study examined misinformation-sharing behavior of 3,277 politicians during 2020-2021 . authors: political affiliation, governance level, crisis contexts influence proliferation of misinformation . they say politicians in italy and the u.s. exhibited the highest rates of disinformation sharing . writers: it's critical to understand how political actors influence the spread of misinfo ."
"Survey of Natural Language Processing for Education: Taxonomy,   Systematic Review, and Future Trends","Natural Language Processing (NLP) aims to analyze text or speech via techniques in the computer science field. It serves the applications in domains of healthcare, commerce, education and so on. Particularly, NLP has been widely applied to the education domain and its applications have enormous potential to help teaching and learning. In this survey, we review recent advances in NLP with the focus on solving problems relevant to the education domain. In detail, we begin with introducing the related background and the real-world scenarios in education where NLP techniques could contribute. Then, we present a taxonomy of NLP in the education domain and highlight typical NLP applications including question answering, question construction, automated assessment, and error correction. Next, we illustrate the task definition, challenges, and corresponding cutting-edge techniques based on the above taxonomy. In particular, LLM-involved methods are included for discussion due to the wide usage of LLMs in diverse NLP applications. After that, we showcase some off-the-shelf demonstrations in this domain. At last, we conclude with six promising directions for future research, including more datasets in education domain, controllable usage of LLMs, intervention of difficulty-level control, interpretable educational NLP, methods with adaptive learning, and integrated systems for education. We organize all relevant datasets and papers in the open-available Github Link for better review~\url{https://github.com/LiXinyuan1015/NLP-for-Education}.","['Yunshi Lan', 'Xinyuan Li', 'Hanyue Du', 'Xuesong Lu', 'Ming Gao', 'Weining Qian', 'Aoying Zhou']",2024-01-15T07:48:42Z,http://arxiv.org/abs/2401.07518v3,Education & Social Science,Educational NLP,"natural language processing (NLP) aims to analyze text or speech via techniques in the computer science field . it serves the applications in domains of healthcare, commerce, education and so on . particularly, its applications have enormous potential to help teaching and learning ."
A Review of Digital Learning Environments for Teaching Natural Language   Processing in K-12 Education,"Natural Language Processing (NLP) plays a significant role in our daily lives and has become an essential part of Artificial Intelligence (AI) education in K-12. As children grow up with NLP-powered applications, it is crucial to introduce NLP concepts to them, fostering their understanding of language processing, language generation, and ethical implications of AI and NLP. This paper presents a comprehensive review of digital learning environments for teaching NLP in K-12. Specifically, it explores existing digital learning tools, discusses how they support specific NLP tasks and procedures, and investigates their explainability and evaluation results in educational contexts. By examining the strengths and limitations of these tools, this literature review sheds light on the current state of NLP learning tools in K-12 education. It aims to guide future research efforts to refine existing tools, develop new ones, and explore more effective and inclusive strategies for integrating NLP into K-12 educational contexts.","['Xiaoyi Tian', 'Kristy Elizabeth Boyer']",2023-10-02T19:54:30Z,http://arxiv.org/abs/2310.01603v1,Education & Social Science,Educational NLP,"this paper presents a comprehensive review of digital learning environments for teaching NLP . it explores existing digital learning tools, discusses how they support specific NLP tasks . this literature review sheds light on the current state of NLP learning tools in K-12 ."
Training an NLP Scholar at a Small Liberal Arts College: A Backwards   Designed Course Proposal,"The rapid growth in natural language processing (NLP) over the last couple years has generated student interest and excitement in learning more about the field. In this paper, we present two types of students that NLP courses might want to train. First, an ""NLP engineer"" who is able to flexibly design, build and apply new technologies in NLP for a wide range of tasks. Second, an ""NLP scholar"" who is able to pose, refine and answer questions in NLP and how it relates to the society, while also learning to effectively communicate these answers to a broader audience. While these two types of skills are not mutually exclusive -- NLP engineers should be able to think critically, and NLP scholars should be able to build systems -- we think that courses can differ in the balance of these skills. As educators at Small Liberal Arts Colleges, the strengths of our students and our institution favors an approach that is better suited to train NLP scholars. In this paper we articulate what kinds of skills an NLP scholar should have, and then adopt a backwards design to propose course components that can aid the acquisition of these skills.","['Grusha Prasad', 'Forrest Davis']",2024-08-11T00:50:59Z,http://arxiv.org/abs/2408.05664v1,Education & Social Science,Educational NLP,"rapid growth in natural language processing (NLP) over last couple years has generated student interest . authors present two types of students that NLP courses might want to train . ""nLP scholar"" who is able to pose, refine and answer questions in NLP ."
"Uncovering shifts in the history of Physics education: a systematic,   NLP-based, thematic analysis of articles from The Physics Teacher and Physics   Education journals (1966-2019)","This study explores the thematic evolution of articles in The Physics Teacher and Physics Education journals, over a critical period in modern history, from the Cold War era to the pre-pandemic world (1966 - 2019). Using an NLP-based inductive topic modeling approach, we identify recurring themes that have shaped the physics education literature, including content-based topics, teaching methodologies, laboratory practices, curriculum development, and the influence of Physics Education Research (PER). Our findings reveal both overarching trends and distinct thematic preferences between the journals. Physics Education has historically emphasized curriculum structures, social aspects of education, and interdisciplinary connections, whereas The Physics Teacher has focused more on pedagogical strategies, demonstrations, and practical teaching tools. Over the past three decades, both journals have increasingly incorporated discussions on technology, computation, and PER-driven instructional practices. By tracing these developments over five decades, this study provides a broader perspective on how physics education has responded to changing educational priorities, technological advancements, and research developments.","['Martina Caramaschi', 'Tor Ole B. Odden']",2025-04-03T15:38:34Z,http://arxiv.org/abs/2504.02703v1,Education & Social Science,Educational NLP,"study examines thematic evolution of articles in The Physics Teacher and Physics Education journals . recurring themes identified include content-based topics, teaching methodologies, laboratory practices . findings reveal overarching trends and distinct thematic preferences between journals, authors say ."
A Review of the Trends and Challenges in Adopting Natural Language   Processing Methods for Education Feedback Analysis,"Artificial Intelligence (AI) is a fast-growing area of study that stretching its presence to many business and research domains. Machine learning, deep learning, and natural language processing (NLP) are subsets of AI to tackle different areas of data processing and modelling. This review article presents an overview of AI impact on education outlining with current opportunities. In the education domain, student feedback data is crucial to uncover the merits and demerits of existing services provided to students. AI can assist in identifying the areas of improvement in educational infrastructure, learning management systems, teaching practices and study environment. NLP techniques play a vital role in analyzing student feedback in textual format. This research focuses on existing NLP methodologies and applications that could be adapted to educational domain applications like sentiment annotations, entity annotations, text summarization, and topic modelling. Trends and challenges in adopting NLP in education were reviewed and explored. Contextbased challenges in NLP like sarcasm, domain-specific language, ambiguity, and aspect-based sentiment analysis are explained with existing methodologies to overcome them. Research community approaches to extract the semantic meaning of emoticons and special characters in feedback which conveys user opinion and challenges in adopting NLP in education are explored.","['Thanveer Shaik', 'Xiaohui Tao', 'Yan Li', 'Christopher Dann', 'Jacquie Mcdonald', 'Petrea Redmond', 'Linda Galligan']",2023-01-20T23:38:58Z,http://arxiv.org/abs/2301.08826v1,Education & Social Science,Educational NLP,"this review article presents an overview of AI impact on education . machine learning, deep learning, and natural language processing (NLP) are subsets of AI . research focuses on existing NLP methodologies and applications ."
From Objectives to Questions: A Planning-based Framework for Educational   Mathematical Question Generation,"Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers' problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a ""plan-evaluate-optimize"" approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.","['Cheng Cheng', 'Zhenya Huang', 'Guanhao Zhao', 'Yuxiang Guo', 'Xin Lin', 'Jinze Wu', 'Xin Li', 'Shijin Wang']",2025-06-01T11:23:18Z,http://arxiv.org/abs/2506.00963v1,Education & Social Science,Educational NLP,"traditional generation methods focus primarily on textual quality, but they often overlook educational objectives . a dataset of 16k mathematical questions with multi-dimensional educational objectives was used to create EQGEVAL . the method is designed to assess the ability of models to generate educational mathematical questions ."
"Towards Process-Oriented, Modular, and Versatile Question Generation   that Meets Educational Needs","NLP-powered automatic question generation (QG) techniques carry great pedagogical potential of saving educators' time and benefiting student learning. Yet, QG systems have not been widely adopted in classrooms to date. In this work, we aim to pinpoint key impediments and investigate how to improve the usability of automatic QG techniques for educational purposes by understanding how instructors construct questions and identifying touch points to enhance the underlying NLP models. We perform an in-depth need finding study with 11 instructors across 7 different universities, and summarize their thought processes and needs when creating questions. While instructors show great interests in using NLP systems to support question design, none of them has used such tools in practice. They resort to multiple sources of information, ranging from domain knowledge to students' misconceptions, all of which missing from today's QG systems. We argue that building effective human-NLP collaborative QG systems that emphasize instructor control and explainability is imperative for real-world adoption. We call for QG systems to provide process-oriented support, use modular design, and handle diverse sources of input.","['Xu Wang', 'Simin Fan', 'Jessica Houghton', 'Lu Wang']",2022-04-30T22:24:39Z,http://arxiv.org/abs/2205.00355v1,Education & Social Science,Educational NLP,authors identify key impediments to use NLP-powered automatic question generation (QG) techniques in classrooms . they analyze how instructors construct questions and identify touch points to enhance NLP models .
Fairness Certification for Natural Language Processing and Large   Language Models,"Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes to certify fairness, both from the perspective of the auditor and the audited organization.","['Vincent Freiberger', 'Erik Buchmann']",2024-01-02T16:09:36Z,http://arxiv.org/abs/2401.01262v2,Education & Social Science,Educational NLP,"natural language processing (NLP) plays an important role in our daily lives . it has many fairness-critical use cases, e.g., as an expert system in recruitment . but potentially harmful biases can diffuse into NLP systems and produce unfair results . we have systematically devised six fairness criteria for NLP ."
K-12BERT: BERT for K-12 education,"Online education platforms are powered by various NLP pipelines, which utilize models like BERT to aid in content curation. Since the inception of the pre-trained language models like BERT, there have also been many efforts toward adapting these pre-trained models to specific domains. However, there has not been a model specifically adapted for the education domain (particularly K-12) across subjects to the best of our knowledge. In this work, we propose to train a language model on a corpus of data curated by us across multiple subjects from various sources for K-12 education. We also evaluate our model, K12-BERT, on downstream tasks like hierarchical taxonomy tagging.","['Vasu Goel', 'Dhruv Sahnan', 'Venktesh V', 'Gaurav Sharma', 'Deep Dwivedi', 'Mukesh Mohania']",2022-05-24T19:35:41Z,http://arxiv.org/abs/2205.12335v1,Education & Social Science,Educational NLP,online education platforms are powered by various NLP pipelines . there has not been a model specifically adapted for the education domain . we propose to train a language model on a corpus of data curated by us .
Automatic assessment of text-based responses in post-secondary   education: A systematic review,"Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered. All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.","['Rujun Gao', 'Hillary E. Merzdorf', 'Saira Anwar', 'M. Cynthia Hipwell', 'Arun Srinivasa']",2023-08-30T17:16:45Z,http://arxiv.org/abs/2308.16151v2,Education & Social Science,Educational NLP,"text-based open-ended questions in academic formative and summative assessments help students become deep learners . however, grading texts-based questions, especially in large courses, is tedious and time-consuming for instructors . this systematic review adopts a scientific and reproducible literature search strategy using explicit inclusion and exclusion criteria ."
The Utility of Large Language Models and Generative AI for Education   Research,"The use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text. While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals. This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests. The specific assignment prompted students to define and explain their career goals as engineers. Using text embedding representations of student responses, we clustered the responses together to identify thematically similar statements from students. The clustered responses were then summarized to quickly identify career interest themes. We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing. The results of this study demonstrate the feasibility and usefulness of NLP techniques in engineering education research. By automating the initial analysis of student essays, researchers and educators can more efficiently and accurately identify key themes and patterns in student writing. The methods presented in this paper have broader applications for engineering education and research purposes beyond analyzing student essays. By explaining these methods to the engineering education community, readers can utilize them in their own contexts.","['Andrew Katz', 'Umair Shakir', 'Ben Chambers']",2023-05-29T14:42:28Z,http://arxiv.org/abs/2305.18125v1,Education & Social Science,Educational NLP,"use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text . this study explores and evaluates a combination of clustering, summarization, and prompting techniques ."
"Big Data and Learning Analytics in Higher Education: Demystifying   Variety, Acquisition, Storage, NLP and Analytics","Different sectors have sought to take advantage of opportunities to invest in big data analytics and Natural language processing, in order to improve their productivity and competitiveness. Current challenges facing the higher education sector include a rapidly changing and evolving environment, which necessitates the development of new ways of thinking. Interest has therefore increased in analytics as part of the solution to many issues in higher education, including the rate of student attrition and learner support. This study provides a comprehensive discussion of big data, learning analytics and use of NLP in higher education. In addition, it introduces an integrated learning analytics solution leveraging a distributed technology system capable of supporting academic authorities and advisors at educational institutions in making decisions concerning individual students.",['Amal S. Alblawi'],2018-01-03T22:26:17Z,http://arxiv.org/abs/1801.06052v1,Education & Social Science,Educational NLP,"interest has increased in analytics as part of the solution to many issues in higher education . study provides a comprehensive discussion of big data, learning analytics and use of NLP ."
"LLMs in Education: Novel Perspectives, Challenges, and Opportunities","The role of large language models (LLMs) in education is an increasing area of interest today, considering the new opportunities they offer for teaching, learning, and assessment. This cutting-edge tutorial provides an overview of the educational applications of NLP and the impact that the recent advances in LLMs have had on this field. We will discuss the key challenges and opportunities presented by LLMs, grounding them in the context of four major educational applications: reading, writing, and speaking skills, and intelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for researchers and practitioners interested in the educational applications of NLP and the role LLMs have to play in this area. It is the first of its kind to address this timely topic.","['Bashar Alhafni', 'Sowmya Vajjala', 'Stefano Bannò', 'Kaushal Kumar Maurya', 'Ekaterina Kochmar']",2024-09-18T12:29:22Z,http://arxiv.org/abs/2409.11917v1,Education & Social Science,Educational NLP,this tutorial provides an overview of the educational applications of large language models . we will discuss the key challenges and opportunities presented by LLMs . COLING 2025 tutorial is designed for researchers and practitioners .
EduChat: A Large-Scale Language Model-based Chatbot System for   Intelligent Education,"EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education.","['Yuhao Dan', 'Zhikai Lei', 'Yiyang Gu', 'Yong Li', 'Jianghao Yin', 'Jiaju Lin', 'Linhao Ye', 'Zhiyan Tie', 'Yougen Zhou', 'Yilei Wang', 'Aimin Zhou', 'Ze Zhou', 'Qin Chen', 'Jie Zhou', 'Liang He', 'Xipeng Qiu']",2023-08-05T02:55:35Z,http://arxiv.org/abs/2308.02773v1,Education & Social Science,Educational NLP,"eduChat is a large-scale language model (LLM)-based chatbot system in the education domain . its goal is to support personalized, fair, and compassionate intelligent education . it further strengthens educational functions such as open question answering ."
Large Language Models in Computer Science Education: A Systematic   Literature Review,"Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.","['Nishat Raihan', 'Mohammed Latif Siddiq', 'Joanna C. S. Santos', 'Marcos Zampieri']",2024-10-21T17:49:50Z,http://arxiv.org/abs/2410.16349v1,Education & Social Science,Educational NLP,"foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances . several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications ."
Striking a Balance between Classical and Deep Learning Approaches in   Natural Language Processing Pedagogy,"While deep learning approaches represent the state-of-the-art of natural language processing (NLP) today, classical algorithms and approaches still find a place in NLP textbooks and courses of recent years. This paper discusses the perspectives of conveners of two introductory NLP courses taught in Australia and India, and examines how classical and deep learning approaches can be balanced within the lecture plan and assessments of the courses. We also draw parallels with the objects-first and objects-later debate in CS1 education. We observe that teaching classical approaches adds value to student learning by building an intuitive understanding of NLP problems, potential solutions, and even deep learning models themselves. Despite classical approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today.","['Aditya Joshi', 'Jake Renzella', 'Pushpak Bhattacharyya', 'Saurav Jha', 'Xiangyu Zhang']",2024-05-16T07:14:13Z,http://arxiv.org/abs/2405.09854v2,Education & Social Science,Educational NLP,the paper examines how classical and deep learning approaches can be balanced . teaching classical approaches adds value to student learning . the paper makes a case for their inclusion in NLP courses today .
The Promises and Pitfalls of Using Language Models to Measure   Instruction Quality in Education,"Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers' expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings. Our results suggest that pretrained Language Models (PLMs) demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices. Interestingly, using only teachers' utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings. Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration.","['Paiheng Xu', 'Jing Liu', 'Nathan Jones', 'Julie Cohen', 'Wei Ai']",2024-04-03T04:15:29Z,http://arxiv.org/abs/2404.02444v1,Education & Social Science,Educational NLP,"study uses natural language processing (NLP) techniques to assess instructional practices . pretrained Language Models (PLMs) demonstrate performances comparable to human raters . but their efficacy diminishes with more complex teaching practices, authors say ."
Leveraging Large Language Models for Concept Graph Recovery and Question   Answering in NLP Education,"In the domain of Natural Language Processing (NLP), Large Language Models (LLMs) have demonstrated promise in text-generation tasks. However, their educational applications, particularly for domain-specific queries, remain underexplored. This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA). We assess LLMs' zero-shot performance in creating domain-specific concept graphs and introduce TutorQA, a new expert-verified NLP-focused benchmark for scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating concept graphs with LLMs for answering diverse questions. Our results indicate that LLMs' zero-shot concept graph recovery is competitive with supervised methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs achieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis show that CGLLM generates answers with more fine-grained concepts.","['Rui Yang', 'Boming Yang', 'Sixun Ouyang', 'Tianwei She', 'Aosong Feng', 'Yuang Jiang', 'Freddy Lecue', 'Jinghui Lu', 'Irene Li']",2024-02-22T05:15:27Z,http://arxiv.org/abs/2402.14293v1,Education & Social Science,Educational NLP,"study assesses LLMs' zero-shot performance in creating domain-specific concept graphs . introduces TutorQA, a new expert-verified benchmark for scientific graph reasoning and QA . results show that LLM's Zero-shot concept graph recovery is competitive with supervised methods ."
Leveraging Generative AI for Enhancing Automated Assessment in   Programming Education Contests,"Competitive programming contests play a crucial role in cultivating computational thinking and algorithmic skills among learners. However, generating comprehensive test cases to effectively assess programming solutions remains resource-intensive and challenging for educators. This paper introduces an innovative NLP-driven method leveraging generative AI (large language models) to automate the creation of high-quality test cases for competitive programming assessments. We extensively evaluated our approach on diverse datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for 5th graders, recent competitions hosted on the Kilonova.ro platform, and the International Informatics Olympiad in Teams (IIOT). Our results demonstrate that AI-generated test cases substantially enhanced assessments, notably identifying previously undetected errors in 67% of the OJI 5th grade programming problems. These improvements underscore the complementary educational value of our technique in formative assessment contexts. By openly sharing our prompts, translated datasets, and methodologies, we offer practical NLP-based tools that educators and contest organizers can readily integrate to enhance assessment quality, reduce workload, and deepen insights into learner performance.","['Stefan Dascalescu', 'Adrian Marius Dumitran', 'Mihai Alexandru Vasiluta']",2025-06-06T11:20:04Z,http://arxiv.org/abs/2506.05990v1,Education & Social Science,Educational NLP,this paper introduces an innovative NLP-driven method leveraging generative AI . it automates the creation of high-quality test cases for competitive programming assessments . the technique identifies previously undetected errors in 67% of the 5th grade programming problems .
Automated essay scoring using efficient transformer-based language   models,"Automated Essay Scoring (AES) is a cross-disciplinary effort involving Education, Linguistics, and Natural Language Processing (NLP). The efficacy of an NLP model in AES tests it ability to evaluate long-term dependencies and extrapolate meaning even when text is poorly written. Large pretrained transformer-based language models have dominated the current state-of-the-art in many NLP tasks, however, the computational requirements of these models make them expensive to deploy in practice. The goal of this paper is to challenge the paradigm in NLP that bigger is better when it comes to AES. To do this, we evaluate the performance of several fine-tuned pretrained NLP models with a modest number of parameters on an AES dataset. By ensembling our models, we achieve excellent results with fewer parameters than most pretrained transformer-based models.","['Christopher M Ormerod', 'Akanksha Malhotra', 'Amir Jafari']",2021-02-25T19:28:39Z,http://arxiv.org/abs/2102.13136v1,Education & Social Science,Educational NLP,the efficacy of an NLP model in AES tests it ability to evaluate long-term dependencies . large pretrained transformer-based language models have dominated the current state-of-the-art . the computational requirements of these models make them expensive to deploy in practice .
Using Conversational Agents To Support Learning By Teaching,"Conversational agents are becoming increasingly popular for supporting and facilitating learning. Conventional pedagogical agents are designed to play the role of human teachers by giving instructions to the students. In this paper, we investigate the use of conversational agents to support the 'learning-by-teaching' paradigm where the agent receives instructions from students. In particular, we introduce Curiosity Notebook: an educational application that harvests conversational interventions to facilitate students' learning. Recognizing such interventions can not only help in engaging students within learning interactions, but also provide a deeper insight into the intricacies involved in designing conversational agents for educational purposes.","['Nalin Chhibber', 'Edith Law']",2019-09-30T03:40:59Z,http://arxiv.org/abs/1909.13443v1,Education & Social Science,Conversational Agents in Education,"conventional pedagogical agents are designed to play the role of human teachers by giving instructions to the students . in this paper, we investigate the use of conversational agents to support the 'learning-by-teaching' paradigm ."
Anticipating User Needs: Insights from Design Fiction on Conversational   Agents for Computational Thinking,"Computational thinking, and by extension, computer programming, is notoriously challenging to learn. Conversational agents and generative artificial intelligence (genAI) have the potential to facilitate this learning process by offering personalized guidance, interactive learning experiences, and code generation. However, current genAI-based chatbots focus on professional developers and may not adequately consider educational needs. Involving educators in conceiving educational tools is critical for ensuring usefulness and usability. We enlisted nine instructors to engage in design fiction sessions in which we elicited abilities such a conversational agent supported by genAI should display. Participants envisioned a conversational agent that guides students stepwise through exercises, tuning its method of guidance with an awareness of the educational background, skills and deficits, and learning preferences. The insights obtained in this paper can guide future implementations of tutoring conversational agents oriented toward teaching computational thinking and computer programming.","['Jacob Penney', 'João Felipe Pimentel', 'Igor Steinmacher', 'Marco A. Gerosa']",2023-11-12T16:19:03Z,http://arxiv.org/abs/2311.06887v2,Education & Social Science,Conversational Agents in Education,genAI-based chatbots focus on professional developers and may not adequately consider educational needs . enlisted nine instructors to engage in design fiction sessions . participants envisioned a conversational agent that guides students stepwise through exercises . the insights obtained can guide future implementations of tutoring conversational agents .
Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward,"Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.","['Yanming Wan', 'Jiaxing Wu', 'Marwa Abdulhai', 'Lior Shani', 'Natasha Jaques']",2025-04-04T06:35:02Z,http://arxiv.org/abs/2504.03206v2,Education & Social Science,Conversational Agents in Education,a curiosity-based intrinsic reward is incorporated into multi-turn RLHF . the reward mechanism encourages the LLM agent to actively infer user traits . this improves personalization performance in a conversational recommendation task .
The AI Teacher Test: Measuring the Pedagogical Ability of Blender and   GPT-3 in Educational Dialogues,"How can we test whether state-of-the-art generative models, such as Blender and GPT-3, are good AI teachers, capable of replying to a student in an educational dialogue? Designing an AI teacher test is challenging: although evaluation methods are much-needed, there is no off-the-shelf solution to measuring pedagogical ability. This paper reports on a first attempt at an AI teacher test. We built a solution around the insight that you can run conversational agents in parallel to human teachers in real-world dialogues, simulate how different agents would respond to a student, and compare these counterpart responses in terms of three abilities: speak like a teacher, understand a student, help a student. Our method builds on the reliability of comparative judgments in education and uses a probabilistic model and Bayesian sampling to infer estimates of pedagogical ability. We find that, even though conversational agents (Blender in particular) perform well on conversational uptake, they are quantifiably worse than real teachers on several pedagogical dimensions, especially with regard to helpfulness (Blender: {\Delta} ability = -0.75; GPT-3: {\Delta} ability = -0.93).","['Anaïs Tack', 'Chris Piech']",2022-05-16T09:36:30Z,http://arxiv.org/abs/2205.07540v1,Education & Social Science,Conversational Agents in Education,this paper reports on a first attempt at an AI teacher test . we built a solution around the insight that you can run conversational agents in parallel to human teachers . our method builds on the reliability of comparative judgments in education .
Learning Affects Trust: Design Recommendations and Concepts for Teaching   Children -- and Nearly Anyone -- about Conversational Agents,"Research has shown that human-agent relationships form in similar ways to human-human relationships. Since children do not have the same critical analysis skills as adults (and may over-trust technology, for example), this relationship-formation is concerning. Nonetheless, little research investigates children's perceptions of conversational agents in-depth, and even less investigates how education might change these perceptions. We present K-12 workshops with associated conversational AI concepts to encourage healthier understanding and relationships with agents. Through studies with the curriculum, and children and parents from various countries, we found participants' perceptions of agents -- specifically their partner models and trust -- changed. When participants discussed changes in trust of agents, we found they most often mentioned learning something. For example, they frequently mentioned learning where agents obtained information, what agents do with this information and how agents are programmed. Based on the results, we developed recommendations for teaching conversational agent concepts, including emphasizing the concepts students found most challenging, like training, turn-taking and terminology; supplementing agent development activities with related learning activities; fostering appropriate levels of trust towards agents; and fostering accurate partner models of agents. Through such pedagogy, students can learn to better understand conversational AI and what it means to have it in the world.","['Jessica Van Brummelen', 'Mingyan Claire Tian', 'Maura Kelleher', 'Nghi Hoang Nguyen']",2022-09-12T07:50:01Z,http://arxiv.org/abs/2209.05063v1,Education & Social Science,Conversational Agents in Education,research shows that human-agent relationships form in similar ways to human-human relationships . little research investigates children's perceptions of conversational agents in-depth . we present K-12 workshops with associated conversational AI concepts .
Towards Generalizable Agents in Text-Based Educational Environments: A   Study of Integrating RL with LLMs,"There has been a growing interest in developing learner models to enhance learning and teaching experiences in educational environments. However, existing works have primarily focused on structured environments relying on meticulously crafted representations of tasks, thereby limiting the agent's ability to generalize skills across tasks. In this paper, we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments by integrating Reinforcement Learning (RL) with Large Language Models (LLMs). We investigate three types of agents: (i) RL-based agents that utilize natural language for state and action representations to find the best interaction strategy, (ii) LLM-based agents that leverage the model's general knowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL agents that combine these two strategies to improve agents' performance and generalization. To support the development and evaluation of these agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations. Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions. In contrast, LLM-based agents perform better in asking diagnostic questions but fall short of completing the task. Finally, hybrid LLM-assisted RL agents enable us to overcome these limitations, highlighting the potential of combining RL and LLMs to develop high-performing agents for open-ended learning environments.","['Bahar Radmehr', 'Adish Singla', 'Tanja Käser']",2024-04-29T14:53:48Z,http://arxiv.org/abs/2404.18978v1,Education & Social Science,Conversational Agents in Education,"we aim to enhance the generalization capabilities of agents in open-ended text-based learning environments . integrating Reinforcement Learning (RL) with Large Language Models (LLMs) we introduce PharmaSimText, a benchmark derived from a virtual pharmacy environment ."
The Use of Multiple Conversational Agent Interlocutors in Learning,"With growing capabilities of large language models (LLMs) comes growing affordances for human-like and context-aware conversational partners. On from this, some recent work has investigated the use of LLMs to simulate multiple conversational partners, such as to assist users with problem solving or to simulate an environment populated entirely with LLMs. Beyond this, we are interested in discussing and exploring the use of LLMs to simulate multiple personas to assist and augment users in educational settings that could benefit from multiple interlocutors. We discuss prior work that uses LLMs to simulate multiple personas sharing the same environment, and discuss example scenarios where multiple conversational agent partners could be used in education.",['Samuel Rhys Cox'],2023-12-27T11:30:33Z,http://arxiv.org/abs/2312.16534v1,Education & Social Science,Conversational Agents in Education,the use of large language models (LLMs) has led to the development of human-like conversational partners . we discuss examples of scenarios where multiple conversational agent partners could be used in education .
Education Policy and Intergenerational Educational Persistence: Evidence   from rural Benin,"This paper employs a nonlinear difference-in-differences approach to empirically examine the Maximally Maintained Inequality (MMI) hypothesis in rural Benin. The findings of this study confirm the MMI hypothesis. In particular, it is observed that when 76% of educated parents choose to educate their daughters in the absence of educational programs, in contrast to only 37% among non-educated parents, the average impact of tuition fee subsidy on enrollment probability in primary schools stands at 3.8\% for non-educated households and 0.27% for educated households. Conversely, in cases where only 27% of educated parents decide to educate their daughters without education programs, the average effect of tuition fee waivers on enrollment probability in primary schools increases to 19.64\% for non-educated households and 24\% for educated households. From the analysis of household education decisions influenced by a preference for education and budget constraints, three key conclusions emerge to explain the mechanism behind the MMI. Firstly, when the income advantage of educated households compared to non-educated households is significantly high, irrespective of the level of their preference advantage, reducing the financial cost of education induces a greater shift in education decisions among non-educated households. Secondly, in situations where educated households do not possess an income advantage relative to non-educated households, the reduction in education-related financial costs leads to a more pronounced change in education decisions among educated households. Lastly, for the low-income advantage of educated households, as the income advantage of educated households increases, non-educated households respond more to education policy than educated parents, if the preference advantage of educated households is relatively smaller.",['Christelle Zozoungbo'],2024-01-30T19:18:44Z,http://arxiv.org/abs/2401.17391v1,Education & Social Science,Conversational Agents in Education,this paper employs a nonlinear difference-in-differences approach to empirically examine the Maximally Maintained Inequality hypothesis . 76% of educated parents choose to educate their daughters in the absence of educational programs . average impact of tuition fee subsidy on enrollment probability in primary schools stands at 3.8% .
The Effects of Embodiment and Personality Expression on Learning in   LLM-based Educational Agents,"This work investigates how personality expression and embodiment affect personality perception and learning in educational conversational agents. We extend an existing personality-driven conversational agent framework by integrating LLM-based conversation support tailored to an educational application. We describe a user study built on this system to evaluate two distinct personality styles: high extroversion and agreeableness and low extroversion and agreeableness. For each personality style, we assess three models: (1) a dialogue-only model that conveys personality through dialogue, (2) an animated human model that expresses personality solely through dialogue, and (3) an animated human model that expresses personality through both dialogue and body and facial animations. The results indicate that all models are positively perceived regarding both personality and learning outcomes. Models with high personality traits are perceived as more engaging than those with low personality traits. We provide a comprehensive quantitative and qualitative analysis of perceived personality traits, learning parameters, and user experiences based on participant ratings of the model types and personality styles, as well as users' responses to open-ended questions.","['Sinan Sonlu', 'Bennie Bendiksen', 'Funda Durupinar', 'Uğur Güdükbay']",2024-06-24T09:38:26Z,http://arxiv.org/abs/2407.10993v1,Education & Social Science,Conversational Agents in Education,this work investigates how personality expression and embodiment affect personality perception and learning in educational conversational agents . we describe a user study built on an existing personality-driven conversational agent framework . results indicate that all models are positively perceived regarding both personality and learning outcomes .
Dialogic Pedagogy for Large Language Models: Aligning Conversational AI   with Proven Theories of Learning,"Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",['Russell Beale'],2025-06-24T10:19:09Z,http://arxiv.org/abs/2506.19484v1,Education & Social Science,Conversational Agents in Education,"this article provides a comprehensive review of how LLMs are being used in higher education, with extensions to secondary and lifelong learning contexts . we map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges traditional pedagogical assumptions ."
Adapting to Educate: Conversational AI's Role in Mathematics Education   Across Different Educational Contexts,"As educational settings increasingly integrate artificial intelligence (AI), understanding how AI tools identify -- and adapt their responses to -- varied educational contexts becomes paramount. This study examines conversational AI's effectiveness in supporting K-12 mathematics education across various educational contexts. Through qualitative content analysis, we identify educational contexts and key instructional needs present in educator prompts and assess AI's responsiveness. Our findings indicate that educators focus their AI conversations on assessment methods, how to set the cognitive demand level of their instruction, and strategies for making meaningful real-world connections. However, educators' conversations with AI about instructional practices do vary across revealed educational contexts; they shift their emphasis to tailored, rigorous content that addresses their students' unique needs. Educators often seek actionable guidance from AI and reject responses that do not align with their inquiries. While AI can provide accurate, relevant, and useful information when educational contexts or instructional practices are specified in conversation queries, its ability to consistently adapt responses along these evaluation dimensions varies across different educational settings. Significant work remains to realize the response-differentiating potential of conversational AI tools in complex educational use cases. This research contributes insights into developing AI tools that are responsive, proactive, and anticipatory, adapting to evolving educational needs before they are explicitly stated, and provides actionable recommendations for both developers and educators to enhance AI integration in educational practices.","['Alex Liu', 'Lief Esbenshade', 'Min Sun', 'Shawon Sarkar', 'Jian He', 'Victor Tian', 'Zachary Zhang']",2025-03-04T20:47:15Z,http://arxiv.org/abs/2503.02999v1,Education & Social Science,Conversational Agents in Education,this study examines conversational AI's effectiveness in supporting K-12 mathematics education . educators' conversations with AI about instructional practices do vary across revealed contexts . Educators often seek actionable guidance from AI and reject responses that do not align .
"What Do Children and Parents Want and Perceive in Conversational Agents?   Towards Transparent, Trustworthy, Democratized Agents","Historically, researchers have focused on analyzing WEIRD, adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries' perspectives on an emerging technology: conversational agents. We aim to better understand participants' trust of agents, partner models, and their ideas of ""ideal future agents"" such that researchers can better design for these users. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents' competence and predictability indicators, as well as increasing transparency in terms of agents' information sources.","['Jessica Van Brummelen', 'Maura Kelleher', 'Mingyan Claire Tian', 'Nghi Hoang Nguyen']",2022-09-16T11:33:53Z,http://arxiv.org/abs/2209.07862v2,Education & Social Science,Conversational Agents in Education,"a new study analyzes children's trust of conversational agents . children trust agents more than parents or friends for correct information, the study finds . researchers hope to better understand participants' trust of agents, partner models ."
The Effects of Generative AI Agents and Scaffolding on Enhancing   Students' Comprehension of Visual Learning Analytics,"Visual learning analytics (VLA) is becoming increasingly adopted in educational technologies and learning analytics dashboards to convey critical insights to students and educators. Yet many students experienced difficulties in comprehending complex VLA due to their limited data visualisation literacy. While conventional scaffolding approaches like data storytelling have shown effectiveness in enhancing students' comprehension of VLA, these approaches remain difficult to scale and adapt to individual learning needs. Generative AI (GenAI) technologies, especially conversational agents, offer potential solutions by providing personalised and dynamic support to enhance students' comprehension of VLA. This study investigates the effectiveness of GenAI agents, particularly when integrated with scaffolding techniques, in improving students' comprehension of VLA. A randomised controlled trial was conducted with 117 higher education students to compare the effects of two types of GenAI agents: passive agents, which respond to student queries, and proactive agents, which utilise scaffolding questions, against standalone scaffolding in a VLA comprehension task. The results show that passive agents yield comparable improvements to standalone scaffolding both during and after the intervention. Notably, proactive GenAI agents significantly enhance students' VLA comprehension compared to both passive agents and standalone scaffolding, with these benefits persisting beyond the intervention. These findings suggest that integrating GenAI agents with scaffolding can have lasting positive effects on students' comprehension skills and support genuine learning.","['Lixiang Yan', 'Roberto Martinez-Maldonado', 'Yueqiao Jin', 'Vanessa Echeverria', 'Mikaela Milesi', 'Jie Fan', 'Linxuan Zhao', 'Riordan Alfredo', 'Xinyu Li', 'Dragan Gašević']",2024-09-18T02:15:55Z,http://arxiv.org/abs/2409.11645v2,Education & Social Science,Conversational Agents in Education,"visual learning analytics (VLA) is becoming increasingly adopted in educational technologies and learning analytics dashboards . many students experienced difficulties in comprehending complex VLA due to limited data visualisation literacy . Generative AI (GenAI) technologies, especially conversational agents, offer potential solutions by providing personalised and dynamic support ."
Target-Guided Open-Domain Conversation,"Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches.","['Jianheng Tang', 'Tiancheng Zhao', 'Chenyan Xiong', 'Xiaodan Liang', 'Eric P. Xing', 'Zhiting Hu']",2019-05-28T00:55:25Z,http://arxiv.org/abs/1905.11553v2,Education & Social Science,Conversational Agents in Education,we want a conversational system to chat naturally with human and guide the conversation to a designated target subject . no public data is available for learning such a target-guided strategy . we introduce coarse-grained keywords to control the intended content of system responses .
Conversational Agents: Theory and Applications,"In this chapter, we provide a review of conversational agents (CAs), discussing chatbots, intended for casual conversation with a user, as well as task-oriented agents that generally engage in discussions intended to reach one or several specific goals, often (but not always) within a specific domain. We also consider the concept of embodied conversational agents, briefly reviewing aspects such as character animation and speech processing. The many different approaches for representing dialogue in CAs are discussed in some detail, along with methods for evaluating such agents, emphasizing the important topics of accountability and interpretability. A brief historical overview is given, followed by an extensive overview of various applications, especially in the fields of health and education. We end the chapter by discussing benefits and potential risks regarding the societal impact of current and future CA technology.","['Mattias Wahde', 'Marco Virgolin']",2022-02-07T13:48:14Z,http://arxiv.org/abs/2202.03164v1,Education & Social Science,Conversational Agents in Education,"in this chapter, we provide a review of conversational agents (CAs) chatbots are discussed, as well as task-oriented agents . we conclude the chapter by discussing benefits and potential risks ."
Conversational Education at Scale: A Multi-LLM Agent Workflow for   Procedural Learning and Pedagogic Quality Assessment,"Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.","['Jiahuan Pei', 'Fanghua Ye', 'Xin Sun', 'Wentao Deng', 'Koen Hindriks', 'Junxiao Wang']",2025-07-07T22:56:37Z,http://arxiv.org/abs/2507.05528v1,Education & Social Science,Conversational Agents in Education,large language models (LLMs) have advanced virtual educators and learners . we propose a multi-agent workflow leveraging LLMs to simulate teaching-learning conversations . the evaluation protocol combines computational and rubric-based metrics with human judgment alignment .
Generating A Crowdsourced Conversation Dataset to Combat Cybergrooming,"Cybergrooming emerges as a growing threat to adolescent safety and mental health. One way to combat cybergrooming is to leverage predictive artificial intelligence (AI) to detect predatory behaviors in social media. However, these methods can encounter challenges like false positives and negative implications such as privacy concerns. Another complementary strategy involves using generative artificial intelligence to empower adolescents by educating them about predatory behaviors. To this end, we envision developing state-of-the-art conversational agents to simulate the conversations between adolescents and predators for educational purposes. Yet, one key challenge is the lack of a dataset to train such conversational agents. In this position paper, we present our motivation for empowering adolescents to cope with cybergrooming. We propose to develop large-scale, authentic datasets through an online survey targeting adolescents and parents. We discuss some initial background behind our motivation and proposed design of the survey, such as situating the participants in artificial cybergrooming scenarios, then allowing participants to respond to the survey to obtain their authentic responses. We also present several open questions related to our proposed approach and hope to discuss them with the workshop attendees.","['Xinyi Zhang', 'Pamela J. Wisniewski', 'Jin-hee Cho', 'Lifu Huang', 'Sang Won Lee']",2024-05-21T18:49:02Z,http://arxiv.org/abs/2405.13154v1,Education & Social Science,Conversational Agents in Education,"cybergrooming is a growing threat to adolescent safety and mental health . we propose to develop large-scale, authentic datasets through an online survey . generative artificial intelligence can empower adolescents by educating them about predators ."
Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent   on programming education,"This study investigates the potential of using ChatGPT as a teachable agent to support students' learning by teaching process, specifically in programming education. While learning by teaching is an effective pedagogical strategy for promoting active learning, traditional teachable agents have limitations, particularly in facilitating natural language dialogue. Our research explored whether ChatGPT, with its ability to engage learners in natural conversations, can support this process. The findings reveal that interacting with ChatGPT improves students' knowledge gains and programming abilities, particularly in writing readable and logically sound code. However, it had limited impact on developing learners' error-correction skills, likely because ChatGPT tends to generate correct code, reducing opportunities for students to practice debugging. Additionally, students' self-regulated learning (SRL) abilities improved, suggesting that teaching ChatGPT fosters learners' higher self-efficacy and better implementation of SRL strategies. This study discussed the role of natural dialogue in fostering socialized learning by teaching, and explored ChatGPT's specific contributions in supporting students' SRL through the learning by teaching process. Overall, the study highlights ChatGPT's potential as a teachable agent, offering insights for future research on ChatGPT-supported education.","['Angxuan Chen', 'Yuang Wei', 'Huixiao Le', 'Yan Zhang']",2024-12-05T04:12:03Z,http://arxiv.org/abs/2412.15226v1,Education & Social Science,Conversational Agents in Education,"study explores whether ChatGPT can support students' learning by teaching process . chatGPT improves students' knowledge gains and programming abilities . however, it had limited impact on developing learners' error-correction skills ."
SimPal: Towards a Meta-Conversational Framework to Understand Teacher's   Instructional Goals for K-12 Physics,"Simulations are widely used to teach science in grade schools. These simulations are often augmented with a conversational artificial intelligence (AI) agent to provide real-time scaffolding support for students conducting experiments using the simulations. AI agents are highly tailored for each simulation, with a predesigned set of Instructional Goals (IGs), making it difficult for teachers to adjust IGs as the agent may no longer align with the revised IGs. Additionally, teachers are hesitant to adopt new third-party simulations for the same reasons. In this research, we introduce SimPal, a Large Language Model (LLM) based meta-conversational agent, to solve this misalignment issue between a pre-trained conversational AI agent and the constantly evolving pedagogy of instructors. Through natural conversation with SimPal, teachers first explain their desired IGs, based on which SimPal identifies a set of relevant physical variables and their relationships to create symbolic representations of the desired IGs. The symbolic representations can then be leveraged to design prompts for the original AI agent to yield better alignment with the desired IGs. We empirically evaluated SimPal using two LLMs, ChatGPT-3.5 and PaLM 2, on 63 Physics simulations from PhET and Golabz. Additionally, we examined the impact of different prompting techniques on LLM's performance by utilizing the TELeR taxonomy to identify relevant physical variables for the IGs. Our findings showed that SimPal can do this task with a high degree of accuracy when provided with a well-defined prompt.","['Effat Farhana', 'Souvika Sarkar', 'Ralph Knipper', 'Indrani Dey', 'Hari Narayanan', 'Sadhana Puntambekar', 'Shubhra Kanti Karmaker']",2024-07-08T05:42:59Z,http://arxiv.org/abs/2407.06241v1,Education & Social Science,Conversational Agents in Education,"teachers are hesitant to adopt new third-party simulations for the same reasons . we introduce SimPal, a large language model (LLM) based meta-conversational agent . the agent can do this task with a high degree of accuracy ."
Salespeople vs SalesBot: Exploring the Role of Educational Value in   Conversational Recommender Systems,"Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context. We release our code and make all data available.","[""Lidiya Murakhovs'ka"", 'Philippe Laban', 'Tian Xie', 'Caiming Xiong', 'Chien-Sheng Wu']",2023-10-26T19:44:06Z,http://arxiv.org/abs/2310.17749v1,Education & Social Science,Conversational Agents in Education,existing conversational recommender systems often overlook users' lack of background knowledge . salesBot and ShopperBot can simulate either side of the framework . a comprehensive human study compares salesBot against professional salespeople .
"Climate Science and Control Engineering: Insights, Parallels, and   Connections","Climate science is the multidisciplinary field that studies the Earth's climate and its evolution. At the very core of climate science are indispensable climate models that predict future climate scenarios, inform policy decisions, and dictate how a country's economy should change in light of the changing climate. Climate models capture a wide range of interacting dynamic processes via extremely complex ordinary and partial differential equations. To model these large-scale complex processes, climate science leverages supercomputers, advanced simulations, and statistical methods to predict future climate. An area of engineering that is rarely studied in climate science is control engineering. Given that climate systems are inherently dynamic, it is intuitive to analyze them within the framework of dynamic system science. This perspective has been underexplored in the literature. In this manuscript, we provide a tutorial that: (i) introduces the control engineering community to climate dynamics and modeling, including spatiotemporal scales and challenges in climate modeling; (ii) offers a fresh perspective on climate models from a control systems viewpoint; and (iii) explores the relevance and applicability of various advanced graph and network control-based approaches in building a physics-informed framework for learning, control and estimation in climate systems. We also present simple and then more complex climate models, depicting fundamental ideas and processes that are instrumental in building climate change projections. This tutorial also builds parallels and observes connections between various contemporary problems at the forefront of climate science and their control theoretic counterparts. We specifically observe that an abundance of climate science problems can be linguistically reworded and mathematically framed as control theoretic ones.","['Salma M. Elsherif', 'Ahmad F. Taha']",2025-04-29T20:16:41Z,http://arxiv.org/abs/2504.21153v2,"Sustainability, Industry & Robotics",Climate Modeling,climate science is the multidisciplinary field that studies the Earth's climate and its evolution . climate models predict future climate scenarios and dictate how a country's economy should change . an area of engineering that is rarely studied is control engineering . this tutorial introduces the control engineering community to climate dynamics and modeling .
Baumol's Climate Disease,"We investigate optimal carbon abatement in a dynamic general equilibrium climate-economy model with endogenous structural change. By differentiating the production of investment from consumption, we show that social cost of carbon can be conceived as a reduction in physical capital. In addition, we distinguish two final sectors in terms of productivity growth and climate vulnerability. We theoretically show that heterogeneous climate vulnerability results in a climate-induced version of Baumol's cost disease. Further, if climate-vulnerable sectors have high (low) productivity growth, climate impact can either ameliorate (aggravate) the Baumol's cost disease, call for less (more) stringent climate policy. We conclude that carbon abatement should not only factor in unpriced climate capital, but also be tailored to Baumol's cost and climate diseases.","['Fangzhi Wang', 'Hua Liao', 'Richard S. J. Tol']",2023-11-30T19:45:13Z,http://arxiv.org/abs/2312.00160v1,"Sustainability, Industry & Robotics",Climate Modeling,"carbon abatement should be tailored to Baumol's cost and climate diseases . if climate-vulnerable sectors have high (low) productivity growth, climate impact can either ameliorate (aggravate) the Baumol’s cost disease ."
Mapping the Climate Change Landscape on TikTok,"Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikTok's climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate ""gateway"" topics that could draw new audiences into climate discussions.","['Alessia Galdeman', 'Luca Maria Aiello']",2025-05-02T13:21:33Z,http://arxiv.org/abs/2505.03813v1,"Sustainability, Industry & Robotics",Climate Modeling,we collect the first TikTok dataset on climate topics . we map the topics discussed on the platform on a climate taxonomy . results show creators primarily approach climate through lifestyle and dietary choices .
ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning,"Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists' efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a ""super emulator"" can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.","['Julia Kaltenborn', 'Charlotte E. E. Lange', 'Venkatesh Ramesh', 'Philippe Brouillard', 'Yaniv Gurwicz', 'Chandni Nagda', 'Jakob Runge', 'Peer Nowack', 'David Rolnick']",2023-11-07T04:55:36Z,http://arxiv.org/abs/2311.03721v1,"Sustainability, Industry & Robotics",Climate Modeling,climate models have been key for assessing the impact of climate change . climateSet dataset contains inputs and outputs of 36 climate models . dataset can be used to train an ML emulator on several climate models instead of just one .
"Detection, attribution, and modeling of climate change: key open issues","The CMIP global climate models (GCMs) assess that nearly 100% of global surface warming observed between 1850-1900 and 2011-2020 is attributable to anthropogenic drivers like greenhouse gas emissions. These models also generate future climate projections based on shared socioeconomic pathways (SSPs), aiding in risk assessment and the development of costly Net-Zero climate mitigation strategies. Yet, the CMIP GCMs face significant scientific challenges in attributing and modeling climate change, particularly in capturing natural climate variability over multiple timescales throughout the Holocene. Other key concerns include the reliability of global surface temperature records, the accuracy of solar irradiance models, and the robustness of climate sensitivity estimates. Global warming estimates may be overstated due to uncorrected non-climatic biases, and the GCMs may significantly underestimate solar and astronomical influences on climate variations. The equilibrium climate sensitivity (ECS) to radiative forcing could be lower than commonly assumed; empirical findings suggest ECS values lower than 3 K and possibly even closer to 1.1 +/- 0.4 K. Empirical models incorporating natural variability suggest that the 21st-century global warming may remain moderate, even under SSP scenarios that do not necessitate Net-Zero emission policies. These findings raise important questions regarding the necessity and urgency of implementing aggressive climate mitigation strategies. While GCMs remain essential tools for climate research and policymaking, their scientific limitations underscore the need for more refined modeling approaches to ensure accurate future climate assessments. Addressing uncertainties related to climate change detection, natural variability, solar influences, and climate sensitivity to radiative forcing will enhance predictions and better inform sustainable climate strategies.",['Nicola Scafetta'],2025-05-21T18:31:14Z,http://arxiv.org/abs/2506.13994v1,"Sustainability, Industry & Robotics",Climate Modeling,"the CMIP global climate models (GCMs) assess that nearly 100% of global surface warming is attributable to anthropogenic drivers like greenhouse gas emissions . the models also generate future climate projections based on shared socioeconomic pathways (SSPs) authors: but challenges remain in attributing and modeling climate change, particularly in capturing natural climate variability ."
Predictability of climate tipping focusing on the internal variability   of the Earth system,"Prediction of climate tipping is challenging due to the lack of recent observation of actual climate tipping. Despite many previous efforts to accurately predict the existence and timing of climate tippings under specific climate scenarios, the predictability of climate tipping, the necessary conditions under which climate tipping can be predicted, has yet to be explored. In this study, the predictability of climate tipping is analyzed by Observation System Simulation Experiment (OSSE), in which the value of observation for prediction is assessed through the idealized experiment of data assimilation, using a simplified dynamic vegetation model and an Atlantic Meridional Overturning Circulation (AMOC) two box model. We find that the ratio of internal variability to observation error, or signal-to-noise ratio, should be large enough to accurately predict climate tippings. When observation can accurately resolve the internal variability of the system, assimilating these observations into process-based models can effectively improve the skill of predicting climate tippings. Our quantitative estimation of required observation accuracy to predict climate tipping implies that the existing observation network is not always sufficient to accurately project climate tipping.","['Amane Kubo', 'Yohei Sawada']",2024-06-28T04:05:58Z,http://arxiv.org/abs/2406.19639v1,"Sustainability, Industry & Robotics",Climate Modeling,"the predictability of climate tipping is analyzed by Observation System Simulation Experiment . internal variability to observation error, or signal-to-noise ratio, should be large enough . when observation can resolve internal variability, assimilating observations into process-based models can improve skill ."
Climate Modeling and Bifurcation,"Many papers and monographs were written about the modeling the Earth climate and its variability. However there is still an obvious need for a module that presents the fundamentals of climate modeling to students at the undergraduate level. The present educational paper attempts to fill in this gap. To this end we collect in this paper the relevant climate data and present a simple zero and one dimensional models for the mean temperature of the Earth. These models can exhibit bifurcations from the present Earth climate to an ice age or a ""Venus type of climate"". The models are accompanied by Matlab programs which enable the user to change the models parameters and explore the impact that these changes might have on their predictions on Earth climate.",['Mayer Humi'],2019-07-15T15:45:48Z,http://arxiv.org/abs/1907.11067v2,"Sustainability, Industry & Robotics",Climate Modeling,"the paper presents a simple zero and one dimensional models for the mean temperature of the Earth . the models can exhibit bifurcations from the present Earth climate to an ice age or a ""Venus type of climate"""
The physics of climate change: simple models in climate science,There is a perception that climate science can only be approached with complex computer simulations. But working climate scientists often use simple models to understand their simulations and make order-of-magnitude estimates. This article presents some of these simple models with the goal of making climate science more accessible and comprehensible.,['Nadir Jeevanjee'],2018-02-08T02:51:51Z,http://arxiv.org/abs/1802.02695v2,"Sustainability, Industry & Robotics",Climate Modeling,working climate scientists often use simple models to understand their simulations . this article presents some of these simple models with the goal of making climate science more accessible .
EpiClim: Weekly District-Wise all-India multi-epidemics Climate-Health   Dataset for accelerated GeoHealth research,"Climate change significantly impacts public health, driving the emergence and spread of epidemics. Climate health models are essential for assessing and predicting disease outbreaks influenced by climatic variables like temperature and precipitation. For instance, dengue and malaria correlate with temperature changes, while cholera is linked to precipitation anomalies. Advances in AI-enabled weather prediction (AI-NWP) have improved forecasting, but integrating climate models with health systems is hindered by the lack of comprehensive, granular health datasets. This study introduces EpiClim: India's Epidemic-Climate Dataset, the first weekly district-wise dataset for major epidemics in India from 2009 to the present, sourced from the Integrated Disease Surveillance Programme (IDSP). The dataset, covering diseases like dengue, malaria, and acute-diarrheal disease, bridges the gap between climate and health data, enabling the integration of climate forecasts with epidemic prediction models. This work lays the foundation for coupling predictive climate health models with weather and climate models, advancing efforts to mitigate climate-induced public health crises.","['Gurleen Kaur', 'Shubham Ghoshal', 'Reena Marbate', 'Neetiraj Malviya', 'Arshmehar Kaur', 'Vaisakh SB', 'Amit Kumar Srivastava', 'Manmeet Singh']",2025-01-17T23:12:08Z,http://arxiv.org/abs/2501.18602v2,"Sustainability, Industry & Robotics",Climate Modeling,"climate health models are essential for assessing and predicting disease outbreaks . dengue and malaria correlate with temperature changes, while cholera is linked to precipitation anomalies . lack of comprehensive, granular health datasets hinders integrating climate models with health systems ."
Link Climate: An Interoperable Knowledge Graph Platform for Climate Data,"Climate science has become more ambitious in recent years as global awareness about the environment has grown. To better understand climate, historical climate (e.g. archived meteorological variables such as temperature, wind, water, etc.) and climate-related data (e.g. geographical features and human activities) are widely used by today's climate research to derive models for an explainable climate change and its effects. However, such data sources are often dispersed across a multitude of disconnected data silos on the Web. Moreover, there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis, therefore, researchers must face a stern challenge in collecting and analyzing multi-source data. In this paper, we address this problem by proposing a climate knowledge graph for the integration of multiple climate data and other data sources into one service, leveraging Web technologies (e.g. HTTP) for multi-source climate data analysis. The proposed knowledge graph is primarily composed of data from the National Oceanic and Atmospheric Administration's daily climate summaries, OpenStreetMap, and Wikidata, and it supports joint data queries on these widely used databases. This paper shows, with a use case in Ireland and the United Kingdom, how climate researchers could benefit from this platform as it allows them to easily integrate datasets from different domains and geographical locations.","['Jiantao Wu', 'Fabrizio Orlandi', ""Declan O'Sullivan"", 'Soumyabrata Dev']",2022-10-28T10:52:50Z,http://arxiv.org/abs/2210.16050v1,"Sustainability, Industry & Robotics",Climate Modeling,climate science has become more ambitious in recent years as global awareness about the environment has grown . historical climate and climate-related data are widely used to derive models for an explainable climate change and its effects . there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis . this paper proposes a climate knowledge graph to integrate multiple climate data and other data sources into one service .
The Snowball Earth transition in a climate model with drifting   parameters,"Using an intermediate complexity climate model (Planet Simulator), we investigate the so-called Snowball Earth transition. For certain values of the solar constant, the climate system allows two different stable states: one of them is the Snowball Earth, covered by ice and snow, and the other one is today's climate. In our setup, we consider the case when the climate system starts from its warm attractor (the stable climate we experience today), and the solar constant is decreased continuously in finite time, according to a parameter drift scenario, to a state, where only the Snowball Earth's attractor remains stable. This induces an inevitable transition, or climate tipping from the warm climate. The reverse transition is also discussed. Increasing the solar constant back to its original value on individual simulations, we find that the system stays stuck in the Snowball state. However, using ensemble methods i.e., using an ensemble of climate realizations differing only slightly in their initial conditions we show that the transition from the Snowball Earth to the warm climate is also possible with a certain probability. From the point of view of dynamical systems theory, we can say that the system's snapshot attractor splits between the warm climate's and the Snowball Earth's attractor.","['Bálint Kaszás', 'Tímea Haszpra', 'Mátyás Herein']",2019-05-31T20:56:11Z,http://arxiv.org/abs/1906.00952v1,"Sustainability, Industry & Robotics",Climate Modeling,"the climate system allows two different stable states: the Snowball Earth and today's climate . the system starts from its warm attractor, and the solar constant is decreased continuously . this induces an inevitable transition, or climate tipping from the warm climate."
State-dependence of climate sensitivity: attractor constraints and   palaeoclimate regimes,"Equilibrium climate sensitivity (ECS) is a key predictor of climate change. However, it is not very well constrained, either by climate models or by observational data. The reasons for this include strong internal variability and forcing on many time scales. In practise this means that the 'equilibrium' will only be relative to fixing the slow feedback processes before comparing palaeoclimate sensitivity estimates with estimates from model simulations. In addition, information from the late Pleistocene ice age cycles indicates that the climate cycles between cold and warm regimes, and the climate sensitivity varies considerably between regime because of fast feedback processes changing relative strength and time scales over one cycle.   In this paper we consider climate sensitivity for quite general climate dynamics. Using a conceptual Earth system model of Gildor and Tziperman (2001) (with Milankovich forcing and dynamical ocean biogeochemistry) we explore various ways of quantifying the state-dependence of climate sensitivity from unperturbed and perturbed model time series. Even without considering any perturbations, we suggest that climate sensitivity can be usefully thought of as a distribution that quantifies variability within the 'climate attractor' and where there is a strong dependence on climate state and more specificially on the 'climate regime' where fast processes are approximately in equilibrium. We also consider perturbations by instantaneous doubling of CO$_2$ and similarly find a strong dependence on the climate state using our approach.","['Anna S. von der Heydt', 'Peter Ashwin']",2016-04-12T09:05:20Z,http://arxiv.org/abs/1604.03311v2,"Sustainability, Industry & Robotics",Climate Modeling,"palaeoclimate sensitivity (ECS) is a key predictor of climate change . but it is not very well constrained, either by climate models or by observational data . the reasons for this include strong internal variability and forcing on many time scales ."
Trend and Thoughts: Understanding Climate Change Concern using Machine   Learning and Social Media Data,"Nowadays social media platforms such as Twitter provide a great opportunity to understand public opinion of climate change compared to traditional survey methods. In this paper, we constructed a massive climate change Twitter dataset and conducted comprehensive analysis using machine learning. By conducting topic modeling and natural language processing, we show the relationship between the number of tweets about climate change and major climate events; the common topics people discuss climate change; and the trend of sentiment. Our dataset was published on Kaggle (\url{https://www.kaggle.com/leonshangguan/climate-change-tweets-ids-until-aug-2021}) and can be used in further research.","['Zhongkai Shangguan', 'Zihe Zheng', 'Lei Lin']",2021-11-06T19:59:03Z,http://arxiv.org/abs/2111.14929v1,"Sustainability, Industry & Robotics",Climate Modeling,"in this paper, we constructed a massive climate change Twitter dataset . we conducted comprehensive analysis using machine learning . our dataset was published on Kaggle ."
Leveraging machine learning to enhance climate models: a review,"Recent achievements in machine learning (Ml) have had a significant impact on various fields, including climate science. Climate modeling is very important and plays a crucial role in shaping the decisions of governments and individuals in mitigating the impact of climate change. Climate change poses a serious threat to humanity, however, current climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMS) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ml has been utilized in the last 5 years to boost the current state-of-the-art climate models. We invite the ml community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ml as a powerful tool in this endeavor.","['Ahmed Elsayed', 'Shrouk Wally', 'Islam Alkabbany', 'Asem Ali', 'Aly Farag']",2023-11-15T22:30:32Z,http://arxiv.org/abs/2311.09413v1,"Sustainability, Industry & Robotics",Climate Modeling,"recent advances in machine learning (ml) have had a significant impact on various fields, including climate science . climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy . this review paper invites the ml community to join in the global effort to accurately model the earth climate ."
A new framework for climate sensitivity and prediction: a modelling   perspective,"The sensitivity of climate models to increasing CO2 concentration and the climate response at decadal time scales are still major factors of uncertainty for the assessment of the long and short term effects of anthropogenic climate change. While the relative slow progress on these issues is partly due to the inherent inaccuracies of numerical climate models, this also hints at the need for stronger theoretical foundations to the problem of studying climate sensitivity and performing climate change predictions with numerical models. Here we demonstrate that it is possible to use Ruelle's response theory to predict the impact of an arbitrary CO2 forcing scenario on the global surface temperature of a general circulation model. Response theory puts the concept of climate sensitivity on firm theoretical grounds, and addresses rigorously the problem of predictability at different time scales. Conceptually, our results show that performing climate change experiments with general circulation models is a well defined problem from a physical and mathematical point of view. Practically, our results show that considering one single CO2 forcing scenario is enough to construct operators able to predict the response of climatic observables to any other CO2 forcing scenario, without the need to perform additional numerical simulations. We also introduce a general relationship between climate sensitivity and climate response at different time scales, thus providing an explicit definition of the inertia of the system at different time scales. While what we report here refers to the linear response, the general theory allows for treating nonlinear effects as well. Our results pave the way for redesigning and interpreting climate change experiments from a radically new perspective.","['Francesco Ragone', 'Valerio Lucarini', 'Frank Lunkeit']",2014-03-19T18:35:09Z,http://arxiv.org/abs/1403.4908v2,"Sustainability, Industry & Robotics",Climate Modeling,sensitivity of climate models to increasing CO2 concentration still major factors of uncertainty . response theory can be used to predict impact of arbitrary CO2 forcing scenario . results pave the way for redesigning and interpreting climate change experiments .
Social dynamics can delay or prevent climate tipping points by speeding   the adoption of climate change mitigation,"Social behaviour models are increasingly integrated into climate change studies, and the significance of climate tipping points for `runaway' climate change is well recognised. However, there has been insufficient focus on tipping points in social-climate dynamics. We developed a coupled social-climate model consisting of an Earth system model and a social behaviour model, both with tipping elements. The social model explores opinion formation by analysing social learning rates, the net cost of mitigation, and the strength of social norms. Our results indicate that the net cost of mitigation and social norms have minimal impact on tipping points when social norms are weak. As social norms strengthen, the climate tipping point can trigger a tipping element in the social model. However, faster social learning can delay or prevent the climate tipping point: sufficiently fast social learning means growing climate change mitigation can outpace the oncoming climate tipping point, despite social-climate feedback. By comparing high- and low-risk scenarios, we demonstrated high-risk scenarios increase the likelihood of tipping points. We also illustrate the role of a critical temperature anomaly in triggering tipping points. In conclusion, understanding social behaviour dynamics is vital for predicting climate tipping points and mitigating their impacts.","['Yazdan Babazadeh Maghsoodlo', 'Madhur Anand', 'Chris T. Bauch']",2025-01-23T21:05:22Z,http://arxiv.org/abs/2501.14096v1,"Sustainability, Industry & Robotics",Climate Modeling,"social behaviour models are increasingly integrated into climate change studies . the significance of climate tipping points for runaway' climate change is well recognised . however, there has been insufficient focus on tipping point in social-climate dynamics ."
GCM Simulations of Unstable Climates in the Habitable Zone,"It has recently been proposed that Earth-like planets in the outer regions of the habitable zone experience unstable climates, repeatedly cycling between glaciated and deglaciated climatic states (Menou 2015). While this result has been confirmed and also extended to explain early Mars climate records (Haqq-Misra et al. 2016; Batalha et al. 2016), all existing work relies on highly idealized low-dimensional climate models. Here, we confirm that the phenomenology of climate cycles remains in 3D Earth climate models with considerably more degrees of freedom. To circumvent the computational barrier of integrating climate on Gyr timescales, we design a hybrid 0D-3D integrator which uses a general circulation model (GCM) as a short relaxation step along a long evolutionary climate sequence. We find that GCM climate cycles are qualitatively consistent with reported low-dimensional results. This establishes on a firmer ground the notion that outer habitable zone planets may be preferentially found in transiently glaciated states.","['Adiv Paradise', 'Kristen Menou']",2017-04-14T20:36:42Z,http://arxiv.org/abs/1704.04535v4,"Sustainability, Industry & Robotics",Climate Modeling,"the phenomenology of climate cycles remains in 3D Earth climate models with considerably more degrees of freedom . to circumvent the computational barrier of integrating climate on Gyr timescales, we design a hybrid 0D-3D integrator ."
Machine learning for climate physics and simulations,"We discuss the emerging advances and opportunities at the intersection of machine learning (ML) and climate physics, highlighting the use of ML techniques, including supervised, unsupervised, and equation discovery, to accelerate climate knowledge discoveries and simulations. We delineate two distinct yet complementary aspects: (1) ML for climate physics and (2) ML for climate simulations. While physics-free ML-based models, such as ML-based weather forecasting, have demonstrated success when data is abundant and stationary, the physics knowledge and interpretability of ML models become crucial in the small-data/non-stationary regime to ensure generalizability. Given the absence of observations, the long-term future climate falls into the small-data regime. Therefore, ML for climate physics holds a critical role in addressing the challenges of ML for climate simulations. We emphasize the need for collaboration among climate physics, ML theory, and numerical analysis to achieve reliable ML-based models for climate applications.","['Ching-Yao Lai', 'Pedram Hassanzadeh', 'Aditi Sheshadri', 'Maike Sonnewald', 'Raffaele Ferrari', 'Venkatramani Balaji']",2024-04-20T01:37:29Z,http://arxiv.org/abs/2404.13227v2,"Sustainability, Industry & Robotics",Climate Modeling,"machine learning (ML) and climate physics are emerging areas of convergence . physics knowledge and interpretability of ML models become crucial in the small-data regime . the long-term future climate falls into the small data regime, so ML for climate phy holds a critical role ."
Towards Specialized Supercomputers for Climate Sciences: Computational   Requirements of the Icosahedral Nonhydrostatic Weather and Climate Model,"We discuss the computational challenges and requirements for high-resolution climate simulations using the Icosahedral Nonhydrostatic Weather and Climate Model (ICON). We define a detailed requirements model for ICON which emphasizes the need for specialized supercomputers to accurately predict climate change impacts and extreme weather events. Based on the requirements model, we outline computational demands for km-scale simulations, and suggests machine learning techniques to enhance model accuracy and efficiency. Our findings aim to guide the design of future supercomputers for advanced climate science.","['Torsten Hoefler', 'Alexandru Calotoiu', 'Anurag Dipankar', 'Thomas Schulthess', 'Xavier Lapillonne', 'Oliver Fuhrer']",2024-05-18T08:03:31Z,http://arxiv.org/abs/2405.13043v1,"Sustainability, Industry & Robotics",Climate Modeling,the Icosahedral Nonhydrostatic Weather and Climate Model (ICON) is a supercomputer model . it emphasizes the need for specialized supercomputers to accurately predict climate change impacts and extreme weather events . our findings aim to guide the design of future supercomputer systems for advanced climate science .
Dimensional analysis identifies contrasting dynamics of past climate   states and critical transitions,"While one can unequivocally identify past climate transitions, we lack comprehensive knowledge about their underlying mechanisms and timescales. Our study employs a dimensional analysis of benthic stable isotope records to uncover, across different timescales, how the climatic fluctuation of the Cenozoic are associated with changes in the number of effective degrees of freedom. Precession timescales dominate the Hothouse and Warmhouse states, while the Icehouse climate is primarily influenced by obliquity and eccentricity timescales. Notably, the Coolhouse state lacks dominant timescales. Our analysis proves effective in objectively identifying abrupt climate shifts and extremes. This is also demonstrated using high-resolution data from the last glacial cycle, revealing abrupt climate shifts within a single climate state. These findings significantly impact our understanding of the inherent stability of each climate state and the evaluation of (paleo-)climate models' ability to replicate key features of past/future climate states and transitions.","['Tommaso Alberti', 'Fabio Florindo', 'Eelco J. Rohling', 'Valerio Lucarini', 'Davide Faranda']",2023-09-22T08:05:59Z,http://arxiv.org/abs/2309.12693v1,"Sustainability, Industry & Robotics",Climate Modeling,"a dimensional analysis of benthic stable isotope records uncovers how climatic fluctuation of the Cenozoic is associated with changes in the number of effective degrees of freedom . Precession timescales dominate the Hothouse and Warmhouse states, while the Icehouse climate is primarily influenced by obliquity and eccentricity time scales ."
Pricing Energy in the Presence of Renewables,"At present, electricity markets largely ignore the fact that renewable power producers impose significant externalities on non-renewable energy producers. This is because consumers are generally guaranteed electricity within certain load parameters. The intermittent nature of production by renewable energy producers implies that they rely on non-renewable producers so that the aggregate power delivered meets the promised quality of service. This implicit insurance provided by the non-renewable power sector to consumers is not currently priced and leads to an often ignored, hidden monetary transfer from non-renewable producers to renewable producers. As the fraction of energy supplied by renewable resources increases, these externalities also increase. In this paper, we quantify these externalities by developing the market clearing price of energy in the presence of renewable energy. We consider a day-ahead electricity market where renewable and non-renewable generators bid by proposing their asking price per unit of energy to an independent system operator (ISO). The ISO's problem is a multi-stage stochastic optimization problem to dispatch energy from each generator to minimize the cost of purchased energy on behalf of the consumers. We incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in the day-ahead electricity market to ensure that the generators are able to meet the load within a desired confidence level. We analytically derive the market clearing price of energy as a function of CVAR. It is shown that a higher penetration level of the renewable energies may increase the market clearing price of energy.","['Ashkan Zeinalzadeh', 'Indraneel Chakraborty', 'Vijay Gupta']",2016-11-23T21:26:17Z,http://arxiv.org/abs/1611.08006v1,"Sustainability, Industry & Robotics",Renewable Energy,renewable power producers impose externalities on non-renewable energy producers . consumers generally guaranteed electricity within certain load parameters . higher penetration of renewable energies may increase the market clearing price of energy .
Control Strategies for Microgrids with Renewable Energy Generation and   Battery Energy Storage Systems,"In this report, several control strategies for microgrids with renewable energy resources and battery energy storage systems are proposed. Renewable energy has the potential to reduce global carbon emissions, while higher penetration of renewable energy is hindered by the inherent variability and intermittency of renewable resources. This motivates the development of microgrids supplied by renewable energy resources. With proper control of storage units and communications with the energy market, the non-dispatchable energy can be regulated to reduce the difficulty of power scheduling in the grid operation. Battery energy storage systems (BESS) are widely used to make renewable energy more controllable and usable on demand. With suitable prediction techniques and control algorithms, BESS can store part of the generated renewable energy and supply some stored energy at different periods, so that the actual power dispatched can meet the required operating criteria. The control algorithms introduced in this report are developed to maximize the profit and minimize the energy cost in microgrids, which require predicted data on renewable power, temperature, electricity price, and load demand.",['Wenhao Zhuo'],2019-11-05T22:57:29Z,http://arxiv.org/abs/1911.02126v1,"Sustainability, Industry & Robotics",Renewable Energy,report proposes several control strategies for microgrids with renewable energy resources . renewable energy has the potential to reduce global carbon emissions . higher penetration of renewable energy is hindered by inherent variability and intermittency .
The Benefits of Hydrogen Energy Transmission and Conversion Systems to   the Renewable Power Grids: Day-ahead Unit Commitment,"The curtailment of renewable energy is more frequently observed as the renewable penetration levels are rising rapidly in modern power systems. It is a waste of free and green renewable energy and implies current power grids are unable to accommodate more renewable sources. One major reason is that higher power transmission capacity is required for higher renewable penetration level. Another major reason is the volatility of the renewable generation. The hydrogen mix or pure hydrogen pipeline can both transfer and store the energy in the form of hydrogen. However, its potential of accelerating renewable integration has not been investigated. In this paper, hydrogen pipeline networks, combined with power-to-hydrogen (P2H) and hydrogen-to-power (H2P) facilities, are organized to form a Hydrogen Energy Transmission and Conversion System (HETCS). We investigate the operation of power systems coupled with HETCS, and propose the day-ahead security-constrained unit commitment (SCUC) with HETCS. The SCUC simulation is conducted on a modified IEEE 24-bus power system with HETCS. Simulation results show HETCS can substantially reduce the renewable curtailment, CO2 emission, load payment and total operational cost. This study validates the HETCS can be a promising solution to achieve net-zero renewable grids.","['Jin Lu', 'Xingpeng Li']",2022-06-28T20:25:16Z,http://arxiv.org/abs/2206.14279v1,"Sustainability, Industry & Robotics",Renewable Energy,the curtailment of renewable energy is more frequently observed . renewable penetration levels are rising rapidly in modern power systems . higher power transmission capacity is required for higher renewable penetration level .
HouYi: An open-source large language model specially designed for   renewable energy and carbon neutrality field,"Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE Bot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.","['Mingliang Bai', 'Zhihao Zhou', 'Ruidong Wang', 'Yusheng Yang', 'Zizhen Qin', 'Yunxiao Chen', 'Chunjin Mu', 'Jinfu Liu', 'Daren Yu']",2023-07-31T06:59:36Z,http://arxiv.org/abs/2308.01414v1,"Sustainability, Industry & Robotics",Renewable Energy,a paper published the first open-source dataset for non-commercial LLM research of renewable energy . HouYi model demonstrated powerful academic paper paragraph generation ability in renewable energy field .
Communication Systems for Grid Integration of Renewable Energy Resources,"There is growing interest in renewable energy around the world. Since most renewable sources are intermittent in nature, it is a challenging task to integrate renewable energy resources into the power grid infrastructure. In this grid integration, communication systems are crucial technologies, which enable the accommodation of distributed renewable energy generation and play extremely important role in monitoring, operating, and protecting both renewable energy generators and power systems. In this paper, we review some communication technologies available for grid integration of renewable energy resources. Then, we present the communication systems used in a real renewable energy project, Bear Mountain Wind Farm (BMW) in British Columbia, Canada. In addition, we present the communication systems used in Photovoltaic Power Systems (PPS). Finally, we outline some research challenges and possible solutions about the communication systems for grid integration of renewable energy resources.","['F. Richard Yu', 'Peng Zhang', 'Weidong Xiao', 'Paul Choudhury']",2011-07-17T16:00:36Z,http://arxiv.org/abs/1107.3313v1,"Sustainability, Industry & Robotics",Renewable Energy,"communication systems are crucial technologies for grid integration of renewable energy resources . communication systems play extremely important role in monitoring, operating and protecting renewable energy generators and power systems ."
Energy Management for Renewable-Colocated Artificial Intelligence Data   Centers,"We develop an energy management system (EMS) for artificial intelligence (AI) data centers with colocated renewable generation. Under a profit-maximizing framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI workload scheduling, on-site renewable utilization, and electricity market participation. Within both wholesale and retail market participation models, the economic benefit of the RCDC operation is maximized. Empirical evaluations using real-world traces of electricity prices, data center power consumption, and renewable generation demonstrate significant profit gains from renewable and AI data center colocations.","['Siying Li', 'Lang Tong', 'Timothy D. Mount']",2025-07-04T18:25:42Z,http://arxiv.org/abs/2507.08011v1,"Sustainability, Industry & Robotics",Renewable Energy,"the EMS of renewable-colocated data center (RCDC) co-optimizes AI workload scheduling, on-site renewable utilization, and electricity market participation . the economic benefit of the RCDC operation is maximized ."
Renewable levelized cost of energy available for export: An indicator   for exploring global renewable energy trade potential,"Renewable energy resources are widely available, yet they are unevenly distributed globally. In a renewable future, countries lacking high-quality renewable resources may choose to import energy from other countries. To assess the resource-dependent and techno-economic basis for global renewable energy trade and identify potential importers and exporters, this study introduces two new metrics: Renewable Levelized Cost of Energy available for Export (RLCOE_Ex) and Potential Energy Export Volume (PEEV). These metrics are computed based on regional resource potential, domestic energy demand and varying financial costs across countries, without the need for any energy system modeling. By applying these two metrics to 165 countries/regions, we identify countries with significant potential for exporting renewable energy (e.g., the US, China) and those that lack the domestic resources to satisfy demand (e.g., South Korea, Japan). The RLCOE_Ex and PEEV metrics are validated through a separate analysis, employing a comprehensive energy system model for each country/region.","['Xiaoming Kan', 'Lina Reichenberg', 'Fredrik Hedenus', 'David Daniels']",2022-02-02T16:50:33Z,http://arxiv.org/abs/2202.02257v2,"Sustainability, Industry & Robotics",Renewable Energy,"renewable energy resources are widely available, yet they are unevenly distributed globally . this study introduces two new metrics: Renewable Levelized Cost of Energy available for Export (RLCOE_Ex) and Potential Energy Export Volume (PEEV) metrics are computed based on regional resource potential, domestic energy demand and varying financial costs across countries ."
Reliability and Market Price of Energy in the Presence of Intermittent   and Non-Dispatchable Renewable Energies,"The intermittent nature of the renewable energies increases the operation costs of conventional generators. As the share of energy supplied by renewable sources increases, these costs also increase. In this paper, we quantify these costs by developing a market clearing price of energy in the presence of renewable energy and congestion constraints. We consider an electricity market where generators propose their asking price per unit of energy to an independent system operator (ISO). The ISO solve an optimization problem to dispatch energy from each generator to minimize the total cost of energy purchased on behalf of the consumers.   To ensure that the generators are able to meet the load within a desired confidence level, we incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in an electricity market and we derive the amount of committed power and market clearing price of energy as a function of CVAR. It is shown that a higher penetration of renewable energies may increase the committed power, market clearing price of energy and consumer cost of energy due to renewable generation uncertainties. We also obtain an upper-bound on the amount that congestion constraints can affect the committed power. We present descriptive simulations to illustrate the impact of renewable energy penetration and reliability levels on committed power by the non-renewable generators, difference between the dispatched and committed power, market price of energy and profit of renewable and non-renewable generators.","['Ashkan Zeinalzadeh', 'Donya Ghavidel', 'Vijay Gupta']",2018-02-05T19:22:23Z,http://arxiv.org/abs/1802.08286v1,"Sustainability, Industry & Robotics",Renewable Energy,"a higher penetration of renewable energies may increase the committed power, market clearing price of energy and consumer cost of energy due to renewable generation uncertainties . the paper presents descriptive simulations to illustrate the impact of renewable energy penetration and reliability levels on committed power ."
Brownian motion with stochastic energy renewals,"We investigate the impact of intermittent energy injections on a Brownian particle, modeled as stochastic renewals of its kinetic energy to a fixed value. Between renewals, the particle follows standard underdamped Langevin dynamics. For energy renewals occurring at a constant rate, we find non-Boltzmannian energy distributions that undergo a shape transition driven by the competition between the velocity relaxation timescale and the renewal timescale. In the limit of rapid renewals, the dynamics mimics one-dimensional run-and-tumble motion, while at finite renewal rates, the effective diffusion coefficient exhibits non-monotonic behavior. To quantify the system's departure from equilibrium, we derive a modified fluctuation-response relation and demonstrate the absence of a consistent effective temperature. The dissipation is characterized by deviations from equilibrium-like response, captured via the Harada-Sasa relation. Finally, we extend the analysis to non-Poissonian renewal processes and introduce a dimensionless conversion coefficient that quantifies the thermodynamic cost of diffusion.","['Ion Santra', 'Kristian Stølevik Olsen']",2025-06-10T15:07:07Z,http://arxiv.org/abs/2506.08876v1,"Sustainability, Industry & Robotics",Renewable Energy,"intermittent energy injections on a Brownian particle are investigated . the particle follows standard underdamped Langevin dynamics . at finite renewal rates, the effective diffusion coefficient exhibits non-monotonic behavior."
Unlocking the Potential of Renewable Energy Through Curtailment   Prediction,A significant fraction (5-15%) of renewable energy generated goes into waste in the grids around the world today due to oversupply issues and transmission constraints. Being able to predict when and where renewable curtailment occurs would improve renewable utilization. The core of this work is to enable the machine learning community to help decarbonize electricity grids by unlocking the potential of renewable energy through curtailment prediction.,"['Bilge Acun', 'Brent Morgan', 'Henry Richardson', 'Nat Steinsultz', 'Carole-Jean Wu']",2024-05-28T18:48:39Z,http://arxiv.org/abs/2405.18526v1,"Sustainability, Industry & Robotics",Renewable Energy,a fraction (5-15%) of renewable energy goes into waste in the grids around the world . the core of this work is to enable the machine learning community to help decarbonize electricity grids .
Energy Allocation Policy for Cellular Networks Powered by Renewable   Energy,"The explosive wireless data service requirement accompanied with carbon dioxide emission and consumption of traditional energy has put pressure on both industry and academia. Wireless networks powered with the uneven and intermittent generated renewable energy have been widely researched and lead to a new research paradigm called green communication. In this paper, we comprehensively consider the total generated renewable energy, QoS requirement and channel quality, then propose a utility based renewable energy allocation policy. The utility here means the satisfaction degree of users with a certain amount allocated renewable energy. The energy allocation problem is formulated as a constraint optimization problem and a heuristic algorithm with low complexity is derived to solve the raised problem. Numerical results show that the renewable energy allocation policy is applicable for any situation. When the renewable energy is very scarce, only users with good channel quality can achieve allocated energy.","['Qiao Li', 'Yifei Wei', 'Mei Song', 'F. Richard Yu']",2016-12-11T18:35:10Z,http://arxiv.org/abs/1612.03452v1,"Sustainability, Industry & Robotics",Renewable Energy,wireless networks powered with uneven and intermittent generated renewable energy . a utility based renewable energy allocation policy is proposed . the energy allocation problem is formulated as a constraint optimization problem .
An Insurance Contract Design to Boost Storage Participation in the   Electricity Market,"Energy storage technologies are key to improving grid flexibility in the presence of increasing amounts of intermittent renewable generation. We propose an insurance contract that suitably compensates energy storage systems for providing flexibility. Such a contract provides a wider range of market opportunities for these systems while also incentivizing higher renewable penetration in the grid. We consider a day-ahead market in which generators, including renewables and storage owners, bid to be scheduled for the next operating day. Due to production uncertainty, renewable generators may be unable to meet their day-ahead production schedule, and thus be subject to a penalty. As a hedge against these penalties, we propose an insurance contract between a renewable producer and a storage owner, in which the storage reserves some energy to be used in case of renewable shortfalls. We show that such a contract incentivizes the renewable player to bid higher, thus increasing renewable participation in the electricity mix. It also provides an extra source of revenue for storage owners that may not be profitable with a purely arbitrage-based strategy in the day-ahead market. Further, we prove this contract is economically beneficial for both players. We validate our analysis through two case studies.","['Nayara Aguiar', 'Vijay Gupta']",2020-07-29T21:17:30Z,http://arxiv.org/abs/2007.15115v1,"Sustainability, Industry & Robotics",Renewable Energy,energy storage technologies are key to improving grid flexibility . we propose an insurance contract that suitably compensates energy storage systems . such a contract provides a wider range of market opportunities .
Value of Storage for Renewable Portfolio Standard,"The ambitious targets for renewable energy penetration warrant huge flexibility in the power system. Such flexibility does not come free. In this paper, we examine the possibility of utilizing storage systems for achieving high renewable energy penetration, and identify the trade-off between providing flexibility and arbitrage against real-time prices. More precisely, we investigate the relationship among the operation cost, storage capacity, and the renewable penetration level. This illustrates the value of storage as well as the true cost induced by the high renewable penetration targets.","['Jiasheng Zhang', 'Nan Gu', 'Yang Yu', 'Chenye Wu']",2019-12-07T09:38:41Z,http://arxiv.org/abs/1912.03476v1,"Sustainability, Industry & Robotics",Renewable Energy,"in this paper, we examine the possibility of utilizing storage systems for achieving high renewable energy penetration . we investigate the relationship between operation cost, storage capacity, and the renewable penetration level ."
"Integrating Renewable Energy Sources as Reserve Providers: Modeling,   Pricing, and Properties","In pursuit of carbon neutrality, many countries have adopted renewable portfolio standards to facilitate the integration of renewable energy. However, increasing penetration of renewable energy resources will also pose higher requirements on system flexibility. Allowing renewable themselves to participate in the reserve market could be a viable solution. To this end, this paper proposes an optimal dispatch model for joint energy-reserve procurement that incorporates renewable portfolio standards and RES serve as reserve providers. Potential generator outages and deviations in renewable and load power are modelled through a given number of probability-weighted scenarios. In particular, reserve resources are initially booked in the base case and then activated in non-base scenarios through the re-dispatch process. Marginal pricing is used to derive energy, reserve, and power deviation prices. Next, we develop the associated settlement process and establish several market properties. The proposed pricing scheme establishes equivalence between thermal generators and renewable units by accounting for their uncertainties, including thermal generator outages and renewable power deviations, and their flexibility, namely reserve and re-dispatch. We have shown that for renewable resources, supplying reserve according to the dispatch results compared to generating as much as possible leads to better profits. Simulations validate the effectiveness of the proposed method and properties established.","['Wenli Wu', 'Ye Guo', 'Jiantao Shi']",2023-12-24T07:16:34Z,http://arxiv.org/abs/2312.15424v1,"Sustainability, Industry & Robotics",Renewable Energy,the paper proposes an optimal dispatch model for joint energy-reserve procurement . it incorporates renewable portfolio standards and RES serve as reserve providers . potential generator outages and deviations in renewable and load power are modelled .
Towards Low-carbon Power Networks: Optimal Integration of Renewable   Energy Sources and Hydrogen Storage,"This paper proposes a new optimization model and solution method for determining optimal locations and sizing of renewable energy sources and hydrogen storage in a power network. We obtain these strategic decisions based on the multi-period alternating current optimal power (AC OPF) flow problem that considers the uncertainty of renewable output, electricity demand, and electricity prices. We develop a second-order cone programming approach within a Benders decomposition framework to provide globally optimal solutions. To the best of our knowledge, our paper is the first to propose a systematic optimization framework based on AC OPF that jointly analyzes power network, renewable, and hydrogen storage interactions in order to provide optimal locations and sizing decisions of renewables and hydrogen storage. In a test case, we show that the joint integration of renewable sources and hydrogen storage and consideration of the AC OPF model significantly reduces the operational cost of the power network. In turn, our findings can provide quantitative insights to decision-makers on how to integrate renewable sources and hydrogen storage under different settings of the hydrogen selling price, renewable curtailment costs, emission tax prices, and conversion efficiency.","['Sezen Ece Kayacık', 'Albert H. Schrotenboer', 'Evrim Ursavas', 'Iris F. A. Vis']",2022-07-22T12:55:49Z,http://arxiv.org/abs/2207.11054v1,"Sustainability, Industry & Robotics",Renewable Energy,the paper proposes a new method for determining optimal locations and sizing of renewables and hydrogen storage in a power network . the paper is the first to propose a systematic optimization framework based on AC OPF .
Optimal Workload Allocation for Distributed Edge Clouds With Renewable   Energy and Battery Storage,"This paper studies an optimal workload allocation problem for a network of renewable energy-powered edge clouds that serve users located across various geographical areas. Specifically, each edge cloud is furnished with both an on-site renewable energy generation unit and a battery storage unit. Due to the discrepancy in electricity pricing and the diverse temporal-spatial characteristics of renewable energy generation, how to optimally allocate workload to different edge clouds to minimize the total operating cost while maximizing renewable energy utilization is a crucial and challenging problem. To this end, we introduce and formulate an optimization-based framework designed for Edge Service Providers (ESPs) with the overarching goal of simultaneously reducing energy costs and environmental impacts through the integration of renewable energy sources and battery storage systems, all while maintaining essential quality-of-service standards. Numerical results demonstrate the effectiveness of the proposed model and solution in maintaining service quality as well as reducing operational costs and emissions. Furthermore, the impacts of renewable energy generation and battery storage on optimal system operations are rigorously analyzed.","['Duong Thuy Anh Nguyen', 'Jiaming Cheng', 'Ni Trieu', 'Duong Tung Nguyen']",2023-10-01T17:38:31Z,http://arxiv.org/abs/2310.00742v2,"Sustainability, Industry & Robotics",Renewable Energy,this paper studies an optimal workload allocation problem for a network of renewable energy-powered edge clouds . each edge cloud is furnished with both an on-site renewable energy generation unit and a battery storage unit . how to optimally allocate workload to different edge clouds is a crucial and challenging problem .
Exploring the Efficiency of Renewable Energy-based Modular Data Centers   at Scale,"Modular data centers (MDCs) that can be placed right at the energy farms and powered mostly by renewable energy, are proven to be a flexible and effective approach to lowering the carbon footprint of data centers. However, the main challenge of using renewable energy is the high variability of power produced, which implies large volatility in powering computing resources at MDCs, and degraded application performance due to the task evictions and migrations. This causes challenges for platform operators to decide the MDC deployment. To this end, we present SkyBox, a framework that employs a holistic and learning-based approach for platform operators to explore the efficient use of renewable energy with MDC deployment across geographical regions. SkyBox is driven by the insights based on our study of real-world power traces from a variety of renewable energy farms -- the predictable production of renewable energy and the complementary nature of energy production patterns across different renewable energy sources and locations. With these insights, SkyBox first uses the coefficient of variation metric to select the qualified renewable farms, and proposes a subgraph identification algorithm to identify a set of farms with complementary energy production patterns. After that, SkyBox enables smart workload placement and migrations to further tolerate the power variability. Our experiments with real power traces and datacenter workloads show that SkyBox has the lowest carbon emissions in comparison with current MDC deployment approaches. SkyBox also minimizes the impact of the power variability on cloud virtual machines, enabling rMDCs a practical solution of efficiently using renewable energy.","['Jinghan Sun', 'Zibo Gong', 'Anup Agarwal', 'Shadi Noghabi', 'Ranveer Chandra', 'Marc Snir', 'Jian Huang']",2024-06-04T12:18:14Z,http://arxiv.org/abs/2406.02252v1,"Sustainability, Industry & Robotics",Renewable Energy,the main challenge of using renewable energy is the high variability of power produced . this implies large volatility in powering computing resources at MDCs . SkyBox is a framework that employs a learning-based approach to explore the efficient use of renewable energy .
Enhanced Renewable Energy Forecasting and Operations through   Probabilistic Forecast Aggregation,"Accurate and reliable forecasting of renewable energy generation is crucial for the efficient integration of renewable sources into the power grid. In particular, probabilistic forecasts are becoming essential for managing the intrinsic variability and uncertainty of renewable energy production, especially wind and solar generation. This paper considers the setting where probabilistic forecasts are provided for individual renewable energy sites using, e.g., quantile regression models, but without any correlation information between sites. This setting is common if, e.g., such forecasts are provided by each individual site, or by multiple vendors. However, to effectively manage a fleet of renewable generators, it is necessary to aggregate these individual forecasts to the fleet level, while ensuring that the aggregated probabilistic forecast is statistically consistent and reliable. To address this challenge, this paper presents the integrated use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts into a statistically calibrated, probabilistic forecast at the fleet level. The proposed framework is validated using synthetic data from several large-scale systems in the United States. This work has important implications for grid operators and energy planners, providing them with better tools to manage the variability and uncertainty inherent in renewable energy production.","['Alireza Moradi', 'Mathieu Tanneau', 'Reza Zandehshahvar', 'Pascal Van Hentenryck']",2025-02-10T20:15:02Z,http://arxiv.org/abs/2502.07010v1,"Sustainability, Industry & Robotics",Renewable Energy,this paper presents the use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts . the proposed framework is validated using synthetic data from several large-scale systems in the u.s.
Optimizing hydrogen and e-methanol production through Power-to-X   integration in biogas plants,"The European Union strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure. These fuels will be crucial as energy carriers and balancing agents for renewable energy variability. Large scale production requires more renewable capacity, and various Power to X (PtX) concepts are emerging in renewable rich countries. However, sourcing renewable carbon to scale carbon based electro fuels is a significant challenge. This study explores a PtX hub that sources renewable CO2 from biogas plants, integrating renewable energy, hydrogen production, and methanol synthesis on site. This concept creates an internal market for energy and materials, interfacing with the external energy system. The size and operation of the PtX hub were optimized, considering integration with local energy systems and a potential hydrogen grid. The levelized costs of hydrogen and methanol were estimated for a 2030 start, considering new legislation on renewable fuels of non biological origin (RFNBOs). Our results show the PtX hub can rely mainly on on site renewable energy, selling excess electricity to the grid. A local hydrogen grid connection improves operations, and the behind the meter market lowers energy prices, buffering against market variability. We found methanol costs could be below 650 euros per ton and hydrogen production costs below 3 euros per kg, with standalone methanol plants costing 23 per cent more. The CO2 recovery to methanol production ratio is crucial, with over 90 per cent recovery requiring significant investment in CO2 and H2 storage. Overall, our findings support planning PtX infrastructures integrated with the agricultural sector as a cost effective way to access renewable carbon.","['Alberto Alamia', 'Behzad Partoon', 'Eoghan Rattigan', 'Gorm Brunn Andresen']",2024-06-01T13:40:36Z,http://arxiv.org/abs/2406.00442v1,"Sustainability, Industry & Robotics",Renewable Energy,"eu strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure . various Power to X (PtX) concepts are emerging in renewable rich countries . this study explores a PtX hub that sources renewable CO2 from biogas plants . it integrates renewable energy, hydrogen production and methanol synthesis on site ."
Delay-optimal Data Transmission in Renewable Energy Aided Cognitive   Radio Networks,"Renewable energy powered cognitive radio (CR) network has gained much attention due to its combination of the CR's spectrum efficiency and the renewable energy's ""green"" nature. In the paper, we investigate the delay-optimal data transmission in the renewable energy aided CR networks. Specifically, a primary user (PU) and a secondary user (SU) share the same frequency in an area. The SU's interference to the PU is controlled by interference-signal-ratio (ISR) constraint, which means that the ISR at the PU receiver (Rx) should be less than a threshold. Under this constraint, the renewable energy powered SU aims to minimize the average data buffer delay by scheduling the renewable allocations in each slot. A constrained stochastic optimization problem is formulated when the randomness of the renewable arrival, the uncertainty of the SU's data generation, and the variability of the fading channel are taken into account. By analyzing the formulated problem, we propose two practical algorithms that is optimal for two special scenarios. And the two algorithms respectively give an upper and a lower bound for the general scenario. In addition, the availability of the PU's private information at the SU is discussed. Finally, numerical simulations verify the effectiveness of the proposed algorithm.","['Tian Zhang', 'Wei Chen']",2015-10-03T09:18:09Z,http://arxiv.org/abs/1510.00798v1,"Sustainability, Industry & Robotics",Renewable Energy,a primary user (PU) and a secondary user (SU) share the same frequency in an area . the SU's interference to the PU is controlled by interference-signal-ratio (iSR) constraint . a constrained stochastic optimization problem is formulated .
A Formal Specification Framework for Smart Grid Components,"Smart grid can be considered as the next step in the evolution of power systems. It comprises of different entities and objects ranging from smart appliances, smart meters, generators, smart storages, and more. One key problem in modeling smart grid is that while currently there is a considerable focus on the practical aspects of smart grid, there are very few modeling attempts and even lesser attempts at formalization. To the best of our knowledge, among other formal methods, formal specification has previously not been applied in the domain of smart grid. In this paper, we attempt to bridge this gap by presenting a novel approach to modeling smart grid components using a formal specification approach. We use a state-based formal specification language namely Z (pronounced as `Zed') since we believe Z is particularly suited for modeling smart grid components.We demonstrate the application of Z on key smart grid components. The presented formal specification can be considered as first steps towards modeling of smart grid using a Software Engineering formalism. It also demonstrates how complex systems, such as the smart grid, can be modeled elegantly using formal specification.","['Waseem Akram', 'Muaz A. Niazi']",2017-11-25T04:06:24Z,http://arxiv.org/abs/1711.09184v1,"Sustainability, Industry & Robotics",Smart Grids,smart grid can be considered as the next step in the evolution of power systems . there are very few modeling attempts and even lesser attempts at formalization . we present a novel approach to modeling smart grid components using a formal specification approach .
Wireless Communications and Networking Technologies for Smart Grid:   Paradigms and Challenges,"Smart grid, regarded as the next generation power grid, uses two-way flows of electricity and information to create a widely distributed automated energy delivery network. In this work we present our vision on smart grid from the perspective of wireless communications and networking technologies. We present wireless communication and networking paradigms for four typical scenarios in the future smart grid and also point out the research challenges of the wireless communication and networking technologies used in smart grid","['Xi Fang', 'Dejun Yang', 'Guoliang Xue']",2011-12-06T04:55:32Z,http://arxiv.org/abs/1112.1158v1,"Sustainability, Industry & Robotics",Smart Grids,smart grid is regarded as the next generation power grid . it uses two-way flows of electricity and information to create a widely distributed energy delivery network . we present wireless communication and networking paradigms for four typical scenarios .
Smart Grid Information Security (IS) Functional Requirement,"It is important to implement safe smart grid environment to enhance people's lives and livelihoods. This paper provides information on smart grid IS functional requirement by illustrating some discussion points to the sixteen identified requirements. This paper introduces the smart grid potential hazards that can be referred as a triggering factor to improve the system and security of the entire grid. The background of smart information infrastructure and the needs for smart grid IS is described with the adoption of hermeneutic circle as methodology. Grid information technology and security-s session discusses that grid provides the chance of a simple and transparent access to different information sources. In addition, the transformation between traditional versus smart grid networking trend and the IS importance on the communication field reflects the criticality of grid IS functional requirement identification is introduces. The smart grid IS functional requirements described in this paper are general and can be adopted or modified to suit any smart grid system. This paper has tutorial contents where some related backgrounds were provided, especially for networking community, covering the cyber security requirement of smart grid information infrastructure.","['Amy Poh Ai Ling', 'Mukaidono Masao']",2011-09-21T04:33:49Z,http://arxiv.org/abs/1109.4474v1,"Sustainability, Industry & Robotics",Smart Grids,this paper introduces the smart grid potential hazards that can be referred to as a triggering factor . the background of smart information infrastructure and the needs for smart grid IS is described with the adoption of hermeneutic circle .
Modeling Smart Grid using Generalized Stochastic Petri Net,"Building smart grid for power system is a major challenge for safe, automated and energy efficient usage of electricity. The full implementation of the smart grid will evolve over time. However, before a new set of infrastructures are invested to build the smart grid, proper modeling and analysis is needed to avoid wastage of resources. Modeling also helps to identify and prioritize appropriate systems parameters. In this paper, an all comprehensive model of smart grid have been proposed using Generalized Stochastic Petri Nets (GSPN). The model is used to analyze the constraints and deliverables of the smart power grid of future.","['Amrita Dey', 'Nabendu Chaki', 'Sugata Sanyal']",2011-08-20T19:06:35Z,http://arxiv.org/abs/1108.4139v1,"Sustainability, Industry & Robotics",Smart Grids,building smart grid for power system is a major challenge . proper modeling and analysis is needed to avoid wastage of resources . the full implementation of the smart grid will evolve over time .
Big Data: Perspektiven fuer Smart Grids und Smart Buildings,"This paper gives a short survey of recent trends in the emerging field of big data. It explains the definitions and useful methods. In addition, application fields of smart buildings and smart grids are discussed.",['Ralf Mikut'],2016-10-21T16:50:11Z,http://arxiv.org/abs/1610.06855v1,"Sustainability, Industry & Robotics",Smart Grids,"this paper gives a short survey of recent trends in the emerging field of big data . it explains the definitions and useful methods . in addition, application fields of smart buildings are discussed ."
"Smart Grid: Cyber Attacks, Critical Defense Approaches, and Digital Twin","As a national critical infrastructure, the smart grid has attracted widespread attention for its cybersecurity issues. The development towards an intelligent, digital, and Internet-connected smart grid has attracted external adversaries for malicious activities. It is necessary to enhance its cybersecurity by both improving the existing defense approaches and introducing novel developed technologies to the smart grid context. As an emerging technology, digital twin (DT) is considered as an enabler for enhanced security. However, the practical implementation is quite challenging. This is due to the knowledge barriers among smart grid designers, security experts, and DT developers. Each single domain is a complicated system covering various components and technologies. As a result, works are needed to sort out relevant contents so that DT can be better embedded in the security architecture design of smart grid.   In order to meet this demand, our paper covers the above three domains, i.e., smart grid, cybersecurity, and DT. Specifically, the paper i) introduces the background of the smart grid; ii) reviews external cyber attacks from attack incidents and attack methods; iii) introduces critical defense approaches in industrial cyber systems, which include device identification, vulnerability discovery, intrusion detection systems (IDSs), honeypots, attribution, and threat intelligence (TI); iv) reviews the relevant content of DT, including its basic concepts, applications in the smart grid, and how DT enhances the security. In the end, the paper puts forward our security considerations on the future development of DT-based smart grid. The survey is expected to help developers break knowledge barriers among smart grid, cybersecurity, and DT, and provide guidelines for future security design of DT-based smart grid.","['Tianming Zheng', 'Ping Yi', 'Yue Wu']",2022-05-24T04:46:09Z,http://arxiv.org/abs/2205.11783v2,"Sustainability, Industry & Robotics",Smart Grids,digital twin (DT) is considered as an enabler for enhanced security . practical implementation of DT-based smart grid is quite challenging . paper reviews external cyber attacks from attack incidents and attack methods .
Smart Grids Secured By Dynamic Watermarking: How Secure?,"Unconditional security for smart grids is defined. Cryptanalyses of the watermarked security of smart grids indicate that watermarking cannot guarantee unconditional security unless the communication within the grid system is unconditionally secure. The successful attack against the dynamically watermarked smart grid remains valid even with the presence of internal noise from the grid. An open question arises: if unconditionally authenticated secure communications within the grid, together with tamper resistance of the critical elements, are satisfactory conditions to provide unconditional security for the grid operation.","['Kate Davis', 'Laszlo B. Kish', 'Chanan Singh']",2024-03-05T18:51:50Z,http://arxiv.org/abs/2404.16849v1,"Sustainability, Industry & Robotics",Smart Grids,watermarking cannot guarantee unconditional security unless communication is secure . successful attack against the dynamically watermarked smart grid remains valid even with internal noise .
Autonomous Smart Grid Fault Detection,"Smart grid plays a crucial role for the smart society and the upcoming carbon neutral society. Achieving autonomous smart grid fault detection is critical for smart grid system state awareness, maintenance and operation. This paper focuses on fault monitoring in smart grid and discusses the inherent technical challenges and solutions. In particular, we first present the basic principles of smart grid fault detection. Then, we explain the new requirements for autonomous smart grid fault detection, the technical challenges and their possible solutions. A case study is introduced, as a preliminary study for autonomous smart grid fault detection. In addition, we highlight relevant directions for future research.","['Qiyue Li', 'Yuxing Deng', 'Xin Liu', 'Wei Sun', 'Weitao Li', 'Jie Li', 'Zhi Liu']",2022-05-27T09:11:31Z,http://arxiv.org/abs/2206.14150v1,"Sustainability, Industry & Robotics",Smart Grids,"smart grid plays a crucial role for the smart society and the upcoming carbon neutral society . Achieving autonomous smart grid fault detection is critical for smart grid system state awareness, maintenance and operation ."
"ASIA: An Access Control, Session Invocation and Authorization   Architecture for Home Energy Appliances in Smart Energy Grid Environments","With the advent of the smart energy grid - an energy transportation and distribution network being combined with an IT network for its monitoring and control - information security has gained tremendous importance for energy distribution and energy automa- tion systems. Integrated security functionality is crucial to ensure a reliable and continuous operation of the smart energy grid. Further security related challenges arise from the integration of millions of smart homes into the smart grid. This paper gives an overview of the smart energy grid environment and its challenges. Many future use cases are centered around the smart home, using an ICT gateway. Approaches to protect the access and data exchange are described, preventing manipulation of ICT gateway operation. The paper presents ASIA - an Authentication, Session Invocation, and Authorization component to be used in the smart energy grid, to protect ICT gateways and to cope with problems like ICT gateway discovery and ICT gateway addressing.","['Rainer Falk', 'Steffen Fries', 'Hans-Joachim Hof']",2015-07-07T08:41:42Z,http://arxiv.org/abs/1507.01706v1,"Sustainability, Industry & Robotics",Smart Grids,Integrated security functionality is crucial to ensure a reliable and continuous operation of the smart energy grid . further security related challenges arise from the integration of millions of smart homes into the smart grid.
"Generating Connected, Simple, and Realistic Cyber Graphs for Smart Grids","Smart grids integrate communication systems with power networks to enable power grids operation and command through real-time data collection and control signals. Designing, analyzing, and simulating smart grid infrastructures as well as predicting the impact of power network failures strongly depend on the topologies of the underlying power network and communication system. Despite the substantial impact that the communication systems bring to smart grid operation, the topology of communication systems employed in smart grids was less studied. The power community lacks realistic generative communication system models that can be calibrated to match real-world data. To address this issue, this paper proposes a framework to generate the underlying topological graphs for the communication systems deployed in smart grids by mimicking the topology of real-world smart grids. In this regard, we have updated the Chung-Lu algorithm to guarantee the communication network connectivity and to match the degree distribution of a real-world smart grid rather than following an expected degree distribution. In addition, key characteristics of communication systems such as diameter, average shortest paths, clustering coefficients, assortativity, and spectral gap were taken into consideration to generate the most similar real-world communication network for smart grid studies. We believe that the proposed algorithm to generate realistic cyber graphs for smart grid studies will benefit the power community.","['Osman Boyaci', 'M. Rasoul Narimani', 'Katherine Davis', 'Erchin Serpedin']",2022-01-12T00:31:00Z,http://arxiv.org/abs/2201.04252v1,"Sustainability, Industry & Robotics",Smart Grids,smart grids integrate communication systems with power networks . power community lacks realistic generative communication system models . proposed algorithm to generate realistic cyber graphs for smart grid studies .
"A Review of Blockchain-based Smart Grid: Applications,Opportunities, and   Future Directions","The Smart Grid (SG) concept presented an unprecedented opportunity to move the energy sector to more availability, reliability, and efficiency to improve our economic and environmental conditions. Renewable energy sources (Solar & Wind) are such technologies that are used in the smart grid to figure out the environmental and economic issues and challenges. Smart grids provide energy in different crowded sectors with the efficient and timely transmission of electricity. But the traditional power grids follow a centralized approach for energy transactions with a large number of growing connections and become more challenging to handle power disturbance in the grid. Blockchain as a decentralized and distributed technology provides promising applications in the smart grid infrastructure with its excellent and salient features. In this paper, we provide a concise review of blockchain architecture, concepts, and applications in smart grids. Different potential opportunities for blockchain technology with smart grids are also discussed. Some future directions concluded the paper.","['H. Sami Ullah', 'S. Aslam']",2020-01-31T07:00:10Z,http://arxiv.org/abs/2002.05650v3,"Sustainability, Industry & Robotics",Smart Grids,smart grids provide energy in different crowded sectors with efficient transmission of electricity . traditional power grids follow centralized approach for energy transactions with a large number of growing connections . blockchain as a decentralized and distributed technology provides promising applications in the smart grid infrastructure .
When Blockchain Meets Smart Grids: A Comprehensive Survey,"Recent years have witnessed an increasing interest in the blockchain technology, and many blockchain-based applications have been developed to take advantage of its decentralization, transparency, fault tolerance, and strong security. In the field of smart grids, a plethora of proposals have emerged to utilize blockchain for augmenting intelligent energy management, energy trading, security and privacy protection, microgrid management, and energy vehicles. Compared with traditional centralized approaches, blockchain-based solutions are able to exploit the advantages of blockchain to realize better functionality in smart grids. However, the blockchain technology itself has its disadvantages in low processing throughput and weak privacy protection. Therefore, it is of paramount importance to study how to integrate blockchain with smart grids in a more effective way so that the advantages of blockchain can be maximized and its disadvantages can be avoided.   This article surveys the state-of-the-art solutions aiming to integrate the emergent blockchain technology with smart grids. The goal of this survey is to discuss the necessity of applying blockchain in different components of smart grids, identify the challenges encountered by current solutions, and highlight the frameworks and techniques used to integrate blockchain with smart grids. We also present thorough comparison studies among blockchain-based solutions for smart grids from different perspectives, with the aim to provide insights on integrating blockchain with smart grids for different smart grid management tasks. Finally, we list the current projects and initiatives demonstrating the current effort from the practice side. Additionally, we draw attention to open problems that have not yet been tackled by existing solutions, and point out possible future research directions.","['Yihao Guo', 'Zhiguo Wan', 'Xiuzhen Cheng']",2021-09-29T01:37:42Z,http://arxiv.org/abs/2109.14130v1,"Sustainability, Industry & Robotics",Smart Grids,this article surveys the state-of-the-art solutions aiming to integrate blockchain with smart grids . the goal of this article is to discuss the necessity of applying blockchain in different components .
The Enhancement of Communication Technologies and Networks for Smart   Grid Applications,"The current electrical grid is perhaps the greatest engineering achievement of the 20th century. However, it is increasingly outdated and overburdened, leading to costly blackouts and burnouts. For this and various other reasons,transformation efforts are underway to make the current electrical grid smarter. A reliable, universal and secure communication infrastructure is mandatory for the implementation and deployment of the future smart grid. A special interest is given to the design of efficient and robust network architecture capable of managing operation and control of the next generation power grid. For this purpose new wired and wireless technologies are emerging in addition to the formerly applied to help upgrade the current power grid. In this paper we will give an overview of smart grid reference model, and a comprehensive survey of the available networks for the smart grid and a critical review of the progress of wired and wireless communication technologies for smart grid communication infrastructure. And we propose end to end communication architecture for Home Area Networks (HANs), Neighborhood Area Networks (NANs) and Wide Area Networks (WANs) for smart grid applications. We believe that this work will provide appreciated insights for the novices who would like to follow related research in the SG domain.","['Saida Elyengui', 'Riadh Bouhouchi', 'Tahar Ezzedine']",2014-03-03T19:21:16Z,http://arxiv.org/abs/1403.0530v1,"Sustainability, Industry & Robotics",Smart Grids,the current electrical grid is increasingly outdated and overburdened . new wired and wireless technologies are emerging to help upgrade the current power grid . we propose end to end communication architecture for smart grid applications .
Blockchain for IOT-based NANs and HANs in Smart Grid,"Smart Grid, an intelligent connected grid consisting of millions of smart devices, used to collect data from the grid to improve the efficiency of its operation. These smart devices communicating wirelessly are susceptible to attacks and hence the security and privacy of the smart devices along with the smart grid is a major challenge. Blockchain-based systems provide improved security and privacy and hence gained a lot of attention in the recent past. This paper proposes a blockchain-based architecture for Neighborhood Area Networks (NANs) and Home Area networks (HANs) in Smart Grid. The paper presents a security analysis in terms of confidentiality, integrity and availability to show that the proposed blockchain-based Smart Grid architecture is secure. Also, the impact of the improved security on packet delays, energy and computational overhead is discussed.",['Shravan Garlapati'],2020-01-01T16:05:39Z,http://arxiv.org/abs/2001.00230v1,"Sustainability, Industry & Robotics",Smart Grids,smart grid is an intelligent connected grid consisting of millions of smart devices . smart devices communicating wirelessly are susceptible to attacks . paper proposes a blockchain-based architecture for Neighborhood Area Networks .
SecGrid: A Secure and Efficient SGX-enabled Smart Grid System with Rich   Functionalities,"Smart grid adopts two-way communication and rich functionalities to gain a positive impact on the sustainability and efficiency of power usage, but on the other hand, also poses serious challenges to customers' privacy. Existing solutions in smart grid usually use cryptographic tools, such as homomorphic encryption, to protect individual privacy, which, however, can only support limited and simple functionalities. Moreover, the resource-constrained smart meters need to perform heavy asymmetric cryptography in these solutions, which is not applied to smart grid. In this paper, we present a practical and secure SGX-enabled smart grid system, named SecGrid. Our system leverage trusted hardware SGX to ensure that grid utilities can efficiently execute rich functionalities on customers' private data, while guaranteeing their privacy. With the designed security protocols, the SecGrid only require the smart meters to perform AES encryption. Security analysis shows that SecGrid can thwart various attacks from malicious adversaries. Experimental results show that SecGrid is much faster than the existing privacy-preserving schemes in smart grid.","['Shaohua Li', 'Kaiping Xue']",2018-10-03T09:21:32Z,http://arxiv.org/abs/1810.01651v1,"Sustainability, Industry & Robotics",Smart Grids,secgrid is a secure SGX-enabled smart grid system . it only requires smart meters to perform AES encryption . the system can thwart various attacks from malicious adversaries .
Boosting 5G on Smart Grid Communication: A Smart RAN Slicing Approach,"Fifth-generation (5G) and beyond systems are expected to accelerate the ongoing transformation of power systems towards the smart grid. However, the inherent heterogeneity in smart grid services and requirements pose significant challenges towards the definition of a unified network architecture. In this context, radio access network (RAN) slicing emerges as a key 5G enabler to ensure interoperable connectivity and service management in the smart grid. This article introduces a novel RAN slicing framework which leverages the potential of artificial intelligence (AI) to support IEC 61850 smart grid services. With the aid of deep reinforcement learning, efficient radio resource management for RAN slices is attained, while conforming to the stringent performance requirements of a smart grid self-healing use case. Our research outcomes advocate the adoption of emerging AI-native approaches for RAN slicing in beyond-5G systems, and lay the foundations for differentiated service provisioning in the smart grid.","['Dick Carrillo', 'Charalampos Kalalas', 'Petra Raussi', 'Diomidis S. Michalopoulos', 'Demóstenes Z. Rodríguez', 'Heli Kokkoniemi-Tarkkanen', 'Kimmo Ahola', 'Pedro H. J. Nardelli', 'Gustavo Fraidenraich', 'Petar Popovski']",2022-08-30T21:05:33Z,http://arxiv.org/abs/2208.14538v1,"Sustainability, Industry & Robotics",Smart Grids,radio access network (RAN) slicing emerges as a key 5G enabler . it leverages the potential of artificial intelligence (AI) to support smart grid services . research outcomes advocate adoption of emerging AI-native approaches .
Detection of Compromised Smart Grid Devices with Machine Learning and   Convolution Techniques,"The smart grid concept has transformed the traditional power grid into a massive cyber-physical system that depends on advanced two-way communication infrastructure to integrate a myriad of different smart devices. While the introduction of the cyber component has made the grid much more flexible and efficient with so many smart devices, it also broadened the attack surface of the power grid. Particularly, compromised devices pose a great danger to the healthy operations of the smart-grid. For instance, the attackers can control the devices to change the behaviour of the grid and can impact the measurements. In this paper, to detect such misbehaving malicious smart grid devices, we propose a machine learning and convolution-based classification framework. Our framework specifically utilizes system and library call lists at the kernel level of the operating system on both resource-limited and resource-rich smart grid devices such as RTUs, PLCs, PMUs, and IEDs. Focusing on the types and other valuable features extracted from the system calls, the framework can successfully identify malicious smart-grid devices. In order to test the efficacy of the proposed framework, we built a representative testbed conforming to the IEC-61850 protocol suite and evaluated its performance with different system calls. The proposed framework in different evaluation scenarios yields very high accuracy (avg. 91%) which reveals that the framework is effective to overcome compromised smart grid devices problem.","['Cengiz Kaygusuz', 'Leonardo Babun', 'Hidayet Aksu', 'A. Selcuk Uluagac']",2018-04-13T20:11:02Z,http://arxiv.org/abs/1804.05106v1,"Sustainability, Industry & Robotics",Smart Grids,the smart grid concept has transformed the traditional power grid into a massive cyber-physical system . the introduction of the cyber component has made the grid much more flexible and efficient . compromised smart grid devices pose a great danger to the healthy operations of the smart-grid .
MLD: An Intelligent Memory Leak Detection Scheme Based on Defect Modes   in Smart Grids,"With the expansion of the software scale and complexity of smart grid systems, the detection of smart grid software defects has become a research hotspot. Because of the large scale of the existing smart grid software code, the efficiency and accuracy of the existing smart grid defect detection algorithms are not high. We propose an intelligent memory leak detection scheme based on defect modes MLD in smart grid. Based on the analysis of existing memory leak defect modes, we summarize memory operation behaviors (allocation, release and transfer) and present a state machine model. We employ a fuzzy matching algorithm based on regular expression to determine the memory operation behaviors and then analyze the change in the state machine to assess the vulnerability in the source code. To improve the efficiency of detection and solve the problem of repeated detection at the function call point, we propose a function summary method for memory operation behaviors. The experimental results demonstrate that the method we proposed has high detection speed and accuracy. The algorithm we proposed can identify the defects of the smart grid operation software and ensure the safe operation of the grid.","['Ling Yuan', 'Siyuan Zhou', 'Neal Xiong']",2020-08-22T05:19:35Z,http://arxiv.org/abs/2008.09758v1,"Sustainability, Industry & Robotics",Smart Grids,the detection of smart grid software defects has become a research hotspot . we propose an intelligent memory leak detection scheme based on defect modes . the method we proposed has high detection speed and accuracy .
Threat Landscape for Smart Grid Systems,"Smart Grids are energy delivery networks, constituting an evolution of power grids, in which a bidirectional flow between power providers and consumers is established. These flows support the transfer of electricity and information, in order to support automation actions in the context of the energy delivery network. Insofar, many smart grid implementations and implementation proposals have emerged, with varying degrees of feature delivery and sophistication. While smart grids offer many advantages, their distributed nature and information flow streams between energy producers and consumers enable the launching of a number of attacks against the smart grid infrastructure, where the related consequences may range from economic loss to complete failure of the smart grid. In this paper, we survey the threat landscape of smart grids, identifying threats that are specific to this infrastructure, providing an assessment of the severity of the consequences of each attack type, discerning features that can be utilized to detect attacks and listing methods that can be used to mitigate them.","['Christos-Minas Mathas', 'Konstantinos-Panagiotis Grammatikakis', 'Costas Vassilakis', 'Nicholas Kolokotronis', 'Vasiliki-Georgia Bilali', 'Dimitris Kavallieros']",2021-05-10T11:03:05Z,http://arxiv.org/abs/2105.04264v1,"Sustainability, Industry & Robotics",Smart Grids,smart grids support the transfer of electricity and information in order to support automation . many smart grid implementations and implementation proposals have emerged . their distributed nature and information flow streams allow attacks against the smart grid .
A System-level Behavioral Detection Framework for Compromised CPS   Devices: Smart-Grid Case,"Cyber-Physical Systems (CPS) play a significant role in our critical infrastructure networks from power-distribution to utility networks. The emerging smart-grid concept is a compelling critical CPS infrastructure that relies on two-way communications between smart devices to increase efficiency, enhance reliability, and reduce costs. However, compromised devices in the smart grid poses several security challenges. Consequences of propagating fake data or stealing sensitive smart grid information via compromised devices are costly. Hence, early behavioral detection of compromised devices is critical for protecting the smart grid's components and data. To address these concerns, in this paper, we introduce a novel and configurable system-level framework to identify compromised smart grid devices. The framework combines system and function call tracing techniques with signal processing and statistical analysis to detect compromised devices based on their behavioral characteristics. We measure the efficacy of our framework with a realistic smart grid substation testbed that includes both resource-limited and resource-rich devices. In total, using our framework, we analyze six different types of compromised device scenarios with different resources and attack payloads. To the best of our knowledge, the proposed framework is the first in detecting compromised CPS smart grid devices with system and function-level call tracing techniques. The experimental results reveal an excellent rate for the detection of compromised devices. Specifically, performance metrics include accuracy values between 95% and 99% for the different attack scenarios. Finally, the performance analysis demonstrates that the use of the proposed framework has minimal overhead on the smart grid devices' computing resources.","['Leonardo Babun', 'Hidayet Aksu', 'A. Selcuk Uluagac']",2019-12-02T01:00:54Z,http://arxiv.org/abs/1912.00533v1,"Sustainability, Industry & Robotics",Smart Grids,cyber-physical systems (CPS) play a significant role in our critical infrastructure networks . the emerging smart-grid concept relies on two-way communications between smart devices . compromised devices in the smart grid poses several security challenges .
"Harnessing Artificial Intelligence for Sustainable Agricultural   Development in Africa: Opportunities, Challenges, and Impact","This paper explores the transformative potential of artificial intelligence (AI) in the context of sustainable agricultural development across diverse regions in Africa. Delving into opportunities, challenges, and impact, the study navigates through the dynamic landscape of AI applications in agriculture. Opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined, alongside challenges related to technological infrastructure, data accessibility, and skill gaps. The article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth. Ethical considerations and policy implications are also discussed, offering insights into responsible AI integration. By providing a nuanced understanding, this paper contributes to the ongoing discourse on leveraging AI for fostering sustainability in African agriculture.",['Kinyua Gikunda'],2024-01-03T23:02:13Z,http://arxiv.org/abs/2401.06171v1,"Sustainability, Industry & Robotics",AI in Agriculture,"this paper explores the transformative potential of artificial intelligence (AI) in agriculture . opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined . article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth ."
AgroLLM: Connecting Farmers and Agricultural Practices through Large   Language Models for Enhanced Knowledge Transfer and Practical Application,"AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and education in agriculture using Large Language Models (LLMs) and a Retrieval-Augmented Generation (RAG) framework. By using a comprehensive open-source agricultural database, AgroLLM provides accurate, contextually relevant responses while reducing incorrect information retrieval. The system utilizes the FAISS vector database for efficient similarity searches, ensuring rapid access to agricultural knowledge. A comparative study of three advanced models: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was conducted to evaluate performance across four key agricultural domains: Agriculture and Life Sciences, Agricultural Management, Agriculture and Forestry, and Agriculture Business. Key evaluation metrics included embedding quality, search efficiency, and response relevance. Results indicated that ChatGPT-4o Mini with RAG achieved the highest accuracy at 93%. Continuous feedback mechanisms enhance response quality, making AgroLLM a benchmark AI-driven educational tool for farmers, researchers, and professionals, promoting informed decision-making and improved agricultural practices.","['Dinesh Jackson Samuel', 'Inna Skarga-Bandurova', 'David Sikolia', 'Muhammad Awais']",2025-02-28T04:13:18Z,http://arxiv.org/abs/2503.04788v1,"Sustainability, Industry & Robotics",AI in Agriculture,AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing . it uses large language models (LLMs) and a Retrieval-Augmented Generation (RAG) system utilizes the FAISS vector database for efficient similarity searches .
Smart Sustainable Agriculture (SSA) Solution Underpinned by Internet of   Things (IoT) and Artificial Intelligence (AI),"The Internet of Things (IoT) and Artificial Intelligence (AI) have been employed in agriculture over a long period of time, alongside other advanced computing technologies. However, increased attention is currently being paid to the use of such smart technologies. Agriculture has provided an important source of food for human beings over many thousands of years, including the development of appropriate farming methods for different types of crops. The emergence of new advanced IoT technologies has the potential to monitor the agricultural environment to ensure high-quality products. However, there remains a lack of research and development in relation to Smart Sustainable Agriculture (SSA), accompanied by complex obstacles arising from the fragmentation of agricultural processes, i.e. the control and operation of IoT/AI machines; data sharing and management; interoperability; and large amounts of data analysis and storage. This study firstly, explores existing IoT/AI technologies adopted for SSA and secondly, identifies IoT/AI technical architecture capable of underpinning the development of SSA platforms. As well as contributing to the current body of knowledge, this research reviews research and development within SSA and provides an IoT/AI architecture to establish a Smart, Sustainable Agriculture platform as a solution.",['Eissa Alreshidi'],2019-05-30T18:56:38Z,http://arxiv.org/abs/1906.03106v1,"Sustainability, Industry & Robotics",AI in Agriculture,IoT and AI have been employed in agriculture over a long period of time . lack of research and development in relation to Smart Sustainable Agriculture (SSA)
OpenAg: Democratizing Agricultural Intelligence,"Agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies. However, current agricultural intelligence systems often lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose large language models (LLMs), while powerful, typically lack the domain-specific knowledge and contextual reasoning needed for practical decision support in farming. They tend to produce recommendations that are too generic or unrealistic for real-world applications. To address these challenges, we present OpenAg, a comprehensive framework designed to advance agricultural artificial general intelligence (AGI). OpenAg combines domain-specific foundation models, neural knowledge graphs, multi-agent reasoning, causal explainability, and adaptive transfer learning to deliver context-aware, explainable, and actionable insights. The system includes: (i) a unified agricultural knowledge base that integrates scientific literature, sensor data, and farmer-generated knowledge; (ii) a neural agricultural knowledge graph for structured reasoning and inference; (iii) an adaptive multi-agent reasoning system where AI agents specialize and collaborate across agricultural domains; and (iv) a causal transparency mechanism that ensures AI recommendations are interpretable, scientifically grounded, and aligned with real-world constraints. OpenAg aims to bridge the gap between scientific knowledge and the tacit expertise of experienced farmers to support scalable and locally relevant agricultural decision-making.","['Srikanth Thudumu', 'Jason Fisher']",2025-06-05T02:44:38Z,http://arxiv.org/abs/2506.04571v2,"Sustainability, Industry & Robotics",AI in Agriculture,"agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies . general-purpose large language models (LLMs) typically lack the domain-specific knowledge needed for practical decision support in farming . openAg delivers context-aware, explainable, and actionable insights ."
Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with   AI,"Farms produce hundreds of thousands of data points on the ground daily. Farming technique which combines farming practices with the insights uncovered in these data points using AI technology is called precision farming. Precision farming technology augments and extends farmers' deep knowledge about their land, making production more sustainable and profitable. As part of the larger effort at Microsoft for empowering agricultural labor force to be more productive and sustainable, this paper presents the AI technology for predicting micro-climate conditions on the farm.   This article is a chapter in publication by Food and Agriculture Organization of the United Nations and International Telecommunication Union Bangkok, 2021. This publication on artificial intelligence (AI) for agriculture is the fifth in the E-agriculture in Action series, launched in 2016 and jointly produced by FAO and ITU. It aims to raise awareness about existing AI applications in agriculture and to inspire stakeholders to develop and replicate the new ones. Improvement of capacity and tools for capturing and processing data and substantial advances in the field of machine learning open new horizons for data-driven solutions that can support decision-making, facilitate supervision and monitoring, improve the timeliness and effectiveness of safety measures (e.g. use of pesticides), and support automation of many resource-consuming tasks in agriculture. This publication presents the reader with a collection of informative applications highlighting various ways AI is used in agriculture and offering valuable insights on the implementation process, success factors, and lessons learnt.","['Peeyush Kumar', 'Andrew Nelson', 'Zerina Kapetanovic', 'Ranveer Chandra']",2023-03-04T02:29:52Z,http://arxiv.org/abs/2303.06049v1,"Sustainability, Industry & Robotics",AI in Agriculture,this paper presents the AI technology for predicting micro-climate conditions on the farm . it is part of the larger effort at Microsoft for empowering agricultural labor force . publication aims to raise awareness about existing AI applications in agriculture .
Self-Consistency in Vision-Language Models for Precision Agriculture:   Multi-Response Consensus for Crop Disease Management,"Precision agriculture relies heavily on accurate image analysis for crop disease identification and treatment recommendation, yet existing vision-language models (VLMs) often underperform in specialized agricultural domains. This work presents a domain-aware framework for agricultural image processing that combines prompt-based expert evaluation with self-consistency mechanisms to enhance VLM reliability in precision agriculture applications. We introduce two key innovations: (1) a prompt-based evaluation protocol that configures a language model as an expert plant pathologist for scalable assessment of image analysis outputs, and (2) a cosine-consistency self-voting mechanism that generates multiple candidate responses from agricultural images and selects the most semantically coherent diagnosis using domain-adapted embeddings. Applied to maize leaf disease identification from field images using a fine-tuned PaliGemma model, our approach improves diagnostic accuracy from 82.2\% to 87.8\%, symptom analysis from 38.9\% to 52.2\%, and treatment recommendation from 27.8\% to 43.3\% compared to standard greedy decoding. The system remains compact enough for deployment on mobile devices, supporting real-time agricultural decision-making in resource-constrained environments. These results demonstrate significant potential for AI-driven precision agriculture tools that can operate reliably in diverse field conditions.","['Mihir Gupta', 'Abhay Mangla', 'Ross Greer', 'Pratik Desai']",2025-07-08T18:32:21Z,http://arxiv.org/abs/2507.08024v1,"Sustainability, Industry & Robotics",AI in Agriculture,precision agriculture relies heavily on accurate image analysis for crop disease identification . existing vision-language models (VLMs) often underperform in specialized agricultural domains . this work presents a domain-aware framework for agricultural image processing .
Leveraging Synthetic Data for Question Answering with Multilingual LLMs   in the Agricultural Domain,"Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Although large language models (LLMs) can be used to implement Question Answering (QA) systems, simply using publicly available general-purpose LLMs in agriculture typically offer generic advisories, lacking precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets. Our study addresses these limitations by generating multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tuning language-specific LLMs. Our evaluation on curated multilingual datasets demonstrates significant improvements in factual accuracy, relevance, and agricultural consensus for the fine-tuned models compared to their baseline counterparts. These results highlight the efficacy of synthetic data-driven, language-specific fine-tuning as an effective strategy to improve the performance of LLMs in agriculture, especially in multilingual and low-resource settings. By enabling more accurate and localized agricultural advisory services, this study provides a meaningful step toward bridging the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.","['Rishemjit Kaur', 'Arshdeep Singh Bhankhar', 'Surangika Ranathunga', 'Jashanpreet Singh Salh', 'Sudhir Rajput', 'Vidhi', 'Kashish Mahendra', 'Bhavika Berwal', 'Ritesh Kumar']",2025-07-22T19:25:10Z,http://arxiv.org/abs/2507.16974v1,"Sustainability, Industry & Robotics",AI in Agriculture,"large language models (LLMs) can be used to implement Question Answering (QA) systems . but they lack precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets . our study addresses these limitations by generating multilingual synthetic agricultural datasets from agriculture-specific documents ."
"LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and   Future Directions","The emerging field of smart agriculture leverages the Internet of Things (IoT) to revolutionize farming practices. This paper investigates the transformative potential of Long Range (LoRa) technology as a key enabler of long-range wireless communication for agricultural IoT systems. By reviewing existing literature, we identify a gap in research specifically focused on LoRa's prospects and challenges from a communication perspective in smart agriculture. We delve into the details of LoRa-based agricultural networks, covering network architecture design, Physical Layer (PHY) considerations tailored to the agricultural environment, and channel modeling techniques that account for soil characteristics. The paper further explores relaying and routing mechanisms that address the challenges of extending network coverage and optimizing data transmission in vast agricultural landscapes. Transitioning to practical aspects, we discuss sensor deployment strategies and energy management techniques, offering insights for real-world deployments. A comparative analysis of LoRa with other wireless communication technologies employed in agricultural IoT applications highlights its strengths and weaknesses in this context. Furthermore, the paper outlines several future research directions to leverage the potential of LoRa-based agriculture 4.0. These include advancements in channel modeling for diverse farming environments, novel relay routing algorithms, integrating emerging sensor technologies like hyper-spectral imaging and drone-based sensing, on-device Artificial Intelligence (AI) models, and sustainable solutions. This survey can guide researchers, technologists, and practitioners to understand, implement, and propel smart agriculture initiatives using LoRa technology.","['Lameya Aldhaheri', 'Noor Alshehhi', 'Irfana Ilyas Jameela Manzil', 'Ruhul Amin Khalil', 'Shumaila Javaid', 'Nasir Saeed', 'Mohamed-Slim Alouini']",2024-09-17T13:55:44Z,http://arxiv.org/abs/2409.11200v1,"Sustainability, Industry & Robotics",AI in Agriculture,paper examines potential of LoRa technology as key enabler of long-range wireless communication . paper outlines several future research directions to leverage potential of loRa .
Agricultural 4.0 Leveraging on Technological Solutions: Study for Smart   Farming Sector,"By 2050, it is predicted that there will be 9 billion people on the planet, which will call for more production, lower costs, and the preservation of natural resources. It is anticipated that atypical occurrences and climate change will pose severe risks to agricultural output. It follows that a 70% or more significant rise in food output is anticipated. Smart farming, often known as agriculture 4.0, is a tech-driven revolution in agriculture with the goal of raising industry production and efficiency. Four primary trends are responsible for it: food waste, climate change, population shifts, and resource scarcity. The agriculture industry is changing as a result of the adoption of emerging technologies. Using cutting-edge technology like IoT, AI, and other sensors, smart farming transforms traditional production methods and international agricultural policies. The objective is to establish a value chain that is optimized to facilitate enhanced monitoring and decreased labor expenses. The agricultural sector has seen tremendous transformation as a result of the fourth industrial revolution, which has combined traditional farming methods with cutting-edge technology to increase productivity, sustainability, and efficiency. To effectively utilize the potential of technology gadgets in the agriculture sector, collaboration between governments, private sector entities, and other stakeholders is necessary. This paper covers Agriculture 4.0, looks at its possible benefits and drawbacks of the implementation methodologies, compatibility, reliability, and investigates the several digital tools that are being utilized to change the agriculture industry and how to mitigate the challenges.","['Emmanuel Kojo Gyamfi', 'Zag ElSayed', 'Jess Kropczynski', 'Mustapha Awinsongya Yakubu', 'Nelly Elsayed']",2024-01-01T17:02:49Z,http://arxiv.org/abs/2401.00814v1,"Sustainability, Industry & Robotics",AI in Agriculture,"agriculture 4.0 is a tech-driven revolution in agriculture with the goal of raising industry production and efficiency . four primary trends are responsible for it: food waste, climate change, population shifts, and resource scarcity . the agricultural sector has seen tremendous transformation as a result of the fourth industrial revolution ."
Information Fusion in Smart Agriculture: Machine Learning Applications   and Future Research Directions,"Machine learning (ML) is a rapidly evolving technology with expanding applications across various fields. This paper presents a comprehensive survey of recent ML applications in agriculture for sustainability and efficiency. Existing reviews mainly focus on narrow subdomains or lack a fusion-driven perspectives. This study provides a combined analysis of ML applications in agriculture, structured around five key objectives: (i) Analyzing ML techniques across pre-harvesting, harvesting, and post-harvesting phases. (ii) Demonstrating how ML can be used with agricultural data and data fusion. (iii) Conducting a bibliometric and statistical analysis to reveal research trends and activity. (iv) Investigating real-world case studies of leading artificial intelligence (AI)-driven agricultural companies that use different types of multisensors and multisource data. (v) Compiling publicly available datasets to support ML model training. Going beyond existing previous reviews, this review focuses on how machine learning (ML) techniques, combined with multi-source data fusion (integrating remote sensing, IoT, and climate analytics), enhance precision agriculture by improving predictive accuracy and decision-making. Case studies and statistical insights illustrate the evolving landscape of AI driven smart farming, while future research directions also discusses challenges associated with data fusion for heterogeneous datasets. This review bridges the gap between AI research and agricultural applications, offering a roadmap for researchers, industry professionals, and policymakers to harness information fusion and ML for advancing precision agriculture.","['Aashu Katharria', 'Kanchan Rajwar', 'Millie Pant', 'Juan D. Velásquez', 'Václav Snášel', 'Kusum Deep']",2024-05-23T17:53:31Z,http://arxiv.org/abs/2405.17465v2,"Sustainability, Industry & Robotics",AI in Agriculture,this paper presents a comprehensive survey of recent ML applications in agriculture . existing reviews mainly focus on narrow subdomains or lack a fusion-driven perspective . this review bridges the gap between AI research and agricultural applications .
Developing and Integrating Trust Modeling into Multi-Objective   Reinforcement Learning for Intelligent Agricultural Management,"Precision agriculture, enhanced by artificial intelligence (AI), offers promising tools such as remote sensing, intelligent irrigation, fertilization management, and crop simulation to improve agricultural efficiency and sustainability. Reinforcement learning (RL), in particular, has outperformed traditional methods in optimizing yields and resource management. However, widespread AI adoption is limited by gaps between algorithmic recommendations and farmers' practical experience, local knowledge, and traditional practices. To address this, our study emphasizes Human-AI Interaction (HAII), focusing on transparency, usability, and trust in RL-based farm management. We employ a well-established trust framework - comprising ability, benevolence, and integrity - to develop a novel mathematical model quantifying farmers' confidence in AI-based fertilization strategies. Surveys conducted with farmers for this research reveal critical misalignments, which are integrated into our trust model and incorporated into a multi-objective RL framework. Unlike prior methods, our approach embeds trust directly into policy optimization, ensuring AI recommendations are technically robust, economically feasible, context-aware, and socially acceptable. By aligning technical performance with human-centered trust, this research supports broader AI adoption in agriculture.","['Zhaoan Wang', 'Wonseok Jang', 'Bowen Ruan', 'Jun Wang', 'Shaoping Xiao']",2025-05-16T02:52:16Z,http://arxiv.org/abs/2505.10803v1,"Sustainability, Industry & Robotics",AI in Agriculture,"widespread adoption is limited by gaps between algorithmic recommendations and farmers' practical experience, local knowledge, and traditional practices . our study emphasizes human-AI interaction (HAII), focusing on transparency, usability, and trust in RL-based farm management ."
Dynamic Evolutionary Game Analysis of How Fintech in Banking Mitigates   Risks in Agricultural Supply Chain Finance,"This paper explores the impact of banking fintech on reducing financial risks in the agricultural supply chain, focusing on the secondary allocation of commercial credit. The study constructs a three-player evolutionary game model involving banks, core enterprises, and SMEs to analyze how fintech innovations, such as big data credit assessment, blockchain, and AI-driven risk evaluation, influence financial risks and access to credit. The findings reveal that banking fintech reduces financing costs and mitigates financial risks by improving transaction reliability, enhancing risk identification, and minimizing information asymmetry. By optimizing cooperation between banks, core enterprises, and SMEs, fintech solutions enhance the stability of the agricultural supply chain, contributing to rural revitalization goals and sustainable agricultural development. The study provides new theoretical insights and practical recommendations for improving agricultural finance systems and reducing financial risks.   Keywords: banking fintech, agricultural supply chain, financial risk, commercial credit, SMEs, evolutionary game model, big data, blockchain, AI-driven risk evaluation.","['Qiang Wan', 'Jun Cui']",2024-11-12T07:25:27Z,http://arxiv.org/abs/2411.07604v1,"Sustainability, Industry & Robotics",AI in Agriculture,"study examines impact of banking fintech on reducing financial risks in the agricultural supply chain . it uses a three-player evolutionary game model involving banks, core enterprises, and SMEs . findings reveal that fintech reduces financing costs and mitigates financial risks ."
"Towards technological adaptation of advanced farming through AI, IoT,   and Robotics: A Comprehensive overview","The population explosion of the 21st century has adversely affected the natural resources with restricted availability of cultivable land, increased average temperatures due to global warming, and carbon footprint resulting in a drastic increase in floods as well as droughts thus making food security significant anxiety for most countries. The traditional methods were no longer sufficient which paved the way for technological ascents such as a substantial rise in Artificial Intelligence (AI), Internet of Things (IoT), as well as Robotics that provides high productivity, functional efficiency, flexibility, cost-effectiveness in the domain of agriculture. AI, IoT, and Robotics-based devices and methods have produced new paradigms and opportunities in agriculture. AI's existing approaches are soil management, crop diseases identification, weed identification, and management in collaboration with IoT devices. IoT has utilized automatic agricultural operations and real-time monitoring with few personnel employed in real-time. The major existing applications of agricultural robotics are for the function of soil preparation, planting, monitoring, harvesting, and storage. In this paper, researchers have explored a comprehensive overview of recent implementation, scopes, opportunities, challenges, limitations, and future research instructions of AI, IoT, and Robotics based methodology in the agriculture sector.","['Md. Mahadi Hasan', 'Muhammad Usama Islam', 'Muhammad Jafar Sadeq']",2022-02-21T07:47:43Z,http://arxiv.org/abs/2202.10459v1,"Sustainability, Industry & Robotics",AI in Agriculture,"the population explosion of the 21st century has adversely affected the natural resources . paved the way for technological ascents such as a substantial rise in Artificial Intelligence (AI), Internet of Things (IoT), and Robotics ."
A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis,"While conversational generative AI has shown considerable potential in enhancing decision-making for agricultural professionals, its exploration has predominantly been anchored in text-based interactions. The evolution of multimodal conversational AI, leveraging vast amounts of image-text data from diverse sources, marks a significant stride forward. However, the application of such advanced vision-language models in the agricultural domain, particularly for crop disease diagnosis, remains underexplored. In this work, we present the crop disease domain multimodal (CDDM) dataset, a pioneering resource designed to advance the field of agricultural research through the application of multimodal learning techniques. The dataset comprises 137,000 images of various crop diseases, accompanied by 1 million question-answer pairs that span a broad spectrum of agricultural knowledge, from disease identification to management practices. By integrating visual and textual data, CDDM facilitates the development of sophisticated question-answering systems capable of providing precise, useful advice to farmers and agricultural professionals. We demonstrate the utility of the dataset by finetuning state-of-the-art multimodal models, showcasing significant improvements in crop disease diagnosis. Specifically, we employed a novel finetuning strategy that utilizes low-rank adaptation (LoRA) to finetune the visual encoder, adapter and language model simultaneously. Our contributions include not only the dataset but also a finetuning strategy and a benchmark to stimulate further research in agricultural technology, aiming to bridge the gap between advanced AI techniques and practical agricultural applications. The dataset is available at https: //github.com/UnicomAI/UnicomBenchmark/tree/main/CDDMBench.","['Xiang Liu', 'Zhaoxiang Liu', 'Huan Hu', 'Zezhou Chen', 'Kohou Wang', 'Kai Wang', 'Shiguo Lian']",2025-03-10T06:37:42Z,http://arxiv.org/abs/2503.06973v1,"Sustainability, Industry & Robotics",AI in Agriculture,"crop disease domain multimodal (CDDM) dataset is a pioneering resource . dataset comprises 137,000 images of various crop diseases . 1 million question-answer pairs span a broad spectrum of agricultural knowledge ."
Enabling Adoption of Regenerative Agriculture through Soil Carbon   Copilots,"Mitigating climate change requires transforming agriculture to minimize environ mental impact and build climate resilience. Regenerative agricultural practices enhance soil organic carbon (SOC) levels, thus improving soil health and sequestering carbon. A challenge to increasing regenerative agriculture practices is cheaply measuring SOC over time and understanding how SOC is affected by regenerative agricultural practices and other environmental factors and farm management practices. To address this challenge, we introduce an AI-driven Soil Organic Carbon Copilot that automates the ingestion of complex multi-resolution, multi-modal data to provide large-scale insights into soil health and regenerative practices. Our data includes extreme weather event data (e.g., drought and wildfire incidents), farm management data (e.g., cropland information and tillage predictions), and SOC predictions. We find that integrating public data and specialized models enables large-scale, localized analysis for sustainable agriculture. In comparisons of agricultural practices across California counties, we find evidence that diverse agricultural activity may mitigate the negative effects of tillage; and that while extreme weather conditions heavily affect SOC, composting may mitigate SOC loss. Finally, implementing role-specific personas empowers agronomists, farm consultants, policymakers, and other stakeholders to implement evidence-based strategies that promote sustainable agriculture and build climate resilience.","['Margaret Capetz', 'Swati Sharma', 'Rafael Padilha', 'Peder Olsen', 'Jessica Wolk', 'Emre Kiciman', 'Ranveer Chandra']",2024-11-25T19:11:41Z,http://arxiv.org/abs/2411.16872v2,"Sustainability, Industry & Robotics",AI in Agriculture,"mitigating climate change requires transforming agriculture to minimize environ mental impact . regenerative agricultural practices enhance soil organic carbon (SOC) levels . extreme weather event data, farm management data and SOC predictions help ."
Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data   Analytics Framework with Lightweight LLMs,"Amid the challenges posed by global population growth and climate change, traditional agricultural Internet of Things (IoT) systems is currently undergoing a significant digital transformation to facilitate efficient big data processing. While smart agriculture utilizes artificial intelligence (AI) technologies to enable precise control, it still encounters significant challenges, including excessive reliance on agricultural expert knowledge, difficulties in fusing multimodal data, poor adaptability to dynamic environments, and bottlenecks in real-time decision-making at the edge. Large language models (LLMs), with their exceptional capabilities in knowledge acquisition and semantic understanding, provide a promising solution to address these challenges. To this end, we propose Farm-LightSeek, an edge-centric multimodal agricultural IoT data analytics framework that integrates LLMs with edge computing. This framework collects real-time farmland multi-source data (images, weather, geographic information) via sensors, performs cross-modal reasoning and disease detection at edge nodes, conducts low-latency management decisions, and enables cloud collaboration for model updates. The main innovations of Farm-LightSeek include: (1) an agricultural ""perception-decision-action"" closed-loop architecture; (2) cross-modal adaptive monitoring; and (3)a lightweight LLM deployment strategy balancing performance and efficiency. Experiments conducted on two real-world datasets demonstrate that Farm-LightSeek consistently achieves reliable performance in mission-critical tasks, even under the limitations of edge computing resources. This work advances intelligent real-time agricultural solutions and highlights the potential for deeper integration of agricultural IoT with LLMs.","['Dawen Jiang', 'Zhishu Shen', 'Qiushi Zheng', 'Tiehua Zhang', 'Wei Xiang', 'Jiong Jin']",2025-05-28T14:09:36Z,http://arxiv.org/abs/2506.03168v1,"Sustainability, Industry & Robotics",AI in Agriculture,farm-LightSeek is an edge-centric multimodal agricultural IoT data analytics framework . the framework collects real-time farmland multi-source data via sensors . it performs cross-modal reasoning and disease detection at edge nodes .
Artificial Intelligence in Sustainable Vertical Farming,"As global challenges of population growth, climate change, and resource scarcity intensify, the agricultural landscape is at a critical juncture. Sustainable vertical farming emerges as a transformative solution to address these challenges by maximizing crop yields in controlled environments. This paradigm shift necessitates the integration of cutting-edge technologies, with Artificial Intelligence (AI) at the forefront. The paper provides a comprehensive exploration of the role of AI in sustainable vertical farming, investigating its potential, challenges, and opportunities. The review synthesizes the current state of AI applications, encompassing machine learning, computer vision, the Internet of Things (IoT), and robotics, in optimizing resource usage, automating tasks, and enhancing decision-making. It identifies gaps in research, emphasizing the need for optimized AI models, interdisciplinary collaboration, and the development of explainable AI in agriculture. The implications extend beyond efficiency gains, considering economic viability, reduced environmental impact, and increased food security. The paper concludes by offering insights for stakeholders and suggesting avenues for future research, aiming to guide the integration of AI technologies in sustainable vertical farming for a resilient and sustainable future in agriculture.","['Hribhu Chowdhury', 'Debo Brata Paul Argha', 'Md Ashik Ahmed']",2023-11-17T22:15:41Z,http://arxiv.org/abs/2312.00030v1,"Sustainability, Industry & Robotics",AI in Agriculture,"the paper provides a comprehensive exploration of the role of AI in sustainable vertical farming . it identifies gaps in research, emphasizing the need for optimized AI models . the implications extend beyond efficiency gains, considering economic viability ."
"Large Language Models and Foundation Models in Smart Agriculture:   Basics, Opportunities, and Challenges","The past decade has witnessed the rapid development and adoption of ML & DL methodologies in agricultural systems, showcased by great successes in agricultural applications. However, these conventional ML/DL models have certain limitations: they heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, large pre-trained models, also known as FMs, have demonstrated remarkable successes in language, vision, and decision-making tasks across various domains. These models are trained on a large amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture AI. Thus, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, conceptual tools and technical background are presented to help the understanding of the problem space and uncover new research directions. To this end, recent FMs in the general CS domain are reviewed, and the models are categorized into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Then, the steps of developing agriculture FMs (AFMs) are outlined and potential applications in smart agriculture are discussed. Moreover, challenges and risks associated with developing AFMs are discussed, including model training, validation, and deployment. In summary, the advancement of AI in agriculture is explored by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.","['Jiajia Li', 'Mingle Xu', 'Lirong Xiang', 'Dong Chen', 'Weichao Zhuang', 'Xunyuan Yin', 'Zhaojian Li']",2023-08-13T02:59:36Z,http://arxiv.org/abs/2308.06668v4,"Sustainability, Industry & Robotics",AI in Agriculture,"large pre-trained models, also known as FMs, have demonstrated successes in language, vision, and decision-making tasks . once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data ."
OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence   for Digital Agriculture,"Recent machine learning approaches have been effective in Artificial Intelligence (AI) applications. They produce robust results with a high level of accuracy. However, most of these techniques do not provide human-understandable explanations for supporting their results and decisions. They usually act as black boxes, and it is not easy to understand how decisions have been made. Explainable Artificial Intelligence (XAI), which has received much interest recently, tries to provide human-understandable explanations for decision-making and trained AI models. For instance, in digital agriculture, related domains often present peculiar or input features with no link to background knowledge. The application of the data mining process on agricultural data leads to results (knowledge), which are difficult to explain. In this paper, we propose a knowledge map model and an ontology design as an XAI framework (OAK4XAI) to deal with this issue. The framework does not only consider the data analysis part of the process, but it takes into account the semantics aspect of the domain knowledge via an ontology and a knowledge map model, provided as modules of the framework. Many ongoing XAI studies aim to provide accurate and verbalizable accounts for how given feature values contribute to model decisions. The proposed approach, however, focuses on providing consistent information and definitions of concepts, algorithms, and values involved in the data mining models. We built an Agriculture Computing Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has a well-designed structure and includes a wide range of concepts and transformations suitable for agriculture and computing domains.","['Quoc Hung Ngo', 'Tahar Kechadi', 'Nhien-An Le-Khac']",2022-09-29T21:20:25Z,http://arxiv.org/abs/2209.15104v1,"Sustainability, Industry & Robotics",AI in Agriculture,"recent machine learning approaches have been effective in Artificial Intelligence (AI) applications . but most of these techniques do not provide human-understandable explanations for their results and decisions . explainable artificial intelligence (XAI) tries to provide explanations . in digital agriculture, related domains often present peculiar or input features ."
Leaf-Based Plant Disease Detection and Explainable AI,"The agricultural sector plays an essential role in the economic growth of a country. Specifically, in an Indian context, it is the critical source of livelihood for millions of people living in rural areas. Plant Disease is one of the significant factors affecting the agricultural sector. Plants get infected with diseases for various reasons, including synthetic fertilizers, archaic practices, environmental conditions, etc., which impact the farm yield and subsequently hinder the economy. To address this issue, researchers have explored many applications based on AI and Machine Learning techniques to detect plant diseases. This research survey provides a comprehensive understanding of common plant leaf diseases, evaluates traditional and deep learning techniques for disease detection, and summarizes available datasets. It also explores Explainable AI (XAI) to enhance the interpretability of deep learning models' decisions for end-users. By consolidating this knowledge, the survey offers valuable insights to researchers, practitioners, and stakeholders in the agricultural sector, fostering the development of efficient and transparent solutions for combating plant diseases and promoting sustainable agricultural practices.","['Saurav Sagar', 'Mohammed Javed', 'David S Doermann']",2023-12-17T03:40:12Z,http://arxiv.org/abs/2404.16833v1,"Sustainability, Industry & Robotics",AI in Agriculture,the agricultural sector plays an essential role in the economic growth of a country . research survey provides a comprehensive understanding of common plant leaf diseases . it evaluates traditional and deep learning techniques for disease detection .
A Game Theoretic Analysis for Cooperative Smart Farming,"The application of Internet of Things (IoT) and Machine Learning (ML) to the agricultural industry has enabled the development and creation of smart farms and precision agriculture. The growth in the number of smart farms and potential cooperation between these farms has given rise to the Cooperative Smart Farming (CSF) where different connected farms collaborate with each other and share data for their mutual benefit. This data sharing through CSF has various advantages where individual data from separate farms can be aggregated by ML models and be used to produce actionable outputs which then can be utilized by all the farms in CSFs. This enables farms to gain better insights for enhancing desired outputs, such as crop yield, managing water resources and irrigation schedules, as well as better seed applications. However, complications may arise in CSF when some of the farms do not transfer high-quality data and rather rely on other farms to feed ML models. Another possibility is the presence of rogue farms in CSFs that want to snoop on other farms without actually contributing any data. In this paper, we analyze the behavior of farms participating in CSFs using game theory approach, where each farm is motivated to maximize its profit. We first present the problem of defective farms in CSFs due to lack of better data, and then propose a ML framework that segregates farms and automatically assign them to an appropriate CSF cluster based on the quality of data they provide. Our proposed model rewards the farms supplying better data and penalize the ones that do not provide required data or are malicious in nature, thus, ensuring the model integrity and better performance all over while solving the defective farms problem.","['Deepti Gupta', 'Paras Bhatt', 'Smriti Bhatt']",2020-11-22T20:26:33Z,http://arxiv.org/abs/2011.11098v1,"Sustainability, Industry & Robotics",Precision Farming,the application of IoT and machine learning (ML) to the agricultural industry has enabled the development and creation of smart farms and precision agriculture . the cooperative smart farming (csf) allows different connected farms to collaborate and share data for their mutual benefit . complications may arise when some of the farms do not transfer high-quality data and rather rely on other farms .
Learning from Data to Optimize Control in Precision Farming,"Precision farming is one way of many to meet a 70 percent increase in global demand for agricultural products on current agricultural land by 2050 at reduced need of fertilizers and efficient use of water resources. The catalyst for the emergence of precision farming has been satellite positioning and navigation followed by Internet-of-Things, generating vast information that can be used to optimize farming processes in real-time. Statistical tools from data mining, predictive modeling, and machine learning analyze pattern in historical data, to make predictions about future events as well as intelligent actions. This special issue presents the latest development in statistical inference, machine learning and optimum control for precision farming.","['Alexander Kocian', 'Luca Incrocci']",2020-07-07T12:44:17Z,http://arxiv.org/abs/2007.05493v1,"Sustainability, Industry & Robotics",Precision Farming,precision farming is one way of many to meet a 70 percent increase in global demand for agricultural products by 2050 . the catalyst for the emergence of precision farming has been satellite positioning and navigation followed by internet-of-things .
Towards The Creation Of The Future Fish Farm,"A fish farm is an area where fish raise and bred for food. Fish farm environments support the care and management of seafood within a controlled environment. Over the past few decades, there has been a remarkable increase in the calorie intake of protein attributed to seafood. Along with this, there are significant opportunities within the fish farming industry for economic development. Determining the fish diseases, monitoring the aquatic organisms, and examining the imbalance in the water element are some key factors that require precise observation to determine the accuracy of the acquired data. Similarly, due to the rapid expansion of aquaculture, new technologies are constantly being implemented in this sector to enhance efficiency. However, the existing approaches have often failed to provide an efficient method of farming fish. This work has kept aside the traditional approaches and opened up new dimensions to perform accurate analysis by adopting a distributed ledger technology. Our work analyses the current state-of-the-art of fish farming and proposes a fish farm ecosystem that relies on a private-by-design architecture based on the Hyperledger Fabric private-permissioned distributed ledger technology. The proposed method puts forward accurate and secure storage of the retrieved data from multiple sensors across the ecosystem so that the adhering entities can exercise their decision based on the acquired data. This study demonstrates a proof-of-concept to signify the efficiency and usability of the future fish farm.","['Pavlos Papadopoulos', 'William J Buchanan', 'Sarwar Sayeed', 'Nikolaos Pitropakis']",2023-01-02T21:41:06Z,http://arxiv.org/abs/2301.01618v1,"Sustainability, Industry & Robotics",Precision Farming,a fish farm is an area where fish raise and bred for food . there has been a remarkable increase in the calorie intake of protein attributed to seafood . new technologies are constantly being implemented in this sector to enhance efficiency .
Internet of Things-Based Smart Precision Farming in Soilless   Agriculture: Opportunities and Challenges for Global Food Security,"The rapid growth of the global population and the continuous decline in cultivable land pose significant threats to food security. This challenge worsens as climate change further reduces the availability of farmland. Soilless agriculture, such as hydroponics, aeroponics, and aquaponics, offers a sustainable solution by enabling efficient crop cultivation in controlled environments. The integration of the Internet of Things (IoT) with smart precision farming improves resource efficiency, automates environmental control, and ensures stable and high-yield crop production. IoT-enabled smart farming systems utilize real-time monitoring, data-driven decision-making, and automation to optimize water and nutrient usage while minimizing human intervention. This paper explores the opportunities and challenges of IoT-based soilless farming, highlighting its role in sustainable agriculture, urban farming, and global food security. These advanced farming methods ensure greater productivity, resource conservation, and year-round cultivation. However, they also face challenges such as high initial investment, technological dependency, and energy consumption. Through a comprehensive study, bibliometric analysis, and comparative analysis, this research highlights current trends and research gaps. It also outlines future directions for researchers, policymakers, and industry stakeholders to drive innovation and scalability in IoT-driven soilless agriculture. By emphasizing the benefits of vertical farming and Controlled Environment Agriculture (CEA)-enabled soilless techniques, this paper supports informed decision-making to address food security challenges and promote sustainable agricultural innovations.","['Monica Dutta', 'Deepali Gupta', 'Sumegh Tharewal', 'Deepam Goyal', 'Jasminder Kaur Sandhu', 'Manjit Kaur', 'Ahmad Ali Alzubi', 'Jazem Mutared Alanazi']",2025-03-15T03:40:32Z,http://arxiv.org/abs/2503.13528v3,"Sustainability, Industry & Robotics",Precision Farming,"internet of things (IoT) with smart precision farming improves resource efficiency . paper explores opportunities and challenges of IoT-based soilless farming . advanced farming methods ensure greater productivity, resource conservation, and year-round cultivation ."
Combating Fraud in Online Social Networks: Detecting Stealthy Facebook   Like Farms,"As businesses increasingly rely on social networking sites to engage with their customers, it is crucial to understand and counter reputation manipulation activities, including fraudulently boosting the number of Facebook page likes using like farms. To this end, several fraud detection algorithms have been proposed and some deployed by Facebook that use graph co-clustering to distinguish between genuine likes and those generated by farm-controlled profiles. However, as we show in this paper, these tools do not work well with stealthy farms whose users spread likes over longer timespans and like popular pages, aiming to mimic regular users. We present an empirical analysis of the graph-based detection tools used by Facebook and highlight their shortcomings against more sophisticated farms. Next, we focus on characterizing content generated by social networks accounts on their timelines, as an indicator of genuine versus fake social activity. We analyze a wide range of features extracted from timeline posts, which we group into two main classes: lexical and non-lexical. We postulate and verify that like farm accounts tend to often re-share content, use fewer words and poorer vocabulary, and more often generate duplicate comments and likes compared to normal users. We extract relevant lexical and non-lexical features and and use them to build a classifier to detect like farms accounts, achieving significantly higher accuracy, namely, at least 99% precision and 93% recall.","['Muhammad Ikram', 'Lucky Onwuzurike', 'Shehroze Farooqi', 'Emiliano De Cristofaro', 'Arik Friedman', 'Guillaume Jourjon', 'Mohammad Ali Kaafar', 'M. Zubair Shafiq']",2015-06-01T14:24:57Z,http://arxiv.org/abs/1506.00506v3,"Sustainability, Industry & Robotics",Precision Farming,like farms are fraudulently boosting the number of likes on facebook pages . this paper shows that these tools do not work well with stealthy farms . we focus on characterizing content generated by social networks accounts on their timelines .
Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with   AI,"Farms produce hundreds of thousands of data points on the ground daily. Farming technique which combines farming practices with the insights uncovered in these data points using AI technology is called precision farming. Precision farming technology augments and extends farmers' deep knowledge about their land, making production more sustainable and profitable. As part of the larger effort at Microsoft for empowering agricultural labor force to be more productive and sustainable, this paper presents the AI technology for predicting micro-climate conditions on the farm.   This article is a chapter in publication by Food and Agriculture Organization of the United Nations and International Telecommunication Union Bangkok, 2021. This publication on artificial intelligence (AI) for agriculture is the fifth in the E-agriculture in Action series, launched in 2016 and jointly produced by FAO and ITU. It aims to raise awareness about existing AI applications in agriculture and to inspire stakeholders to develop and replicate the new ones. Improvement of capacity and tools for capturing and processing data and substantial advances in the field of machine learning open new horizons for data-driven solutions that can support decision-making, facilitate supervision and monitoring, improve the timeliness and effectiveness of safety measures (e.g. use of pesticides), and support automation of many resource-consuming tasks in agriculture. This publication presents the reader with a collection of informative applications highlighting various ways AI is used in agriculture and offering valuable insights on the implementation process, success factors, and lessons learnt.","['Peeyush Kumar', 'Andrew Nelson', 'Zerina Kapetanovic', 'Ranveer Chandra']",2023-03-04T02:29:52Z,http://arxiv.org/abs/2303.06049v1,"Sustainability, Industry & Robotics",Precision Farming,this paper presents the AI technology for predicting micro-climate conditions on the farm . it is part of the larger effort at Microsoft for empowering agricultural labor force . publication aims to raise awareness about existing AI applications in agriculture .
Mapping Methane -- The Impact of Dairy Farm Practices on Emissions   Through Satellite Data and Machine Learning,"This study investigates the correlation between dairy farm characteristics and methane concentrations as derived from satellite observations in Eastern Canada. Utilizing data from 11 dairy farms collected between January 2020 and December 2022, we integrated Sentinel-5P satellite methane data with critical farm-level attributes, including herd genetics, feeding practices, and management strategies. Initial analyses revealed significant correlations with methane concentrations, leading to the application of Variance Inflation Factor (VIF) and Principal Component Analysis (PCA) to address multicollinearity and enhance model stability. Subsequently, machine learning models - specifically Random Forest and Neural Networks - were employed to evaluate feature importance and predict methane emissions. Our findings indicate a strong negative correlation between the Estimated Breeding Value (EBV) for protein percentage and methane concentrations, suggesting that genetic selection for higher milk protein content could be an effective strategy for emissions reduction. The integration of atmospheric transport models with satellite data further refined our emission estimates, significantly enhancing accuracy and spatial resolution. This research underscores the potential of advanced satellite monitoring, machine learning techniques, and atmospheric modeling in improving methane emission assessments within the dairy sector. It emphasizes the critical role of farm-specific characteristics in developing effective mitigation strategies. Future investigations should focus on expanding the dataset and incorporating inversion modeling for more precise emission quantification. Balancing ecological impacts with economic viability will be essential for fostering sustainable dairy farming practices.","['Hanqing Bi', 'Suresh Neethirajan']",2024-11-13T16:52:30Z,http://arxiv.org/abs/2411.08766v1,"Sustainability, Industry & Robotics",Precision Farming,"this study investigates the correlation between dairy farm characteristics and methane concentrations as derived from satellite observations in eastern canada . using data from 11 dairy farms collected between January 2020 and December 2022, we integrated Sentinel-5P satellite data with critical farm-level attributes, including herd genetics, feeding practices, and management strategies . our findings indicate a strong negative correlation between the Estimated Breeding Value (EBV) for protein percentage . the integration of atmospheric transport models with satellite data further refined our emission estimates, significantly"
"Measuring, Characterizing, and Detecting Facebook Like Farms","Social networks offer convenient ways to seamlessly reach out to large audiences. In particular, Facebook pages are increasingly used by businesses, brands, and organizations to connect with multitudes of users worldwide. As the number of likes of a page has become a de-facto measure of its popularity and profitability, an underground market of services artificially inflating page likes, aka like farms, has emerged alongside Facebook's official targeted advertising platform. Nonetheless, there is little work that systematically analyzes Facebook pages' promotion methods. Aiming to fill this gap, we present a honeypot-based comparative measurement study of page likes garnered via Facebook advertising and from popular like farms. First, we analyze likes based on demographic, temporal, and social characteristics, and find that some farms seem to be operated by bots and do not really try to hide the nature of their operations, while others follow a stealthier approach, mimicking regular users' behavior. Next, we look at fraud detection algorithms currently deployed by Facebook and show that they do not work well to detect stealthy farms which spread likes over longer timespans and like popular pages to mimic regular users. To overcome their limitations, we investigate the feasibility of timeline-based detection of like farm accounts, focusing on characterizing content generated by Facebook accounts on their timelines as an indicator of genuine versus fake social activity. We analyze a range of features, grouped into two main categories: lexical and non-lexical. We find that like farm accounts tend to re-share content, use fewer words and poorer vocabulary, and more often generate duplicate comments and likes compared to normal users. Using relevant lexical and non-lexical features, we build a classifier to detect like farms accounts that achieves precision higher than 99% and 93% recall.","['Muhammad Ikram', 'Lucky Onwuzurike', 'Shehroze Farooqi', 'Emiliano De Cristofaro', 'Arik Friedman', 'Guillaume Jourjon', 'Dali Kaafar', 'M. Zubair Shafiq']",2017-07-01T18:44:54Z,http://arxiv.org/abs/1707.00190v2,"Sustainability, Industry & Robotics",Precision Farming,"facebook page likes have become a de-facto measure of its popularity and profitability . a market of services artificially inflating likes, aka like farms, has emerged . there is little work that systematically analyzes Facebook pages' promotion methods . we analyze likes based on demographic, temporal, and social characteristics ."
Scene and Environment Monitoring Using Aerial Imagery and Deep Learning,"Unmanned Aerial vehicles (UAV) are a promising technology for smart farming related applications. Aerial monitoring of agriculture farms with UAV enables key decision-making pertaining to crop monitoring. Advancements in deep learning techniques have further enhanced the precision and reliability of aerial imagery based analysis. The capabilities to mount various kinds of sensors (RGB, spectral cameras) on UAV allows remote crop analysis applications such as vegetation classification and segmentation, crop counting, yield monitoring and prediction, crop mapping, weed detection, disease and nutrient deficiency detection and others. A significant amount of studies are found in the literature that explores UAV for smart farming applications. In this paper, a review of studies applying deep learning on UAV imagery for smart farming is presented. Based on the application, we have classified these studies into five major groups including: vegetation identification, classification and segmentation, crop counting and yield predictions, crop mapping, weed detection and crop disease and nutrient deficiency detection. An in depth critical analysis of each study is provided.","['Mahdi Maktabdar Oghaz', 'Manzoor Razaak', 'Hamideh Kerdegari', 'Vasileios Argyriou', 'Paolo Remagnino']",2019-06-06T20:58:39Z,http://arxiv.org/abs/1906.02809v1,"Sustainability, Industry & Robotics",Precision Farming,aerial monitoring of agriculture farms with UAV enables key decision-making . advances in deep learning techniques have enhanced precision and reliability . a significant amount of studies are found in the literature that explores UAV for smart farming .
Estimator Model for Prediction of Power Output of Wave Farms Using   Machine Learning Methods,"The amount of power generated by a wave farm depends on the Wave Energy Converter (WEC) arrangement along with the usual wave conditions. Therefore, forming the appropriate arrangement of WECs in an array is an important factor in maximizing power absorption. Data collected from the test sites is used to design a neural model for predicting wave farm's power output generated. This paper focuses on developing a neural model for the prediction of wave energy based on the data set derived from the four real wave scenarios from the southern coast of Australia. The applied converter model is a fully submerged three-tether converter called CETO. A precise analysis of the WEC placement is investigated to reveal the amount of power generated by the wave farms on the test site.",['Bhavana Burramukku'],2020-11-26T05:05:24Z,http://arxiv.org/abs/2011.13130v1,"Sustainability, Industry & Robotics",Precision Farming,data collected from the test sites is used to design a neural model for predicting wave farm's power output generated . the applied converter model is a fully submerged three-tether converter called CETO .
Assessment of design and analysis frameworks for on-farm experimentation   through a simulation study of wheat yield in Japan,"On-farm experiments can provide farmers with information on more efficient crop management in their own fields. Developments in precision agricultural technologies, such as yield monitoring and variable-rate application technology, allow farmers to implement on-farm experiments. Research frameworks including the experimental design and the statistical analysis method strongly influences the precision of the experiment. Conventional statistical approaches (e.g., ordinary least squares regression) may not be appropriate for on-farm experiments because they are not capable of accurately accounting for the underlying spatial variation in a particular response variable (e.g., yield data). The effects of experimental designs and statistical approaches on type I error rates and estimation accuracy were explored through a simulation study hypothetically conducted on experiments in three wheat fields in Japan. Isotropic and anisotropic spatial linear mixed models were established for comparison with ordinary least squares regression models. The repeated designs were not sufficient to reduce both the risk of a type I error and the estimation bias on their own. A combination of a repeated design and an anisotropic model is sometimes required to improve the precision of the experiments. Model selection should be performed to determine whether the anisotropic model is required for analysis of any specific field. The anisotropic model had larger standard errors than the other models, especially when the estimates had large biases. This finding highlights an advantage of anisotropic models since they enable experimenters to cautiously consider the reliability of the estimates when they have a large bias.",['Takashi S. T. Tanaka'],2020-04-27T12:29:02Z,http://arxiv.org/abs/2004.12741v2,"Sustainability, Industry & Robotics",Precision Farming,on-farm experiments can provide information on more efficient crop management . the effects of experimental designs and statistical approaches on type I error rates were explored . a combination of a repeated design and an anisotropic model is sometimes required .
Privacy Preserving Ultra-Short-term Wind Power Prediction Based on   Secure Multi Party Computation,"Mining the spatial and temporal correlation of wind farm output data is beneficial for enhancing the precision of ultra-short-term wind power prediction. However, if the wind farms are owned by separate entities, they may be reluctant to share their data directly due to privacy concerns as well as business management regulation policies. Although cryptographic approaches have been designed to protect privacy in the process of data sharing, it is still a challenging problem to encrypt the original data while extracting the nonlinear relationship among multiple wind farms in the machine learning process. This paper presents pwXGBoost, a technique based on the machine learning tree model and secure multi-party computation (SMPC) that can successfully extract complicated relationships while preserving data privacy. A maximum mean discrepancy (MMD) based scheme is proposed to effectively choose adjacent candidate wind farms to participate in the collaborative model training, therefore improving the accuracy and reducing the burden of data acquisition. The proposed method was evaluated on real world data collected from a cluster of wind farms in Inner Mongolia, China, demonstrating that it is capable of achieving considerable efficiency and performance improvements while preserving privacy","['Hang Fan', 'Xiaoyu Fan', 'Tianyi Hao', 'Wei Wei', 'Kun Chen', 'Guosai Wang', 'Xiaofeng Jia', 'Yidong Li', 'Wei Xu']",2023-01-31T10:01:51Z,http://arxiv.org/abs/2301.13513v1,"Sustainability, Industry & Robotics",Precision Farming,"pwXGBoost is a technique that can extract complicated relationships while preserving data privacy . if wind farms are owned by separate entities, they may be reluctant to share their data . the proposed method was evaluated on real world data collected from a cluster of wind farms in Inner Mongolia, china ."
Strawberry Detection Using a Heterogeneous Multi-Processor Platform,"Over the last few years, the number of precision farming projects has increased specifically in harvesting robots and many of which have made continued progress from identifying crops to grasping the desired fruit or vegetable. One of the most common issues found in precision farming projects is that successful application is heavily dependent not just on identifying the fruit but also on ensuring that localisation allows for accurate navigation. These issues become significant factors when the robot is not operating in a prearranged environment, or when vegetation becomes too thick, thus covering crop. Moreover, running a state-of-the-art deep learning algorithm on an embedded platform is also very challenging, resulting most of the times in low frame rates. This paper proposes using the You Only Look Once version 3 (YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image processing techniques for the application of precision farming robots targeting strawberry detection, accelerated on a heterogeneous multiprocessor platform. The results show a performance acceleration by five times when implemented on a Field-Programmable Gate Array (FPGA) when compared with the same algorithm running on the processor side with an accuracy of 78.3\% over the test set comprised of 146 images.","['Samuel Brandenburg', 'Pedro Machado', 'Nikesh Lama', 'T. M. McGinnity']",2020-11-07T01:08:21Z,http://arxiv.org/abs/2011.03651v1,"Sustainability, Industry & Robotics",Precision Farming,the number of precision farming projects has increased specifically in harvesting robots . many of which have made continued progress from identifying crops to grasping the desired fruit or vegetable . successful application is heavily dependent on identifying the fruit and ensuring that localisation allows for accurate navigation .
An Efficient Data Warehouse for Crop Yield Prediction,"Nowadays, precision agriculture combined with modern information and communications technologies, is becoming more common in agricultural activities such as automated irrigation systems, precision planting, variable rate applications of nutrients and pesticides, and agricultural decision support systems. In the latter, crop management data analysis, based on machine learning and data mining, focuses mainly on how to efficiently forecast and improve crop yield. In recent years, raw and semi-processed agricultural data are usually collected using sensors, robots, satellites, weather stations, farm equipment, farmers and agribusinesses while the Internet of Things (IoT) should deliver the promise of wirelessly connecting objects and devices in the agricultural ecosystem. Agricultural data typically captures information about farming entities and operations. Every farming entity encapsulates an individual farming concept, such as field, crop, seed, soil, temperature, humidity, pest, and weed. Agricultural datasets are spatial, temporal, complex, heterogeneous, non-standardized, and very large. In particular, agricultural data is considered as Big Data in terms of volume, variety, velocity and veracity. Designing and developing a data warehouse for precision agriculture is a key foundation for establishing a crop intelligence platform, which will enable resource efficient agronomy decision making and recommendations. Some of the requirements for such an agricultural data warehouse are privacy, security, and real-time access among its stakeholders (e.g., farmers, farm equipment manufacturers, agribusinesses, co-operative societies, customers and possibly Government agencies). However, currently there are very few reports in the literature that focus on the design of efficient data warehouses with the view of enabling Agricultural Big Data analysis and data mining. In this paper ...","['Vuong M. Ngo', 'Nhien-An Le-Khac', 'M-Tahar Kechadi']",2018-06-26T15:51:30Z,http://arxiv.org/abs/1807.00035v1,"Sustainability, Industry & Robotics",Precision Farming,agricultural data typically captures information about farming entities and operations . the internet of things (iot) should deliver the promise of wirelessly connecting objects and devices in the agricultural ecosystem . designing and developing a data warehouse for precision agriculture is a key foundation .
Waterberry Farms: A Novel Benchmark For Informative Path Planning,"Recent developments in robotic and sensor hardware make data collection with mobile robots (ground or aerial) feasible and affordable to a wide population of users. The newly emergent applications, such as precision agriculture, weather damage assessment, or personal home security often do not satisfy the simplifying assumptions made by previous research: the explored areas have complex shapes and obstacles, multiple phenomena need to be sensed and estimated simultaneously and the measured quantities might change during observations. The future progress of path planning and estimation algorithms requires a new generation of benchmarks that provide representative environments and scoring methods that capture the demands of these applications.   This paper describes the Waterberry Farms benchmark (WBF) that models a precision agriculture application at a Florida farm growing multiple crop types. The benchmark captures the dynamic nature of the spread of plant diseases and variations of soil humidity while the scoring system measures the performance of a given combination of a movement policy and an information model estimator. By benchmarking several examples of representative path planning and estimator algorithms, we demonstrate WBF's ability to provide insight into their properties and quantify future progress.","['Samuel Matloob', 'Partha P. Datta', 'O. Patrick Kreidl', 'Ayan Dutta', 'Swapnoneel Roy', 'Ladislau Bölöni']",2023-05-10T15:24:25Z,http://arxiv.org/abs/2305.06243v1,"Sustainability, Industry & Robotics",Precision Farming,mobile robots make data collection feasible and affordable to a wide population of users . future progress of path planning and estimation algorithms requires new benchmarks . benchmark captures dynamic nature of spread of plant diseases and variations of soil humidity .
AnimalFormer: Multimodal Vision Framework for Behavior-based Precision   Livestock Farming,"We introduce a multimodal vision framework for precision livestock farming, harnessing the power of GroundingDINO, HQSAM, and ViTPose models. This integrated suite enables comprehensive behavioral analytics from video data without invasive animal tagging. GroundingDINO generates accurate bounding boxes around livestock, while HQSAM segments individual animals within these boxes. ViTPose estimates key body points, facilitating posture and movement analysis. Demonstrated on a sheep dataset with grazing, running, sitting, standing, and walking activities, our framework extracts invaluable insights: activity and grazing patterns, interaction dynamics, and detailed postural evaluations. Applicable across species and video resolutions, this framework revolutionizes non-invasive livestock monitoring for activity detection, counting, health assessments, and posture analyses. It empowers data-driven farm management, optimizing animal welfare and productivity through AI-powered behavioral understanding.","['Ahmed Qazi', 'Taha Razzaq', 'Asim Iqbal']",2024-06-14T04:42:44Z,http://arxiv.org/abs/2406.09711v1,"Sustainability, Industry & Robotics",Precision Farming,"a multimodal vision framework for precision livestock farming is introduced . the framework harnesses the power of GroundingDINO, HQSAM, and ViTPose models . it empowers data-driven farm management, optimizing animal welfare and productivity ."
The Composite Visual-Laser Navigation Method Applied in Indoor Poultry   Farming Environments,"Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accumulation, pose significant challenges. Traditional navigation methods that rely on a single sensor often perform poorly in such environments, resulting in issues like laser drift and inaccuracies in visual navigation line extraction. To overcome these limitations, we propose a novel composite navigation method that integrates both laser and vision technologies. This approach dynamically computes a fused yaw angle based on the real-time reliability of each sensor modality, thereby eliminating the need for physical navigation lines. Experimental validation in actual poultry house environments demonstrates that our method not only resolves the inherent drawbacks of single-sensor systems, but also significantly enhances navigation precision and operational efficiency. As such, it presents a promising solution for improving the performance of inspection robots in complex indoor poultry farming settings.","['Jiafan Lu', 'Dongcheng Hu', 'Yitian Ye', 'Anqi Liu', 'Zixian Zhang', 'Xin Peng']",2025-04-11T10:44:30Z,http://arxiv.org/abs/2504.08431v1,"Sustainability, Industry & Robotics",Precision Farming,indoor poultry farms require inspection robots to maintain precise environmental control . the complex conditions within these facilities pose significant challenges . traditional navigation methods that rely on a single sensor perform poorly in such environments .
Cybersecurity in Smart Farming: Canada Market Research,"The Cyber Science Lab (CSL) and Smart Cyber-Physical System (SCPS) Lab at the University of Guelph conduct a market study of cybersecurity technology adoption and requirements for smart and precision farming in Canada. We conducted 17 stakeholder/key opinion leader interviews in Canada and the USA, as well as conducting extensive secondary research, to complete this study. Each interview generally required 15-20 minutes to complete. Interviews were conducted using a client-approved interview guide. Secondary and primary research focussed on the following areas of investigation: Market size and segmentation Market forecast and growth rate Competitive landscape Market challenges/barriers to entry Market trends/growth drivers Adoption/commercialization of the technology","['Ali Dehghantanha', 'Hadis Karimipour', 'Amin Azmoodeh']",2021-04-12T03:33:45Z,http://arxiv.org/abs/2104.05183v1,"Sustainability, Industry & Robotics",Precision Farming,a market study of cybersecurity adoption and requirements for smart and precision farming in Canada . the study conducted 17 stakeholder/key opinion leader interviews in Canada and the USA . each interview generally required 15-20 minutes to complete .
A Learned Simulation Environment to Model Plant Growth in Indoor Farming,"We developed a simulator to quantify the effect of changes in environmental parameters on plant growth in precision farming. Our approach combines the processing of plant images with deep convolutional neural networks (CNN), growth curve modeling, and machine learning. As a result, our system is able to predict growth rates based on environmental variables, which opens the door for the development of versatile reinforcement learning agents.","['J. Amacker', 'T. Kleiven', 'M. Grigore', 'P. Albrecht', 'C. Horn']",2022-12-06T17:28:13Z,http://arxiv.org/abs/2212.03155v1,"Sustainability, Industry & Robotics",Precision Farming,"we developed a simulator to quantify the effect of changes in environmental parameters on plant growth in precision farming . our system is able to predict growth rates based on environmental variables, which opens the door for versatile reinforcement learning agents ."
Health Detection on Cattle Compressed Images in Precision Livestock   Farming,"The constant population growth brings the needing to make up for food also grows at the same rate. The livestock provides one-third of humans protein base as meat and milk. To improve cattles health and welfare the pastoral farming employs Precision Livestock farming (PLF). This technique implementation brings a challenge to minimize energy consumption due to farmers not having enough energy or devices to transmit large volumes of information at the size are received from their farms monitors. Therefore, in this project, we will design an algorithm to compress and decompress images reducing energy consumption with the less information lost. Initially, the related problems have been read and analyzed to learn about the techniques used in the past and to be updated with the current works. We implemented Seam Carving and LZW algorithms. The compression of all images, around 1000 takes a time of 5 hours 10 min. We got a compression rate of 1.82:1 with 13.75s average time for each file and a decompression rate of 1.64:1 and 7.5 s average time for each file. The memory consumption we obtained was between 146MB and 504 MB and time consumption was between 30,5s for 90MB to 12192s for 24410 MB, it was all files.","['Miguel Angel Calvache', 'Valeria Cardona', 'Sebastian Tapias', 'Simon Marin', 'Mauricio Toro']",2021-11-23T03:46:27Z,http://arxiv.org/abs/2112.01251v1,"Sustainability, Industry & Robotics",Precision Farming,"the livestock provides one-third of humans protein base as meat and milk . this technique implementation brings a challenge to minimize energy consumption . the compression of all images, around 1000 takes a time of 5 hours 10 min."
Smoothing Traffic Flow via Control of Autonomous Vehicles,"The emergence of autonomous vehicles is expected to revolutionize road transportation in the near future. Although large-scale numerical simulations and small-scale experiments have shown promising results, a comprehensive theoretical understanding to smooth traffic flow via autonomous vehicles is lacking. In this paper, from a control-theoretic perspective, we establish analytical results on the controllability, stabilizability, and reachability of a mixed traffic system consisting of human-driven vehicles and autonomous vehicles in a ring road. We show that the mixed traffic system is not completely controllable, but is stabilizable, indicating that autonomous vehicles can not only suppress unstable traffic waves but also guide the traffic flow to a higher speed. Accordingly, we establish the maximum traffic speed achievable via controlling autonomous vehicles. Numerical results show that the traffic speed can be increased by over 6% when there are only 5% autonomous vehicles. We also design an optimal control strategy for autonomous vehicles to actively dampen undesirable perturbations. These theoretical findings validate the high potential of autonomous vehicles to smooth traffic flow.","['Yang Zheng', 'Jiawei Wang', 'Keqiang Li']",2018-12-22T15:25:53Z,http://arxiv.org/abs/1812.09544v2,"Sustainability, Industry & Robotics",Autonomous Vehicles,"the emergence of autonomous vehicles is expected to revolutionize road transportation in the near future . a comprehensive theoretical understanding to smooth traffic flow via autonomous vehicles remains lacking . the mixed traffic system is not completely controllable, but is stabilizable ."
Towards Fully Intelligent Transportation through Infrastructure-Vehicle   Cooperative Autonomous Driving: Challenges and Opportunities,"The infrastructure-vehicle cooperative autonomous driving approach depends on the cooperation between intelligent roads and intelligent vehicles. This approach is not only safer but also more economical compared to the traditional on-vehicle-only autonomous driving approach. In this paper, we introduce our real-world deployment experiences of cooperative autonomous driving, and delve into the details of new challenges and opportunities. Specifically, based on our progress towards commercial deployment, we follow a three-stage development roadmap of the cooperative autonomous driving approach:infrastructure-augmented autonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and infrastructure-planned autonomous driving (IPAD).","['Shaoshan Liu', 'Bo Yu', 'Jie Tang', 'Qi Zhu']",2021-03-03T04:50:43Z,http://arxiv.org/abs/2103.02176v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"this paper introduces our real-world deployment experiences of cooperative autonomous driving . this approach is not only safer but also more economical compared to the traditional on-vehicle-only approach . based on our progress towards commercial deployment, we follow three-stage development roadmap ."
Vision-based Navigation of Autonomous Vehicle in Roadway Environments   with Unexpected Hazards,"Vision-based navigation of autonomous vehicles primarily depends on the Deep Neural Network (DNN) based systems in which the controller obtains input from sensors/detectors, such as cameras and produces a vehicle control output, such as a steering wheel angle to navigate the vehicle safely in a roadway traffic environment. Typically, these DNN-based systems of the autonomous vehicle are trained through supervised learning; however, recent studies show that a trained DNN-based system can be compromised by perturbation or adversarial inputs. Similarly, this perturbation can be introduced into the DNN-based systems of autonomous vehicle by unexpected roadway hazards, such as debris and roadblocks. In this study, we first introduce a roadway hazardous environment (both intentional and unintentional roadway hazards) that can compromise the DNN-based navigational system of an autonomous vehicle, and produces an incorrect steering wheel angle, which can cause crashes resulting in fatality and injury. Then, we develop a DNN-based autonomous vehicle driving system using object detection and semantic segmentation to mitigate the adverse effect of this type of hazardous environment, which helps the autonomous vehicle to navigate safely around such hazards. We find that our developed DNN-based autonomous vehicle driving system including hazardous object detection and semantic segmentation improves the navigational ability of an autonomous vehicle to avoid a potential hazard by 21% compared to the traditional DNN-based autonomous vehicle driving system.","['Mhafuzul Islam', 'Mahsrur Chowdhury', 'Hongda Li', 'Hongxin Hu']",2018-09-27T02:08:21Z,http://arxiv.org/abs/1810.03967v3,"Sustainability, Industry & Robotics",Autonomous Vehicles,a trained DNN-based system can be compromised by perturbation or adversarial inputs . a wrong steering wheel angle can cause crashes resulting in fatality and injury . the system improves the navigational ability of an autonomous vehicle to avoid a potential hazard .
Want a Ride? Attitudes Towards Autonomous Driving and Behavior in   Autonomous Vehicles,"Research conducted previously has focused on either attitudes toward or behaviors associated with autonomous driving. In this paper, we bridge these two dimensions by exploring how attitudes towards autonomous driving influence behavior in an autonomous car. We conducted a field experiment with twelve participants engaged in non-driving related tasks. Our findings indicate that attitudes towards autonomous driving do not affect participants' driving interventions in vehicle control and eye glance behavior. Therefore, studies on autonomous driving technology lacking field tests might be unreliable for assessing the potential behaviors, attitudes, and acceptance of autonomous vehicles.","['Enrico Del Re', 'Leonie Sauer', 'Marco Polli', 'Cristina Olaverri-Monreal']",2024-09-04T09:21:49Z,http://arxiv.org/abs/2409.02556v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,this paper examines how attitudes towards autonomous driving influence behavior . we conducted a field experiment with twelve participants engaged in non-driving related tasks . our findings indicate that attitudes toward autonomous driving do not affect participants' driving interventions .
Intelligent Perception System for Vehicle-Road Cooperation,"With the development of autonomous driving, the improvement of autonomous driving technology for individual vehicles has reached the bottleneck. The advancement of vehicle-road cooperation autonomous driving technology can expand the vehicle's perception range, supplement the perception blind area and improve the perception accuracy, to promote the development of autonomous driving technology and achieve vehicle-road integration. This project mainly uses lidar to develop data fusion schemes to realize the sharing and combination of vehicle and road equipment data and achieve the detection and tracking of dynamic targets. At the same time, some test scenarios for the vehicle-road cooperative system were designed and used to test our vehicle-road cooperative awareness system, which proved the advantages of vehicle-road cooperative autonomous driving over single-vehicle autonomous driving.",['Songbin Chen'],2022-08-30T08:10:34Z,http://arxiv.org/abs/2208.14052v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,the advancement of vehicle-road cooperation autonomous driving technology can expand the vehicle's perception range . this project mainly uses lidar to develop data fusion schemes to realize the sharing of vehicle and road equipment data .
A Highway Toll Lane Framework that Unites Autonomous Vehicles and   High-occupancy Vehicles,"We consider the scenario where human-driven/autonomous vehicles with low/high occupancy are sharing a segment of highway and autonomous vehicles are capable of increasing the traffic throughput by preserving a shorter headway than human-driven vehicles. We propose a toll lane framework where a lane on the highway is reserved freely for autonomous vehicles with high occupancy, which have the greatest capability to increase social mobility, and the other three classes of vehicles can choose to use the toll lane with a toll or use the other regular lanes freely. All vehicles are assumed to be only interested in minimizing their own travel costs. We explore the resulting lane choice equilibria under the framework and establish desirable properties of the equilibria, which implicitly compare high-occupancy vehicles with autonomous vehicles in terms of their capabilities to increase social mobility. We further use numerical examples in the optimal toll design, the occupancy threshold design, and the policy design problems to clarify the various potential applications of this toll lane framework that unites high-occupancy vehicles and autonomous vehicles. To our best knowledge, this is the first work that systematically studies a toll lane framework that unites autonomous vehicles and high-occupancy vehicles on the roads.","['Ruolin Li', 'Philip N. Brown', 'Roberto Horowitz']",2021-07-07T21:01:59Z,http://arxiv.org/abs/2107.03477v2,"Sustainability, Industry & Robotics",Autonomous Vehicles,we propose a toll lane framework where a lane on the highway is reserved freely . autonomous vehicles with high occupancy have the greatest capability to increase social mobility . all vehicles are assumed to be only interested in minimizing their own travel costs . we explore the resulting lane choice equilibria and establish desirable properties
Synthesis of Different Autonomous Vehicles Test Approaches,"Currently, the most prevalent way to evaluate an autonomous vehicle is to directly test it on the public road. However, because of recent accidents caused by autonomous vehicles, it becomes controversial about whether on-road tests should be the best approach. Alternatively, people use test tracks or simulation to assess the safety of autonomous vehicles. These approaches are time-efficient and less costly, however, their credibility varies. In this paper, we propose to use a co-Kriging model to synthesize the results from different evaluation approaches, which allows us to fully utilize the information and provides an accurate, affordable, and safe way to assess a design of an autonomous vehicle.","['Zhiyuan Huang', 'Mansur Arief', 'Henry Lam', 'Ding Zhao']",2018-09-09T01:57:37Z,http://arxiv.org/abs/1809.02911v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"the most prevalent way to evaluate an autonomous vehicle is to test it on the public road . people use test tracks or simulation to assess the safety of autonomous vehicles . these approaches are time-efficient and less costly, but their credibility varies ."
Development of an Autonomous Reverse Engineering Capability for   Controller Area Network Messages to Support Autonomous Control Retrofits,"As the autonomous vehicle industry continues to grow, various companies are exploring the use of aftermarket kits to retrofit existing vehicles with semi-autonomous capabilities. However, differences in implementation of the controller area network (CAN) used by each vehicle manufacturer poses a significant challenge to achieving large-scale implementation of retrofits. To address this challenge, this research proposes a method for reverse engineering the CAN channels associated with a vehicle's accelerator and brake pedals, without any prior knowledge of the vehicle. By simultaneously recording inertial measurement unit (IMU) and CAN data during vehicle operation, the proposed algorithms can identify the CAN channels that correspond to each control. During testing of six vehicles from three manufacturers, the proposed method was shown to successfully identify the CAN channels for the accelerator pedal and brake pedal for each vehicle tested. These promising results demonstrate the potential for using this approach for developing aftermarket autonomous vehicle kits - potentially with additional research to facilitate real-time use. Notably, the proposed system has the potential to maintain its effectiveness despite changes in vehicle CAN standards, and it could potentially be adapted to function with any vehicle communications medium.","['Kevin Setterstrom', 'Jeremy Straub']",2023-07-20T08:00:24Z,http://arxiv.org/abs/2307.11781v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"research proposes a method for reverse engineering the CAN channels associated with a vehicle's accelerator and brake pedals . by simultaneously recording inertial measurement unit (iMU) and CAN data during vehicle operation, the proposed algorithms can identify the channels that correspond to each control . the proposed method has the potential to maintain its effectiveness despite"
Optimal Formation of Autonomous Vehicles in Mixed Traffic Flow,"Platooning of multiple autonomous vehicles has attracted significant attention in both academia and industry. Despite its great potential, platooning is not the only choice for the formation of autonomous vehicles in mixed traffic flow, where autonomous vehicles and human-driven vehicles (HDVs) coexist. In this paper, we investigate the optimal formation of autonomous vehicles that can achieve an optimal system-wide performance in mixed traffic flow. Specifically, we consider the optimal $\mathcal{H}_2$ performance of the entire traffic flow, reflecting the potential of autonomous vehicles in mitigating traffic perturbations. Then, we formulate the optimal formation problem as a set function optimization problem. Numerical results reveal two predominant optimal formations: uniform distribution and platoon formation, depending on traffic parameters. In addition, we show that 1) the prevailing platoon formation is not always the optimal choice; 2) platoon formation might be the worst choice when HDVs have a poor string stability behavior. These results suggest more opportunities for the formation of autonomous vehicles, beyond platooning, in mixed traffic flow.","['Keqiang Li', 'Jiawei Wang', 'Yang Zheng']",2020-04-01T12:48:12Z,http://arxiv.org/abs/2004.00397v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,platooning is not the only option for autonomous vehicles in mixed traffic flow . autonomous vehicles and human-driven vehicles (HDVs) coexist . results suggest more opportunities for the formation of autonomous vehicles .
Drive as You Speak: Enabling Human-Like Interaction with Large Language   Models in Autonomous Vehicles,"The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.","['Can Cui', 'Yunsheng Ma', 'Xu Cao', 'Wenqian Ye', 'Ziran Wang']",2023-09-19T00:47:13Z,http://arxiv.org/abs/2309.10228v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"the future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities . the proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making ."
Physics of Autonomous Driving based on Three-Phase Traffic Theory,"We have revealed physical features of autonomous driving in the framework of the three-phase traffic theory for which there is no fixed time headway to the preceding vehicle. A comparison with the classical model approach to autonomous driving for which an autonomous driving vehicle tries to reach a fixed (desired or ""optimal"") time headway to the preceding vehicle has been made. It turns out that autonomous driving in the framework of the three-phase traffic theory exhibits the following advantages in comparison with the classical model of autonomous driving: (i) The absence of string instability. (ii) Considerably smaller speed disturbances at road bottlenecks. (iii) Autonomous driving vehicles based on the three-phase theory decrease the probability of traffic breakdown at the bottleneck in mixed traffic flow consisting of human driving and autonomous driving vehicles; on the contrary, even a single autonomous driving vehicle based on the classical approach can provoke traffic breakdown at the bottleneck in mixed traffic flow.",['Boris S. Kerner'],2017-10-30T10:26:21Z,http://arxiv.org/abs/1710.10852v3,"Sustainability, Industry & Robotics",Autonomous Vehicles,the absence of string instability and considerably smaller speed disturbances at road bottlenecks are advantages of autonomous driving . even a single autonomous driving vehicle based on the classical approach can provoke traffic breakdown .
Evaluation and Optimization of Adaptive Cruise Control in Autonomous   Vehicles using the CARLA Simulator: A Study on Performance under Wet and Dry   Weather Conditions,"Adaptive Cruise Control ACC can change the speed of the ego vehicle to maintain a safe distance from the following vehicle automatically. The primary purpose of this research is to use cutting-edge computing approaches to locate and track vehicles in real time under various conditions to achieve a safe ACC. The paper examines the extension of ACC employing depth cameras and radar sensors within Autonomous Vehicles AVs to respond in real time by changing weather conditions using the Car Learning to Act CARLA simulation platform at noon. The ego vehicle controller's decision to accelerate or decelerate depends on the speed of the leading ahead vehicle and the safe distance from that vehicle. Simulation results show that a Proportional Integral Derivative PID control of autonomous vehicles using a depth camera and radar sensors reduces the speed of the leading vehicle and the ego vehicle when it rains. In addition, longer travel time was observed for both vehicles in rainy conditions than in dry conditions. Also, PID control prevents the leading vehicle from rear collisions","['Roza Al-Hindaw', 'Taqwa I. Alhadidi', 'Mohammad Adas']",2024-05-02T17:34:23Z,http://arxiv.org/abs/2405.01504v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,Adaptive Cruise Control ACC can change the speed of the ego vehicle to maintain a safe distance from the following vehicle automatically . the paper examines the extension of ACC employing depth cameras and radar sensors within Autonomous Vehicles AVs .
Blue Phase: Optimal Network Traffic Control for Legacy and Autonomous   Vehicles,"With the forecasted emergence of autonomous vehicles in urban traffic networks, new control policies are needed to leverage their potential for reducing congestion. While several efforts have studied the fully autonomous traffic control problem, there is a lack of models addressing the more imminent transitional stage wherein legacy and autonomous vehicles share the urban infrastructure. We address this gap by introducing a new policy for stochastic network traffic control involving both classes of vehicles. We conjecture that network links will have dedicated lanes for autonomous vehicles which provide access to traffic intersections and combine traditional green signal phases with autonomous vehicle-restricted signal phases named blue phases. We propose a new pressure-based, decentralized, hybrid network control policy that activates selected movements at intersections based on the solution of mixed-integer linear programs. We prove that the proposed policy is stable, i.e. maximizes network throughput, under conventional travel demand conditions. We conduct numerical experiments to test the proposed policy under varying proportions of autonomous vehicles. Our experiments reveal that considerable trade-offs exist in terms of vehicle-class travel time based on the level of market penetration of autonomous vehicles. Further, we find that the proposed hybrid network control policy improves on traditional green phase traffic signal control for high levels of congestion, thus helping in quantifying the potential benefits of autonomous vehicles in urban networks.","['David Rey', 'Michael W Levin']",2018-08-09T23:44:18Z,http://arxiv.org/abs/1808.03373v3,"Sustainability, Industry & Robotics",Autonomous Vehicles,"we propose a new policy for stochastic network traffic control involving both classes of vehicles . we conjecture that network links will have dedicated lanes for autonomous cars . the policy is stable, i.e. maximizes network throughput, under conventional travel demand conditions ."
"Autonomous Vehicles: Open-Source Technologies, Considerations, and   Development","Autonomous vehicles are the culmination of advances in many areas such as sensor technologies, artificial intelligence (AI), networking, and more. This paper will introduce the reader to the technologies that build autonomous vehicles. It will focus on open-source tools and libraries for autonomous vehicle development, making it cheaper and easier for developers and researchers to participate in the field. The topics covered are as follows. First, we will discuss the sensors used in autonomous vehicles and summarize their performance in different environments, costs, and unique features. Then we will cover Simultaneous Localization and Mapping (SLAM) and algorithms for each modality. Third, we will review popular open-source driving simulators, a cost-effective way to train machine learning models and test vehicle software performance. We will then highlight embedded operating systems and the security and development considerations when choosing one. After that, we will discuss Vehicle-to-Vehicle (V2V) and Internet-of-Vehicle (IoV) communication, which are areas that fuse networking technologies with autonomous vehicles to extend their functionality. We will then review the five levels of vehicle automation, commercial and open-source Advanced Driving Assistance Systems, and their features. Finally, we will touch on the major manufacturing and software companies involved in the field, their investments, and their partnerships. These topics will give the reader an understanding of the industry, its technologies, active research, and the tools available for developers to build autonomous vehicles.","['Oussama Saoudi', 'Ishwar Singh', 'Hamidreza Mahyar']",2022-01-25T07:13:39Z,http://arxiv.org/abs/2202.03148v2,"Sustainability, Industry & Robotics",Autonomous Vehicles,"this paper will introduce the reader to the technologies that build autonomous vehicles . it will focus on open-source tools and libraries for autonomous vehicle development . topics will give the reader an understanding of the industry, its technologies, active research, and the tools available ."
A Vehicle-Infrastructure Multi-layer Cooperative Decision-making   Framework,"Autonomous driving has entered the testing phase, but due to the limited decision-making capabilities of individual vehicle algorithms, safety and efficiency issues have become more apparent in complex scenarios. With the advancement of connected communication technologies, autonomous vehicles equipped with connectivity can leverage vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications, offering a potential solution to the decision-making challenges from individual vehicle's perspective. We propose a multi-level vehicle-infrastructure cooperative decision-making framework for complex conflict scenarios at unsignalized intersections. First, based on vehicle states, we define a method for quantifying vehicle impacts and their propagation relationships, using accumulated impact to group vehicles through motif-based graph clustering. Next, within and between vehicle groups, a pass order negotiation process based on Large Language Models (LLM) is employed to determine the vehicle passage order, resulting in planned vehicle actions. Simulation results from ablation experiments show that our approach reduces negotiation complexity and ensures safer, more efficient vehicle passage at intersections, aligning with natural decision-making logic.","['Yiming Cui', 'Shiyu Fang', 'Peng Hang', 'Jian Sun']",2025-03-19T14:49:39Z,http://arxiv.org/abs/2503.16552v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"autonomous driving has entered the testing phase, but safety and efficiency issues have become more apparent in complex scenarios . we propose a multi-level vehicle-infrastructure cooperative decision-making framework for complex conflict scenarios at unsignalized intersections ."
Transfer Learning and Organic Computing for Autonomous Vehicles,"Autonomous Vehicles(AV) are one of the brightest promises of the future which would help cut down fatalities and improve travel time while working in harmony. Autonomous vehicles will face with challenging situations and experiences not seen before. These experiences should be converted to knowledge and help the vehicle prepare better in the future. Online Transfer Learning will help transferring prior knowledge to a new task and also keep the knowledge updated as the task evolves. This paper presents the different methods of transfer learning, online transfer learning and organic computing that could be adapted to the domain of autonomous vehicles.",['Christofer Fellicious'],2018-08-16T12:28:11Z,http://arxiv.org/abs/1808.05443v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,autonomous vehicles(AV) are one of the brightest promises of the future . they will face with challenging situations and experiences not seen before . these experiences should be converted into knowledge and help the vehicle prepare better .
Formal Verification of Autonomous Vehicle Platooning,"The coordination of multiple autonomous vehicles into convoys or platoons is expected on our highways in the near future. However, before such platoons can be deployed, the new autonomous behaviors of the vehicles in these platoons must be certified. An appropriate representation for vehicle platooning is as a multi-agent system in which each agent captures the ""autonomous decisions"" carried out by each vehicle. In order to ensure that these autonomous decision-making agents in vehicle platoons never violate safety requirements, we use formal verification. However, as the formal verification technique used to verify the agent code does not scale to the full system and as the global verification technique does not capture the essential verification of autonomous behavior, we use a combination of the two approaches. This mixed strategy allows us to verify safety requirements not only of a model of the system, but of the actual agent code used to program the autonomous vehicles.","['Maryam Kamali', 'Louise A. Dennis', 'Owen McAree', 'Michael Fisher', 'Sandor M. Veres']",2016-02-04T15:50:22Z,http://arxiv.org/abs/1602.01718v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,"the coordination of multiple autonomous vehicles into convoys or platoons is expected on our highways in the near future . a multi-agent system in which each agent captures the ""autonomous decisions"" carried out by each vehicle . formal verification techniques used to verify the agent code do not scale to the full system ."
"Decision-Making Technology for Autonomous Vehicles Learning-Based   Methods, Applications and Future Outlook","Autonomous vehicles have a great potential in the application of both civil and military fields, and have become the focus of research with the rapid development of science and economy. This article proposes a brief review on learning-based decision-making technology for autonomous vehicles since it is significant for safer and efficient performance of autonomous vehicles. Firstly, the basic outline of decision-making technology is provided. Secondly, related works about learning-based decision-making methods for autonomous vehicles are mainly reviewed with the comparison to classical decision-making methods. In addition, applications of decision-making methods in existing autonomous vehicles are summarized. Finally, promising research topics in the future study of decision-making technology for autonomous vehicles are prospected.","['Qi Liu', 'Xueyuan Li', 'Shihua Yuan', 'Zirui Li']",2021-07-02T14:45:52Z,http://arxiv.org/abs/2107.01110v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,this article proposes a brief review on learning-based decision-making technology . it is significant for safer and efficient performance of autonomous vehicles .
Game Theoretic Analysis of Road User Safety Scenarios Involving   Autonomous Vehicles,"Interactions between pedestrians, bikers, and human-driven vehicles have been a major concern in traffic safety over the years. The upcoming age of autonomous vehicles will further raise major problems on whether self-driving cars can accurately avoid accidents; on the other hand, usability issues arise on whether human-driven cars and pedestrians can dominate the road at the expense of the autonomous vehicles which will be programmed to avoid accidents. This paper proposes some game theoretical models applied to related traffic scenarios. In the first two games the reciprocal influence between a pedestrian and a vehicle (either autonomous or not) is analyzed, while the third game investigates the intersection of two vehicles, possibly autonomous. The games have been simulated in order to demonstrate the theoretical analysis and the predicted behaviors. These investigations can shed new lights on how novel urban traffic regulations could be required to allow for a better interaction of vehicles and a general improved management of traffic and communication vehicular networks.","['Umberto Michieli', 'Leonardo Badia']",2018-03-06T19:26:04Z,http://arxiv.org/abs/1803.02393v2,"Sustainability, Industry & Robotics",Autonomous Vehicles,"interactions between pedestrians, bikers and human-driven vehicles have been a major concern in traffic safety over the years . upcoming age of autonomous vehicles will raise major problems on whether self-driving cars can accurately avoid accidents . usability issues arise on whether autonomous vehicles can dominate the road at the expense of the autonomous vehicles "
F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle   Edge Computing System Using 3D Point Clouds,"Autonomous vehicles are heavily reliant upon their sensors to perfect the perception of surrounding environments, however, with the current state of technology, the data which a vehicle uses is confined to that from its own sensors. Data sharing between vehicles and/or edge servers is limited by the available network bandwidth and the stringent real-time constraints of autonomous driving applications. To address these issues, we propose a point cloud feature based cooperative perception framework (F-Cooper) for connected autonomous vehicles to achieve a better object detection precision. Not only will feature based data be sufficient for the training process, we also use the features' intrinsically small size to achieve real-time edge computing, without running the risk of congesting the network. Our experiment results show that by fusing features, we are able to achieve a better object detection result, around 10% improvement for detection within 20 meters and 30% for further distances, as well as achieve faster edge computing with a low communication delay, requiring 71 milliseconds in certain feature selections. To the best of our knowledge, we are the first to introduce feature-level data fusion to connected autonomous vehicles for the purpose of enhancing object detection and making real-time edge computing on inter-vehicle data feasible for autonomous vehicles.",['Qi Chen'],2019-09-13T21:33:18Z,http://arxiv.org/abs/1909.06459v1,"Sustainability, Industry & Robotics",Autonomous Vehicles,a point cloud feature based cooperative perception framework (F-Cooper) is proposed for connected autonomous vehicles . data sharing between vehicles and/or edge servers is limited by the available network bandwidth and stringent real-time constraints of autonomous driving applications.
The Use of Agricultural Robots in Orchard Management,"Book chapter that summarizes recent research on agricultural robotics in orchard management, including Robotic pruning, Robotic thinning, Robotic spraying, Robotic harvesting, Robotic fruit transportation, and future trends.","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']",2019-07-30T17:56:17Z,http://arxiv.org/abs/1907.13114v1,"Sustainability, Industry & Robotics",Robotics and Automation,"book summarizes recent research on agricultural robotics in orchard management . book includes: Robotic pruning, Robotic thinning, Robot"
On Robot Revolution and Taxation,Advances in artificial intelligence are resulting in the rapid automation of the work force. The tools that are used to automate are called robots. Bill Gates proposed that in order to deal with the problem of the loss of jobs and reduction of the tax revenue we ought to tax the robots. The problem with taxing the robots is that it is not easy to know what a robot is. This article studies the definition of a robot and the implication of advances in robotics on taxation. It is evident from this article that it is a difficult task to establish what a robot is and what is not a robot. It concludes that taxing robots is the same as increasing corporate tax.,['Tshilidzi Marwala'],2018-08-05T18:26:34Z,http://arxiv.org/abs/1808.01666v1,"Sustainability, Industry & Robotics",Robotics and Automation,this article studies the definition of a robot and the implication of advances in robotics on taxation . it concludes that taxing robot
"Towards Using Multiple Iterated, Reproduced, and Replicated Experiments   with Robots (MIRRER) for Evaluation and Benchmarking","The robotics research field lacks formalized definitions and frameworks for evaluating advanced capabilities including generalizability (the ability for robots to perform tasks under varied contexts) and reproducibility (the performance of a reproduced robot capability in different labs under the same experimental conditions). This paper presents an initial conceptual framework, MIRRER, that unites the concepts of performance evaluation, benchmarking, and reproduced/replicated experimentation in order to facilitate comparable robotics research. Several open issues with the application of the framework are also presented.","['Adam Norton', 'Brian Flynn']",2024-08-08T19:33:23Z,http://arxiv.org/abs/2408.04736v1,"Sustainability, Industry & Robotics",Robotics and Automation,"this paper presents an initial conceptual framework, MIRRER, that unites the concepts of performance evaluation, benchmarking, and reproduced/replic"
A systematic literature review on Robotic Process Automation security,"The technocrat epoch is overflowing with new technologies and such cutting-edge facilities accompany the risks and pitfalls. Robotic process automation is another innovation that empowers the computerization of high-volume, manual, repeatable, everyday practice, rule-based, and unmotivating human errands. The principal objective of Robotic Process Automation is to supplant monotonous human errands with a virtual labor force or a computerized specialist playing out a similar work as the human laborer used to perform. This permits human laborers to zero in on troublesome undertakings and critical thinking. Robotic Process Automation instruments are viewed as straightforward and strong for explicit business process computerization. Robotic Process Automation comprises intelligence to decide if a process should occur. It has the capability to analyze the data presented and provide a decision based on the logic parameters set in place by the developer. Moreover, it does not demand for system integration, like other forms of automation. Be that as it may since the innovation is yet arising, the Robotic Process Automation faces a few difficulties during the execution.","['Nishith Gajjar', 'Keyur Rathod', 'Khushali Jani']",2022-12-11T17:02:18Z,http://arxiv.org/abs/2212.05544v1,"Sustainability, Industry & Robotics",Robotics and Automation,robotic process automation empowers the computerization of human errands . the principal objective of Robotic Process Automation is to suppl
Kinematic Optimization of a Robotic Arm for Automation Tasks with Human   Demonstration,"Robotic arms are highly common in various automation processes such as manufacturing lines. However, these highly capable robots are usually degraded to simple repetitive tasks such as pick-and-place. On the other hand, designing an optimal robot for one specific task consumes large resources of engineering time and costs. In this paper, we propose a novel concept for optimizing the fitness of a robotic arm to perform a specific task based on human demonstration. Fitness of a robot arm is a measure of its ability to follow recorded human arm and hand paths. The optimization is conducted using a modified variant of the Particle Swarm Optimization for the robot design problem. In the proposed approach, we generate an optimal robot design along with the required path to complete the task. The approach could reduce the time-to-market of robotic arms and enable the standardization of modular robotic parts. Novice users could easily apply a minimal robot arm to various tasks. Two test cases of common manufacturing tasks are presented yielding optimal designs and reduced computational effort by up to 92%.","['Inbar Meir', 'Avital Bechar', 'Avishai Sintov']",2024-01-30T07:45:26Z,http://arxiv.org/abs/2401.16801v1,"Sustainability, Industry & Robotics",Robotics and Automation,the fitness of a robot arm is a measure of its ability to follow recorded human path . the approach could reduce the time-to-
Deformable Tip Mount for Soft Growing Eversion Robots,"Here we present a flexible tip mount for eversion (vine) robots. This soft cap allows attaching a payload to an eversion robot while allowing moving through narrow openings, as well as the eversion of protruding objects, and expanded surfaces.","['Cem Suulker', 'Sophie Skach', 'Danyaal Kaleel', 'Taqi Abrar', 'Zain Murtaza', 'Dilara Suulker', 'Kaspar Althoefer']",2024-01-15T17:34:37Z,http://arxiv.org/abs/2401.07855v1,"Sustainability, Industry & Robotics",Robotics and Automation,"this soft cap allows attaching a payload to an eversion (vine) robot . it allows moving through narrow openings, as"
Enabling the Deployment of Any-Scale Robotic Applications in   Microservice Architectures through Automated Containerization,"In an increasingly automated world -- from warehouse robots to self-driving cars -- streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at https://github.com/ika-rwth-aachen/dorotos.","['Jean-Pierre Busch', 'Lennart Reiher', 'Lutz Eckstein']",2023-09-12T21:32:25Z,http://arxiv.org/abs/2309.06611v3,"Sustainability, Industry & Robotics",Robotics and Automation,automated DevOps processes and microservice architectures have already proven successful . we present and release a tooling suite for automating
Mobile Robot Yielding Cues for Human-Robot Spatial Interaction,"Mobile robots are increasingly being deployed in public spaces such as shopping malls, airports, and urban sidewalks. Most of these robots are designed with human-aware motion planning capabilities but are not designed to communicate with pedestrians. Pedestrians encounter these robots without prior understanding of the robots' behaviour, which can cause discomfort, confusion, and delayed social acceptance. In this research, we explore the common human-robot interaction at a doorway or bottleneck in a structured environment. We designed and evaluated communication cues used by a robot when yielding to a pedestrian in this scenario. We conducted an online user study with 102 participants using videos of a set of robot-to-human yielding cues. Results show that a Robot Retreating cue was the most socially acceptable cue. The results of this work help guide the development of mobile robots for public spaces.","['Nicholas J. Hetherington', 'Ryan Lee', 'Marlene Haase', 'Elizabeth A. Croft', 'H. F. Machiel Van der Loos']",2021-04-06T04:09:28Z,http://arxiv.org/abs/2104.02279v1,"Sustainability, Industry & Robotics",Robotics and Automation,"mobile robots are increasingly being deployed in public spaces such as shopping malls, airports, and urban sidewalks . most of these robots"
Design of a Hybrid Robot Control System using Memristor-Model and   Ant-Inspired Based Information Transfer Protocols,It is not always possible for a robot to process all the information from its sensors in a timely manner and thus quick and yet valid approximations of the robot's situation are needed. Here we design hybrid control for a robot within this limit using algorithms inspired by ant worker placement behaviour and based on memristor-based non-linearity.,"['Ella Gale', 'Ben de Lacy Costello', 'Andrew Adamatzky']",2014-02-17T14:00:01Z,http://arxiv.org/abs/1402.4004v1,"Sustainability, Industry & Robotics",Robotics and Automation,we design hybrid control for a robot within this limit using algorithms inspired by ant worker placement behaviour and based on memristor-based
Intelligent humanoids in manufacturing to address worker shortage and   skill gaps: Case of Tesla Optimus,"Technological evolution in the field of robotics is emerging with major breakthroughs in recent years. This was especially fostered by revolutionary new software applications leading to humanoid robots. Humanoids are being envisioned for manufacturing applications to form human-robot teams. But their implication in manufacturing practices especially for industrial safety standards and lean manufacturing practices have been minimally addressed. Humanoids will also be competing with conventional robotic arms and effective methods to assess their return on investment are needed. To study the next generation of industrial automation, we used the case context of the Tesla humanoid robot. The company has recently unveiled its project on an intelligent humanoid robot named Optimus to achieve an increased level of manufacturing automation. This article proposes a framework to integrate humanoids for manufacturing automation and also presents the significance of safety standards of human-robot collaboration. A case of lean assembly cell for the manufacturing of an open-source medical ventilator was used for human-humanoid automation. Simulation results indicate that humanoids can increase the level of manufacturing automation. Managerial and research implications are presented.","['Ali Ahmad Malik', 'Tariq Masood', 'Alexander Brem']",2023-04-11T03:32:55Z,http://arxiv.org/abs/2304.04949v1,"Sustainability, Industry & Robotics",Robotics and Automation,humanoids are being envisioned for manufacturing applications to form human-robot teams . humanoids will also be competing with conventional robotic arms .
A reconfigurable robot workcell for quick set-up of assembly processes,"High volume production has been a prerequisite in order to invest into automation of the manufacturing process for decades. The high cost of setup and the inflexibility of classical automation meant that low batch productions, often present in Small and Medium-sized Enterprises (SMEs), were dismissed as potential end user of automation technologies. In this extended abstract we present the results of the ReconCell project whose objective was to develop a new type of highly reconfigurable robot workcell for fast set-up of automated assembly processes in SMEs. The high degree of reconfigurability was achieved by the developed reconfigurable hardware and the complementary reconfigurable software, while fast set-up was achieved with technologies for fast robot programming.","['Timotej Gašpar', 'Miha Deniša', 'Aleš Ude']",2020-04-02T08:26:23Z,http://arxiv.org/abs/2004.00865v1,"Sustainability, Industry & Robotics",Robotics and Automation,low batch productions were dismissed as potential end-users of automation technologies . high cost of setup and inflexibility of classical automation meant low
Holistic Construction Automation with Modular Robots: From High-Level   Task Specification to Execution,"In situ robotic automation in construction is challenging due to constantly changing environments, a shortage of robotic experts, and a lack of standardized frameworks bridging robotics and construction practices. This work proposes a holistic framework for construction task specification, optimization of robot morphology, and mission execution using a mobile modular reconfigurable robot. Users can specify and monitor the desired robot behavior through a graphical interface. In contrast to existing, monolithic solutions, we automatically identify a new task-tailored robot for every task by integrating \acf{bim}. Our framework leverages modular robot components that enable the fast adaption of robot hardware to the specific demands of the construction task. Other than previous works on modular robot optimization, we consider multiple competing objectives, which allow us to explicitly model the challenges of real-world transfer, such as calibration errors. We demonstrate our framework in simulation by optimizing robots for drilling and spray painting. Finally, experimental validation demonstrates that our approach robustly enables the autonomous execution of robotic drilling.","['Jonathan Külz', 'Michael Terzer', 'Marco Magri', 'Andrea Giusti', 'Matthias Althoff']",2024-12-30T11:11:13Z,http://arxiv.org/abs/2412.20867v2,"Sustainability, Industry & Robotics",Robotics and Automation,this work proposes a holistic framework for construction task specification . users can specify and monitor the desired robot behavior through a graphical interface 
What is Business Process Automation Anyway?,"Many organizations strive to increase the level of automation in their business processes. While automation historically was mainly concerned with automating physical labor, current automation efforts mostly focus on automation in a digital manner, thus targeting work that is related to the interaction between humans and computers. This type of automation, commonly referred to as business process automation, has many facets. Yet, academic literature mainly focuses on Robotic Process Automation, a specific automation capability. Recognizing that leading vendors offer automation capabilities going way beyond that, we use this paper to develop a detailed understanding of business process automation in industry. To this end, we conduct a structured market analysis of the 18 predominant vendors of business process automation solutions as identified by Gartner. As a result, we provide a comprehensive overview of the business process automation capabilities currently offered by industrial vendors. We show which types and facets of automation exist and which aspects represent promising directions for the future.","['Hoang Vu', 'Henrik Leopold', 'Han van der Aa']",2025-03-24T09:21:07Z,http://arxiv.org/abs/2506.10991v1,"Sustainability, Industry & Robotics",Robotics and Automation,this paper provides a comprehensive overview of business process automation in industry . it shows which types and facets of automation exist and which aspects represent promising
"Robot Design: Formalisms, Representations, and the Role of the Designer","The objective of this paper is to distill the following essential idea from the RSS 2016 Workshop on Minimality and Design Automation and the RSS 2017 Workshop on Minimality and Trade-offs in Automated Robot Design:   The information abstractions popular within robotics, designed as they were to address insulated sub-problems, are currently inadequate for design automation.   This paper's first aim is to draw together multiple threads---specifically those of formalization, minimality, automation, and integration---and to argue that robot design questions involve some of the most interesting and fundamental challenges for the discipline. While most efforts in automating robot design have focused on optimization of hardware, robot design is also inextricably linked to the design of the internal state of the robot, how that internal state interacts with sensors and actuators, and how task specifications are designed within this context. Focusing attention on those considerations is worthwhile for the study of robot design because they are currently in a critical intellectual sweet spot, being out of reach technically, but only just.   The second ingredient of this paper forms a roadmap. It emphasizes two aspects: (1) the role of models in robot design, a reprise of the old chestnut about representation in robotics (namely, that ""the world is its own best model""); (2) a consideration of the human-element within the envisioned scheme.","['Alexandra Q. Nilles', 'Dylan A. Shell', ""Jason M. O'Kane""]",2018-06-13T17:32:34Z,http://arxiv.org/abs/1806.05157v1,"Sustainability, Industry & Robotics",Robotics and Automation,the aim of this paper is to distill the essential idea from the RSS 2016 Workshop on Minimality and Design Automation . it argues that robot
Intuitive Robot Integration via Virtual Reality Workspaces,"As robots become increasingly prominent in diverse industrial settings, the desire for an accessible and reliable system has correspondingly increased. Yet, the task of meaningfully assessing the feasibility of introducing a new robotic component, or adding more robots into an existing infrastructure, remains a challenge. This is due to both the logistics of acquiring a robot and the need for expert knowledge in setting it up. In this paper, we address these concerns by developing a purely virtual simulation of a robotic system. Our proposed framework enables natural human-robot interaction through a visually immersive representation of the workspace. The main advantages of our approach are the following: (i) independence from a physical system, (ii) flexibility in defining the workspace and robotic tasks, and (iii) an intuitive interaction between the operator and the simulated environment. Not only does our system provide an enhanced understanding of 3D space to the operator, but it also encourages a hands-on way to perform robot programming. We evaluate the effectiveness of our method in applying novel automation assignments by training a robot in virtual reality and then executing the task on a real robot.","['Minh Q. Tram', 'Joseph M. Cloud', 'William J. Beksi']",2023-05-25T02:06:24Z,http://arxiv.org/abs/2305.15657v1,"Sustainability, Industry & Robotics",Robotics and Automation,a purely virtual simulation of a robotic system is developed . it enables natural human-robot interaction through a visually immersive representation
Customizing Textile and Tactile Skins for Interactive Industrial Robots,"Tactile skins made from textiles enhance robot-human interaction by localizing contact points and measuring contact forces. This paper presents a solution for rapidly fabricating, calibrating, and deploying these skins on industrial robot arms. The novel automated skin calibration procedure maps skin locations to robot geometry and calibrates contact force. Through experiments on a FANUC LR Mate 200id/7L industrial robot, we demonstrate that tactile skins made from textiles can be effectively used for human-robot interaction in industrial environments, and can provide unique opportunities in robot control and learning, making them a promising technology for enhancing robot perception and interaction.","['Bo Ying Su', 'Zhongqi Wei', 'James McCann', 'Wenzhen Yuan', 'Changliu Liu']",2023-08-06T09:48:19Z,http://arxiv.org/abs/2308.03072v1,"Sustainability, Industry & Robotics",Robotics and Automation,"this paper presents a solution for rapidly fabricating, calibrating, and deploying these skins on industrial robot arms . the novel automated skin"
LLMs can generate robotic scripts from goal-oriented instructions in   biological laboratory automation,"The use of laboratory automation by all researchers may substantially accelerate scientific activities by humans, including those in the life sciences. However, computer programs to operate robots should be written to implement laboratory automation, which requires technical knowledge and skills that may not be part of a researcher's training or expertise. In the last few years, there has been remarkable development in large language models (LLMs) such as GPT-4, which can generate computer codes based on natural language instructions. In this study, we used LLMs, including GPT-4, to generate scripts for robot operations in biological experiments based on ambiguous instructions. GPT-4 successfully generates scripts for OT-2, an automated liquid-handling robot, from simple instructions in natural language without specifying the robotic actions. Conventionally, translating the nuances of biological experiments into low-level robot actions requires researchers to understand both biology and robotics, imagine robot actions, and write robotic scripts. Our results showed that GPT-4 can connect the context of biological experiments with robot operation through simple prompts with expert-level contextual understanding and inherent knowledge. Replacing robot script programming, which is a tedious task for biological researchers, with natural-language LLM instructions that do not consider robot behavior significantly increases the number of researchers who can benefit from automating biological experiments.","['Takashi Inagaki', 'Akari Kato', 'Koichi Takahashi', 'Haruka Ozaki', 'Genki N. Kanda']",2023-04-18T09:15:37Z,http://arxiv.org/abs/2304.10267v1,"Sustainability, Industry & Robotics",Robotics and Automation,computer programs to operate robots should be written to implement laboratory automation . large language models (LLMs) such as GPT-4 can generate computer codes based on natural language instructions . authors: replacing robot script programming with natural-language LLMs increases number of researchers .
Towards Probabilistic Planning of Explanations for Robot Navigation,"In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of decisions of the robot and its actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.","['Amar Halilovic', 'Senka Krivic']",2024-10-26T09:52:14Z,http://arxiv.org/abs/2411.05022v1,"Sustainability, Industry & Robotics",Robotics and Automation,this paper introduces a novel approach that integrates user-centered design principles . it aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs .
Abstract Hardware Grounding towards the Automated Design of Automation   Systems,"Crafting automation systems tailored for specific domains requires aligning the space of human experts' semantics with the space of robot executable actions, and scheduling the required resources and system layout accordingly. Regrettably, there are three major gaps, fine-grained domain-specific knowledge injection, heterogeneity between human knowledge and robot instructions, and diversity of users' preferences, resulting automation system design a case-by-case and labour-intensive effort, thus hindering the democratization of automation. We refer to this challenging alignment as the abstract hardware grounding problem, where we firstly regard the procedural operations in humans' semantics space as the abstraction of hardware requirements, then we ground such abstractions to instantiated hardware devices, subject to constraints and preferences in the real world -- optimizing this problem is essentially standardizing and automating the design of automation systems. On this basis, we develop an automated design framework in a hybrid data-driven and principle-derived fashion. Results on designing self-driving laboratories for enhancing experiment-driven scientific discovery suggest our framework's potential to produce compact systems that fully satisfy domain-specific and user-customized requirements with no redundancy.","['Yu-Zhe Shi', 'Qiao Xu', 'Fanxu Meng', 'Lecheng Ruan', 'Qining Wang']",2024-10-08T03:23:37Z,http://arxiv.org/abs/2410.05663v1,"Sustainability, Industry & Robotics",Robotics and Automation,crafting automation systems tailored for specific domains requires aligning semantics with robot actions . three major gaps make design a case-by-case and labour-intensive effort . authors develop automated design framework in a hybrid data-driven and principle-derived fashion .
GLSO: Grammar-guided Latent Space Optimization for Sample-efficient   Robot Design Automation,"Robots have been used in all sorts of automation, and yet the design of robots remains mainly a manual task. We seek to provide design tools to automate the design of robots themselves. An important challenge in robot design automation is the large and complex design search space which grows exponentially with the number of components, making optimization difficult and sample inefficient. In this work, we present Grammar-guided Latent Space Optimization (GLSO), a framework that transforms design automation into a low-dimensional continuous optimization problem by training a graph variational autoencoder (VAE) to learn a mapping between the graph-structured design space and a continuous latent space. This transformation allows optimization to be conducted in a continuous latent space, where sample efficiency can be significantly boosted by applying algorithms such as Bayesian Optimization. GLSO guides training of the VAE using graph grammar rules and robot world space features, such that the learned latent space focus on valid robots and is easier for the optimization algorithm to explore. Importantly, the trained VAE can be reused to search for designs specialized to multiple different tasks without retraining. We evaluate GLSO by designing robots for a set of locomotion tasks in simulation, and demonstrate that our method outperforms related state-of-the-art robot design automation methods.","['Jiaheng Hu', 'Julian Whiman', 'Howie Choset']",2022-09-23T17:48:24Z,http://arxiv.org/abs/2209.11748v1,"Sustainability, Industry & Robotics",Robotics and Automation,grammar-guided latent space optimization (GLSO) is a framework for design automation . it trains a graph variational autoencoder (VAE) to learn a mapping between design space . the trained VAE can be reused to search for designs specialized to multiple tasks .
Prescriptive maintenance with causal machine learning,"Machine maintenance is a challenging operational problem, where the goal is to plan sufficient preventive maintenance to avoid machine failures and overhauls. Maintenance is often imperfect in reality and does not make the asset as good as new. Although a variety of imperfect maintenance policies have been proposed in the literature, these rely on strong assumptions regarding the effect of maintenance on the machine's condition, assuming the effect is (1) deterministic or governed by a known probability distribution, and (2) machine-independent. This work proposes to relax both assumptions by learning the effect of maintenance conditional on a machine's characteristics from observational data on similar machines using existing methodologies for causal inference. By predicting the maintenance effect, we can estimate the number of overhauls and failures for different levels of maintenance and, consequently, optimize the preventive maintenance frequency to minimize the total estimated cost. We validate our proposed approach using real-life data on more than 4,000 maintenance contracts from an industrial partner. Empirical results show that our novel, causal approach accurately predicts the maintenance effect and results in individualized maintenance schedules that are more accurate and cost-effective than supervised or non-individualized approaches.","['Toon Vanderschueren', 'Robert Boute', 'Tim Verdonck', 'Bart Baesens', 'Wouter Verbeke']",2022-06-03T13:35:57Z,http://arxiv.org/abs/2206.01562v1,"Sustainability, Industry & Robotics",Predictive Maintenance,machine maintenance is a challenging operational problem . the goal is to plan sufficient preventive maintenance to avoid machine failures . this work proposes to relax these assumptions by learning the effect of maintenance .
An Economic Perspective on Predictive Maintenance of Filtration Units,"This paper provides an economic perspective on the predictive maintenance of filtration units. The rise of predictive maintenance is possible due to the growing trend of industry 4.0 and the availability of inexpensive sensors. However, the adoption rate for predictive maintenance by companies remains low. The majority of companies are sticking to corrective and preventive maintenance. This is not due to a lack of information on the technical implementation of predictive maintenance, with an abundance of research papers on state-of-the-art machine learning algorithms that can be used effectively. The main issue is that most upper management has not yet been fully convinced of the idea of predictive maintenance. The economic value of the implementation has to be linked to the predictive maintenance program for better justification by the management. In this study, three machine learning models were trained to demonstrate the economic value of predictive maintenance. Data was collected from a testbed located at the Singapore University of Technology and Design. The testbed closely resembles a real-world water treatment plant. A cost-benefit analysis coupled with Monte Carlo simulation was proposed. It provided a structured approach to document potential costs and savings by implementing a predictive maintenance program. The simulation incorporated real-world risk into a financial model. Financial figures were adapted from CITIC Envirotech Ltd, a leading membrane-based integrated environmental solutions provider. Two scenarios were used to elaborate on the economic values of predictive maintenance. Overall, this study seeks to bridge the gap between technical and business domains of predictive maintenance.","['Denis Tan Jing Yu', 'Adrian Law Wing-Keung']",2020-08-25T14:43:30Z,http://arxiv.org/abs/2008.11070v1,"Sustainability, Industry & Robotics",Predictive Maintenance,paper provides an economic perspective on the predictive maintenance of filtration units . the adoption rate for predictive maintenance by companies remains low . most upper management has not yet been fully convinced of the idea of predictive maintenance .
Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize   Framework for Predictive Maintenance,"Recent research increasingly integrates machine learning (ML) into predictive maintenance (PdM) to reduce operational and maintenance costs in data-rich operational settings. However, uncertainty due to model misspecification continues to limit widespread industrial adoption. This paper proposes a PdM framework in which sensor-driven prognostics inform decision-making under economic trade-offs within a finite decision space. We investigate two key questions: (1) Does higher predictive accuracy necessarily lead to better maintenance decisions? (2) If not, how can the impact of prediction errors on downstream maintenance decisions be mitigated? We first demonstrate that in the traditional estimate-then-optimize (ETO) framework, errors in probabilistic prediction can result in inconsistent and suboptimal maintenance decisions. To address this, we propose an integrated estimate-optimize (IEO) framework that jointly tunes predictive models while directly optimizing for maintenance outcomes. We establish theoretical finite-sample guarantees on decision consistency under standard assumptions. Specifically, we develop a stochastic perturbation gradient descent algorithm suitable for small run-to-failure datasets. Empirical evaluations on a turbofan maintenance case study show that the IEO framework reduces average maintenance regret up to 22% compared to ETO. This study provides a principled approach to managing prediction errors in data-driven PdM. By aligning prognostic model training with maintenance objectives, the IEO framework improves robustness under model misspecification and improves decision quality. The improvement is particularly pronounced when the decision-making policy is misaligned with the decision-maker's target. These findings support more reliable maintenance planning in uncertain operational environments.","['Zhuojun Xie', 'Adam Abdin', 'Yiping Fang']",2025-06-24T15:10:15Z,http://arxiv.org/abs/2506.19698v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"paper proposes a framework in which sensor-driven prognostics inform decision-making . it investigates two key questions: (1) Does higher predictive accuracy lead to better maintenance decisions? if not, how can the impact of prediction errors on downstream maintenance decisions be mitigated?"
Cost-Sensitive Learning for Predictive Maintenance,"In predictive maintenance, model performance is usually assessed by means of precision, recall, and F1-score. However, employing the model with best performance, e.g. highest F1-score, does not necessarily result in minimum maintenance cost, but can instead lead to additional expenses. Thus, we propose to perform model selection based on the economic costs associated with the particular maintenance application. We show that cost-sensitive learning for predictive maintenance can result in significant cost reduction and fault tolerant policies, since it allows to incorporate various business constraints and requirements.","['Stephan Spiegel', 'Fabian Mueller', 'Dorothea Weismann', 'John Bird']",2018-09-28T12:08:51Z,http://arxiv.org/abs/1809.10979v1,"Sustainability, Industry & Robotics",Predictive Maintenance,employing the model with the best performance does not necessarily result in minimum maintenance cost . e.g. highest F1-score can lead to additional expenses . we show that cost-sensitive learning can result in significant cost reduction .
"A Survey of Predictive Maintenance: Systems, Purposes and Approaches","This paper highlights the importance of maintenance techniques in the coming industrial revolution, reviews the evolution of maintenance techniques, and presents a comprehensive literature review on the latest advancement of maintenance techniques, i.e., Predictive Maintenance (PdM), with emphasis on system architectures, optimization objectives, and optimization methods. In industry, any outages and unplanned downtime of machines or systems would degrade or interrupt a company's core business, potentially resulting in significant penalties and immeasurable reputation and economic loss. Existing traditional maintenance approaches, such as Reactive Maintenance (RM) and Preventive Maintenance (PM), suffer from high prevent and repair costs, inadequate or inaccurate mathematical degradation processes, and manual feature extraction. The incoming fourth industrial revolution is also demanding for a new maintenance paradigm to reduce the maintenance cost and downtime, and increase system availability and reliability. Predictive Maintenance (PdM) is envisioned the solution. In this survey, we first provide a high-level view of the PdM system architectures including PdM 4.0, Open System Architecture for Condition Based Monitoring (OSA-CBM), and cloud-enhanced PdM system. Then, we review the specific optimization objectives, which mainly comprise cost minimization, availability/reliability maximization, and multi-objective optimization. Furthermore, we present the optimization methods to achieve the aforementioned objectives, which include traditional Machine Learning (ML) based and Deep Learning (DL) based approaches. Finally, we highlight the future research directions that are critical to promote the application of DL techniques in the context of PdM.","['Tianwen Zhu', 'Yongyi Ran', 'Xin Zhou', 'Yonggang Wen']",2019-12-12T20:11:51Z,http://arxiv.org/abs/1912.07383v2,"Sustainability, Industry & Robotics",Predictive Maintenance,this paper reviews the evolution of maintenance techniques . any outages and unplanned downtime of machines or systems would degrade or interrupt a company's core business . the incoming fourth industrial revolution is demanding for a new maintenance paradigm .
A Large-Scale Annotated Multivariate Time Series Aviation Maintenance   Dataset from the NGAFID,"This paper presents the largest publicly available, non-simulated, fleet-wide aircraft flight recording and maintenance log data for use in predicting part failure and maintenance need. We present 31,177 hours of flight data across 28,935 flights, which occur relative to 2,111 unplanned maintenance events clustered into 36 types of maintenance issues. Flights are annotated as before or after maintenance, with some flights occurring on the day of maintenance. Collecting data to evaluate predictive maintenance systems is challenging because it is difficult, dangerous, and unethical to generate data from compromised aircraft. To overcome this, we use the National General Aviation Flight Information Database (NGAFID), which contains flights recorded during regular operation of aircraft, and maintenance logs to construct a part failure dataset. We use a novel framing of Remaining Useful Life (RUL) prediction and consider the probability that the RUL of a part is greater than 2 days. Unlike previous datasets generated with simulations or in laboratory settings, the NGAFID Aviation Maintenance Dataset contains real flight records and maintenance logs from different seasons, weather conditions, pilots, and flight patterns. Additionally, we provide Python code to easily download the dataset and a Colab environment to reproduce our benchmarks on three different models. Our dataset presents a difficult challenge for machine learning researchers and a valuable opportunity to test and develop prognostic health management methods","['Hong Yang', 'Travis Desell']",2022-10-13T19:43:02Z,http://arxiv.org/abs/2210.07317v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"the paper presents the largest publicly available, non-simulated, fleet-wide aircraft flight data . it is difficult, dangerous, and unethical to generate data from compromised aircraft . the dataset presents a difficult challenge for machine learning researchers ."
Cycling into the workshop: predictive maintenance for Barcelona's   bike-sharing system,"Bike-sharing systems have emerged as a significant element of urban mobility, providing an environmentally friendly transportation alternative. With the increasing integration of electric bikes alongside mechanical bikes, it is crucial to illuminate distinct usage patterns and their impact on maintenance. Accordingly, this research aims to develop a comprehensive understanding of mobility dynamics, distinguishing between different mobility modes, and introducing a novel predictive maintenance system tailored for bikes. By utilising a combination of trip information and maintenance data from Barcelona's bike-sharing system, Bicing, this study conducts an extensive analysis of mobility patterns and their relationship to failures of bike components. To accurately predict maintenance needs for essential bike parts, this research delves into various mobility metrics and applies statistical and machine learning survival models, including deep learning models. Due to their complexity, and with the objective of bolstering confidence in the system's predictions, interpretability techniques explain the main predictors of maintenance needs. The analysis reveals marked differences in the usage patterns of mechanical bikes and electric bikes, with a growing user preference for the latter despite their extra costs. These differences in mobility were found to have a considerable impact on the maintenance needs within the bike-sharing system. Moreover, the predictive maintenance models proved effective in forecasting these maintenance needs, capable of operating across an entire bike fleet. Despite challenges such as approximated bike usage metrics and data imbalances, the study successfully showcases the feasibility of an accurate predictive maintenance system capable of improving operational costs, bike availability, and security.","['Jordi Grau-Escolano', 'Aleix Bassolas', 'Julian Vicens']",2024-04-26T07:46:25Z,http://arxiv.org/abs/2404.17217v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"this research aims to develop a comprehensive understanding of mobility dynamics . it uses trip information and maintenance data from Barcelona's bike-sharing system, Bicing . to accurately predict maintenance needs for essential bike parts, this research delves into various mobility metrics ."
A Decision Making Framework for Recommended Maintenance of Road Segments,"Due to limited budgets allocated for road maintenance projects in various countries, road management departments face difficulties in making scientific maintenance decisions. This paper aims to provide road management departments with more scientific decision tools and evidence. The framework proposed in this paper mainly has the following four innovative points: 1) Predicting pavement performance deterioration levels of road sections as decision basis rather than accurately predicting specific indicator values; 2) Determining maintenance route priorities based on multiple factors; 3) Making maintenance plan decisions by establishing deep reinforcement learning models to formulate predictive strategies based on past maintenance performance evaluations, while considering both technical and management indicators; 4) Determining repair section priorities according to actual and suggested repair effects. By resolving these four issues, the framework can make intelligent decisions regarding optimal maintenance plans and sections, taking into account limited funds and historical maintenance management experiences.","['Haoyu Sun', 'Yan Yan']",2023-07-19T15:55:25Z,http://arxiv.org/abs/2307.10085v3,"Sustainability, Industry & Robotics",Predictive Maintenance,this paper aims to provide road management departments with more scientific maintenance decisions . it mainly has the following four points: 1) Predicting pavement performance deterioration levels of road sections . 2) Determining maintenance route priorities based on multiple factors .
Sensor-Driven Predictive Vehicle Maintenance and Routing Problem with   Time Windows,"Advancements in sensor technology offer significant insights into vehicle conditions, unlocking new venues to enhance fleet operations. While current vehicle health management models provide accurate predictions of vehicle failures, they often fail to integrate these forecasts into operational decision-making, limiting their practical impact. This paper addresses this gap by incorporating sensor-driven failure predictions into a single-vehicle routing problem with time windows. A maintenance cost function is introduced to balance two critical trade-offs: premature maintenance, which leads to underutilization of remaining useful life, and delayed maintenance, which increases the likelihood of breakdowns. Routing problems with time windows are inherently challenging, and integrating maintenance considerations adds significantly to its computational complexity. To address this, we develop a new solution method, called the Iterative Alignment Method (IAM), building on the structural properties of the problem. IAM generates high-quality solutions even in large-size instances where Gurobi cannot find any solutions. Moreover, compared to the traditional periodic maintenance strategy, our sensor-driven approach to maintenance decisions shows improvements in operational and maintenance costs as well as in overall vehicle reliability.","['Iman Kazemian', 'Bahar Cavdar', 'Murat Yildirim']",2024-12-05T17:09:53Z,http://arxiv.org/abs/2412.04350v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"sensor-driven failure predictions are incorporated into a single-vehicle routing problem . premature maintenance leads to underutilization of remaining useful life, and delayed maintenance increases likelihood of breakdowns . iterative alignment method generates high-quality solutions even in large-size instances ."
The Co-Evolution of Test Maintenance and Code Maintenance through the   lens of Fine-Grained Semantic Changes,"Automatic testing is a widely adopted technique for improving software quality. Software developers add, remove and update test methods and test classes as part of the software development process as well as during the evolution phase, following the initial release. In this work we conduct a large scale study of 61 popular open source projects and report the relationships we have established between test maintenance, production code maintenance, and semantic changes (e.g, statement added, method removed, etc.). performed in developers' commits.   We build predictive models, and show that the number of tests in a software project can be well predicted by employing code maintenance profiles (i.e., how many commits were performed in each of the maintenance activities: corrective, perfective, adaptive). Our findings also reveal that more often than not, developers perform code fixes without performing complementary test maintenance in the same commit (e.g., update an existing test or add a new one). When developers do perform test maintenance, it is likely to be affected by the semantic changes they perform as part of their commit.   Our work is based on studying 61 popular open source projects, comprised of over 240,000 commits consisting of over 16,000,000 semantic change type instances, performed by over 4,000 software engineers.","['Stanislav Levin', 'Amiram Yehudai']",2017-09-26T14:13:33Z,http://arxiv.org/abs/1709.09029v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"automatic testing is a widely adopted technique for improving software quality . software developers add, remove and update test methods and test classes . a large scale study of 61 popular open source projects is conducted ."
Predictive Maintenance using Machine Learning,"Predictive maintenance (PdM) is a concept, which is implemented to effectively manage maintenance plans of the assets by predicting their failures with data driven techniques. In these scenarios, data is collected over a certain period of time to monitor the state of equipment. The objective is to find some correlations and patterns that can help predict and ultimately prevent failures. Equipment in manufacturing industry are often utilized without a planned maintenance approach. Such practise frequently results in unexpected downtime, owing to certain unexpected failures. In scheduled maintenance, the condition of the manufacturing equipment is checked after fixed time interval and if any fault occurs, the component is replaced to avoid unexpected equipment stoppages. On the flip side, this leads to increase in time for which machine is non-functioning and cost of carrying out the maintenance. The emergence of Industry 4.0 and smart systems have led to increasing emphasis on predictive maintenance (PdM) strategies that can reduce the cost of downtime and increase the availability (utilization rate) of manufacturing equipment. PdM also has the potential to bring about new sustainable practices in manufacturing by fully utilizing the useful lives of components.","['Archit P. Kane', 'Ashutosh S. Kore', 'Advait N. Khandale', 'Sarish S. Nigade', 'Pranjali P. Joshi']",2022-05-19T09:05:37Z,http://arxiv.org/abs/2205.09402v1,"Sustainability, Industry & Robotics",Predictive Maintenance,the emergence of Industry 4.0 and smart systems have led to increasing emphasis on predictive maintenance (PdM) strategies . the objective is to find some correlations and patterns that can help predict and ultimately prevent failures .
Predicting Future Machine Failure from Machine State Using Logistic   Regression,Accurately predicting machine failures in advance can decrease maintenance cost and help allocate maintenance resources more efficiently. Logistic regression was applied to predict machine state 24 hours in the future given the current machine state.,"['Matthew Battifarano', 'David DeSmet', 'Achyuth Madabhushi', 'Parth Nabar']",2018-04-17T03:03:06Z,http://arxiv.org/abs/1804.06022v1,"Sustainability, Industry & Robotics",Predictive Maintenance,logistic regression was applied to predict machine state 24 hours in the future given the current machine state . predicting machine failures in advance can decrease maintenance cost .
A Multi-state Markov Model to Infer the Latent Deterioration Process   From the Maintenance Effect on Reliability Engineering of Ships,"Maintenance optimization of naval ship equipment is crucial in terms of national defense. However, the mixed effect of the maintenance and the pure deterioration processes in the observed data hinders an exact comparison between candidate maintenance policies. That is, the observed data-annual failure counts of naval ships reflect counteracting actions between the maintenance and deterioration. The inference of the latent deteriorating process is needed in advance for choosing an optimal maintenance policy to be carried out. This study proposes a new framework for the separation of the true deterioration effect by predicting it from the current maintenance effect through the multi-state Markov model. Using an annual engine failure count of 99 ships in the Korean navy, we construct the framework consisting of imputation, transition matrix design, optimization, and validation. The hierarchical Gaussian process model is used for the imputation and the three-state Markov model is applied for the estimation of parameters in the deterioration and maintenance effect. To consider the natural (deterioration) and artificial (maintenance) effect respectively, the Bayesian HMM model with a categorical distribution is employed. Computational experiments under multiple settings showed the robustness of the estimated parameters, as well as an accurate recovery of the observed data, thereby confirming the credibility of our model. The framework could further be employed to establish a reliable maintenance system and to reduce an overall maintenance cost.","['Hyunji Moon', 'Jungin Choi', 'Seoyeon Cha']",2021-11-29T07:56:07Z,http://arxiv.org/abs/2111.14368v2,"Sustainability, Industry & Robotics",Predictive Maintenance,the observed data-annual failure counts of naval ships reflect counteracting actions between maintenance and deterioration . the inference of the latent deteriorating process is needed in advance for choosing an optimal maintenance policy . this study proposes a new framework for the separation of the true deteriorations effect .
Data Strategies for Fleetwide Predictive Maintenance,"For predictive maintenance, we examine one of the largest public datasets for machine failures derived along with their corresponding precursors as error rates, historical part replacements, and sensor inputs. To simplify the time and accuracy comparison between 27 different algorithms, we treat the imbalance between normal and failing states with nominal under-sampling. We identify 3 promising regression and discriminant algorithms with both higher accuracy (96%) and twenty-fold faster execution times than previous work. Because predictive maintenance success hinges on input features prior to prediction, we provide a methodology to rank-order feature importance and show that for this dataset, error counts prove more predictive than scheduled maintenance might imply solely based on more traditional factors such as machine age or last replacement times.",['David Noever'],2018-12-11T14:57:57Z,http://arxiv.org/abs/1812.04446v1,"Sustainability, Industry & Robotics",Predictive Maintenance,we examine one of the largest public datasets for machine failures . we treat the imbalance between normal and failing states with nominal under-sampling .
Realtime Predictive Maintenance with Lambda Architecture,"Recently, IoT technologies have been progressed and applications of maintenance area are expected. However, IoT maintenance applications are not spread in Japan yet because of insufficient analysis of real time situation, high cost to collect sensing data and to configure failure detection rules. In this paper, using lambda architecture concept, we propose a maintenance platform in which edge nodes analyze sensing data, detect anomaly, extract a new detection rule in real time and a cloud orders maintenance automatically, also analyzes whole data collected by batch process in detail, updates learning model of edge nodes to improve analysis accuracy.","['Yoji Yamato', 'Hiroki Kumazaki', 'Yoshifumi Fukumoto']",2016-12-08T13:42:49Z,http://arxiv.org/abs/1612.02640v1,"Sustainability, Industry & Robotics",Predictive Maintenance,"in this paper, we propose a maintenance platform in which edge nodes analyze sensing data, detect anomaly, extract a new detection rule in real time and a cloud orders maintenance automatically ."
An Optimized and Safety-aware Maintenance Framework: A Case Study on   Aircraft Engine,"The COVID-19 pandemic has recently exacerbated the fierce competition in the transportation businesses. The airline industry took one of the biggest hits as the closure of international borders forced aircraft operators to suspend their international routes, keeping aircraft on the ground without generating revenues while at the same time still requiring adequate maintenance. To maintain their operational sustainability, finding a good balance between cost reductions measure and safety standards fulfillment, including its maintenance procedure, becomes critical. This paper proposes an AI-assisted predictive maintenance scheme that synthesizes prognostics modeling and simulation-based optimization to help airlines decide their optimal engine maintenance approach. The proposed method enables airlines to utilize their diagnostics measurements and operational settings to design a more customized maintenance strategy that takes engine operations conditions into account. Our numerical experiments on the proposed approach resulted in significant cost savings without compromising the safety standards. The experiments also show that maintenance strategies tailored to the failure mode and operational settings (that our framework enables) yield 13% more cost savings than generic optimal maintenance strategies. The generality of our proposed framework allows the extension to other intelligent, safety-critical transportation systems.","['Muhammad Ziyad', 'Kenrick Tjandra', 'Zulvah', 'Mushonnifun Faiz Sugihartanto', 'Mansur Arief']",2022-09-06T17:47:39Z,http://arxiv.org/abs/2209.02678v1,"Sustainability, Industry & Robotics",Predictive Maintenance,the COVID-19 pandemic has exacerbated the fierce competition in the transportation industries . the closure of international borders forced aircraft operators to suspend their international routes . this paper proposes an AI-assisted predictive maintenance scheme .
"Explainable Predictive Maintenance: A Survey of Current Methods,   Challenges and Opportunities","Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM.","['Logan Cummins', 'Alex Sommers', 'Somayeh Bakhtiari Ramezani', 'Sudip Mittal', 'Joseph Jabour', 'Maria Seale', 'Shahram Rahimi']",2024-01-15T18:06:59Z,http://arxiv.org/abs/2401.07871v1,"Sustainability, Industry & Robotics",Predictive Maintenance,predictive maintenance is a collection of techniques that aims to prolong the life of a mechanical system . the methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep . this survey discusses the current methods of XAI as applied to predictive maintenance .
Acela: Predictable Datacenter-level Maintenance Job Scheduling,"Datacenter operators ensure fair and regular server maintenance by using automated processes to schedule maintenance jobs to complete within a strict time budget. Automating this scheduling problem is challenging because maintenance job duration varies based on both job type and hardware. While it is tempting to use prior machine learning techniques for predicting job duration, we find that the structure of the maintenance job scheduling problem creates a unique challenge. In particular, we show that prior machine learning methods that produce the lowest error predictions do not produce the best scheduling outcomes due to asymmetric costs. Specifically, underpredicting maintenance job duration has results in more servers being taken offline and longer server downtime than overpredicting maintenance job duration. The system cost of underprediction is much larger than that of overprediction.   We present Acela, a machine learning system for predicting maintenance job duration, which uses quantile regression to bias duration predictions toward overprediction. We integrate Acela into a maintenance job scheduler and evaluate it on datasets from large-scale, production datacenters. Compared to machine learning based predictors from prior work, Acela reduces the number of servers that are taken offline by 1.87-4.28X, and reduces the server offline time by 1.40-2.80X.","['Yi Ding', 'Aijia Gao', 'Thibaud Ryden', 'Kaushik Mitra', 'Sukumar Kalmanje', 'Yanai Golany', 'Michael Carbin', 'Henry Hoffmann']",2022-12-10T00:22:49Z,http://arxiv.org/abs/2212.05155v1,"Sustainability, Industry & Robotics",Predictive Maintenance,the structure of the maintenance job scheduling problem creates a unique challenge . underpredicting maintenance job duration has results in more servers being taken offline . the system cost of underprediction is much larger than that of overprediction .
Predictive Maintenance of Armoured Vehicles using Machine Learning   Approaches,"Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the suggested ensemble, this study highlights the potential of machine learning-based predictive maintenance solutions.","['Prajit Sengupta', 'Anant Mehta', 'Prashant Singh Rana']",2023-07-26T18:50:32Z,http://arxiv.org/abs/2307.14453v1,"Sustainability, Industry & Robotics",Predictive Maintenance,this study proposes a predictive maintenance-based ensemble system . the proposed system achieves an accuracy of 98.93% and precision of 99.80% .
VisioRed: A Visualisation Tool for Interpretable Predictive Maintenance,"The use of machine learning rapidly increases in high-risk scenarios where decisions are required, for example in healthcare or industrial monitoring equipment. In crucial situations, a model that can offer meaningful explanations of its decision-making is essential. In industrial facilities, the equipment's well-timed maintenance is vital to ensure continuous operation to prevent money loss. Using machine learning, predictive and prescriptive maintenance attempt to anticipate and prevent eventual system failures. This paper introduces a visualisation tool incorporating interpretations to display information derived from predictive maintenance models, trained on time-series data.","['Spyridon Paraschos', 'Ioannis Mollas', 'Nick Bassiliades', 'Grigorios Tsoumakas']",2021-03-31T11:35:51Z,http://arxiv.org/abs/2103.17003v2,"Sustainability, Industry & Robotics",Predictive Maintenance,"use of machine learning increases in high-risk scenarios where decisions are required . a model that can offer meaningful explanations of its decision-making is essential . in industrial facilities, the equipment's well-timed maintenance is vital to ensure continuous operation ."
Time-Sensitive Networking (TSN) for Industrial Automation: Current   Advances and Future Directions,"With the introduction of Cyber-Physical Systems (CPS) and Internet of Things (IoT) technologies, the automation industry is undergoing significant changes, particularly in improving production efficiency and reducing maintenance costs. Industrial automation applications often need to transmit time- and safety-critical data to closely monitor and control industrial processes. Several Ethernet-based fieldbus solutions, such as PROFINET IRT, EtherNet/IP, and EtherCAT, are widely used to ensure real-time communications in industrial automation systems. These solutions, however, commonly incorporate additional mechanisms to provide latency guarantees, making their interoperability a grand challenge. The IEEE 802.1 Time Sensitive Networking (TSN) task group was formed to enhance and optimize IEEE 802.1 network standards, particularly for Ethernet-based networks. These solutions can be evolved and adapted for cross-industry scenarios, such as large-scale distributed industrial plants requiring multiple industrial entities to work collaboratively. This paper provides a comprehensive review of current advances in TSN standards for industrial automation. It presents the state-of-the-art IEEE TSN standards and discusses the opportunities and challenges of integrating TSN into the automation industry. Some promising research directions are also highlighted for applying TSN technologies to industrial automation applications.","['Tianyu Zhang', 'Gang Wang', 'Chuanyu Xue', 'Jiachen Wang', 'Mark Nixon', 'Song Han']",2023-06-06T14:03:00Z,http://arxiv.org/abs/2306.03691v4,"Sustainability, Industry & Robotics",Industrial Automation,the automation industry is undergoing significant changes with the introduction of cyber-physical systems (CPS) and Internet of Things (IoT) technologies . the IEEE 802.1 Time Sensitive Networking (TSN) task group was formed .
What is Business Process Automation Anyway?,"Many organizations strive to increase the level of automation in their business processes. While automation historically was mainly concerned with automating physical labor, current automation efforts mostly focus on automation in a digital manner, thus targeting work that is related to the interaction between humans and computers. This type of automation, commonly referred to as business process automation, has many facets. Yet, academic literature mainly focuses on Robotic Process Automation, a specific automation capability. Recognizing that leading vendors offer automation capabilities going way beyond that, we use this paper to develop a detailed understanding of business process automation in industry. To this end, we conduct a structured market analysis of the 18 predominant vendors of business process automation solutions as identified by Gartner. As a result, we provide a comprehensive overview of the business process automation capabilities currently offered by industrial vendors. We show which types and facets of automation exist and which aspects represent promising directions for the future.","['Hoang Vu', 'Henrik Leopold', 'Han van der Aa']",2025-03-24T09:21:07Z,http://arxiv.org/abs/2506.10991v1,"Sustainability, Industry & Robotics",Industrial Automation,this paper provides a comprehensive overview of the business process automation capabilities offered by industrial vendors . the paper shows which types and facets of automation exist and which aspects represent promising directions .
Poster: Towards an Automated Security Testing Framework for Industrial   UEs,"With the ongoing adoption of 5G for communication in industrial systems and critical infrastructure, the security of industrial UEs such as 5G-enabled industrial robots becomes an increasingly important topic. Most notably, to meet the stringent security requirements of industrial deployments, industrial UEs not only have to fully comply with the 5G specifications but also implement and use correctly secure communication protocols such as TLS. To ensure the security of industrial UEs, operators of industrial 5G networks rely on security testing before deploying new devices to their production networks. However, currently only isolated tests for individual security aspects of industrial UEs exist, severely hindering comprehensive testing. In this paper, we report on our ongoing efforts to alleviate this situation by creating an automated security testing framework for industrial UEs to comprehensively evaluate their security posture before deployment. With this framework, we aim to provide stakeholders with a fully automated-method to verify that higher-layer security protocols are correctly implemented, while simultaneously ensuring that the UE's protocol stack adheres to 3GPP specifications.","['Sotiris Michaelides', 'Daniel Eguiguren Chavez', 'Martin Henze']",2025-05-22T06:54:38Z,http://arxiv.org/abs/2505.16300v1,"Sustainability, Industry & Robotics",Industrial Automation,"security of industrial UEs such as 5G-enabled industrial robots becomes an increasingly important topic . operators of industrial 5G networks rely on security testing before deploying new devices to production networks . currently only isolated tests for individual security aspects exist, severely hindering comprehensive testing . this paper aims to provide stakeholders with a fully automated-method to verify that higher-layer security protocols are correctly implemented ."
An Example for BeSpaceD and its Use for Decision Support in Industrial   Automation,"We describe our formal methods-based spatial reasoning framework BeSpaceD and its application in decision support for industrial automation. In particular we are supporting analysis and decisions based on formal models for industrial plant and mining operations. BeSpaceD is a framework for deciding geometric and topological properties of spatio-temporal models. We present an example and report on our ongoing experience with applications in different projects around software and cyber-physical systems engineering. The example features abstracted aspects of a production plant model. Using the example we motivate the use of our framework in the context of an existing software platform supporting monitoring, incident handling and maintenance of industrial automation facilities in remote locations.",['Jan Olaf Blech'],2015-12-15T06:10:14Z,http://arxiv.org/abs/1512.04656v1,"Sustainability, Industry & Robotics",Industrial Automation,beSpaceD is a framework for deciding geometric and topological properties of spatio-temporal models . the framework supports analysis and decisions based on formal models for industrial plant and mining operations.
Visoes da Industria 4.0,"Industry is part of an economy that produces highly mechanized and automated material goods. Since the beginning of industrialization, there have been several stages and paradigm shifts that today are ex-post-so-called industrial revolutions: in the field of mechanization (called the 1st industrial revolution), the intensive use of electrical energy (called the 2nd industrial revolution) and widespread digitization (called the 3rd industrial revolution). In this sense, for this future expectation, the term (Industry 4.0) was established for a 4th industrial revolution. Developments especially in Europe, but also in the United States, coined as the Industrial Internet, are often compared with the continuation of disruptive increases in industrial production, such as revolutions initiated by steam, electricity, etc. Aspects of continuous workforce training, and the use of sustainability resources in industrial, economic and general IT governance policies are not widespread and are the main problems and challenges in paradigms in Industry 4.0. Directions for future thematic research that will be covered in this article.","['Wallace Camacho', 'Cristina Dias']",2021-05-07T00:33:46Z,http://arxiv.org/abs/2105.08544v1,"Sustainability, Industry & Robotics",Industrial Automation,industry is part of an economy that produces highly mechanized and automated material goods . there have been several stages and paradigm shifts that today are ex-post-so-called industrial revolutions . the term (Industry 4.0) was established for a 4th industrial revolution .
Towards Automated Acceptance testing for industrial robots,"Industrial robots are important machines applied in numerous modern industries that execute repetitive tasks with high accuracy, replacing or supporting dangerous jobs. In this kind of system, with increased complexity in which cost is related to the time the system keeps working, the system must operate with a minimum number of failures. In other words, a quality aspect important in industry is reliability. We hypothesize that Automated Acceptance Testing improves reliability for industrial robot program. We present the research question, the motivation for this study, our hypothesis and future research efforts.","['Marcela G. dos Santos', 'Fabio Petrillo']",2021-04-22T23:29:06Z,http://arxiv.org/abs/2104.11351v1,"Sustainability, Industry & Robotics",Industrial Automation,"industrial robots are important machines applied in many modern industries . they perform repetitive tasks with high accuracy, replacing or supporting dangerous jobs . a quality aspect important in industry is reliability . we hypothesize that Automated Acceptance Testing improves reliability for robot program ."
Machine learning's own Industrial Revolution,"Machine learning is expected to enable the next Industrial Revolution. However, lacking standardized and automated assembly networks, ML faces significant challenges to meet ever-growing enterprise demands and empower broad industries. In the Perspective, we argue that ML needs to first complete its own Industrial Revolution, elaborate on how to best achieve its goals, and discuss new opportunities to enable rapid translation from ML's innovation frontier to mass production and utilization.","['Yuan Luo', 'Song Han', 'Jingjing Liu']",2023-11-04T00:00:13Z,http://arxiv.org/abs/2311.02278v1,"Sustainability, Industry & Robotics",Industrial Automation,"machine learning is expected to enable the next industrial revolution . lacking standardized and automated assembly networks, ML faces significant challenges . in the Perspective, we argue that ML needs to complete its own Industrial Revolution ."
IASelect: Finding Best-fit Agent Practices in Industrial CPS Using Graph   Databases,"The ongoing fourth Industrial Revolution depends mainly on robust Industrial Cyber-Physical Systems (ICPS). ICPS includes computing (software and hardware) abilities to control complex physical processes in distributed industrial environments. Industrial agents, originating from the well-established multi-agent systems field, provide complex and cooperative control mechanisms at the software level, allowing us to develop larger and more feature-rich ICPS. The IEEE P2660.1 standardisation project, ""Recommended Practices on Industrial Agents: Integration of Software Agents and Low Level Automation Functions"" focuses on identifying Industrial Agent practices that can benefit ICPS systems of the future. A key problem within this project is identifying the best-fit industrial agent practices for a given ICPS. This paper reports on the design and development of a tool to address this challenge. This tool, called IASelect, is built using graph databases and provides the ability to flexibly and visually query a growing repository of industrial agent practices relevant to ICPS. IASelect includes a front-end that allows industry practitioners to interactively identify best-fit practices without having to write manual queries.","['Chandan Sharma', 'Roopak Sinha', 'Paulo Leitao']",2021-08-03T11:10:02Z,http://arxiv.org/abs/2108.01413v1,"Sustainability, Industry & Robotics",Industrial Automation,the fourth industrial revolution depends mainly on robust industrial cyber-physical systems . industrial agents provide complex and cooperative control mechanisms at the software level . a tool to address this challenge is called IASelect .
On a Uniform Causality Model for Industrial Automation,"The increasing complexity of Cyber-Physical Systems (CPS) makes industrial automation challenging. Large amounts of data recorded by sensors need to be processed to adequately perform tasks such as diagnosis in case of fault. A promising approach to deal with this complexity is the concept of causality. However, most research on causality has focused on inferring causal relations between parts of an unknown system. Engineering uses causality in a fundamentally different way: complex systems are constructed by combining components with known, controllable behavior. As CPS are constructed by the second approach, most data-based causality models are not suited for industrial automation. To bridge this gap, a Uniform Causality Model for various application areas of industrial automation is proposed, which will allow better communication and better data usage across disciplines. The resulting model describes the behavior of CPS mathematically and, as the model is evaluated on the unique requirements of the application areas, it is shown that the Uniform Causality Model can work as a basis for the application of new approaches in industrial automation that focus on machine learning.","['Maria Krantz', 'Alexander Windmann', 'Rene Heesch', 'Lukas Moddemann', 'Oliver Niggemann']",2022-09-20T11:23:51Z,http://arxiv.org/abs/2209.09618v1,"Sustainability, Industry & Robotics",Industrial Automation,large amounts of data recorded by sensors make industrial automation challenging . a promising approach to deal with this complexity is the concept of causality . most research on causality has focused on inferring causal relations between parts of an unknown system .
From Mechatronic Components to Industrial Automation Things - An IoT   model for cyber-physical manufacturing systems,"IoT is considered as one of the key enabling technologies for the fourth industrial revolution, that is known as Industry 4.0. In this paper, we consider the mechatronic component as the lowest level in the system composition hierarchy that tightly integrates mechanics with the electronics and software required to convert the mechanics to intelligent (smart) object offering well defined services to its environment. For this mechatronic component to be integrated in the IoT-based industrial automation environment, a software layer is required on top of it to convert its conventional interface to an IoT compliant one. This layer, that we call IoTwrapper, transforms the conventional mechatronic component to an Industrial Automation Thing (IAT). The IAT is the key element of an IoT model specifically developed in the context of this work for the manufacturing domain. The model is compared to existing IoT models and its main differences are discussed. A model-to-model transformer is presented to automatically transform the legacy mechatronic component to an IAT ready to be integrated in the IoT-based industrial automation environment. The UML4IoT profile is used in the form of a Domain Specific Modeling Language to automate this transformation. A prototype implementation of an Industrial Automation Thing using C and the Contiki operating system demonstrates the effectiveness of the proposed approach.","['Kleanthis Thramboulidis', 'Theodoros Foradis']",2016-06-03T14:52:45Z,http://arxiv.org/abs/1606.01120v1,"Sustainability, Industry & Robotics",Industrial Automation,"IoT is considered as one of the key enabling technologies for the fourth industrial revolution, that is known as Industry 4.0 . in this paper, we consider the mechatronic component as the lowest level in the system composition hierarchy that tightly integrates mechanics with electronics . a software layer is required on top of it to convert its conventional interface to an industrial automation Thing (IAT)"
Developing a Safety Management System for the Autonomous Vehicle   Industry,"Safety Management Systems (SMSs) have been used in many safety-critical industries and are now being developed and deployed in the automated driving system (ADS)-equipped vehicle (AV) sector. Industries with decades of SMS deployment have established frameworks tailored to their specific context. Several frameworks for an AV industry SMS have been proposed or are currently under development. These frameworks borrow heavily from the aviation industry although the AV and aviation industries differ in many significant ways. In this context, there is a need to review the approach to develop an SMS that is tailored to the AV industry, building on generalized lessons learned from other safety-sensitive industries. A harmonized AV-industry SMS framework would establish a single set of SMS practices to address management of broad safety risks in an integrated manner and advance the establishment of a more mature regulatory framework. This paper outlines a proposed SMS framework for the AV industry based on robust taxonomy development and validation criteria and provides rationale for such an approach. Keywords: Safety Management System (SMS), Automated Driving System (ADS), ADS-Equipped Vehicle, Autonomous Vehicles (AV)","['David Wichner', 'Jeffrey Wishart', 'Jason Sergent', 'Sunder Swaminathan']",2024-11-08T23:05:17Z,http://arxiv.org/abs/2411.06010v2,"Sustainability, Industry & Robotics",Industrial Automation,safety management systems (SMSs) have been used in many safety-critical industries . they are now being developed and deployed in the automated driving system (ADS)-equipped vehicle (AV) sector . there is a need to review the approach to develop an SMS tailored to the AV industry .
Hyper-automation-The next peripheral for automation in IT industries,"The extension of legacy business process automation beyond the bounds of specific processes is known as hyperautomation. Hyperautomation provides automation for nearly any repetitive action performed by business users by combining AI tools with RPA. It automates complex IT business processes that a company's top brains might not be able to complete. This is an end-to-end automation of a standard business process deployment. It enables automation to perform task digitalization by combining a brain computer interface (BCI) with AI and RPA automation tools. BCI, in conjunction with automation tools, will advance the detection and generation of automation processes to the next level. It allows enterprises to combine business intelligence systems, address complex requirements, and enhance human expertise and automation experience. Hyperautomation and its importance in today's environment are briefly discussed in this paper. The article then goes on to discuss how BCI and sensors might aid Hyperautomation. The specific sectors of solicitations were examined using a variety of flexible technologies associated to this concept, as well as dedicated workflow techniques, which are also diagrammatically illustrated. Hyperautomation is being utilized to improve the efficiency, accuracy, and human enhancement of automated tasks dramatically. It incorporates a number of automated tools in its discovery, implementation, and automation phases. As a result, it's well-suited to integrating cutting-edge technologies and experimenting with new methods of working. Keywords- Hyperautomation, Brain computer Interface (BCI), Technology, Used case, Sensors, Industries.","['Ayush Singh Rajput', 'Richa Gupta']",2023-05-14T11:48:27Z,http://arxiv.org/abs/2305.11896v1,"Sustainability, Industry & Robotics",Industrial Automation,"hyperautomation is an end-to-end automation of a standard business process deployment . it combines a brain computer interface (BCI) with AI and RPA automation tools . the concept is being utilized to improve efficiency, accuracy, and human enhancement ."
Increasing System Test Coverage in Production Automation Systems,"An approach is introduced, which supports a testing technician in the identification of possibly untested behavior of control software of fully integrated automated production systems (aPS). Based on an approach for guided semi-automatic system testing, execution traces are recorded during testing, allowing a subsequent coverage assessment. As the behavior of an aPS is highly dependent on the software, omitted system behavior can be identified and assessed for criticality. Through close cooperation with industry, this approach represents the first coverage assessment approach for system testing in production automation to be applied on real industrial objects and evaluated by industrial experts.","['Sebastian Ulewicz', 'Birgit Vogel-Heuser']",2022-12-07T13:55:00Z,http://arxiv.org/abs/2212.04355v1,"Sustainability, Industry & Robotics",Industrial Automation,the behavior of an aPS is highly dependent on the software . omitted system behavior can be identified and assessed for criticality .
Resiliency Analysis of LLM generated models for Industrial Automation,"This paper proposes a study of the resilience and efficiency of automatically generated industrial automation and control systems using Large Language Models (LLMs). The approach involves modeling the system using percolation theory to estimate its resilience and formulating the design problem as an optimization problem subject to constraints. Techniques from stochastic optimization and regret analysis are used to find a near-optimal solution with provable regret bounds. The study aims to provide insights into the effectiveness and reliability of automatically generated systems in industrial automation and control, and to identify potential areas for improvement in their design and implementation.","['Oluwatosin Ogundare', 'Gustavo Quiros Araya', 'Ioannis Akrotirianakis', 'Ankit Shukla']",2023-08-23T13:35:36Z,http://arxiv.org/abs/2308.12129v1,"Sustainability, Industry & Robotics",Industrial Automation,the paper proposes a study of the resilience and efficiency of industrial automation and control systems using Large Language Models (LLMs) techniques from stochastic optimization and regret analysis are used to find a near-optimal solution with provable regret bounds .
Using industrial robot to manipulate the measured object in CMM,"Coordinate measuring machines (CMMs) are widely used to check dimensions of manufactured parts, especially in automotive industry. The major obstacles in automation of these measurements are fixturing and clamping assemblies, which are required in order to position the measured object within the CMM. This paper describes how an industrial robot can be used to manipulate the measured object within the CMM work space, in order to enable automation of complex geometry measurement.","['Samir Lemes', 'Damir Strbac', 'Malik Cabaravdic']",2014-04-14T19:29:02Z,http://arxiv.org/abs/1404.3706v1,"Sustainability, Industry & Robotics",Industrial Automation,this paper describes how an industrial robot can be used to manipulate the measured object . fixturing and clamping assemblies are required to position the object within the CMM .
Coyote C++: An Industrial-Strength Fully Automated Unit Testing Tool,"Coyote C++ is an automated testing tool that uses a sophisticated concolic-execution-based approach to realize fully automated unit testing for C and C++. While concolic testing has proven effective for languages such as C and Java, tools have struggled to achieve a practical level of automation for C++ due to its many syntactical intricacies and overall complexity. Coyote C++ is the first automated testing tool to breach the barrier and bring automated unit testing for C++ to a practical level suitable for industrial adoption, consistently reaching around 90% code coverage. Notably, this testing process requires no user involvement and performs test harness generation, test case generation and test execution with ""one-click"" automation. In this paper, we introduce Coyote C++ by outlining its high-level structure and discussing the core design decisions that shaped the implementation of its concolic execution engine. Finally, we demonstrate that Coyote C++ is capable of achieving high coverage results within a reasonable timespan by presenting the results from experiments on both open-source and industrial software.","['Sanghoon Rho', 'Philipp Martens', 'Seungcheol Shin', 'Yeoneo Kim', 'Hoon Heo', 'SeungHyun Oh']",2023-10-23T02:22:14Z,http://arxiv.org/abs/2310.14500v1,"Sustainability, Industry & Robotics",Industrial Automation,coyote C++ is an automated testing tool that uses a concolic-execution-based approach to realize fully automated unit testing for C and C++ . tools have struggled to achieve a practical level of automation for C++ due to its many syntactical intricacies .
Competing Advanced Process Control via an Industrial Automation Cloud   Platform,"This paper proposes an innovative approach for the advanced control of an industrial process via an automation cloud platform. Increased digital transformation and advances in Industrial Internet of Things (IIoT) technologies make it possible for multiple vendors to compete to control an industrial process. An industrial automation cloud platform facilitates the interaction between advanced process control (APC) vendors and the process. A selector, which forms part of the platform, is used to determine the best controller for a process for any given time period. The article starts with a general overview of platform businesses, platforms aimed at industry, and the steps required to build such platforms. Issues that need to be addressed to make APC via an automation platform practically viable are discussed including what process information to provide to APC vendors, continuous evaluation of controllers even when not in control of the process, bumpless transfer, closed-loop stability, constraint handling, and platform security and trust. A case study is given of competing APCs via an industrial automation cloud platform. The process used in the study is a surge tank from a bulk tailings treatment plant, the aim of which is to keep the density of the tank out flow constant while maintaining a steady tank level. A platform facilitates the competition of three vendors for control of this process. It is shown that the cloud platform approach can provide the plant access to a superior controller without the need for directly procuring the services of an exclusive vendor.","['L. L. Rokebrand', 'J. J. Burchell', 'L. E. Olivier', 'I. K. Craig']",2020-11-26T08:57:39Z,http://arxiv.org/abs/2011.13184v1,"Sustainability, Industry & Robotics",Industrial Automation,an industrial automation cloud platform facilitates the interaction between vendors . a selector is used to determine the best controller for a process for any given time period . issues discussed include continuous evaluation of controllers even when not in control of the process .
Automated Security Findings Management: A Case Study in Industrial   DevOps,"In recent years, DevOps, the unification of development and operation workflows, has become a trend for the industrial software development lifecycle. Security activities turned into an essential field of application for DevOps principles as they are a fundamental part of secure software development in the industry. A common practice arising from this trend is the automation of security tests that analyze a software product from several perspectives. To effectively improve the security of the analyzed product, the identified security findings must be managed and looped back to the project team for stakeholders to take action. This management must cope with several challenges ranging from low data quality to a consistent prioritization of findings while following DevOps aims. To manage security findings with the same efficiency as other activities in DevOps projects, a methodology for the management of industrial security findings minding DevOps principles is essential.   In this paper, we propose a methodology for the management of security findings in industrial DevOps projects, summarizing our research in this domain and presenting the resulting artifact. As an instance of the methodology, we developed the Security Flama, a semantic knowledge base for the automated management of security findings. To analyze the impact of our methodology on industrial practice, we performed a case study on two DevOps projects of a multinational industrial enterprise. The results emphasize the importance of using such an automated methodology in industrial DevOps projects, confirm our approach's usefulness and positive impact on the studied projects, and identify the communication strategy as a crucial factor for usability in practice.","['Markus Voggenreiter', 'Florian Angermeir', 'Fabiola Moyón', 'Ulrich Schöpp', 'Pierre Bonvin']",2024-01-12T14:35:51Z,http://arxiv.org/abs/2401.06602v1,"Sustainability, Industry & Robotics",Industrial Automation,devOps is the unification of development and operation workflows . security activities are an essential part of secure software development in the industry . a methodology for the management of industrial security findings is essential . the results emphasize the importance of using such an automated methodology .
FABLE : Fabric Anomaly Detection Automation Process,"Unsupervised anomaly in industry has been a concerning topic and a stepping stone for high performance industrial automation process. The vast majority of industry-oriented methods focus on learning from good samples to detect anomaly notwithstanding some specific industrial scenario requiring even less specific training and therefore a generalization for anomaly detection. The obvious use case is the fabric anomaly detection, where we have to deal with a really wide range of colors and types of textile and a stoppage of the production line for training could not be considered. In this paper, we propose an automation process for industrial fabric texture defect detection with a specificity-learning process during the domain-generalized anomaly detection. Combining the ability to generalize and the learning process offer a fast and precise anomaly detection and segmentation. The main contributions of this paper are the following: A domain-generalization texture anomaly detection method achieving the state-of-the-art performances, a fast specific training on good samples extracted by the proposed method, a self-evaluation method based on custom defect creation and an automatic detection of already seen fabric to prevent re-training.","['Simon Thomine', 'Hichem Snoussi', 'Mahmoud Soua']",2023-06-16T13:35:46Z,http://arxiv.org/abs/2306.10089v1,"Sustainability, Industry & Robotics",Industrial Automation,the vast majority of industry-oriented methods focus on learning from good samples . the obvious use case is the fabric anomaly detection . a stoppage of the production line could not be considered .
Agentic AI for Intent-Based Industrial Automation,"The recent development of Agentic AI systems, empowered by autonomous large language models (LLMs) agents with planning and tool-usage capabilities, enables new possibilities for the evolution of industrial automation and reduces the complexity introduced by Industry 4.0. This work proposes a conceptual framework that integrates Agentic AI with the intent-based paradigm, originally developed in network research, to simplify human-machine interaction (HMI) and better align automation systems with the human-centric, sustainable, and resilient principles of Industry 5.0. Based on the intent-based processing, the framework allows human operators to express high-level business or operational goals in natural language, which are decomposed into actionable components. These intents are broken into expectations, conditions, targets, context, and information that guide sub-agents equipped with specialized tools to execute domain-specific tasks. A proof of concept was implemented using the CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the feasibility of intent decomposition, agent orchestration, and autonomous decision-making in predictive maintenance scenarios. The results confirm the potential of this approach to reduce technical barriers and enable scalable, intent-driven automation, despite data quality and explainability concerns.","['Marcos Lima Romero', 'Ricardo Suyama']",2025-06-05T12:50:54Z,http://arxiv.org/abs/2506.04980v1,"Sustainability, Industry & Robotics",Industrial Automation,"the framework allows human operators to express high-level business or operational goals in natural language . intents are broken into expectations, conditions, targets, context, and information . this information guides sub-agents equipped with specialized tools ."
Collaborative Framework with Shared Responsibility for Relief Management   in Disaster Scenarios,"Disasters instances have been increasing both in frequency and intensity causing the tragic loss of life and making life harder for the survivors. Disaster relief management plays a crucial role in enhancing the lifestyle of disaster victims by managing the disaster impacts. Disaster relief management is a process with many collaborative sectors where different stakeholders should operate in all major phases of the disaster management progression. In the different phases of the disaster management process, many collaborative government organisations along with nongovernment organisations, leadership, community, and media at different levels need to share the responsibility with disaster victims to achieve effective disaster relief management. Shared responsibility enhances disaster relief management effectiveness and reduces the disaster's impact on the victims. Considering the diverse roles of different stakeholders, there has been a need for a framework that can bind different stakeholders together during disaster management. this paper shows a framework with major stakeholders of disaster relief management and how different stakeholders can take part in an effective disaster relief management process. The framework also highlights how each stakeholder can contribute to relief management at different phases after a disaster. The paper also explores some of the shared responsibility collaborative practices that have been implemented around the world in response to the disaster as a disaster relief management process. In addition, the paper highlights the knowledge obtained from those disaster instances and how this knowledge can be transferred and can be helpful in disaster mitigation and preparedness for future disaster scenarios.","['Bhupesh Kumar Mishra', 'Keshav Dahal']",2024-06-15T09:17:50Z,http://arxiv.org/abs/2406.10572v1,"Sustainability, Industry & Robotics",Disaster Response,paper shows framework with major stakeholders of disaster relief management . highlights how each stakeholder can contribute to relief management at different phases after a disaster .
DisasterQA: A Benchmark for Assessing the performance of LLMs in   Disaster Response,"Disasters can result in the deaths of many, making quick response times vital. Large Language Models (LLMs) have emerged as valuable in the field. LLMs can be used to process vast amounts of textual information quickly providing situational context during a disaster. However, the question remains whether LLMs should be used for advice and decision making in a disaster. To evaluate the capabilities of LLMs in disaster response knowledge, we introduce a benchmark: DisasterQA created from six online sources. The benchmark covers a wide range of disaster response topics. We evaluated five LLMs each with four different prompting methods on our benchmark, measuring both accuracy and confidence levels through Logprobs. The results indicate that LLMs require improvement on disaster response knowledge. We hope that this benchmark pushes forth further development of LLMs in disaster response, ultimately enabling these models to work alongside. emergency managers in disasters.",['Rajat Rawat'],2024-10-09T00:13:06Z,http://arxiv.org/abs/2410.20707v1,"Sustainability, Industry & Robotics",Disaster Response,large language models (LLMs) can be used to process vast amounts of textual information . the question remains whether LLMs should be used for advice and decision making in a disaster . a benchmark: DisasterQA was created from six online sources .
Social Media Information Sharing for Natural Disaster Response,"Social media has become an essential channel for posting disaster-related information, which provide governments and relief agencies real-time data for better disaster management. However, research in this field has not received sufficient attention and extracting useful information is still challenging. This paper aims to improve disaster relief efficiency via mining and analyzing social media data like public attitudes towards disaster response and public demands for targeted relief supplies during different types of disasters. We focus on different natural disasters based on properties such as types, durations, and damages, which contains a total of 41,993 tweets. In this paper, public perception is assessed qualitatively by manually classified tweets, which contain information like the demand for targeted relief supplies, satisfactions of disaster response, and public fear. Public attitudes to natural disasters are studied via a quantitative analysis using eight machine learning models. To better provide decision-makers with the appropriate model, the comparison of machine learning models based on computational time and prediction accuracy is conducted. The change of public opinion during different natural disasters and the evolution of people's behavior of using social media for disaster relief in the face of the identical type of natural disasters as Twitter continues to evolve are studied. The results in this paper demonstrate the feasibility and validation of the proposed research approach and provide relief agencies with insights into better disaster management.","['Zhijie Sasha Dong', 'Lingyu Meng', 'Lauren Christenson', 'Lawrence Fulton']",2020-05-08T21:11:39Z,http://arxiv.org/abs/2005.07019v5,"Sustainability, Industry & Robotics",Disaster Response,"this paper aims to improve disaster relief efficiency via mining and analyzing social media data . we focus on different natural disasters based on properties such as types, durations, and damages ."
Social Media Analytics in Disaster Response: A Comprehensive Review,"Social media has emerged as a valuable resource for disaster management, revolutionizing the way emergency response and recovery efforts are conducted during natural disasters. This review paper aims to provide a comprehensive analysis of social media analytics for disaster management. The abstract begins by highlighting the increasing prevalence of natural disasters and the need for effective strategies to mitigate their impact. It then emphasizes the growing influence of social media in disaster situations, discussing its role in disaster detection, situational awareness, and emergency communication. The abstract explores the challenges and opportunities associated with leveraging social media data for disaster management purposes. It examines methodologies and techniques used in social media analytics, including data collection, preprocessing, and analysis, with a focus on data mining and machine learning approaches. The abstract also presents a thorough examination of case studies and best practices that demonstrate the successful application of social media analytics in disaster response and recovery. Ethical considerations and privacy concerns related to the use of social media data in disaster scenarios are addressed. The abstract concludes by identifying future research directions and potential advancements in social media analytics for disaster management. The review paper aims to provide practitioners and researchers with a comprehensive understanding of the current state of social media analytics in disaster management, while highlighting the need for continued research and innovation in this field.",['Mohammadsepehr Karimiziarani'],2023-07-08T20:49:18Z,http://arxiv.org/abs/2307.04046v2,"Sustainability, Industry & Robotics",Disaster Response,"social media has emerged as a valuable resource for disaster management . the abstract aims to provide a comprehensive analysis of social media analytics . it discusses its role in disaster detection, situational awareness, and emergency communication ."
Enhancing Trustworthiness and Minimising Bias Issues in Leveraging   Social Media Data for Disaster Management Response,"Disaster events often unfold rapidly, necessitating a swift and effective response. Developing action plans, resource allocation, and resolution of help requests in disaster scenarios is time-consuming and complex since disaster-relevant information is often uncertain. Leveraging real-time data can significantly deal with data uncertainty and enhance disaster response efforts. To deal with real-time data uncertainty, social media appeared as an alternative effective source of real-time data as there has been extensive use of social media during and after the disasters. However, it also brings forth challenges regarding trustworthiness and bias in these data. To fully leverage social media data for disaster management, it becomes crucial to mitigate biases that may arise due to specific disaster types or regional contexts. Additionally, the presence of misinformation within social media data raises concerns about the reliability of data sources, potentially impeding actionable insights and leading to improper resource utilization. To overcome these challenges, our research aimed to investigate how to ensure trustworthiness and address biases in social media data. We aim to investigate and identify the factors that can be used to enhance trustworthiness and minimize bias to make an efficient and scalable disaster management system utilizing real-time social media posts, identify disaster-related keywords, and assess the severity of the disaster. By doing so, the integration of real-time social data can improve the speed and accuracy of disaster management systems","['Samia Abid', 'Bhupesh Kumar Mishra', 'Dhavalkumar Thakker', 'Nishikant Mishra']",2024-08-15T10:59:20Z,http://arxiv.org/abs/2409.00004v1,"Sustainability, Industry & Robotics",Disaster Response,"real-time data can significantly deal with data uncertainty and enhance disaster response efforts . social media brings forth challenges regarding trustworthiness and bias in these data . to fully leverage social media data for disaster management, it becomes crucial to mitigate biases ."
A Correlation Analysis and Visualization of Climate Change using   Post-Disaster Heterogeneous Datasets,"There are numerous geo-climatic and human factors that contribute to the occurrence of natural disasters in the real-world scenario. Besides the study of causes and preconditions of such calamities, post-disaster analysis is essential for the efficient management of the disaster situation. This process needs timely and accurate data in light of the increasing frequency and severity of climate change-related extreme weather events. The analysis of disaster data involves the challenging task of integrating multiple heterogeneous sources, data ingestion and visualization. This paper aims at providing a three-dimensional analytical view of disaster data as time-series charts and a statistical model to evaluate the correlation between the occurrence of disasters, climate change and the corresponding economic damages (as a percentage of GDP). Therefore, statistical methodologies are leveraged to play important role in managing disasters, from preparation to recovery and reporting. The graphical representations provide insights on regional trends that follow, related to factors such as the proportion of each type of disaster in the various losses incurred. Therefore, obtaining reliable information about the population, the economy and climate are crucial both for risk management and preparedness and for responding to disasters. The research puts forth a detailed statistical methodology with a spatial dimension to study the impacts on the economy and infrastructure in the aftermath of a disaster thereby ensuring specialized assistance. The inference of the analysis confirmed a positive correlation between climate change and occurrence of natural disasters. Therefore, statistical evidence of an important phenomenon like climate change affecting natural disasters brings awareness among the population in the society to be more environmentally responsible.","['Sukeerthi Mandyam', 'Shanmuga Priya', 'Shalini Suresh', 'Kavitha Srinivasan']",2022-05-25T03:56:16Z,http://arxiv.org/abs/2205.12474v1,"Sustainability, Industry & Robotics",Disaster Response,"the paper aims to provide a three-dimensional analytical view of disaster data . it evaluates correlation between occurrence of disasters, climate change and economic damages . the research puts forth a detailed statistical methodology with a spatial dimension ."
From Satellite Imagery to Disaster Insights,"The use of satellite imagery has become increasingly popular for disaster monitoring and response. After a disaster, it is important to prioritize rescue operations, disaster response and coordinate relief efforts. These have to be carried out in a fast and efficient manner since resources are often limited in disaster-affected areas and it's extremely important to identify the areas of maximum damage. However, most of the existing disaster mapping efforts are manual which is time-consuming and often leads to erroneous results. In order to address these issues, we propose a framework for change detection using Convolutional Neural Networks (CNN) on satellite images which can then be thresholded and clustered together into grids to find areas which have been most severely affected by a disaster. We also present a novel metric called Disaster Impact Index (DII) and use it to quantify the impact of two natural disasters - the Hurricane Harvey flood and the Santa Rosa fire. Our framework achieves a top F1 score of 81.2% on the gridded flood dataset and 83.5% on the gridded fire dataset.","['Jigar Doshi', 'Saikat Basu', 'Guan Pang']",2018-12-17T20:08:23Z,http://arxiv.org/abs/1812.07033v1,"Sustainability, Industry & Robotics",Disaster Response,"satellite imagery has become increasingly popular for disaster monitoring and response . it is important to prioritize rescue operations, disaster response and coordinate relief efforts . but most of the existing disaster mapping efforts are manual which is time-consuming and often leads to erroneous results . we propose a framework for change detection using Convolutional Neural Networks (CNN) on satellite images which can then be thresholded and clustered together into grids ."
Collective Dynamics of Hierarchical Networks,"In an increasingly complex, mobile and interconnected world, we face growing threats of disasters, whether by chance or deliberately. Disruption of coordinated response and recovery efforts due to organizational, technical, procedural, random or deliberate attack could result in the risk of massive loss of life. This requires urgent action to explore the development of optimal information-sharing environments for promoting collective disaster response and preparedness using multijurisdictional hierarchical networks. Innovative approaches to information flow modeling and analysis for dealing with challenges of coordinating across multi layered agency structures as well as development of early warnings through social systems using social media analytics may be pivotal to timely responses to dealing with large scale disasters where response strategies need to be viewed as a shared responsibility. How do facilitate the development of collective disaster response in a multijurisdictional setting? How do we develop and test the level and effectiveness of shared multijurisdictional hierarchical networks for improved preparedness and response? What is the role of multi layered training and exercises in building the shared learning space for collective disaster preparedness and response? The aim of this is therefore to determine factors that may be responsible for affecting disaster response.","['Liaquat Hossain', 'Rolf T. Wigand']",2015-03-28T04:30:27Z,http://arxiv.org/abs/1503.08264v1,"Sustainability, Industry & Robotics",Disaster Response,"we face growing threats of disasters, whether by chance or deliberately . early warnings may be pivotal to dealing with large scale disasters . how do we facilitate the development of collective disaster response in a multijurisdictional setting?"
DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage   Assessment and Response,"Large vision-language models (VLMs) have made great achievements in Earth vision. However, complex disaster scenes with diverse disaster types, geographic regions, and satellite sensors have posed new challenges for VLM applications. To fill this gap, we curate a remote sensing vision-language dataset (DisasterM3) for global-scale disaster assessment and response. DisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction pairs across 5 continents, with three characteristics: 1) Multi-hazard: DisasterM3 involves 36 historical disaster events with significant impacts, which are categorized into 10 common natural and man-made disasters. 2)Multi-sensor: Extreme weather during disasters often hinders optical sensor imaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery for post-disaster scenes. 3) Multi-task: Based on real-world scenarios, DisasterM3 includes 9 disaster-related visual perception and reasoning tasks, harnessing the full potential of VLM's reasoning ability with progressing from disaster-bearing body recognition to structural damage assessment and object relational reasoning, culminating in the generation of long-form disaster reports. We extensively evaluated 14 generic and remote sensing VLMs on our benchmark, revealing that state-of-the-art models struggle with the disaster tasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap, and damage object counting insensitivity. Focusing on these issues, we fine-tune four VLMs using our dataset and achieve stable improvements across all tasks, with robust cross-sensor and cross-disaster generalization capabilities.","['Junjue Wang', 'Weihao Xuan', 'Heli Qi', 'Zhihao Liu', 'Kunyi Liu', 'Yuhan Wu', 'Hongruixuan Chen', 'Jian Song', 'Junshi Xia', 'Zhuo Zheng', 'Naoto Yokoya']",2025-05-27T12:16:07Z,http://arxiv.org/abs/2505.21089v1,"Sustainability, Industry & Robotics",Disaster Response,"large vision-language models (VLMs) have made great achievements in Earth vision . however, complex disaster scenes with diverse disaster types, geographic regions, and satellite sensors have posed new challenges for VLM applications . DisasterM3 includes 26,988 bi-temporal satellite images across 5 continents ."
Achieving Disaster-Resilient Distribution Systems via Emergency Response   Resources: A Practical Approach,"This paper presents a practical approach to utilizing emergency response resources (ERRs) and post-disaster available distributed energy resources (PDA-DERs) to improve the resilience of power distribution systems against natural disasters. The proposed approach consists of two sequential steps: first, the minimum amount of ERRs is determined in a pre-disaster planning model; second, a post-disaster restoration model is proposed to co-optimize the dispatch of pre-planned ERRs and PDA-DERs to minimize the impact of disasters on customers, i.e., unserved energy for the entire restoration window. Compared with existing restoration strategies using ERRs, the proposed approach is more tractable since 1) in the pre-disaster stage, the needed EERs are determined based on the prediction of energy shortage and disaster-induced damages using machine learning-based algorithms (i.e., cost-sensitive-RFQRF for prediction of outage customers, random forest for prediction of outage duration, and CART for prediction of disaster-induced damages); 2) in the post-disaster stage, the super-node approximation (SNA) and the convex hull relaxation (CHR) of distribution networks are introduced to achieve the best trade-off between computational burden and accuracy. Tests of the proposed approach on IEEE test feeders demonstrated that a combination of SNA and CHR remarkably reduces the solution time of the post-disaster restoration model.","['Santosh Sharma', 'Qifeng Li', 'Qiuhua Huang', 'Ahmad Tbaileh']",2020-08-21T15:34:56Z,http://arxiv.org/abs/2008.09539v1,"Sustainability, Industry & Robotics",Disaster Response,paper presents a practical approach to utilizing emergency response resources (ERRs) and post-disaster available distributed energy resources (PDA-DERs) the proposed approach is more tractable than existing restoration strategies using ERRs .
CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for   Multi-label Social Media Text Classification in Disaster Informatics,"In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.","['Kai Yin', 'Chengkai Liu', 'Ali Mostafavi', 'Xia Hu']",2024-06-16T23:01:10Z,http://arxiv.org/abs/2406.15477v2,"Sustainability, Industry & Robotics",Disaster Response,social media is increasingly being used for improving situational awareness . current methods mostly rely on single-label text classification models . but this fails to capture different insights embedded in disaster-related social media data .
Nowcasting Disaster Damage,"Could social media data aid in disaster response and damage assessment? Countries face both an increasing frequency and intensity of natural disasters due to climate change. And during such events, citizens are turning to social media platforms for disaster-related communication and information. Social media improves situational awareness, facilitates dissemination of emergency information, enables early warning systems, and helps coordinate relief efforts. Additionally, spatiotemporal distribution of disaster-related messages helps with real-time monitoring and assessment of the disaster itself. Here we present a multiscale analysis of Twitter activity before, during, and after Hurricane Sandy. We examine the online response of 50 metropolitan areas of the United States and find a strong relationship between proximity to Sandy's path and hurricane-related social media activity. We show that real and perceived threats -- together with the physical disaster effects -- are directly observable through the intensity and composition of Twitter's message stream. We demonstrate that per-capita Twitter activity strongly correlates with the per-capita economic damage inflicted by the hurricane. Our findings suggest that massive online social networks can be used for rapid assessment (""nowcasting"") of damage caused by a large-scale disaster.","['Yury Kryvasheyeu', 'Haohui Chen', 'Nick Obradovich', 'Esteban Moro', 'Pascal Van Hentenryck', 'James Fowler', 'Manuel Cebrian']",2015-04-26T14:15:36Z,http://arxiv.org/abs/1504.06827v1,"Sustainability, Industry & Robotics",Disaster Response,"a multiscale analysis of twitter activity before, during, and after hurricane sandy . per-capita Twitter activity strongly correlates with economic damage inflicted by the hurricane . results suggest massive online social networks can be used for rapid assessment ."
MONITRS: Multimodal Observations of Natural Incidents Through Remote   Sensing,"Natural disasters cause devastating damage to communities and infrastructure every year. Effective disaster response is hampered by the difficulty of accessing affected areas during and after events. Remote sensing has allowed us to monitor natural disasters in a remote way. More recently there have been advances in computer vision and deep learning that help automate satellite imagery analysis, However, they remain limited by their narrow focus on specific disaster types, reliance on manual expert interpretation, and lack of datasets with sufficient temporal granularity or natural language annotations for tracking disaster progression. We present MONITRS, a novel multimodal dataset of more than 10,000 FEMA disaster events with temporal satellite imagery and natural language annotations from news articles, accompanied by geotagged locations, and question-answer pairs. We demonstrate that fine-tuning existing MLLMs on our dataset yields significant performance improvements for disaster monitoring tasks, establishing a new benchmark for machine learning-assisted disaster response systems. Code can be found at: https://github.com/ShreelekhaR/MONITRS","['Shreelekha Revankar', 'Utkarsh Mall', 'Cheng Perng Phoo', 'Kavita Bala', 'Bharath Hariharan']",2025-07-22T04:59:09Z,http://arxiv.org/abs/2507.16228v1,"Sustainability, Industry & Robotics",Disaster Response,"MONITRS is a new multimodal dataset of more than 10,000 FEMA disaster events . it contains temporal satellite imagery and natural language annotations from news articles . MONitRS provides a benchmark for machine learning-assisted disaster response systems ."
Crowdsourced bi-directional disaster reporting and alerting on   smartphones in Lao PDR,"Natural disasters are a large threat for people especially in developing countries such as Laos. ICT-based disaster management systems aim at supporting disaster warning and response efforts. However, the ability to directly communicate in both directions between local and administrative level is often not supported, and a tight integration into administrative workflows is missing. In this paper, we present the smartphone-based disaster and reporting system Mobile4D. It allows for bi-directional communication while being fully involved in administrative processes. We present the system setup and discuss integration into administrative structures in Lao PDR.","['Lutz Frommberger', 'Falko Schmid']",2013-12-20T16:47:37Z,http://arxiv.org/abs/1312.6036v1,"Sustainability, Industry & Robotics",Disaster Response,a smartphone-based disaster and reporting system Mobile4D is presented . it allows for bi-directional communication while being fully involved in administrative processes . a tight integration into administrative workflows is missing .
Behaviour in social media for floods and heat waves in disaster response   via Artificial Intelligence,"This paper analyses social media data in multiple disaster-related collections of floods and heat waves in the UK. The proposed method uses machine learning classifiers based on deep bidirectional neural networks trained on benchmark datasets of disaster responses and extreme events. The resulting models are applied to perform sentiment and qualitative analysis of inferred topics in text data. We further analyse a set of behavioural indicators and match them with climate variables via decoding synoptical records to analyse thermal comfort. We highlight the advantages of aligning behavioural indicators along with climate variables to provide with additional valuable information to be considered especially in different phases of a disaster and applicable to extreme weather periods. The positiveness of messages is around 8% for disaster, 1% for disaster and medical response, 7% for disaster and humanitarian related messages. This shows the reliability of such data for our case studies. We show the transferability of this approach to be applied to any social media data collection.","['Victor Ponce-López', 'Catalina Spataru']",2022-03-16T17:09:49Z,http://arxiv.org/abs/2203.08753v1,"Sustainability, Industry & Robotics",Disaster Response,"this paper analyses social media data in multiple disaster-related collections of floods and heat waves in the UK . proposed method uses machine learning classifiers trained on benchmark datasets of disaster responses and extreme events . positiveness of messages is around 8% for disaster, 1% for disaster and medical response, 7% for disaster related messages ."
On-Demand HAPS-Assisted Communication System for Public Safety in   Emergency and Disaster Response,"Natural disasters often disrupt communication networks and severely hamper emergency response and disaster management. Existing solutions, such as portable communication units and cloud-based network architectures, have improved disaster resilience but fall short if both the Radio Access Network (RAN) and backhaul infrastructure become inoperable. To address these challenges, we propose a demand-driven communication system supported by High Altitude Platform Stations (HAPS) to restore communication in an affected area and enable effective disaster relief. The proposed emergency response network is a promising solution as it provides a rapidly deployable, resilient communications infrastructure. The proposed HAPS-based communication can play a crucial role not only in ensuring connectivity for mobile users but also in restoring backhaul connections when terrestrial networks fail. As a bridge between the disaster management center and the affected areas, it can facilitate the exchange of information in real time, collect data from the affected regions, and relay crucial updates to emergency responders. Enhancing situational awareness, coordination between relief agencies, and ensuring efficient resource allocation can significantly strengthen disaster response capabilities. In this paper, simulations show that HAPS with hybrid optical/THz links boosts backhaul capacity and resilience, even in harsh conditions. HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first responders and disaster-affected populations. This paper also explores the integration of HAPS into emergency communication frameworks and standards, as it has the potential to improve network resilience and support effective disaster management.","['Bilal Karaman', 'Ilhan Baştürk', 'Ferdi Kara', 'Engin Zeydan', 'Esra Aycan Beyazıt', 'Sezai Taşkın', 'Emil Björnson', 'Halim Yanikomeroglu']",2025-07-12T05:55:56Z,http://arxiv.org/abs/2507.09153v1,"Sustainability, Industry & Robotics",Disaster Response,a demand-driven communication system is proposed to restore communication in an affected area . high altitude platform stations (HAPS) can play a crucial role in restoring backhaul connections . paper explores integration of HAPS into emergency communication frameworks and standards .
Natural Hazards Twitter Dataset,"With the development of the Internet, social media has become an important channel for posting disaster-related information. Analyzing attitudes hidden in these texts, known as sentiment analysis, is crucial for the government or relief agencies to improve disaster response efficiency, but it has not received sufficient attention. This paper aims to fill this gap by focusing on investigating attitudes towards disaster response and analyzing targeted relief supplies during disaster response. The contributions of this paper are fourfold. First, we propose several machine learning models for classifying public sentiment concerning disaster-related social media data. Second, we create a natural disaster dataset with sentiment labels, which contains nearly 50,00 Twitter data about different natural disasters in the United States (e.g., a tornado in 2011, a hurricane named Sandy in 2012, a series of floods in 2013, a hurricane named Matthew in 2016, a blizzard in 2016, a hurricane named Harvey in 2017, a hurricane named Michael in 2018, a series of wildfires in 2018, and a hurricane named Dorian in 2019). We are making our dataset available to the research community: https://github.com/Dong-UTIL/Natural-Hazards-Twitter-Dataset. It is our hope that our contribution will enable the study of sentiment analysis in disaster response. Third, we focus on extracting public attitudes and analyzing the essential needs (e.g., food, housing, transportation, and medical supplies) for the public during disaster response, instead of merely targeting on studying positive or negative attitudes of the public to natural disasters. Fourth, we conduct this research from two different dimensions for a comprehensive understanding of public opinion on disaster response, since disparate hazards caused by different types of natural disasters.","['Lingyu Meng', 'Zhijie Sasha Dong']",2020-04-29T20:09:46Z,http://arxiv.org/abs/2004.14456v2,"Sustainability, Industry & Robotics",Disaster Response,"social media has become an important channel for posting disaster-related information . this paper aims to fill this gap by focusing on investigating attitudes towards disaster response . we create a dataset with sentiment labels, which contains nearly 50,00 Twitter data ."
Resilience for Landslide Geohazards and Promoting Strategies in the   Three Gorges Reservoir Area,"Recently, resilience is increasingly used as a concept for understanding natural disaster systems. Landslide is one of the most frequent geohazards in the Three Gorges Reservoir Area (TGRA).However, it is difficult to measure local disaster resilience, because of special geographical location in the TGRA and the special disaster landslide. Current approaches to disaster resilience evaluation are usually limited either by the qualitative method or properties of different disaster. Therefore, practical evaluating methods for the disaster resilience are needed. In this study, we developed an indicator system to evaluate landslides disaster resilience in the TGRE at the county level. It includes two properties of inherent geological stress and external social response, which are summarized into physical stress and social forces. The evaluated disaster resilience can be simulated for promoting strategies with fuzzy cognitive map (FCM).","['Yuanyue Huang', 'Haixiang Guo', 'Jing Yu', 'Shicheng Li', 'Zuozhi Zuo']",2019-10-24T06:46:16Z,http://arxiv.org/abs/1910.10938v2,"Sustainability, Industry & Robotics",Disaster Response,landslide is one of the most frequent geohazards in the Three Gorges Reservoir Area (TGRE) it is difficult to measure local disaster resilience because of special geographical location in the TGRA . current approaches to disaster resilience evaluation are usually limited by qualitative method .
Empirical Insights for Designing Information and Communication   Technology for International Disaster Response,"Due to the increase in natural disasters in the past years, Disaster Response Organizations (DROs) are faced with the challenge of coping with more and larger operations. Currently appointed Information and Communications Technology (ICT) used for coordination and communication is sometimes outdated and does not scale, while novel technologies have the potential to greatly improve disaster response efficiency. To allow adoption of these novel technologies, ICT system designers have to take into account the particular needs of DROs and characteristics of International Disaster Response (IDR). This work attempts to bring the humanitarian and ICT communities closer together. In this work, we analyze IDR-related documents and conduct expert interviews. Using open coding, we extract empirical insights and translate the peculiarities of DRO coordination and operation into tangible ICT design requirements. This information is based on interviews with active IDR staff as well as DRO guidelines and reports. Ultimately, the goal of this paper is to serve as a reference for future ICT research endeavors to support and increase the efficiency of IDR operations.","['Milan Stute', 'Max Maass', 'Tom Schons', 'Marc-André Kaufhold', 'Christian Reuter', 'Matthias Hollick']",2020-05-11T08:05:02Z,http://arxiv.org/abs/2005.04910v1,"Sustainability, Industry & Robotics",Disaster Response,the goal of this paper is to serve as a reference for future ICT research endeavors . this paper analyzes IDR-related documents and conducts expert interviews . we extract empirical insights and translate peculiarities of DRO operation into tangible ICT design requirements .
Intelligent Disaster Response via Social Media Analysis - A Survey,"The success of a disaster relief and response process is largely dependent on timely and accurate information regarding the status of the disaster, the surrounding environment, and the affected people. This information is primarily provided by first responders on-site and can be enhanced by the firsthand reports posted in real-time on social media. Many tools and methods have been developed to automate disaster relief by extracting, analyzing, and visualizing actionable information from social media. However, these methods are not well integrated in the relief and response processes and the relation between the two requires exposition for further advancement. In this survey, we review the new frontier of intelligent disaster relief and response using social media, show stages of disasters which are reflected on social media, establish a connection between proposed methods based on social media and relief efforts by first responders, and outline pressing challenges and future research directions.","['Tahora H. Nazer', 'Guoliang Xue', 'Yusheng Ji', 'Huan Liu']",2017-09-07T19:50:54Z,http://arxiv.org/abs/1709.02426v1,"Sustainability, Industry & Robotics",Disaster Response,"the success of a disaster relief and response process is largely dependent on timely and accurate information . many tools and methods have been developed to automate disaster relief by extracting, analyzing, and visualizing actionable information from social media . the relation between the two requires exposition for further advancement ."
